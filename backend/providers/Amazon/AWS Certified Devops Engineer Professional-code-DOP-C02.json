[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/amazon/view/105229-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has a mobile application that makes HTTP API calls to an Application Load Balancer (ALB). The ALB routes requests to an AWS Lambda function. Many different versions of the application are in use at any given time, including versions that are in testing by a subset of users. The version of the application is defined in the user-agent header that is sent with all requests to the API.<br>After a series of recent changes to the API, the company has observed issues with the application. The company needs to gather a metric for each API operation by response code for each version of the application that is in use. A DevOps engineer has modified the Lambda function to extract the API operation name, version information from the user-agent header and response code.<br>Which additional set of actions should the DevOps engineer take to gather the required metrics?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Lambda function to write the API operation name, response code, and version number as a log line to an Amazon CloudWatch Logs log group. Configure a CloudWatch Logs Insights query to populate CloudWatch metrics from the log lines. Specify response code and application version as dimensions for the metric.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ALB access logs to write to an Amazon CloudWatch Logs log group. Modify the Lambda function to respond to the ALB with the API operation name, response code, and version number as response metadata. Configure a CloudWatch Logs metric filter that increments a metric for each API operation name. Specify response code and application version as dimensions for the metric.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS X-Ray integration on the Lambda function. Modify the Lambda function to create an X-Ray subsegment with the API operation name, response code, and version number. Configure X-Ray insights to extract an aggregated metric for each API operation name and to publish the metric to Amazon CloudWatch. Specify response code and application version as dimensions for the metric."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-31T15:49:00.000Z",
        "voteCount": 5,
        "content": "Option A: This option is the most efficient way to gather the required metrics. It does not require any additional infrastructure and can be easily implemented.\nOption B: This option is more complex than Option A and requires configuring a CloudWatch Logs Insights query. This can be more time-consuming to set up and can be less efficient if the query is not optimized.\nOption C: This option requires configuring the ALB access logs to write to CloudWatch Logs. This can add additional latency to the requests.\nOption D: This option requires configuring AWS X-Ray integration. This is a more complex solution that is not necessary in this case."
      },
      {
        "date": "2024-09-22T06:59:00.000Z",
        "voteCount": 4,
        "content": "The other options are either incomplete or involve unnecessary complexity:\n\nOption B requires using CloudWatch Logs Insights queries, which may introduce additional complexity and potential performance overhead.\nOption C involves modifying the ALB access logs, which may not provide the required level of granularity or flexibility for capturing the application version information.\nOption D requires integrating AWS X-Ray, which is primarily designed for distributed tracing and may not be necessary for this specific use case of gathering metrics by response code and application version.\n\nTherefore, option A is the most appropriate and straightforward solution for the given requirements."
      },
      {
        "date": "2024-02-25T11:57:00.000Z",
        "voteCount": 1,
        "content": "The answer is b based on the scenario. \n\nWhen to Choose Which\n\nChoose A (Metric Filters) if you need basic metrics with straightforward patterns (e.g., counting occurrences of specific API operations and response codes).\n\nChoose B (Insights Queries) if  you require more complex metric calculations, such as:\nAggregations (averages, sums, etc.) over time\nFiltering metrics based on conditions within the logs\nCreating custom metrics not directly defined in log lines"
      },
      {
        "date": "2024-01-31T03:54:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A."
      },
      {
        "date": "2024-01-27T19:50:00.000Z",
        "voteCount": 1,
        "content": "A is correct: Because only need to gather metric but not parse log or advanced analyzing the log."
      },
      {
        "date": "2024-01-26T20:54:00.000Z",
        "voteCount": 1,
        "content": "A definitely"
      },
      {
        "date": "2023-12-31T19:54:00.000Z",
        "voteCount": 1,
        "content": "Cloudwatch log metrics filter can apply on pattern and populate metrics, CW insight significant to get it. B should be correct one"
      },
      {
        "date": "2023-12-28T17:11:00.000Z",
        "voteCount": 3,
        "content": "CloudWatch Logs Insights is a powerful tool for analyzing log data, but it's more suited for ad-hoc exploration and troubleshooting rather than continuous metric collection and monitoring."
      },
      {
        "date": "2023-12-12T07:18:00.000Z",
        "voteCount": 1,
        "content": "The company needs to gather a metric for each API operation by response code for each version of the application that is in use\n=&gt; A"
      },
      {
        "date": "2023-11-18T10:44:00.000Z",
        "voteCount": 3,
        "content": "Option A seems to be the most straightforward and effective method. By writing the required information as a log line to CloudWatch Logs and configuring a metric filter to increment metrics based on this data, the company can efficiently gather the metrics it needs with minimal complexity. This approach leverages existing AWS services in a simple and direct manner, aligning well with the company's requirements."
      },
      {
        "date": "2023-11-02T07:18:00.000Z",
        "voteCount": 1,
        "content": "Answer A looks pretty good except one thing \n\"Configure a CloudWatch Logs metric filter that increments a metric for each API operation name.\"\nCan the metric filter \"increment\" the metric or just send metric value?\nAnswer B looks weird to me - I don't know the way to populate Logs Insights query result as the CW metric"
      },
      {
        "date": "2023-10-26T10:03:00.000Z",
        "voteCount": 1,
        "content": "The right option is A. In the question, the requirement is to get a metric. In option B, we are not creating any metric."
      },
      {
        "date": "2023-10-12T07:41:00.000Z",
        "voteCount": 1,
        "content": "For me its B. From the question \"The company needs to gather a metric for each API operation by response code for each version of the application that is in use\". Only CloudWatch Logs Insight is querying for response code and application version. CloudWatch Logs metric filter is using API operation name to create an aggregate metric"
      },
      {
        "date": "2023-10-09T09:52:00.000Z",
        "voteCount": 2,
        "content": "It should be between A and B but the thing is the difference use cases between CW Logs Metric and CW Logs Insights. Metrics will provide Aggregated data while insights give you direct access to raw log event data. Hence I would go for A"
      },
      {
        "date": "2023-09-06T00:19:00.000Z",
        "voteCount": 1,
        "content": "More efficient"
      },
      {
        "date": "2023-09-03T06:44:00.000Z",
        "voteCount": 1,
        "content": "real response : A"
      },
      {
        "date": "2023-07-04T05:06:00.000Z",
        "voteCount": 2,
        "content": "A is correct. For event-based tracking, metric filters in CW are more automatic. Insights query need some intervention still. Devops = AUTOMATION. \n\nX-Ray is used extensively for development, especially backend APIs. By production it should already had resolved any API health issues."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/amazon/view/105230-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company provides an application to customers. The application has an Amazon API Gateway REST API that invokes an AWS Lambda function. On initialization, the Lambda function loads a large amount of data from an Amazon DynamoDB table. The data load process results in long cold-start times of 8-10 seconds. The DynamoDB table has DynamoDB Accelerator (DAX) configured.<br>Customers report that the application intermittently takes a long time to respond to requests. The application receives thousands of requests throughout the day. In the middle of the day, the application experiences 10 times more requests than at any other time of the day. Near the end of the day, the application's request volume decreases to 10% of its normal total.<br>A DevOps engineer needs to reduce the latency of the Lambda function at all times of the day.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure provisioned concurrency on the Lambda function with a concurrency value of 1. Delete the DAX cluster for the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure reserved concurrency on the Lambda function with a concurrency value of 0.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure provisioned concurrency on the Lambda function. Configure AWS Application Auto Scaling on the Lambda function with provisioned concurrency values set to a minimum of 1 and a maximum of 100.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure reserved concurrency on the Lambda function. Configure AWS Application Auto Scaling on the API Gateway API with a reserved concurrency maximum value of 100."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-24T02:08:00.000Z",
        "voteCount": 9,
        "content": "To reduce the latency of the Lambda function at all times of the day, the best solution is to configure provisioned concurrency on the Lambda function with a concurrency value of 1 and also configure AWS Application Auto Scaling on the Lambda function with provisioned concurrency values set to a minimum of 1 and a maximum of 100 (Option C).\n\nProvisioned concurrency will ensure that the Lambda function has a set number of instances always available, which will reduce the cold start time. By setting the provisioned concurrency values to a minimum of 1 and a maximum of 100, the Lambda function can handle sudden spikes in traffic and can scale down during low-traffic periods, thus minimizing costs."
      },
      {
        "date": "2024-09-24T02:09:00.000Z",
        "voteCount": 2,
        "content": "Lambda integrates with Application Auto Scaling, allowing you to manage provisioned concurrency on a schedule or based on utilization.\nhttps://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\nhttps://docs.aws.amazon.com/autoscaling/application/userguide/services-that-can-integrate-lambda.html"
      },
      {
        "date": "2024-09-24T02:09:00.000Z",
        "voteCount": 2,
        "content": "By implementing provisioned concurrency with auto-scaling and retaining the DynamoDB DAX cluster, the DevOps engineer can effectively reduce the latency of the Lambda function at all times of the day while ensuring that the application can handle varying request volumes."
      },
      {
        "date": "2024-09-24T02:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is to is to use application autoscaling to create a Lambda scaling policy for Provisioned Concurrency based on a re-occuring schedule\nHere is reference that explains exactly how to do it CLI: https://aws.amazon.com/blogs/compute/scheduling-aws-lambda-provisioned-concurrency-for-recurring-peak-usage/\nHere are truncated command examples from the reference:\naws application-autoscaling register-scalable-target --service-namespace lambda [...] --min-capacity 1 --max-capacity 100 --scalable-dimension lambda:function:ProvisionedConcurrency\naws application-autoscaling put-scheduled-action --service-namespace lambda --scalable-dimension lambda:function:ProvisionedConcurrency --scalable-target-action MinCapacity=100 [...]"
      },
      {
        "date": "2024-08-20T08:59:00.000Z",
        "voteCount": 1,
        "content": "C is right answer."
      },
      {
        "date": "2024-08-14T05:26:00.000Z",
        "voteCount": 1,
        "content": "C is right answer. Omamko."
      },
      {
        "date": "2024-01-27T03:55:00.000Z",
        "voteCount": 1,
        "content": "C definitely"
      },
      {
        "date": "2024-01-07T07:10:00.000Z",
        "voteCount": 1,
        "content": "Agree answer C. Provisioned concurrency \u2013 This is the number of pre-initialized execution environments allocated to your function. These execution environments are ready to respond immediately to incoming function requests."
      },
      {
        "date": "2023-11-28T09:35:00.000Z",
        "voteCount": 1,
        "content": "Auto Scaling makes it easy to dynamically adjust the provisioned concurrency based on metrics and hence option C is a good choice to dynamically adjust based on the changing demand levels of the lambda function throughout the day."
      },
      {
        "date": "2023-06-11T18:22:00.000Z",
        "voteCount": 2,
        "content": "C, This can help to optimize the number of instances available to serve requests, reducing the likelihood of cold starts and improving overall performance."
      },
      {
        "date": "2023-04-15T10:39:00.000Z",
        "voteCount": 1,
        "content": "C it is"
      },
      {
        "date": "2023-04-14T11:09:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-04-04T17:29:00.000Z",
        "voteCount": 1,
        "content": "D / C - Provisioned concurrency is a manually set fixed value."
      },
      {
        "date": "2024-05-17T22:53:00.000Z",
        "voteCount": 1,
        "content": "Somewhat true, except (in researching this), I discovered it the setting can also be dynamically adjusted up or down by application auto-scaling based on a schedule.  Look at my other references, and you'll understand."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/amazon/view/105490-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is adopting AWS CodeDeploy to automate its application deployments for a Java-Apache Tomcat application with an Apache Webserver. The development team started with a proof of concept, created a deployment group for a developer environment, and performed functional tests within the application. After completion, the team will create additional deployment groups for staging and production.<br>The current log level is configured within the Apache settings, but the team wants to change this configuration dynamically when the deployment occurs, so that they can set different log level configurations depending on the deployment group without having a different application revision for each group.<br>How can these requirements be met with the LEAST management overhead and without requiring different script versions for each deployment group?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTag the Amazon EC2 instances depending on the deployment group. Then place a script into the application revision that calls the metadata service and the EC2 API to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference the script as part of the AfterInstall lifecycle hook in the appspec.yml file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ NAME to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the BeforeInstall lifecycle hook in the appspec.yml file.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodeDeploy custom environment variable for each environment. Then place a script into the application revision that checks this environment variable to identify which deployment group the instance is part of. Use this information to configure the log level settings. Reference this script as part of the ValidateService lifecycle hook in the appspec.yml file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_ID to identify which deployment group the instance is part of to configure the log level settings. Reference this script as part of the Install lifecycle hook in the appspec.yml file."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-18T00:18:00.000Z",
        "voteCount": 15,
        "content": "B. In the docs: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html you'll find a Note: \"The Start, Install, TestTraffic, AllowTraffic, and End events in the deployment cannot be scripted, which is why they appear in gray in this diagram.\" Thats why its not D."
      },
      {
        "date": "2024-09-24T02:09:00.000Z",
        "voteCount": 9,
        "content": "Answer: B\nRead this blog. \nhttps://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/\n\nif [ \"$DEPLOYMENT_GROUP_NAME\" == \"Staging\" ]\nthen\n    sed -i -e 's/LogLevel warn/LogLevel debug/g' /etc/httpd/conf/httpd.conf\nfi"
      },
      {
        "date": "2024-09-24T02:09:00.000Z",
        "voteCount": 2,
        "content": "B is correct.\nDEPLOYMENT_ID :  This variables contains the deployment ID of the current deployment.\n\nDEPLOYMENT_GROUP_NAME :  This variable contains the name of the deployment group. A deployment group is a set of instances associated with an application that you target for a deployment."
      },
      {
        "date": "2024-09-24T02:09:00.000Z",
        "voteCount": 2,
        "content": "Answer B. You only need to consider options B and D which are the least complex ones. The option B gives the CodeDeploy environment variable DEPLOYMENT_GROUP_ NAME that points to different instances and gives the option to set different log-level configurations in the same script depending on the deployment group without having a different application revision for each group. Also, The BeforeInstall lifecycle hook in the appspec.yml file refers to a script that will run on the instance before the application revision files are installed."
      },
      {
        "date": "2024-09-24T02:09:00.000Z",
        "voteCount": 1,
        "content": "version: 0.0\nos: linux\nfiles:\n  - source: /\n    destination: /var/www/html/\nhooks:\n  BeforeInstall:\n    - location: scripts/update_log_level.sh\n      timeout: 300\n      runas: root"
      },
      {
        "date": "2024-09-24T02:09:00.000Z",
        "voteCount": 1,
        "content": "This reference specifies the exact scenario described in \"B\", so I have to go with that\n\"Set the log level according to the deployment group.\"\nhttps://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/\n\ncat install_dependencies.sh\n   [...]\n\tif [ \"$DEPLOYMENT_GROUP_NAME\" == \"Staging\" ]\n   [...]\n\ncat appspec.yml\nhooks:\n   BeforeInstall:\n     - location: scripts/install_dependencies"
      },
      {
        "date": "2024-01-27T04:15:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-02-08T23:01:00.000Z",
        "voteCount": 1,
        "content": "B is correct: &lt;without having a different application revision for each group&gt; means A and C is incorrect. \nA and C: &lt;place a script into the application revision&gt; both mention this, indicating a revision of the app, which is contradicted to the question\nD: there is no Install lifecycle hook"
      },
      {
        "date": "2023-12-31T20:28:00.000Z",
        "voteCount": 3,
        "content": "Answer D. \nYou only need to consider options B and D which are the least complex ones. The option B gives the CodeDeploy environment variable DEPLOYMENT_GROUP_ NAME, but DEPLOYMENT_GROUP_ID is recommended because it's more reliable and less prone to changes or inconsistencies. Answer D with settings DEPLOYMENT_GROUP_ID environment variable, which contains the unique identifier for the deployment group. This allows you to identify the deployment group without relying on custom scripts or metadata services."
      },
      {
        "date": "2023-11-18T10:52:00.000Z",
        "voteCount": 1,
        "content": "Based on the analysis, Option B is the most efficient and straightforward approach. It uses the built-in DEPLOYMENT_GROUP_NAME environment variable provided by CodeDeploy and involves minimal management overhead. The script can easily read this variable to determine the deployment group and set the log level accordingly, eliminating the need for different script versions for each group. This method aligns well with the requirement of least management overhead and simplicity."
      },
      {
        "date": "2023-06-11T18:26:00.000Z",
        "voteCount": 2,
        "content": "B is the most straightforward and efficient solution to meet the requirements with the least management overhead and without requiring different script versions for each deployment group."
      },
      {
        "date": "2023-08-06T00:54:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/devops/using-codedeploy-environment-variables/"
      },
      {
        "date": "2023-06-10T00:23:00.000Z",
        "voteCount": 1,
        "content": "Answer is B practical use case\n"
      },
      {
        "date": "2023-06-08T02:35:00.000Z",
        "voteCount": 2,
        "content": "Running the hook during the Install or AfterInstall would make more sense but hooks for Install are not available (like in answer D) and AfterInstall is not included in answers so the best answer is B."
      },
      {
        "date": "2023-05-03T01:54:00.000Z",
        "voteCount": 4,
        "content": "B is the only correct answer"
      },
      {
        "date": "2023-04-14T13:48:00.000Z",
        "voteCount": 3,
        "content": "B. Create a script that uses the CodeDeploy environment variable DEPLOYMENT_GROUP_NAME to identify which deployment group the instance is part of, and use this information to configure the log level settings. Reference this script as part of the BeforeInstall lifecycle hook in the appspec.yml file, would be the option with the least management overhead and without requiring different script versions for each deployment group"
      },
      {
        "date": "2023-04-14T11:10:00.000Z",
        "voteCount": 1,
        "content": "Option B is the best solution for this use case. By using the CodeDeploy environment variable DEPLOYMENT_GROUP_NAME, the script can identify the deployment group that the instance is part of, without requiring any additional configuration or management overhead. The script can then dynamically configure the log level settings based on the identified deployment group."
      },
      {
        "date": "2023-04-12T17:06:00.000Z",
        "voteCount": 1,
        "content": "B  See: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html"
      },
      {
        "date": "2023-04-07T02:41:00.000Z",
        "voteCount": 1,
        "content": "D is valid and LEAST management overhead"
      },
      {
        "date": "2023-04-22T01:15:00.000Z",
        "voteCount": 2,
        "content": "typo, B is the right: BeforeInstall lifecycle hook"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/amazon/view/105332-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company requires its developers to tag all Amazon Elastic Block Store (Amazon EBS) volumes in an account to indicate a desired backup frequency. This requirement Includes EBS volumes that do not require backups. The company uses custom tags named Backup_Frequency that have values of none, dally, or weekly that correspond to the desired backup frequency. An audit finds that developers are occasionally not tagging the EBS volumes.<br>A DevOps engineer needs to ensure that all EBS volumes always have the Backup_Frequency tag so that the company can perform backups at least weekly unless a different value is specified.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS Config in the account. Create a custom rule that returns a compliance failure for all Amazon EC2 resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS Config in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS CloudTrail in the account. Create an Amazon EventBridge rule that reacts to EBS CreateVolume events or EBS ModifyVolume events. Configure a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly. Specify the runbook as the target of the rule."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-24T02:11:00.000Z",
        "voteCount": 2,
        "content": "Only takes few minutes to login &gt; Config &gt; Managed rulename = BACKUP_PLAN_MIN_FREQUENCY_AND_MIN_RETENTION_CHECK\nA = tags everything in EC2, thats EC2::* which includes ELB/EIP/etc. Nope.\nOption B you can specify the tags to match &amp; expected values = answer."
      },
      {
        "date": "2024-09-24T02:11:00.000Z",
        "voteCount": 1,
        "content": "A: It works, but it uses a custom rule.\nB: It is simpler than option A as it uses a managed rule which already exists.\nC: It only applies to new volumes and does not address existing resources.\nD: It is better than C but still does not fully meet the requirement to check all EBS volumes and enforce compliance.\nBest Answer is B."
      },
      {
        "date": "2024-09-24T02:11:00.000Z",
        "voteCount": 1,
        "content": "By leveraging the AWS Config managed rule and automated remediation action, the DevOps engineer can ensure that all EBS volumes in the account always have the required Backup_Frequency tag, enabling the company to perform backups at least weekly unless a different value is explicitly specified. This solution provides continuous monitoring and automated remediation, reducing the risk of human error and ensuring compliance with the company's backup policy."
      },
      {
        "date": "2024-09-24T02:11:00.000Z",
        "voteCount": 2,
        "content": "Option B --&gt; AWS config managed rule on EC2::Volume resource + custom SSM automation document\nNot Option A --&gt; because it says custom config rule on all EC2::Instance + Managed SSM automation document\nNot options C &amp; D --&gt; As it says cloudtrail which is for logging API actions"
      },
      {
        "date": "2024-06-30T14:19:00.000Z",
        "voteCount": 1,
        "content": "sorry, a typo.. Option A also says custom SSM automation document, but it is wrong where it says custom config rule on all Ec2::Instance"
      },
      {
        "date": "2024-02-13T14:43:00.000Z",
        "voteCount": 1,
        "content": "Answer is A.\nChecks if your resources have the tags that you specify. For example, you can check whether your Amazon EC2 instances have the CostCenter tag, while also checking if all your RDS instance have one set of Keys tag. Separate multiple values with commas. You can check up to 6 tags at a time.\n\nThe AWS-managed AWS Systems Manager automation document AWS-SetRequiredTags does not work as a remediation with this rule. You will need to create your own custom Systems Manager automation documentation for remediation\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/required-tags.html"
      },
      {
        "date": "2024-02-18T06:07:00.000Z",
        "voteCount": 1,
        "content": "We don't need to create a custom AWS Config rule, we can utilize the managed rule to detect for non-compliance on the EBS volumes. Otherwise the options indicate to use a custom runbook for AWS Systems Manager to remediate the missing tags."
      },
      {
        "date": "2024-02-08T23:06:00.000Z",
        "voteCount": 4,
        "content": "B is correct: We should use AWS config for this task \nC and D: cloud trail is for auditing account activities, which is irrelevant\nA: &lt;returns a compliance failure for all Amazon EC2 resources&gt; : we need to remediate EC2 volumes only, not all EC2 resources"
      },
      {
        "date": "2024-01-24T23:19:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer \"The AWS-managed AWS Systems Manager automation document AWS-SetRequiredTags does not work as a remediation with this rule. You will need to create your own custom Systems Manager automation documentation for remediation.\" from this link : https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html"
      },
      {
        "date": "2023-11-28T13:32:00.000Z",
        "voteCount": 3,
        "content": "B is the best choice. If you look at Config Managed Rules you can find - ebs-in-backup-plan - Check if Amazon Elastic Block Store (Amazon EBS) volumes are added in backup plans of AWS Backup. The rule is NON_COMPLIANT if Amazon EBS volumes are not included in backup plans."
      },
      {
        "date": "2023-06-11T18:36:00.000Z",
        "voteCount": 3,
        "content": "B is the most straightforward and efficient solution to ensure that all EBS volumes always have the Backup_Frequency tag applied with the least amount of effort.\n\nA This approach requires more effort than using a managed rule provided by AWS."
      },
      {
        "date": "2023-08-06T01:05:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/config/latest/developerguide/required-tags.html"
      },
      {
        "date": "2023-06-08T02:56:00.000Z",
        "voteCount": 1,
        "content": "B makes sense, you can use managed rule \"required-tags\" to identify non-compliant volumes and custom SSM document to fix it."
      },
      {
        "date": "2023-05-03T01:56:00.000Z",
        "voteCount": 1,
        "content": "B makes sense"
      },
      {
        "date": "2023-04-14T13:50:00.000Z",
        "voteCount": 1,
        "content": "B. Set up AWS Config in the account. Use a managed rule that returns a compliance failure for EC2::Volume resources that do not have a Backup Frequency tag applied. Configure a remediation action that uses a custom AWS Systems Manager Automation runbook to apply the Backup_Frequency tag with a value of weekly."
      },
      {
        "date": "2023-04-07T03:10:00.000Z",
        "voteCount": 1,
        "content": "Answer B: Config has a managed rule for type AWS EC2 Volume for tag compliance check."
      },
      {
        "date": "2023-04-05T13:45:00.000Z",
        "voteCount": 2,
        "content": "B for me. https://aws.amazon.com/ru/blogs/mt/build-an-aws-config-custom-rule-to-optimize-amazon-ebs-volume-types/"
      },
      {
        "date": "2023-04-05T13:46:00.000Z",
        "voteCount": 2,
        "content": "Sorry A is the answer. This is custom rule"
      },
      {
        "date": "2023-04-05T13:48:00.000Z",
        "voteCount": 1,
        "content": "But very strange that custom rule for all ec2 instances , it should be only ec2 volumes"
      },
      {
        "date": "2023-04-14T17:23:00.000Z",
        "voteCount": 4,
        "content": "Option A creates a custom rule that applies to all EC2 resources, not just volumes, which may create additional overhead. The custom AWS Systems Manager Automation runbook is used to apply the Backup_Frequency tag with a value of weekly, but this approach can result in inconsistent tagging if the developers specify a different desired backup frequency. Therefore, Option A is not the correct answer.\n\nOption B is the correct answer because it uses a managed rule specifically for EC2 volumes, which simplifies the configuration effort and ensures that all volumes have the Backup_Frequency tag applied consistently. The custom AWS Systems Manager Automation runbook is used to automatically apply the Backup_Frequency tag with a value of weekly, which reduces the risk of data loss due to missing backups. Your comment that the managed rule should only apply to volumes is correct, and Option B addresses that requirement."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/amazon/view/105457-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is using an Amazon Aurora cluster as the data store for its application. The Aurora cluster is configured with a single DB instance. The application performs read and write operations on the database by using the cluster's instance endpoint.<br>The company has scheduled an update to be applied to the cluster during an upcoming maintenance window. The cluster must remain available with the least possible interruption during the maintenance window.<br>What should a DevOps engineer do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a reader instance to the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster's reader endpoint for reads.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a reader instance to the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster\u2019s reader endpoint for reads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the Multi-AZ option on the Aurora cluster. Create a custom ANY endpoint for the cluster. Update the application to use the Aurora cluster's custom ANY endpoint for read and write operations"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-04T09:41:00.000Z",
        "voteCount": 12,
        "content": "B and D are incorrect because Aurora cluster provides cluster and read endpoints, but does not support creating custom ANY endpoints.\n\n\n\nC and D are incorrect because Amazon Aurora's multi-AZ option must be set when the DB instance is created.\n\n\n\nTherefore, A is correct."
      },
      {
        "date": "2023-07-05T05:17:00.000Z",
        "voteCount": 11,
        "content": "Option A is the right choise for an existing Cluster without Multi-AZ!\nRefer to: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html\n \n#Read the Tip Box#\n\"You can set up a Multi-AZ cluster by making a simple choice when you create the cluster. The choice is simple whether you use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also make an existing Aurora cluster into a Multi-AZ cluster by adding a new reader instance and specifying a different Availability Zone.\""
      },
      {
        "date": "2024-09-24T02:13:00.000Z",
        "voteCount": 2,
        "content": "C. Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster\u2019s reader endpoint for reads.\n\nEnabling Multi-AZ ensures that the data in the Aurora cluster is replicated across multiple Availability Zones (AZs), providing high availability and durability. During maintenance, the update will be applied to one AZ at a time, allowing the cluster to remain available. Updating the application to use the cluster endpoint for write operations ensures that writes will continue to be directed to the primary instance in the cluster, while updating the reader endpoint for reads allows read traffic to be routed to the appropriate instance. Adding a reader instance or creating a custom ANY endpoint are not necessary for meeting the requirement of minimizing interruption during maintenance."
      },
      {
        "date": "2024-09-24T02:13:00.000Z",
        "voteCount": 1,
        "content": "To meet the requirement of keeping the Aurora cluster available with the least possible interruption during the maintenance window, the DevOps engineer should choose option C: Turn on the Multi-AZ option on the Aurora cluster. Update the application to use the Aurora cluster endpoint for write operations. Update the Aurora cluster\u2019s reader endpoint for reads.\n\nOption A is incorrect because adding a reader instance alone does not provide high availability during maintenance, and updating the application to use the reader endpoint for reads is unnecessary when the Multi-AZ option is enabled"
      },
      {
        "date": "2024-09-24T02:12:00.000Z",
        "voteCount": 2,
        "content": "Option A is the correct choice, adding a reader instance after provisioning is the same as setting a Multi-AZ during creation. In the event that the primary fails, the reader instance will be promoted to do both reading and writing automatically. \n\nAdditionally, reader instance can also be used for read activity if you use the cluster reader endpoint which you can serve to user/application closer to the region for better performance."
      },
      {
        "date": "2024-09-24T02:12:00.000Z",
        "voteCount": 1,
        "content": "\"You can set up a Multi-AZ DB cluster by making a simple choice when you create the cluster. You can use the AWS Management Console, the AWS CLI, or the Amazon RDS API. You can also convert an existing Aurora DB cluster into a Multi-AZ DB cluster by adding a new reader DB instance and specifying a different Availability Zone.\""
      },
      {
        "date": "2024-09-24T02:12:00.000Z",
        "voteCount": 2,
        "content": "To meet the requirements of the given scenario, the DevOps engineer should do the following:\n\nAdd a reader instance to the Aurora cluster:\n\nThis will allow the application to offload read operations to the reader instance, reducing the load on the primary instance.\nThe application should be updated to use the Aurora cluster endpoint for write operations and the reader endpoint for read operations.\nThe engineer should not turn on the Multi-AZ option on the Aurora cluster.\n\nMulti-AZ is used to provide high availability and failover capabilities, but it does not necessarily minimize interruption during a maintenance window.\nAdding a reader instance is a more appropriate solution to maintain availability and distribute the read workload.\nTherefore, the correct option is A. Add a reader instance to the Aurora cluster and update the application to use the appropriate endpoints for read and write operations."
      },
      {
        "date": "2024-06-20T02:08:00.000Z",
        "voteCount": 1,
        "content": "You cannot change the az option after creation but can deploy a reader instance in another az and use it for reading and writing in your main instance."
      },
      {
        "date": "2024-06-18T02:52:00.000Z",
        "voteCount": 1,
        "content": "There isn't a specific Multi-AZ mode for Aurora it's multi-AZ by default (it uses all three AZs). So I think A or B. A is the most commonly used method but I think B offers less disruption because requests are always routed to the instance with greater availability."
      },
      {
        "date": "2024-05-09T11:54:00.000Z",
        "voteCount": 1,
        "content": "By enabling Multi-AZ deployment, creating a custom ANY endpoint, and updating the application to use this endpoint for all read and write operations, the DevOps engineer can ensure that the Aurora cluster remains available during the maintenance window with minimal interruption. The application will be able to transparently connect to the available instances (primary or read-only replica), and Aurora will automatically fail over to the read-only replica if the primary instance becomes unavailable during the maintenance process."
      },
      {
        "date": "2024-05-04T23:07:00.000Z",
        "voteCount": 1,
        "content": "Can't tell the difference between A and C except Multi-AZ, both should be working if only read is needed during maintenance window.\nAnd also not understand why only read is needed when people choose them.\nSome say there is no custom ANY endpoint, I think it only means you can choose any instance or instances to that endpoint.\nSo I go with B"
      },
      {
        "date": "2024-03-22T19:16:00.000Z",
        "voteCount": 1,
        "content": "answer A).\nthe question doesn't mention what DB is behind the Aurora.\nMulti-AZ config avoids downtime EXCEPT MySQL/MariaDB.\nSo the question mentions \"the least possible interruption\", then A) is the appropriate one"
      },
      {
        "date": "2024-02-20T09:13:00.000Z",
        "voteCount": 1,
        "content": "is corret is  ='c' - &gt; 'a' is not \nThis option leverages Aurora's built-in high availability and failover mechanisms to ensure minimal interruption. By using the cluster endpoint for writes, the application automatically writes to the primary instance. In case of maintenance or failure, Aurora handles failover to another instance with minimal downtime. The reader endpoint distributes read traffic across available replicas, enhancing read scalability and availability without affecting write operations. This setup ensures that the application remains as available as possible during maintenance"
      },
      {
        "date": "2024-01-27T06:49:00.000Z",
        "voteCount": 1,
        "content": "A is good"
      },
      {
        "date": "2024-01-20T22:54:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2024-01-07T07:32:00.000Z",
        "voteCount": 1,
        "content": "Agree answer A. There is no ANY custom endpoint and multy-AZ can be set up during cluster creation"
      },
      {
        "date": "2024-01-02T15:14:00.000Z",
        "voteCount": 2,
        "content": "'A' - since Multi AZ has to be setup when the cluster is created. It cannot be updated later."
      },
      {
        "date": "2024-03-19T22:33:00.000Z",
        "voteCount": 1,
        "content": "I agree with you. C D is the wrong selection. because we can't enable Multi-AZ after cluster is created"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/amazon/view/105265-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company must encrypt all AMIs that the company shares across accounts. A DevOps engineer has access to a source account where an unencrypted custom AMI has been built. The DevOps engineer also has access to a target account where an Amazon EC2 Auto Scaling group will launch EC2 instances from the AMI. The DevOps engineer must share the AMI with the target account.<br>The company has created an AWS Key Management Service (AWS KMS) key in the source account.<br>Which additional steps should the DevOps engineer perform to meet the requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the source account, copy the unencrypted AMI to an encrypted AMI. Specify the KMS key in the copy action.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the source account, copy the unencrypted AMI to an encrypted AMI. Specify the default Amazon Elastic Block Store (Amazon EBS) encryption key in the copy action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the source account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role in the target account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the source account, modify the key policy to give the target account permissions to create a grant. In the target account, create a KMS grant that delegates permissions to the Auto Scaling group service-linked role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the source account, share the unencrypted AMI with the target account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the source account, share the encrypted AMI with the target account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "ACF",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "ACD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-12T05:31:00.000Z",
        "voteCount": 11,
        "content": "ADF seems to be the correct answer"
      },
      {
        "date": "2023-04-05T04:27:00.000Z",
        "voteCount": 9,
        "content": "A D F for me. https://jackiechen.blog/2020/01/29/share-encrypted-ami-across-aws-accounts/"
      },
      {
        "date": "2024-08-04T11:22:00.000Z",
        "voteCount": 1,
        "content": "ADF for me,"
      },
      {
        "date": "2024-03-01T09:54:00.000Z",
        "voteCount": 1,
        "content": "ADF is correct"
      },
      {
        "date": "2024-02-28T04:26:00.000Z",
        "voteCount": 1,
        "content": "A D F for me"
      },
      {
        "date": "2024-01-27T19:54:00.000Z",
        "voteCount": 2,
        "content": "ADF: \nA: cannot be B because using KMS\nD: Must share with the account because grant is only temp\nF: share the AMI with the target"
      },
      {
        "date": "2024-01-27T07:42:00.000Z",
        "voteCount": 1,
        "content": "AFD seem about right"
      },
      {
        "date": "2024-01-20T22:56:00.000Z",
        "voteCount": 1,
        "content": "ADF the correct answer"
      },
      {
        "date": "2024-01-12T19:43:00.000Z",
        "voteCount": 1,
        "content": "ACF. For autoscaling to work a KMS grant is needed"
      },
      {
        "date": "2024-01-12T19:44:00.000Z",
        "voteCount": 1,
        "content": "Should be ADF"
      },
      {
        "date": "2023-12-09T19:07:00.000Z",
        "voteCount": 1,
        "content": "ADF is the right answer"
      },
      {
        "date": "2023-11-02T14:35:00.000Z",
        "voteCount": 2,
        "content": "C is incorrect as the AMI **MUST** be shared with the account.\nnot just the scaling group. So it would make sense for the target account to create the grant."
      },
      {
        "date": "2023-09-24T10:24:00.000Z",
        "voteCount": 1,
        "content": "ADF is the right answer."
      },
      {
        "date": "2023-09-02T04:00:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/security/how-to-create-a-custom-ami-with-encrypted-amazon-ebs-snapshots-and-share-it-with-other-accounts-and-regions/"
      },
      {
        "date": "2023-08-26T12:08:00.000Z",
        "voteCount": 1,
        "content": "ADF is right"
      },
      {
        "date": "2023-07-24T01:47:00.000Z",
        "voteCount": 2,
        "content": "ADF is correct"
      },
      {
        "date": "2023-07-04T05:34:00.000Z",
        "voteCount": 2,
        "content": "Step 1: Always specify the KMS (CMK) key to encrypt with when creating/copying images\nStep 2: Modify the CMK key policy to allow trusted role to assume the key to decrypt image\nStep 3: Use cross-account trust policy to grant the other account access to the encrypted image"
      },
      {
        "date": "2023-06-11T20:07:00.000Z",
        "voteCount": 1,
        "content": "ACD, The question is KMS Permission.\nOption F is not a valid solution because it shares the encrypted AMI with the target account, but it does not address the requirement of delegating permissions to the Auto Scaling group service-linked role to use the KMS key to launch instances from the encrypted AMI."
      },
      {
        "date": "2023-06-11T20:09:00.000Z",
        "voteCount": 1,
        "content": "Sorry as my fault ADF is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/amazon/view/105504-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS CodePipeline pipelines to automate releases of its application A typical pipeline consists of three stages build, test, and deployment. The company has been using a separate AWS CodeBuild project to run scripts for each stage. However, the company now wants to use AWS CodeDeploy to handle the deployment stage of the pipelines.<br>The company has packaged the application as an RPM package and must deploy the application to a fleet of Amazon EC2 instances. The EC2 instances are in an EC2 Auto Scaling group and are launched from a common AMI.<br>Which combination of steps should a DevOps engineer perform to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new version of the common AMI with the CodeDeploy agent installed. Update the IAM role of the EC2 instances to allow access to CodeDeploy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new version of the common AMI with the CodeDeploy agent installed. Create an AppSpec file that contains application deployment scripts and grants access to CodeDeploy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI. Configure CodeDeploy to deploy the newly created AMI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application in CodeDeploy. Configure an in-place deployment type. Specify the Auto Scaling group as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application in CodeDeploy. Configure an in-place deployment type. Specify the EC2 instances that are launched from the common AMI as the deployment target. Update the CodePipeline pipeline to use the CodeDeploy action to deploy the application."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-30T07:44:00.000Z",
        "voteCount": 9,
        "content": "A and D are the correct ones.\n\nE is wrong because it says that the instances are on an ASG.\nC is qrong. You deploy the new RPM on the AMI, you do not create a new AMI every time to install the RPM.\nB is wrong, the appspec has nothing to do with permissions"
      },
      {
        "date": "2024-09-24T02:15:00.000Z",
        "voteCount": 5,
        "content": "A and D are correct: \nA: rebuild the AMI and Update the IAM role of the EC2 instances to allow access to CodeDeploy is necessary \nB: no need to grants AppSpec file access to code CodeDeploy\nC: &lt;Add a step to the CodePipeline pipeline to use EC2 Image Builder to create a new AMI&gt; : this is unnecessary, we have already done it in option A. Addtionally, recreating AIM each time running the CICD pipiline is unnecessary\nD: ok\nE: &lt;Specify the EC2 instances that are launched from the common AMI as the deployment targe&gt;: this is time-consumming. There might by hundreds of EC2 instances and targeting them individually is time-consuming and not effective."
      },
      {
        "date": "2024-01-27T08:18:00.000Z",
        "voteCount": 1,
        "content": "A and D: \nB is incorrect: AppSpec file does not need to be granted access to code deploy. It is code deploy that need the permission to get acess to Appspec file"
      },
      {
        "date": "2023-12-18T21:29:00.000Z",
        "voteCount": 3,
        "content": "A - instances need code deploy agent and role \nD - target as ASG"
      },
      {
        "date": "2023-11-07T06:06:00.000Z",
        "voteCount": 2,
        "content": "A D. \nIAM role/instance profile requirement for EC2 is to allow EC2 access to S3 buckets used by CodeDeploy.\n\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-create-iam-instance-profile.html"
      },
      {
        "date": "2023-10-29T21:52:00.000Z",
        "voteCount": 2,
        "content": "Should be BD. EC2 doesn't need a permission to access CodeDeploy. Instead an IAM role associated with Code Deployment Group should have an permission to launch instances in that autoscaling group."
      },
      {
        "date": "2023-12-18T21:28:00.000Z",
        "voteCount": 3,
        "content": "A is right \nyou have to attach IAM role to EC2 instance , for them to be controlled by Code deploy . The agent running in the ec2 instance needs to talk with code deploy ."
      },
      {
        "date": "2023-10-12T09:15:00.000Z",
        "voteCount": 3,
        "content": "Why EC2 instance need access to CodeDeploy??, in the doc is mentioned only S3: \"Create or locate an IAM instance profile that allows the Amazon EC2 Auto Scaling group to work with Amazon S3\" https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html"
      },
      {
        "date": "2023-12-09T19:15:00.000Z",
        "voteCount": 3,
        "content": "As the codedeploy agent is being installed inside the EC2, This agent facilitates the deployment process by coordinating with the CodeDeploy service. Hence, \nThe EC2 instances must have an IAM role that grants them the necessary permissions to interact with CodeDeploy. This step is critical to ensure that the deployment process can be executed securely and successfully"
      },
      {
        "date": "2023-08-09T08:30:00.000Z",
        "voteCount": 1,
        "content": "AD as explained en the comments"
      },
      {
        "date": "2023-06-08T03:59:00.000Z",
        "voteCount": 2,
        "content": "AD are correct.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html"
      },
      {
        "date": "2023-08-06T01:35:00.000Z",
        "voteCount": 2,
        "content": "An in-place deployment allows you to deploy your application without creating new infrastructure.\nThe deployment type that is specific to the deployment's compute platform or deployments initiated by a CloudFormation stack update.https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-aws-auto-scaling.html"
      },
      {
        "date": "2023-04-23T09:56:00.000Z",
        "voteCount": 1,
        "content": "A, D;  B is wrong, because the AppSpec file cannot grant permissions for CodeDeploy"
      },
      {
        "date": "2023-04-15T10:55:00.000Z",
        "voteCount": 1,
        "content": "A&amp;D it is"
      },
      {
        "date": "2023-04-07T06:12:00.000Z",
        "voteCount": 2,
        "content": "A,D. \nB - wrong, the The 'permissions' section specifies how special permissions, should be applied to the files and directories/folders in the 'files' section\nC wrong, no need as AMI was already built.\nE wrong, as ASG is the target."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/amazon/view/105266-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company\u2019s security team requires that all external Application Load Balancers (ALBs) and Amazon API Gateway APIs are associated with AWS WAF web ACLs. The company has hundreds of AWS accounts, all of which are included in a single organization in AWS Organizations. The company has configured AWS Config for the organization. During an audit, the company finds some externally facing ALBs that are not associated with AWS WAF web ACLs.<br>Which combination of steps should a DevOps engineer take to prevent future violations? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelegate AWS Firewall Manager to a security account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelegate Amazon GuardDuty to a security account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Firewall Manager policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon GuardDuty policy to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Config managed rule to attach AWS WAF web ACLs to any newly created ALBs and API Gateway APIs."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-24T10:32:00.000Z",
        "voteCount": 10,
        "content": "If you see WAF you have to think AWS Firewall Manager."
      },
      {
        "date": "2023-04-15T11:01:00.000Z",
        "voteCount": 9,
        "content": "A and C"
      },
      {
        "date": "2024-09-24T02:16:00.000Z",
        "voteCount": 1,
        "content": "If instead you want to automatically apply the policy to existing in-scope resources, choose Auto remediate any noncompliant resources. This option creates a web ACL in each applicable account within the AWS organization and associates the web ACL with the resources in the accounts.\nWhen you choose Auto remediate any noncompliant resources, you can also choose to remove existing web ACL associations from in-scope resources, for the web ACLs that aren't managed by another active Firewall Manager policy. If you choose this option, Firewall Manager first associates the policy's web ACL with the resources, and then removes the prior associations. If a resource has an association with another web ACL that's managed by a different active Firewall Manager policy, this choice doesn't affect that association."
      },
      {
        "date": "2024-08-04T11:34:00.000Z",
        "voteCount": 1,
        "content": "I think that is best way to centralize manage firewall config"
      },
      {
        "date": "2024-07-24T22:44:00.000Z",
        "voteCount": 1,
        "content": "As my understanding, WAF related with AWS Firewall Manager."
      },
      {
        "date": "2024-05-23T13:59:00.000Z",
        "voteCount": 2,
        "content": "These references indicate this  can all be handled within Firewall manager (w/no references to Config or GuardDuty)\nhttps://aws.amazon.com/blogs/security/how-to-enforce-a-security-baseline-for-an-aws-waf-acl-across-your-organization-using-aws-firewall-manager/\nhttps://aws.amazon.com/solutions/implementations/automations-for-aws-firewall-manager/"
      },
      {
        "date": "2024-05-23T14:07:00.000Z",
        "voteCount": 1,
        "content": "In reading a little further, I suspect that Config may be being used in the background (since Config must be enabled to use WAF.  However, I believe that is totally transparent to the Organization WAF Administrator.  The administration of WAF and enforcement of WAF policies is ALL handled with the Web Application Firewall service."
      },
      {
        "date": "2024-05-05T02:36:00.000Z",
        "voteCount": 1,
        "content": "I think E works, but Firewall manager is designed for the purpose."
      },
      {
        "date": "2024-03-12T03:07:00.000Z",
        "voteCount": 1,
        "content": "A and C:  AWS Config rules are primarily used for monitoring and evaluating the configurations of your AWS resources for compliance with desired configurations. However, AWS Config also supports remediation actions through AWS Systems Manager Automation documents."
      },
      {
        "date": "2024-02-28T04:36:00.000Z",
        "voteCount": 1,
        "content": "Why not E?"
      },
      {
        "date": "2024-05-05T02:35:00.000Z",
        "voteCount": 1,
        "content": "I think E works, but Firewall manager is designed for the purpose."
      },
      {
        "date": "2024-03-12T03:06:00.000Z",
        "voteCount": 1,
        "content": "AWS Config rules are primarily used for monitoring and evaluating the configurations of  AWS resources for compliance with desired configurations. However, AWS Config also supports remediation actions through AWS Systems Manager Automation documents or lambda. Firewall manager is used to apply and enforce WebACLs to all ALBs at an organizational level to all your AWS Organization's accounts, and you can configure auto remidation for any non-compliant resource in any account."
      },
      {
        "date": "2024-01-27T08:24:00.000Z",
        "voteCount": 2,
        "content": "A and C: Config does not have any action, only notifications"
      },
      {
        "date": "2023-08-07T01:12:00.000Z",
        "voteCount": 2,
        "content": "A) is a prerequisites: AWS Firewall Manager prerequisites\nhttps://docs.aws.amazon.com/es_es/waf/latest/developerguide/join-aws-orgs.html"
      },
      {
        "date": "2023-07-04T07:23:00.000Z",
        "voteCount": 2,
        "content": "GuardDuty only posts findings, hence they can be eliminated.\nFrom my knowledge, Config only notifies.\nHence, A and C."
      },
      {
        "date": "2023-04-05T04:30:00.000Z",
        "voteCount": 1,
        "content": "A C for me"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/amazon/view/105235-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Key Management Service (AWS KMS) keys and manual key rotation to meet regulatory compliance requirements. The security team wants to be notified when any keys have not been rotated after 90 days.<br>Which solution will accomplish this?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS KMS to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge event to launch an AWS Lambda function to call the AWS Trusted Advisor API and publish to an Amazon Simple Notification Service (Amazon SNS) topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an AWS Config custom rule that publishes to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Security Hub to publish to an Amazon Simple Notification Service (Amazon SNS) topic when keys are more than 90 days old."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-27T08:27:00.000Z",
        "voteCount": 5,
        "content": "C is correct\nA is not because KMS does not provide this function"
      },
      {
        "date": "2024-01-07T07:50:00.000Z",
        "voteCount": 3,
        "content": "Answer C. AWS Config"
      },
      {
        "date": "2023-07-04T07:24:00.000Z",
        "voteCount": 3,
        "content": "C. Config rules notifies."
      },
      {
        "date": "2023-06-28T10:55:00.000Z",
        "voteCount": 3,
        "content": "Are these questions really came from DOP-C02?"
      },
      {
        "date": "2023-06-08T04:05:00.000Z",
        "voteCount": 2,
        "content": "C makes sense. it should be a custom rule. Rule \"access-keys-rotated\" checks for access keys, not KMS keys."
      },
      {
        "date": "2023-04-15T11:02:00.000Z",
        "voteCount": 1,
        "content": "C it is"
      },
      {
        "date": "2023-04-07T06:22:00.000Z",
        "voteCount": 1,
        "content": "custom config: C"
      },
      {
        "date": "2023-04-06T01:38:00.000Z",
        "voteCount": 4,
        "content": "Looks like C, actually there is a managed rule for this:\nhttps://docs.aws.amazon.com/config/latest/developerguide/access-keys-rotated.html\nanyway trusted advisor cannot be used as there is no such check, also KMS does not have this action, security hub is not conducting any active checks just react to events"
      },
      {
        "date": "2023-07-13T09:44:00.000Z",
        "voteCount": 3,
        "content": "access key?"
      },
      {
        "date": "2024-02-22T11:14:00.000Z",
        "voteCount": 1,
        "content": "IAM Access Key &amp; KMS key are different. The managed rule is for IAM Access key"
      },
      {
        "date": "2023-04-05T13:57:00.000Z",
        "voteCount": 2,
        "content": "C for me. A there no such functionality, B i checked trusted advisor there is no such kms days, d is aggregator for config, guardduty. So you need config for D"
      },
      {
        "date": "2023-04-04T18:32:00.000Z",
        "voteCount": 1,
        "content": "Tell me Why not D.."
      },
      {
        "date": "2023-06-07T10:39:00.000Z",
        "voteCount": 1,
        "content": "\u2022\tOption D is not the correct answer because AWS Security Hub is primarily focused on aggregating and managing security findings, and it does not have a specific feature to monitor the age of AWS KMS keys."
      },
      {
        "date": "2023-08-19T22:56:00.000Z",
        "voteCount": 1,
        "content": "When you enable a control in Security hub it will automatically create a Config. There are 4 KMS related controls in security hub but none of them is about the rotation age. In this case you need to create a custom Config."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/amazon/view/105505-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A security review has identified that an AWS CodeBuild project is downloading a database population script from an Amazon S3 bucket using an unauthenticated request. The security team does not allow unauthenticated requests to S3 buckets for this project.<br>How can this issue be corrected in the MOST secure manner?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the bucket name to the AllowedBuckets section of the CodeBuild project settings. Update the build spec to use the AWS CLI to download the database population script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the S3 bucket settings to enable HTTPS basic authentication and specify a token. Update the build spec to use cURL to pass the token and download the database population script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove unauthenticated access from the S3 bucket with a bucket policy. Modify the service role for the CodeBuild project to include Amazon S3 access. Use the AWS CLI to download the database population script.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove unauthenticated access from the S3 bucket with a bucket policy. Use the AWS CLI to download the database population script using an IAM access key and a secret access key."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-27T08:36:00.000Z",
        "voteCount": 5,
        "content": "C is correct:\n+ Remove unauthenticated access from the S3 bucket with a bucket policy\n+ Modify the service role for the CodeBuild project to include Amazon S3 access."
      },
      {
        "date": "2024-08-04T11:50:00.000Z",
        "voteCount": 1,
        "content": "C is a correct answer.\nInside AWS, using of service roles is the best option."
      },
      {
        "date": "2023-12-18T21:48:00.000Z",
        "voteCount": 3,
        "content": "all these questions seem fairly to be part of aws devops exam"
      },
      {
        "date": "2023-10-26T10:44:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-10-25T05:54:00.000Z",
        "voteCount": 2,
        "content": "Involves using a service role also, which make it the most secure manner"
      },
      {
        "date": "2023-06-11T22:55:00.000Z",
        "voteCount": 4,
        "content": "C is the correct answer because it involves removing unauthenticated access from the S3 bucket with a bucket policy, which ensures that only authorized users or services can access the bucket."
      },
      {
        "date": "2023-06-08T04:06:00.000Z",
        "voteCount": 1,
        "content": "C is the best answer."
      },
      {
        "date": "2023-04-14T14:08:00.000Z",
        "voteCount": 2,
        "content": "c is the answer"
      },
      {
        "date": "2023-04-10T12:00:00.000Z",
        "voteCount": 1,
        "content": "c is the answer."
      },
      {
        "date": "2023-04-07T06:24:00.000Z",
        "voteCount": 1,
        "content": "C most secure"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/amazon/view/105236-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "An ecommerce company has chosen AWS to host its new platform. The company's DevOps team has started building an AWS Control Tower landing zone. The DevOps team has set the identity store within AWS IAM Identity Center (AWS Single Sign-On) to external identity provider (IdP) and has configured SAML 2.0.<br>The DevOps team wants a robust permission model that applies the principle of least privilege. The model must allow the team to build and manage only the team's own resources.<br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate IAM policies that include the required permissions. Include the aws:PrincipalTag condition key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate permission sets. Attach an inline policy that includes the required permissions and uses the aws:PrincipalTag condition key to scope the permissions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a group in the IdP. Place users in the group. Assign the group to accounts and the permission sets in IAM Identity Center.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a group in the IdP. Place users in the group. Assign the group to OUs and IAM policies.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable attributes for access control in IAM Identity Center. Apply tags to users. Map the tags as key-value pairs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable attributes for access control in IAM Identity Center. Map attributes from the IdP as key-value pairs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 32,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-30T07:56:00.000Z",
        "voteCount": 10,
        "content": "I would go with BCF. I cannot make a large comment on why but manage an identity center setup at work and find that these are the correct ones IMHO. Your IdP has attributes, not tags, ou have to rely on the IdP's attributes for instance. And you work with permission sets almost always, so the three answers about the permission sets make the full answer. You do not use IAM directly or tags for this."
      },
      {
        "date": "2023-04-06T02:05:00.000Z",
        "voteCount": 6,
        "content": "This is clearly stated here:\nhttps://aws.amazon.com/blogs/aws/new-attributes-based-access-control-with-aws-single-sign-on/\nAnswers are: BCF - permissions sets + IDP attributes mapping + groups \nFor example a user with IDP attribute of Dep/hr will be able to delete instances with this specific tag"
      },
      {
        "date": "2024-08-06T09:42:00.000Z",
        "voteCount": 1,
        "content": "BCF is correct anwers. \nPermission set + group created in the IdP, and map attributes is key"
      },
      {
        "date": "2024-05-24T03:07:00.000Z",
        "voteCount": 1,
        "content": "While I have no great insights or expertise in this area, I do know how to read (RTFM) and quasi-solve the puzzle in my head.  This reference URL (pdf) seems  to touch all the steps listed in \"B\", \"C\", \"F\" and showed some extra steps not listed.  Search and see for yourself.\nhttps://d1.awsstatic.com/events/aws-reinforce-2022/IAM309_Designing-a-well-architected-identity-and-access-management-solution.pdf"
      },
      {
        "date": "2024-05-24T03:17:00.000Z",
        "voteCount": 2,
        "content": "Also, I might add, rather than just memorize the most votes answer to the question, I'd suggest actually going out to do some research and taking some long term notes you can reference later.  That may take more time, but you also be more competent at work, and maybe keep your job longer.  I love the fact that exam topics gives a forum to discuss and research complex questions and share findings.  It's pretty lame If you come here to just memorize answers long enough to pass an exam."
      },
      {
        "date": "2024-02-22T13:50:00.000Z",
        "voteCount": 1,
        "content": "Permission sets are stored in IAM Identity Center. So you know all answers that mention about permission sets and IAM Identity Center are likely correct"
      },
      {
        "date": "2024-01-27T20:41:00.000Z",
        "voteCount": 1,
        "content": "B, C, E seem more accurate:\nB- need to attach the policy so that it can be usable. A is not true because IAM policies is not the same as in IAM Identity Center\nC- not D because cannot assign group to IAM policies. IAM policies is attached to groups. also, need permission sets in Identity Center\nE- attributes is basically tagging."
      },
      {
        "date": "2023-11-29T04:47:00.000Z",
        "voteCount": 1,
        "content": "correct answer seen as A-B-C. but 11 people sure the correct answer is B-C-F in discussion.\nWhat is the answer? \nCan the system show the correct answer as wrong or are people mistaken?"
      },
      {
        "date": "2024-01-05T10:23:00.000Z",
        "voteCount": 3,
        "content": "The examTopics answers in most cases are wrong, please read discussions, and references that users provide"
      },
      {
        "date": "2024-07-01T02:10:00.000Z",
        "voteCount": 2,
        "content": "Then why do people pay the fee for access, I dont understand. If it is from a discussion the people have to understand the answer (that too not very sure), why do they charge so much for the contributor access?!"
      },
      {
        "date": "2023-07-04T07:40:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/singlesignon/latest/userguide/provision-automatically.html"
      },
      {
        "date": "2023-07-04T07:39:00.000Z",
        "voteCount": 5,
        "content": "Example if I use IdP as my group, and I add users to the group, then my users will be onboarded via the SCIM method. \nIAM roles does not apply to Control Tower landing zone. Hence B and C is secured (only permission sets for AWS SSO)\nDoes not make sense granting RBAC via tags\u2026"
      },
      {
        "date": "2023-08-06T02:15:00.000Z",
        "voteCount": 1,
        "content": "An inline policy is a policy created for a single IAM identity (a user, group, or role). Inline policies maintain a strict one-to-one relationship between a policy and an identity\nA permission set is a template that you create and maintain that defines a collection of one or more IAM policies."
      },
      {
        "date": "2023-08-06T02:18:00.000Z",
        "voteCount": 1,
        "content": "IAM Identity Center helps you securely create, or connect, your workforce identities and manage their access centrally across AWS accounts and applications\nAttribute mappings are used to map attribute types that exist in IAM Identity Center with like attributes in an AWS Managed Microsoft AD directory. IAM Identity Center retrieves user attributes from your Microsoft AD directory and maps them to IAM Identity Center user attributes. These IAM Identity Center user attribute mappings are also used for generating SAML assertions for your cloud applications."
      },
      {
        "date": "2023-06-08T04:25:00.000Z",
        "voteCount": 4,
        "content": "BCF\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/abac.html"
      },
      {
        "date": "2023-05-30T23:27:00.000Z",
        "voteCount": 1,
        "content": "I beleive BCF"
      },
      {
        "date": "2023-05-07T23:19:00.000Z",
        "voteCount": 2,
        "content": "BCF makes more sense here."
      },
      {
        "date": "2023-04-15T11:05:00.000Z",
        "voteCount": 2,
        "content": "ill go with B,C,F"
      },
      {
        "date": "2023-04-07T06:52:00.000Z",
        "voteCount": 2,
        "content": "agree, BCF - permissions sets + IDP attributes mapping + groups"
      },
      {
        "date": "2023-04-04T18:42:00.000Z",
        "voteCount": 1,
        "content": "A, C, E"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/amazon/view/105508-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "An ecommerce company is receiving reports that its order history page is experiencing delays in reflecting the processing status of orders. The order processing system consists of an AWS Lambda function that uses reserved concurrency. The Lambda function processes order messages from an Amazon Simple Queue Service (Amazon SQS) queue and inserts processed orders into an Amazon DynamoDB table. The DynamoDB table has auto scaling enabled for read and write capacity.<br>Which actions should a DevOps engineer take to resolve this delay? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the ApproximateAgeOfOldestMessage metric for the SQS queue. Increase the Lambda function concurrency limit.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the ApproximateAgeOfOldestMessage metnc for the SQS queue Configure a redrive policy on the SQS queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the NumberOfMessagesSent metric for the SQS queue. Increase the SQS queue visibility timeout.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the WriteThrottleEvents metric for the DynamoDB table. Increase the maximum write capacity units (WCUs) for the table's scaling policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the Throttles metric for the Lambda function. Increase the Lambda function timeout."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-08T04:29:00.000Z",
        "voteCount": 10,
        "content": "AD look fine."
      },
      {
        "date": "2024-05-11T14:23:00.000Z",
        "voteCount": 2,
        "content": "Everyone who has commented AD has not provided any reasoning. Bunch of sheeps in the comments."
      },
      {
        "date": "2024-06-13T04:01:00.000Z",
        "voteCount": 2,
        "content": "D: This action is important because if the WriteThrottleEvents metric is high, it indicates that DynamoDB is throttling writes due to insufficient write capacity. By increasing the maximum WCUs, you ensure that the table can handle the increased write throughput required by the Lambda function, thus reducing delays in order processing.\n even though DynamoDB has auto-scaling enabled, it\u2019s still important to monitor the WriteThrottleEvents metric. Auto-scaling adjusts capacity based on the workload, but it may not always keep up with sudden spikes in demand or be configured optimally for this specific use case. Ensuring that the maximum write capacity units (WCUs) are set appropriately can help prevent throttling during peak times."
      },
      {
        "date": "2024-01-27T20:54:00.000Z",
        "voteCount": 3,
        "content": "A and D is correct:\nA: Check ApproximateAgeOfOldestMessage and increase concurrency accordingly\nD: Check throttleevent (the number of rejected requests) and increase max write accordingly."
      },
      {
        "date": "2023-11-18T23:35:00.000Z",
        "voteCount": 3,
        "content": "Answer should be A &amp; C. D is wrong as DynamoDB has autoscaling enabled so Writethrottle should not be the case."
      },
      {
        "date": "2023-12-09T23:29:00.000Z",
        "voteCount": 3,
        "content": "Eventhough the statement say\n\n\n\"The DynamoDB table has auto scaling enabled for read and write capacity.\"\n\nA and D still are a good answer. \n\nOption C looks at the NumberOfMessagesSent metric and suggests increasing the SQS queue visibility timeout. This action is more relevant when messages are not being processed before the visibility timeout expires, but it does not seem to be the primary issue in this scenario."
      },
      {
        "date": "2023-09-10T12:43:00.000Z",
        "voteCount": 4,
        "content": "A: Check the ApproximateAgeOfOldestMessage metric for the SQS queue. If this age is high, increasing the Lambda function's concurrency limit would help speed up processing.\n\nD: Check the WriteThrottleEvents metric for the DynamoDB table. If write operations are being throttled, increasing the maximum WCUs for the table\u2019s scaling policy could help."
      },
      {
        "date": "2023-05-07T23:20:00.000Z",
        "voteCount": 4,
        "content": "AD are accurate for this scenario."
      },
      {
        "date": "2023-04-14T14:11:00.000Z",
        "voteCount": 1,
        "content": "a and d are correct"
      },
      {
        "date": "2023-04-07T06:59:00.000Z",
        "voteCount": 2,
        "content": "upscale both"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/amazon/view/105512-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has a single AWS account that runs hundreds of Amazon EC2 instances in a single AWS Region. New EC2 instances are launched and terminated each hour in the account. The account also includes existing EC2 instances that have been running for longer than a week.<br>The company's security policy requires all running EC2 instances to use an EC2 instance profile. If an EC2 instance does not have an instance profile attached, the EC2 instance must use a default instance profile that has no IAM permissions assigned.<br>A DevOps engineer reviews the account and discovers EC2 instances that are running without an instance profile. During the review, the DevOps engineer also observes that new EC2 instances are being launched without an instance profile.<br>Which solution will ensure that an instance profile is attached to all existing and future EC2 instances in the Region?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge rule that reacts to EC2 RunInstances API calls. Configure the rule to invoke an AWS Lambda function to attach the default instance profile to the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the ec2-instance-profile-attached AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge rule that reacts to EC2 StartInstances API calls. Configure the rule to invoke an AWS Systems Manager Automation runbook to attach the default instance profile to the EC2 instances",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the iam-role-managed-policy-check AWS Config managed rule with a trigger type of configuration changes. Configure an automatic remediation action that invokes an AWS Lambda function to attach the default instance profile to the EC2 instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-20T06:05:00.000Z",
        "voteCount": 8,
        "content": "WS Config, specifically utilizing the \"ec2-instance-profile-attached\" managed rule with the configuration change trigger type. This rule helps monitor the attachment of instance profiles to EC2 instances. An automatic remediation action can be configured within AWS Config to respond when instances are found without an instance profile attached. The remediation action would execute an AWS Systems Manager Automation runbook to attach the default instance profile to those instances."
      },
      {
        "date": "2024-05-24T15:14:00.000Z",
        "voteCount": 1,
        "content": "I concur the best answer seems to be \"B\".  However, I have not been able to trigger exactly what kind of \"configuration change\" triggers the config rule (e.g. \"starting\" or \"running\" an instance isn't a configuration change, but a state change.  The real world answer (IMHO) would be to just kick off the AWS Config rule manually or on a schedule.  I'd also take steps to ensure that all EC2 launch templates specify an instance profile so I'm not running around trying to fix things that shouldn't have been left broken from the start."
      },
      {
        "date": "2024-04-09T08:51:00.000Z",
        "voteCount": 1,
        "content": "The rules AWS Config"
      },
      {
        "date": "2024-01-27T21:05:00.000Z",
        "voteCount": 4,
        "content": "B is correct: AWS config + runbook is the right way for remediation"
      },
      {
        "date": "2024-02-03T20:14:00.000Z",
        "voteCount": 5,
        "content": "B is correct: AWS Config run in combination with SSM Automation run book is the recommended way\nA: this option only remediate new instances\nC: this option only remedidate instances that have been stopped. \nD: automatic remediation action should invoke Automation run book, not lambda"
      },
      {
        "date": "2023-12-13T07:50:00.000Z",
        "voteCount": 3,
        "content": "B\nConfig: ec2-instance-profile-attached\nSSM Automation: AttachedIAMtoinstances"
      },
      {
        "date": "2023-06-08T05:02:00.000Z",
        "voteCount": 2,
        "content": "B is correct.\nhttps://docs.aws.amazon.com/config/latest/developerguide/ec2-instance-profile-attached.html"
      },
      {
        "date": "2023-05-07T23:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-04-14T14:13:00.000Z",
        "voteCount": 1,
        "content": "correct answer is B"
      },
      {
        "date": "2023-04-07T07:11:00.000Z",
        "voteCount": 1,
        "content": "B , no brainer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/amazon/view/105513-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is building a continuous deployment pipeline for a serverless application that uses AWS Lambda functions. The company wants to reduce the customer impact of an unsuccessful deployment. The company also wants to monitor for issues.<br>Which deploy stage configuration will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Serverless Application Model (AWS SAM) template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent15Minutes Deployment Preference Type. Use Amazon CloudWatch alarms to monitor the health of the functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to publish a new stack update, and include Amazon CloudWatch alarms on all resources. Set up an AWS CodePipeline approval action for a developer to verify and approve the AWS CloudFormation change set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to publish a new version on every stack update, and include Amazon CloudWatch alarms on all resources. Use the RoutingConfig property of the AWS::Lambda::Alias resource to update the traffic routing during the stack update.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeBuild to add sample event payloads for testing to the Lambda functions. Publish a new version of the functions, and include Amazon CloudWatch alarms. Update the production alias to point to the new version. Configure rollbacks to occur when an alarm is in the ALARM state."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-17T11:31:00.000Z",
        "voteCount": 15,
        "content": "Certification TIP: 99% of questions regarding lambda and cloudformation the answer is the one that involves SAM"
      },
      {
        "date": "2023-12-09T23:38:00.000Z",
        "voteCount": 2,
        "content": "i couldnt agree more with this"
      },
      {
        "date": "2023-09-10T12:53:00.000Z",
        "voteCount": 6,
        "content": "A\n\nReducing Customer Impact: AWS CodeDeploy with Canary deployments (Canary10Percent15Minutes) will incrementally roll out the new version. Initially, 10% of the traffic will be directed to the new version, and if everything goes well, the rest of the traffic will be shifted over the span of 15 minutes. This cautious rollout minimizes the risk and impact on customers.\n\nMonitoring: Amazon CloudWatch alarms can be configured to track function errors, latency, and other important metrics. If anything goes awry, you can act promptly."
      },
      {
        "date": "2024-07-25T00:11:00.000Z",
        "voteCount": 1,
        "content": "A\nKeywords: Serverless Application related with AWS SAM"
      },
      {
        "date": "2024-02-26T10:45:00.000Z",
        "voteCount": 1,
        "content": "Canary10Percent15Minutes refers to a specific type of deployment strategy used in the context of serverless applications, particularly with tools like AWS SAM (Serverless Application Model)."
      },
      {
        "date": "2024-02-03T20:18:00.000Z",
        "voteCount": 4,
        "content": "A is correct: &lt;a continuous deployment pipeline for a serverless application&gt; means AWS SAM. \nB, C and D: no mention of SAM"
      },
      {
        "date": "2024-01-27T21:08:00.000Z",
        "voteCount": 1,
        "content": "A: use serverless code deployment is the right way"
      },
      {
        "date": "2023-12-19T21:17:00.000Z",
        "voteCount": 1,
        "content": "A - SAM  for Lambda deployment\nReduce Custome Impact - Canary got it covered"
      },
      {
        "date": "2023-09-17T19:50:00.000Z",
        "voteCount": 1,
        "content": "A CodeDepoly is for canary deployment , cloudwatch alarm for monitoring, if aram is raised then codedeploy automatically  rolls back\nD-  Using Codebuild for controlled deployment is not good. Codebuild is for build and testing"
      },
      {
        "date": "2023-09-05T19:46:00.000Z",
        "voteCount": 2,
        "content": "A is the answer: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html"
      },
      {
        "date": "2023-07-19T03:28:00.000Z",
        "voteCount": 3,
        "content": "Selected Answer: A Canary Deployment"
      },
      {
        "date": "2023-06-11T23:40:00.000Z",
        "voteCount": 3,
        "content": "My Answer is D which can help reduce the customer impact of an unsuccessful deployment, also got monitor and rollbacks to occur when an alarm is in the ALARM state.\n\nA, this option does not provide a rollback plan in case of failures, which could further increase the customer impact of a failed deployment.\nThis strategy can help detect any issues early on, it does not guarantee that the impact on customers will be reduced since some customers might still be affected by the issues.\n\nSomeone can tell me more?"
      },
      {
        "date": "2023-09-09T21:52:00.000Z",
        "voteCount": 1,
        "content": "Alarms: These are CloudWatch alarms that are triggered by any errors raised by the deployment. When encountered, they automatically roll back your deployment. For example, if the updated code you're deploying causes errors within the application. Another example is if any AWS Lambda or custom CloudWatch metrics that you specified have breached the alarm threshold.\n\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html"
      },
      {
        "date": "2023-06-26T03:12:00.000Z",
        "voteCount": 2,
        "content": "codebuild is not use for continous deployment"
      },
      {
        "date": "2023-06-12T00:18:00.000Z",
        "voteCount": 6,
        "content": "checked, Answer D does not provide a gradual deployment strategy that reduces the customer impact of a new deployment, which was one of the requirements given in the question.\n\nSo the final answer should be A"
      },
      {
        "date": "2023-06-08T05:07:00.000Z",
        "voteCount": 1,
        "content": "A looks fine."
      },
      {
        "date": "2023-05-01T18:03:00.000Z",
        "voteCount": 2,
        "content": "A. Use an AWS Serverless Application Model (AWS SAM) template to define the serverless application. Use AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent15Minutes Deployment Preference Type. Use Amazon CloudWatch alarms to monitor the health of the functions would be the best deploy stage configuration for meeting the requirements of reducing customer impact of an unsuccessful deployment and monitoring for issues.\n\nOption A uses AWS CodeDeploy to deploy the Lambda functions with the Canary10Percent15Minutes Deployment Preference Type, which gradually deploys the new version of the function to a small subset of users before deploying it to the entire fleet. This approach reduces customer impact of an unsuccessful deployment.\n\nAdditionally, Amazon CloudWatch alarms are used to monitor the health of the functions, which can provide real-time feedback on any issues that arise. This meets the requirement of monitoring for issues."
      },
      {
        "date": "2023-04-14T14:15:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2023-04-07T07:16:00.000Z",
        "voteCount": 1,
        "content": "A looks realistic"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/amazon/view/105514-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "To run an application, a DevOps engineer launches an Amazon EC2 instance with public IP addresses in a public subnet. A user data script obtains the application artifacts and installs them on the instances upon launch. A change to the security classification of the application now requires the instances to run with no access to the internet. While the instances launch successfully and show as healthy, the application does not seem to be installed.<br>Which of the following should successfully install the application while complying with the new rule?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the instances in a public subnet with Elastic IP addresses attached. Once the application is installed and running, run a script to disassociate the Elastic IP addresses afterwards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a NAT gateway. Deploy the EC2 instances to a private subnet. Update the private subnet's route table to use the NAT gateway as the default route.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish the application artifacts to an Amazon S3 bucket and create a VPC endpoint for S3. Assign an IAM instance profile to the EC2 instances so they can read the application artifacts from the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a security group for the application instances and allow only outbound traffic to the artifact repository. Remove the security group rule once the install is complete."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-19T21:22:00.000Z",
        "voteCount": 7,
        "content": "C - in the answer \nThough we can use both B and C , since we only want to download to package at the time of initialization . So there is no need to have continuous access to internet . Therefore, it is cheap and optimal to use S3 ."
      },
      {
        "date": "2024-08-06T09:59:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer.\nno access to the internet but connect to aws services =&gt; private endpoint"
      },
      {
        "date": "2024-01-27T21:20:00.000Z",
        "voteCount": 3,
        "content": "C is correct: all other options utilize internet connections"
      },
      {
        "date": "2023-12-09T23:47:00.000Z",
        "voteCount": 3,
        "content": "C is the correct one.\n\nall other option will allow internet access which is not compliance with the reqs"
      },
      {
        "date": "2023-11-17T11:33:00.000Z",
        "voteCount": 3,
        "content": "C: Can't be B because with the NAT the EC2 still has internet access"
      },
      {
        "date": "2023-11-11T07:27:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer.\n\nA B D are not correct.\nKeywords:\n- requires the instances to run with no access to the internet"
      },
      {
        "date": "2023-11-13T16:07:00.000Z",
        "voteCount": 1,
        "content": "is the dump still valid?"
      },
      {
        "date": "2023-11-05T00:55:00.000Z",
        "voteCount": 2,
        "content": "B is incorrect, the new policy is \"no access to the internet\""
      },
      {
        "date": "2023-10-28T06:33:00.000Z",
        "voteCount": 1,
        "content": "C is the answer. B would enable internet access from the instance."
      },
      {
        "date": "2023-09-29T07:12:00.000Z",
        "voteCount": 1,
        "content": "C is correct and B, which is specifically for NAT. in question they have asked that no internet access from the instance, so If we enable NAT then from outside no one can access the instance but internet will be accessible on the instance using NAT."
      },
      {
        "date": "2023-09-24T10:50:00.000Z",
        "voteCount": 1,
        "content": "C is correct\nB: \"instances to run with no access to the internet.\" so you can not use NAT"
      },
      {
        "date": "2023-09-05T19:58:00.000Z",
        "voteCount": 2,
        "content": "C is the answer, you can use artifacts in s3 with vpc endpoints. With a gateway endpoint, you can access Amazon S3 from your VPC, without requiring an internet gateway or NAT device for your VPC, and with no additional cost.\nhttps://repost.aws/knowledge-center/ec2-systems-manager-vpc-endpoints\nhttps://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html"
      },
      {
        "date": "2023-08-27T10:10:00.000Z",
        "voteCount": 1,
        "content": "C is correct as solution required no internet access"
      },
      {
        "date": "2023-08-10T19:24:00.000Z",
        "voteCount": 1,
        "content": "Correct C."
      },
      {
        "date": "2023-06-08T05:12:00.000Z",
        "voteCount": 1,
        "content": "C is the answer. B gives the instances access to the Internet."
      },
      {
        "date": "2023-05-30T12:41:00.000Z",
        "voteCount": 1,
        "content": "Def C, all others include access to the internet"
      },
      {
        "date": "2023-05-25T21:56:00.000Z",
        "voteCount": 1,
        "content": "This is supposed to be a Choose two answer. BC"
      },
      {
        "date": "2023-05-22T23:52:00.000Z",
        "voteCount": 1,
        "content": "NAT GW for me"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/amazon/view/105519-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A development team is using AWS CodeCommit to version control application code and AWS CodePipeline to orchestrate software deployments. The team has decided to use a remote main branch as the trigger for the pipeline to integrate code changes. A developer has pushed code changes to the CodeCommit repository, but noticed that the pipeline had no reaction, even after 10 minutes.<br>Which of the following actions should be taken to troubleshoot this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that an Amazon EventBridge rule has been created for the main branch to trigger the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that the CodePipeline service role has permission to access the CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck that the developer\u2019s IAM role has permission to push to the CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck to see if the pipeline failed to start because of CodeCommit errors in Amazon CloudWatch Logs."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 53,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 34,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-16T21:12:00.000Z",
        "voteCount": 17,
        "content": "A: EventBridge rules are not a requirement for CodePipeline to trigger from a CodeCommit repository. CodePipeline directly integrates with CodeCommit without needing EventBridge.\n\nB:  is a likely cause. The CodePipeline service role needs permissions to access the CodeCommit repository in order to start the pipeline execution when new code is pushed.\n\nC:If the developer was able to push code changes to the CodeCommit repository, then their IAM role permissions with respect to CodeCommit are likely fine. This isn't the issue.\n\nD:If the pipeline didn't start, CloudWatch Logs could give insights. However, these logs will only exist if the pipeline actually attempted to start but failed. If the pipeline never started, checking logs won't help.\n\nGiven these options, Option B: is the correct answer."
      },
      {
        "date": "2024-01-15T06:04:00.000Z",
        "voteCount": 8,
        "content": "B would throw out \"Permission denied\" error immediately,  rather than no reaction for 10 minutes."
      },
      {
        "date": "2024-07-25T00:29:00.000Z",
        "voteCount": 3,
        "content": "A - EventBridge rule is one of the recommended ways to configure CodePipeline to automatically trigger based on changes in a CodeCommit repository\n\nB - if \"Permission denied\", the error message should prompt immediately, rather than no reaction for 10 minutes for the pipeline. Mean the pipeline not even start\n\nReference:\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html#change-detection-methods\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-simple-codecommit.html"
      },
      {
        "date": "2024-07-15T17:39:00.000Z",
        "voteCount": 2,
        "content": "EventBridge rule is one of the recommended ways to configure CodePipeline to automatically trigger based on changes in a CodeCommit repository"
      },
      {
        "date": "2024-06-28T02:36:00.000Z",
        "voteCount": 1,
        "content": "For CodePipeline to be triggered by changes in a CodeCommit repository, an EventBridge rule (formerly CloudWatch Events rule) needs to be set up. This rule listens for specific events (like commits to the main branch) and triggers the pipeline accordingly."
      },
      {
        "date": "2024-06-25T17:29:00.000Z",
        "voteCount": 1,
        "content": "It's B."
      },
      {
        "date": "2024-08-16T06:39:00.000Z",
        "voteCount": 1,
        "content": "The answer is not B because if the CodePipeline service linked role didn't have permissions to access CodeCommit, you will get a \"Permissions denied\" error immediately but the question said you didn't get any reaction in 10 minutes so the only possible scenario we would be dealing with here is not having an EventBridge rule that triggers the pipeline. When you use the console to create or edit a pipeline, the change detection resources are created for you. If you use the AWS CLI to create the pipeline, you must create the additional resources yourself. \n\nReference: https://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create.html"
      },
      {
        "date": "2024-06-16T02:02:00.000Z",
        "voteCount": 1,
        "content": "the answer is A because if codepipeline has no access to codecommit pipeline is triggered and source stage fails with:\n```\nThe service role or action role doesn\u2019t have the permissions required to access the AWS CodeCommit repository named random-repo. Update the IAM role permissions, and then try again\n```"
      },
      {
        "date": "2024-05-30T15:55:00.000Z",
        "voteCount": 1,
        "content": "B is right."
      },
      {
        "date": "2024-05-12T23:52:00.000Z",
        "voteCount": 1,
        "content": "Just voting to fix the results, because clearly its B, as explained by top 2 comments here."
      },
      {
        "date": "2024-05-19T17:31:00.000Z",
        "voteCount": 2,
        "content": "B wrong."
      },
      {
        "date": "2024-05-10T14:38:00.000Z",
        "voteCount": 1,
        "content": "The first step in troubleshooting this issue should be to check that the CodePipeline service role has the required permissions to access the CodeCommit repository. If the permissions are correct, then you can proceed with other troubleshooting steps, such as checking the CloudWatch Logs for any errors or failures."
      },
      {
        "date": "2024-04-11T07:14:00.000Z",
        "voteCount": 3,
        "content": "Not sure why most people here are even considering A. CodePipeline does not use Amazon EventBridge to trigger pipeline executions based on changes in CodeCommit repositories. Instead, it directly integrates with CodeCommit and monitors repository changes internally."
      },
      {
        "date": "2024-05-02T04:58:00.000Z",
        "voteCount": 2,
        "content": "You are wrong, correct answer is A"
      },
      {
        "date": "2024-05-02T04:59:00.000Z",
        "voteCount": 2,
        "content": "When you create a pipeline from CodePipeline during the step-by-step it creates a CloudWatch Event rule for a given branch and repo\nlike this:\n{\n\"source\": [\n\"aws.codecommit\"\n],\n\"detail-type\": [\n\"CodeCommit Repository State Change\"\n],\n\"resources\": [\n\"arn:aws:codecommit:us-east-1:xxxxx:repo-name\"\n],\n\"detail\": {\n\"event\": [\n\"referenceCreated\",\n\"referenceUpdated\"\n],\n\"referenceType\": [\n\"branch\"\n],\n\"referenceName\": [\n\"master\"\n]\n}"
      },
      {
        "date": "2024-03-22T04:59:00.000Z",
        "voteCount": 3,
        "content": "\"After you select the repository name and branch, a message displays the Amazon CloudWatch Events rule to be created for this pipeline.\"\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-simple-codecommit.html"
      },
      {
        "date": "2024-03-17T22:47:00.000Z",
        "voteCount": 3,
        "content": "I highly believe it is A.\nEven though CodePipeline directly integrates with CodeCommit, this integration automatically creates a EventBridge rule for you if it is created through the console. \nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-trigger-source-repo-changes-console.html\nSince we know that there is no reaction from the pipeline, it would mean that it wasn't triggered at all. \n\nB is about permission which would have thrown an error in the console at that stage, but to even start the first stage, it needs to be trigger first which for the case here.\nC shouldn't be the answer as the question already said that it was pushed into the repository."
      },
      {
        "date": "2024-03-16T21:13:00.000Z",
        "voteCount": 1,
        "content": "BBBBBBBBBBBBB"
      },
      {
        "date": "2024-02-29T09:37:00.000Z",
        "voteCount": 3,
        "content": "Amazon EventBridge (recommended). This is the default for pipelines with an CodeCommit source created or edited in the console.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-about-starting.html#change-detection-methods"
      },
      {
        "date": "2024-02-28T05:12:00.000Z",
        "voteCount": 2,
        "content": "A is most accurate IMO"
      },
      {
        "date": "2024-02-27T15:04:00.000Z",
        "voteCount": 2,
        "content": "I think the answer is B\nB. Check that the CodePipeline service role has permission to access the CodeCommit repository.\nBy default CodePipeline polls repository for changes even if there is no EventBridge configured. But if the CodePipeline Service Role has no permissions to access the CodeCommit repository it will throw error and CodePipeline will have no reaction."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/amazon/view/105520-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's developers use Amazon EC2 instances as remote workstations. The company is concerned that users can create or modify EC2 security groups to allow unrestricted inbound access.<br>A DevOps engineer needs to develop a solution to detect when users create unrestricted security group rules. The solution must detect changes to security group rules in near real time, remove unrestricted rules, and send email notifications to the security team. The DevOps engineer has created an AWS Lambda function that checks for security group ID from input, removes rules that grant unrestricted access, and sends notifications through Amazon Simple Notification Service (Amazon SNS).<br>What should the DevOps engineer do next to meet the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Lambda function to be invoked by the SNS topic. Create an AWS CloudTrail subscription for the SNS topic. Configure a subscription filter for security group modification events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge scheduled rule to invoke the Lambda function. Define a schedule pattern that runs the Lambda function every hour.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge event rule that has the default event bus as the source. Define the rule\u2019s event pattern to match EC2 security group creation and modification events. Configure the rule to invoke the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge custom event bus that subscribes to events from all AWS services. Configure the Lambda function to be invoked by the custom event bus."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-28T07:41:00.000Z",
        "voteCount": 6,
        "content": "C is correct:\nA: lambda should be invoked by Eventbridge\nB: we need to act when there is events, not schedully\nD: subscribing to events from ALL AWS services incurs a huge cost"
      },
      {
        "date": "2024-05-11T07:39:00.000Z",
        "voteCount": 1,
        "content": "C of course.\nBut A seems working, and does Aws Config work in this situation?"
      },
      {
        "date": "2024-05-10T14:49:00.000Z",
        "voteCount": 2,
        "content": "By creating an EventBridge event rule with the appropriate event pattern and configuring it to invoke the Lambda function, the DevOps engineer can effectively detect security group rule changes in near real-time, remove unrestricted rules, and send notifications to the security team. This solution leverages the event-driven architecture of EventBridge and the serverless execution of AWS Lambda, providing a scalable and efficient way to meet the company's security requirements."
      },
      {
        "date": "2023-12-24T15:08:00.000Z",
        "voteCount": 2,
        "content": "selected answer:C"
      },
      {
        "date": "2023-06-08T10:29:00.000Z",
        "voteCount": 4,
        "content": "C the default bus includes events from AWS services.\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-bus.html"
      },
      {
        "date": "2023-05-30T08:37:00.000Z",
        "voteCount": 4,
        "content": "Wrong answers:\nA. SNS is used here to send a notification post-facto\nB. The question requires \"near real time\", an hour is not \"near real time\"\nD. AWS events come on the default event bus, you do not need a custom event bus"
      },
      {
        "date": "2023-08-06T04:13:00.000Z",
        "voteCount": 4,
        "content": "The default event bus in each account receives events from AWS services.\n\nA custom event bus sends events to or receives events from a different account.\n\nA custom event bus sends events to or receives events from a different Region to aggregate events in a single location.\n\nA partner event bus receives events from a SaaS partner."
      },
      {
        "date": "2023-05-01T18:18:00.000Z",
        "voteCount": 4,
        "content": "To meet the requirements, the DevOps engineer should create an Amazon EventBridge event rule that has the default event bus as the source. The rule's event pattern should match EC2 security group creation and modification events, and it should be configured to invoke the Lambda function. This solution will allow for near real-time detection of security group rule changes and will trigger the Lambda function to remove any unrestricted rules and send email notifications to the security team."
      },
      {
        "date": "2023-04-14T14:23:00.000Z",
        "voteCount": 2,
        "content": "C is the answer"
      },
      {
        "date": "2023-04-13T23:37:00.000Z",
        "voteCount": 4,
        "content": "C. Create an Amazon EventBridge event rule that has the default event bus as the source. Define the rule\u2019s event pattern to match EC2 security group creation and modification events. Configure the rule to invoke the Lambda function.\n\nThe solution requires near real-time detection of changes to security group rules and immediate action to remove unrestricted rules and send email notifications to the security team. The AWS Lambda function created by the DevOps engineer can perform these actions, but it needs to be invoked whenever a security group rule is modified.\n\nAmazon EventBridge is a serverless event bus service that can receive and process events from various AWS services, including Amazon EC2 and Amazon SNS. An EventBridge event rule with the default event bus as the source can be created to match EC2 security group creation and modification events. This rule can then be configured to invoke the Lambda function, which can remove unrestricted rules and send email notifications to the security team."
      },
      {
        "date": "2023-04-07T07:48:00.000Z",
        "voteCount": 3,
        "content": "https://repost.aws/knowledge-center/monitor-security-group-changes-ec2"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/amazon/view/105239-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is creating an AWS CloudFormation template to deploy a web service. The web service will run on Amazon EC2 instances in a private subnet behind an Application Load Balancer (ALB). The DevOps engineer must ensure that the service can accept requests from clients that have IPv6 addresses.<br>What should the DevOps engineer do with the CloudFormation template so that IPv6 clients can access the web service?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an IPv6 CIDR block to the VPC and the private subnet for the EC2 instances. Create route table entries for the IPv6 network, use EC2 instance types that support IPv6, and assign IPv6 addresses to each EC2 instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign each EC2 instance an IPv6 Elastic IP address. Create a target group, and add the EC2 instances as targets. Create a listener on port 443 of the ALB, and associate the target group with the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the ALB with a Network Load Balancer (NLB). Add an IPv6 CIDR block to the VPC and subnets for the NLB, and assign the NLB an IPv6 Elastic IP address.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443. and specify the dualstack IP address type on the ALB. Create a target group, and add the EC2 instances as targets. Associate the target group with the ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-22T12:36:00.000Z",
        "voteCount": 6,
        "content": "D\n\"To support IPv6, configure your Application Load Balancers or Network Load Balancers with the \u201cdualstack\u201d IP address type. This means that clients can communicate with the load balancers using both IPv4 and IPv6 addresses. In a dual-stack IP address type, the DNS name of the load balancer provides both IPv4 and IPv6 addresses, and creates A and AAAA records respectively. \"\n\nhttps://docs.aws.amazon.com/whitepapers/latest/ipv6-on-aws/scaling-the-dual-stack-network-design-in-aws.html"
      },
      {
        "date": "2024-05-11T08:09:00.000Z",
        "voteCount": 2,
        "content": "But why is port 443 necessary?"
      },
      {
        "date": "2024-08-16T06:56:00.000Z",
        "voteCount": 1,
        "content": "Port 443 is the TCP port for HTTPS which a secured or encrypted version of HTTP. To enable the ALB handle HTTPS traffic having a listener on port 443 is necessary."
      },
      {
        "date": "2024-05-10T14:59:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D. Add an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443. and specify the dualstack IP address type on the ALB. Create a target group, and add the EC2 instances as targets. Associate the target group with the ALB."
      },
      {
        "date": "2024-02-28T09:29:00.000Z",
        "voteCount": 2,
        "content": "Why is the need for port 443 reference on D and D has no reference to private subnet. That makes me think the answer is A, but A has no reference to ALB."
      },
      {
        "date": "2024-01-28T07:45:00.000Z",
        "voteCount": 4,
        "content": "D is correct: use dual stack + listener on 443\nA: no mention of the ALB\nB: no mention of adding dualstack IP to ALB\nC: cannot replace the ALB"
      },
      {
        "date": "2024-01-06T22:34:00.000Z",
        "voteCount": 2,
        "content": "definitely D"
      },
      {
        "date": "2023-12-24T15:25:00.000Z",
        "voteCount": 2,
        "content": "keyword is \"Dualstack\""
      },
      {
        "date": "2023-12-19T21:50:00.000Z",
        "voteCount": 3,
        "content": "D is correct , To enable ALB to deal with Ipv6 requests , vpc should enable for dual stack, by configuring a ipv6 cidr , and ALB subnet should also adhere to the same , by having ipv4 and 6 cidr \nB is incorrect , we can assisg any public ip to instance , since it is in private subnet ."
      },
      {
        "date": "2023-06-08T10:35:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer. C is wrong, we don't need Elastic IPs for a private app."
      },
      {
        "date": "2023-05-30T23:31:00.000Z",
        "voteCount": 1,
        "content": "D i answer"
      },
      {
        "date": "2023-05-30T08:41:00.000Z",
        "voteCount": 4,
        "content": "I would say it is D. The backend instances serving the data can be IPv4. The ALB shoult serve IPv6 to the public (which is what is required by the question). So the only place in the VPC that needs IPv6 are the ALB subnets."
      },
      {
        "date": "2023-05-08T02:45:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer in this case"
      },
      {
        "date": "2023-05-01T18:21:00.000Z",
        "voteCount": 4,
        "content": "To allow IPv6 clients to access the web service running on Amazon EC2 instances in a private subnet behind an Application Load Balancer (ALB) using an AWS CloudFormation template, the DevOps engineer should choose option D:\n\nAdd an IPv6 CIDR block to the VPC and subnets for the ALB. Create a listener on port 443, and specify the dualstack IP address type on the ALB. Create a target group, add the EC2 instances as targets, and associate the target group with the ALB.\n\nThe dualstack IP address type enables the ALB to support both IPv4 and IPv6 traffic. By adding an IPv6 CIDR block to the VPC and subnets for the ALB, the VPC automatically assigns an IPv6 address to the ALB."
      },
      {
        "date": "2023-04-26T15:35:00.000Z",
        "voteCount": 4,
        "content": "D\nhttps://repost.aws/ja/knowledge-center/elb-configure-with-ipv6"
      },
      {
        "date": "2023-04-14T14:24:00.000Z",
        "voteCount": 1,
        "content": "answer is D"
      },
      {
        "date": "2023-04-07T07:52:00.000Z",
        "voteCount": 2,
        "content": "D right"
      },
      {
        "date": "2023-04-04T20:09:00.000Z",
        "voteCount": 1,
        "content": "I think D"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/amazon/view/105240-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Organizations and AWS Control Tower to manage all the company's AWS accounts. The company uses the Enterprise Support plan.<br>A DevOps engineer is using Account Factory for Terraform (AFT) to provision new accounts. When new accounts are provisioned, the DevOps engineer notices that the support plan for the new accounts is set to the Basic Support plan. The DevOps engineer needs to implement a solution to provision the new accounts with the Enterprise Support plan.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Config conformance pack to deploy the account-part-of-organizations AWS Config rule and to automatically remediate any noncompliant accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to create a ticket for AWS Support to add the account to the Enterprise Support plan. Grant the Lambda function the support:ResolveCase permission.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an additional value to the control_tower_parameters input to set the AWSEnterpriseSupport parameter as the organization's management account number.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. Redeploy AFT and apply the changes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-19T22:08:00.000Z",
        "voteCount": 10,
        "content": "D check docs\nhttps://docs.aws.amazon.com/controltower/latest/userguide/aft-feature-options.html#enterprise-support-option"
      },
      {
        "date": "2023-04-13T23:39:00.000Z",
        "voteCount": 8,
        "content": "D. Set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. Redeploy AFT and apply the changes.\n\nAWS Organizations is a service that helps to manage multiple AWS accounts. AWS Control Tower is a service that makes it easy to set up and govern secure, compliant multi-account AWS environments. Account Factory for Terraform (AFT) is an AWS Control Tower feature that provisions new accounts using Terraform templates.\n\nTo provision new accounts with the Enterprise Support plan, the DevOps engineer can set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration. This flag enables the Enterprise Support plan for newly provisioned accounts."
      },
      {
        "date": "2024-02-09T06:08:00.000Z",
        "voteCount": 5,
        "content": "D is correct: &lt; Account Fachttps://www.examtopics.com/exams/amazon/aws-certified-devops-engineer-professional-dop-c02/view/#tory for Terraform (AFT)&gt; means we need to change AFT config\nA: AWS Config conformance pack should be used with SSM automation document to remediate\nB and C: irrelevant"
      },
      {
        "date": "2023-06-08T10:41:00.000Z",
        "voteCount": 3,
        "content": "D\nhttps://docs.aws.amazon.com/controltower/latest/userguide/aft-feature-options.html"
      },
      {
        "date": "2023-05-01T18:30:00.000Z",
        "voteCount": 3,
        "content": "D. Set the aft_feature_enterprise_support feature flag to True in the AFT deployment input configuration, and then redeploy AFT to apply the changes. This flag is used to enable the Enterprise Support plan for new accounts provisioned by AFT. By default, AFT provisions accounts with the Basic Support plan. Therefore, enabling this flag will provision accounts with the Enterprise Support plan."
      },
      {
        "date": "2023-04-15T11:11:00.000Z",
        "voteCount": 2,
        "content": "D it is"
      },
      {
        "date": "2023-04-07T07:57:00.000Z",
        "voteCount": 1,
        "content": "D: To enable the Enterprise Support option, set the following feature flag to True in your AFT deployment input configuration.\naft_feature_enterprise_support=true\nhttps://docs.aws.amazon.com/controltower/latest/userguide/aft-feature-options.html"
      },
      {
        "date": "2023-04-04T20:26:00.000Z",
        "voteCount": 1,
        "content": "Why not C?"
      },
      {
        "date": "2023-09-19T05:13:00.000Z",
        "voteCount": 1,
        "content": "check this out https://controltower.aws-management.tools/automation/aft_setup/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/amazon/view/105242-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's DevOps engineer uses AWS Systems Manager to perform maintenance tasks during maintenance windows. The company has a few Amazon EC2 instances that require a restart after notifications from AWS Health. The DevOps engineer needs to implement an automated solution to remediate these notifications. The DevOps engineer creates an Amazon EventBridge rule.<br>How should the DevOps engineer configure the EventBridge rule to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an event source of AWS Health, a service of EC2. and an event type that indicates instance maintenance. Target a Systems Manager document to restart the EC2 instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an event source of Systems Manager and an event type that indicates a maintenance window. Target a Systems Manager document to restart the EC2 instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an event source of AWS Health, a service of EC2, and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an event source of EC2 and an event type that indicates instance maintenance. Target a newly created AWS Lambda function that registers an automation task to restart the EC2 instance during a maintenance window."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 47,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 23,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-29T14:33:00.000Z",
        "voteCount": 29,
        "content": "And AWS Training and Certification has A as the correct answer in the practice exam."
      },
      {
        "date": "2023-08-18T16:27:00.000Z",
        "voteCount": 7,
        "content": "It doesn't need to invoke Lambda.\nThere is a SSM document , RestartEC2Instance\nhttps://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html"
      },
      {
        "date": "2024-07-25T01:00:00.000Z",
        "voteCount": 1,
        "content": "A\nUsing SSM document to restart EC2 Instance. Not require to invoke Lambda. \n\nhttps://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#automating-instance-actions"
      },
      {
        "date": "2024-07-07T02:34:00.000Z",
        "voteCount": 1,
        "content": "No need to invoke Lambda."
      },
      {
        "date": "2024-06-27T07:59:00.000Z",
        "voteCount": 1,
        "content": "I'm hesitant between A and C but I'm voting C\n1) SSM document is not a valid target, valid targets for SSM are: Automation, Run Command, OpsItem\n2) If company is already using maitenance windows devops engineer should use them instead of restarting instances immediately"
      },
      {
        "date": "2024-08-16T07:19:00.000Z",
        "voteCount": 1,
        "content": "From the question, the company is already using SSM so there is really no need to create a custom Lambda function. SSM is a valid target action for EventBridge events. You can trigger the running of the AWS-RestartEC2Instance automation document with an EventBridge event which means SSM documents are a valid target."
      },
      {
        "date": "2024-05-30T03:57:00.000Z",
        "voteCount": 1,
        "content": "easiest way to do this"
      },
      {
        "date": "2024-05-27T21:15:00.000Z",
        "voteCount": 1,
        "content": "SSM Runbook:  AWS-RestartEC2Instance  (restart one or more EC2 instances)"
      },
      {
        "date": "2024-05-27T21:26:00.000Z",
        "voteCount": 1,
        "content": "In reading through some of the responses I think \"maintenance windows\" (plural) doesn't imply scheduling through Lambda.  A DevOps engineer can disable automation during production hours.  The scenario is unclear if they want this running all the time, or just enabled to run ONLY in a maintenance window.  What I'm sure of is they are wanting the SSM runbook as the answer.  In the real world, if productin EC2 instance has a health issue, you might just very well want to reboot it automatically if that truly fixes the problem.  Nuff said."
      },
      {
        "date": "2024-05-24T01:49:00.000Z",
        "voteCount": 1,
        "content": "The answer is C because A. This option is incorrect because AWS Health notifications do not trigger Systems Manager maintenance windows directly. Additionally, Systems Manager documents cannot restart EC2 instances directly; they need to be executed through other services like Systems Manager Automation or AWS Lambda."
      },
      {
        "date": "2024-02-28T10:49:00.000Z",
        "voteCount": 3,
        "content": "Answer is A. You can create a maintenance window in AWS SSM and associate the EventBridge rule with the maintenance window. No need to customize the solution with lambda."
      },
      {
        "date": "2024-04-23T18:07:00.000Z",
        "voteCount": 1,
        "content": "A say Target a\"Systems Manager document\" not support by EB =&gt; need to use Lambda =&gt; Answer is C\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-targets.html"
      },
      {
        "date": "2024-05-19T17:51:00.000Z",
        "voteCount": 1,
        "content": "A Systems Manager document defines the actions that Systems Manager performs on your managed instances. An automation document is a type of Systems Manager document that's used to perform common maintenance and deployment tasks. This includes creating or updating an Amazon Machine Image (AMI). This topic outlines how to create, edit, publish, and delete automation documents with AWS Toolkit.\nsorry my mistake, ans is A"
      },
      {
        "date": "2024-02-18T22:35:00.000Z",
        "voteCount": 1,
        "content": "Thus, Option C is the most accurate and effective solution for automating EC2 instance restarts in response to AWS Health notifications, leveraging the combined capabilities of AWS Health, Amazon EventBridge, AWS Lambda, and AWS Systems Manager."
      },
      {
        "date": "2024-05-11T23:53:00.000Z",
        "voteCount": 2,
        "content": "Why does Lambda have to be involved?"
      },
      {
        "date": "2024-01-30T16:18:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#automating-instance-actions"
      },
      {
        "date": "2024-01-28T08:01:00.000Z",
        "voteCount": 2,
        "content": "A is correct:\nB: AWS health should be the event source, not system manager\nC and D: should not use lambda if already have System manager"
      },
      {
        "date": "2024-01-15T06:22:00.000Z",
        "voteCount": 3,
        "content": "The system is already using SSM to manage EC2 instances, why would you create another solution and use Lambda ?   The maintenance window is added to confuse people.  The event is from AWS health and need attention immediately.  option A fits perfectly."
      },
      {
        "date": "2023-12-16T07:17:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html#automating-instance-actions"
      },
      {
        "date": "2023-11-23T21:26:00.000Z",
        "voteCount": 3,
        "content": "Option A appears to be the most suitable:\n\nConfiguring AWS Health as the event source ensures notifications related to EC2 instances are captured.\nTargeting a Systems Manager document to restart the EC2 instance aligns with Systems Manager's capabilities for automated tasks like instance restarts.\n\nOption B focuses on Systems Manager events related to maintenance windows, which might not directly align with notifications triggered by AWS Health for EC2 instance maintenance."
      },
      {
        "date": "2023-11-17T12:18:00.000Z",
        "voteCount": 3,
        "content": "Answer is A, lets breakdown the question. The first part is the DevOps uses system manager for maintenance windows (ok, normal approach) Second part of the question, some EC2 instances requires a restart after AWS Health notification (So, If there is a AWS Health notification the EC2 instance needs a restart), third part of the question, the DevOps should solve the part 2 problem automatically (but it doesn't say when, only a restart is needed), so .. the first part of the question is a catfish, you need to solve the problem automatically an the best way to do it is the A option,"
      },
      {
        "date": "2023-11-09T05:16:00.000Z",
        "voteCount": 1,
        "content": "It can't be A, as the AWS-RestartEC2Instance action is immediate. You need Lambda to schedule it to run during maintenance window."
      },
      {
        "date": "2023-12-11T03:18:00.000Z",
        "voteCount": 1,
        "content": "system manager's automation document can also do it! the rule of thumb is all ec2 related things could directly be run via automation document but for complex task which is not by-default covered using automation document we use lambda."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/amazon/view/105524-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has containerized all of its in-house quality control applications. The company is running Jenkins on Amazon EC2 instances, which require patching and upgrading. The compliance officer has requested a DevOps engineer begin encrypting build artifacts since they contain company intellectual property.<br>What should the DevOps engineer do to accomplish this in the MOST maintainable manner?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomate patching and upgrading using AWS Systems Manager on EC2 instances and encrypt Amazon EBS volumes by default.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Jenkins to an Amazon ECS cluster and copy build artifacts to an Amazon S3 bucket with default encryption enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage AWS CodePipeline with a build action and encrypt the artifacts using AWS Secrets Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeBuild with artifact encryption to replace the Jenkins instance running on EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 51,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-24T16:52:00.000Z",
        "voteCount": 18,
        "content": "The question wants you to know which solution is the easiest to maintain. It's important not to get thrown by information provided about their current environment. Only the question they ask matters. The question asks which solution is the easiest to \"maintain\". The question did not ask whether it would be easy to transition from one solution to another or ask you to leverage containers like other parts of their environment.\n\nAs a managed service, AWS CodeBuild does not require patching and upgrading. AWS CodeBuild, using Amazon S3, provides automatic artifact encryption. So this solution is the easiest to maintain of all the solutions listed.\n\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/welcome.html\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/security-encryption.html"
      },
      {
        "date": "2023-06-08T11:34:00.000Z",
        "voteCount": 11,
        "content": "While B will require less changes to the build process I assume AWS is promoting managed services here and expects D answer."
      },
      {
        "date": "2024-09-22T23:20:00.000Z",
        "voteCount": 1,
        "content": "while option D could be easier for simple projects or when starting from scratch, it may not be the most maintainable solution for a company that already has a significant investment in Jenkins. Option B provides a balanced approach, leveraging Jenkins' capabilities while improving infrastructure management and security."
      },
      {
        "date": "2024-06-07T00:43:00.000Z",
        "voteCount": 1,
        "content": "AWS codebuild use kms encryption key by default"
      },
      {
        "date": "2024-05-28T17:38:00.000Z",
        "voteCount": 1,
        "content": "\"D\" for me based on  sb333's comments, etc."
      },
      {
        "date": "2024-05-15T17:27:00.000Z",
        "voteCount": 1,
        "content": "D isn't cost effective, but most maintainable"
      },
      {
        "date": "2024-02-28T11:07:00.000Z",
        "voteCount": 1,
        "content": "Answer is D\nAWS CodeBuild can be seamlessly integrated with containerized applications deployed on Amazon ECS.\nAWS CodeBuild utilizes multiple layers of encryption to safeguard your data at rest, in transit, and during execution."
      },
      {
        "date": "2024-02-28T05:31:00.000Z",
        "voteCount": 1,
        "content": "D Seems the best option"
      },
      {
        "date": "2024-01-28T20:05:00.000Z",
        "voteCount": 3,
        "content": "D is correct: codebuild has encryption by default -&gt; easiest to maintain\nA: No mention of encrypting build artifacts\nB: Amazon S3 excryption only protect data at rest, not encrypting the data\nC: Using both AWS codepipline and AWS secret manager incurs more costs and makes maintenance much more difficult"
      },
      {
        "date": "2023-12-14T07:47:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2023-11-27T02:09:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2023-11-03T21:55:00.000Z",
        "voteCount": 1,
        "content": "B is the answer .  The ask is not to re engineer the whole solution it's just a simple task which needs encrypt the artifact. \nJenkins on Amazon ECS: Running Jenkins in an Amazon ECS cluster allows you to containerize your Jenkins setup, making it easier to manage and scale. ECS offers high availability, scalability, and easy maintenance.\n\nNormally Jenkin should run on ECS so it can handle multiple agents while use S3 as the default encryption."
      },
      {
        "date": "2023-09-18T01:21:00.000Z",
        "voteCount": 2,
        "content": "MOST maintainable manner is repacing jenkins with Codebuild a fully managed service\nIf the question had been with minimal chnage to the envornment  then  B would be best"
      },
      {
        "date": "2023-09-06T20:27:00.000Z",
        "voteCount": 1,
        "content": "Answer is D: MOST maintainable manner/managed service is the key word and there is no need to patch and upgrade. There is ECS with EC2 instances and ECS with fargate and the question is not explicit. Hence maintenance wise, a managed service is the way to go.\nhttps://jenkinshero.com/jenkins-vs-aws-codebuild-for-building-docker-images/"
      },
      {
        "date": "2023-07-07T21:23:00.000Z",
        "voteCount": 1,
        "content": "Technically CodeBuild runs on a VM\u2026 albeit disposable. Switching on EC2 24/7 is not cost effective either."
      },
      {
        "date": "2023-06-19T14:22:00.000Z",
        "voteCount": 2,
        "content": "D. Use AWS CodeBuild with artifact encryption to replace the Jenkins instance running on EC2 instances.\n\nExplanation: AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. With CodeBuild, you don\u2019t need to provision, manage, and scale your own build servers. It also provides built-in support for artifact encryption, which would satisfy the compliance officer's requirements. This would eliminate the need for patching and upgrading Jenkins on EC2 instances, as well as the need to handle encryption at the storage level."
      },
      {
        "date": "2023-06-16T09:26:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer.. no one said to replace jenkins"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/amazon/view/105243-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "An IT team has built an AWS CloudFormation template so others in the company can quickly and reliably deploy and terminate an application. The template creates an Amazon EC2 instance with a user data script to install the application and an Amazon S3 bucket that the application uses to serve static webpages while it is running.<br>All resources should be removed when the CloudFormation stack is deleted. However, the team observes that CloudFormation reports an error during stack deletion, and the S3 bucket created by the stack is not deleted.<br>How can the team resolve the error in the MOST efficient manner to ensure that all resources are deleted without errors?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a DelelionPolicy attribute to the S3 bucket resource, with the value Delete forcing the bucket to be removed when the stack is deleted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the Lambda function to delete all objects from the bucket when RequestType is Delete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIdentify the resource that was not deleted. Manually empty the S3 bucket and then delete it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the EC2 and S3 bucket resources with a single AWS OpsWorks Stacks resource. Define a custom recipe for the stack to create and delete the EC2 instance and the S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-15T13:49:00.000Z",
        "voteCount": 7,
        "content": "B. As per the AWS DeletionPolicy Options documentation it says, \"For Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed.\"\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html"
      },
      {
        "date": "2024-01-28T20:37:00.000Z",
        "voteCount": 7,
        "content": "B is correct: \n- Cant delete S3 so must check S3\n- There are several DeletionPolition option in ACF: delete, retain, snapshot. For S3, even if there is delete flag, S3 can only be deleted if all objects are removed\nA: wrong - add delete flag to deleteionpolicy cant forcing deletion of S3\nC: should not manually do the task\nD: should not swap to AWS opsworks"
      },
      {
        "date": "2024-06-07T00:44:00.000Z",
        "voteCount": 1,
        "content": "Cloudformation does  not have any behavior to force delete not empty bucket, need to invoke a custom lambda function to delete it"
      },
      {
        "date": "2024-05-11T02:00:00.000Z",
        "voteCount": 1,
        "content": "Keyword \"Custom Resource\""
      },
      {
        "date": "2023-06-08T11:36:00.000Z",
        "voteCount": 1,
        "content": "B is a correct answer. A is wrong, you can't delete a bucket that has any objects."
      },
      {
        "date": "2023-05-01T18:49:00.000Z",
        "voteCount": 2,
        "content": "B. Add a custom resource with an AWS Lambda function with the DependsOn attribute specifying the S3 bucket, and an IAM role. Write the Lambda function to delete all objects from the bucket when RequestType is Delete."
      },
      {
        "date": "2023-04-14T14:45:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-04-07T09:11:00.000Z",
        "voteCount": 3,
        "content": "Because it's B. CFN will not delete non-empty bucket. It must be emptied first. Custom resource will do it."
      },
      {
        "date": "2023-04-04T20:53:00.000Z",
        "voteCount": 1,
        "content": "Why not A?"
      },
      {
        "date": "2023-04-24T08:12:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html\ndeletion policy seems fine as well ..."
      },
      {
        "date": "2024-01-17T03:10:00.000Z",
        "voteCount": 2,
        "content": "As per the linked article: \"For Amazon S3 buckets, you must delete all objects in the bucket for deletion to succeed.\""
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/amazon/view/105247-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an AWS CodePipeline pipeline that is configured with an Amazon S3 bucket in the eu-west-1 Region. The pipeline deploys an AWS Lambda application to the same Region. The pipeline consists of an AWS CodeBuild project build action and an AWS CloudFormation deploy action.<br>The CodeBuild project uses the aws cloudformation package AWS CLI command to build an artifact that contains the Lambda function code\u2019s .zip file and the CloudFormation template. The CloudFormation deploy action references the CloudFormation template from the output artifact of the CodeBuild project\u2019s build action.<br>The company wants to also deploy the Lambda application to the us-east-1 Region by using the pipeline in eu-west-1. A DevOps engineer has already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east-1.<br>Which combination of additional steps should the DevOps engineer take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CloudFormation template to include a parameter for the Lambda function code\u2019s zip file location. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to pass in the us-east-1 artifact location as a parameter override.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket in us-east-1. Configure the S3 bucket policy to allow CodePipeline to have read and write access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket in us-east-1. Configure S3 Cross-Region Replication (CRR) from the S3 bucket in eu-west-1 to the S3 bucket in us-east-1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the pipeline to include the S3 bucket for us-east-1 as an artifact store. Create a new CloudFormation deploy action for us-east-1 in the pipeline. Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-10T01:20:00.000Z",
        "voteCount": 18,
        "content": "As below. You need S# bucket in the new region so C. You need to output artifacts to this new bucket so E.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html"
      },
      {
        "date": "2024-07-25T01:32:00.000Z",
        "voteCount": 1,
        "content": "CE\nNot D because \"A DevOps engineer has already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east-1\"\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html"
      },
      {
        "date": "2024-07-25T00:50:00.000Z",
        "voteCount": 1,
        "content": "C and E are the right answers."
      },
      {
        "date": "2024-07-25T00:51:00.000Z",
        "voteCount": 1,
        "content": "A: It suggests pointing directly to the Lambda function but we also need the Cloudformation template. So we can rule option A out.\nB: Here, we are missing the new region (us-east-1) artifact store where the new Cloudformation deploys action would store the artifacts upon completion.\nE: Includes the missing part in option B and is the right answer.\nD: \"A DevOps engineer has already updated the CodeBuild project to use the AWS CloudFormation package command to produce an additional output artifact for us-east-1.\" So as we are directly producing the artifact in the S3 bucket in us-east-1, I don't see the point of having a cross-replication.\nC: This option is mandatory as we should provide permissions to services (CodePipeline) to access resources (S3 bucket)."
      },
      {
        "date": "2024-05-27T11:15:00.000Z",
        "voteCount": 1,
        "content": "B and C are the answers. \nThe two important things to note here are the use of AWS CLI and artifacts from two different regions."
      },
      {
        "date": "2024-05-27T11:16:00.000Z",
        "voteCount": 1,
        "content": "Apologies, I meant C and E."
      },
      {
        "date": "2024-02-19T18:30:00.000Z",
        "voteCount": 1,
        "content": "a/b correct\na: the cloudformation template should be modified to incllude a parameter that indicates the location of the.zip file containing the lambda function's cod, this allows the cloudformation deploy action to use the correct artifact depending on the region, this is critical because lambda functions needto reference their code artifacts form the same region they are being deployed in, b. you tould also need to create a new cloudformation deploy action fro the us-east-1 region within the pipelinem this action should be confiured to use the cloudformation template from the artiface that was sepcifically created for us-eat-1"
      },
      {
        "date": "2024-01-29T00:13:00.000Z",
        "voteCount": 4,
        "content": "C and E are correct: To achieve the goal. we need an empty S3 in the us-east-1 and create additional stage in the pipline\nA: No mention of S3 - incorrect\nB: no mention of S3 - incoorect\nD: we need an empty S3 to store artifact, Cross-Region Replicate incurs more unnecessary cost. Additionally, this way force the S3 in us-east-1 to be exactly like that of us-west-1, which is incorrect. Each S3 has a different set of artifacts (though they might be very similar)"
      },
      {
        "date": "2024-01-06T15:38:00.000Z",
        "voteCount": 1,
        "content": "Why C and not D?"
      },
      {
        "date": "2024-07-15T14:10:00.000Z",
        "voteCount": 2,
        "content": "\"A DevOps engineer has already updated the CodeBuild project to use the aws cloudformation package command to produce an additional output artifact for us-east-1\""
      },
      {
        "date": "2023-12-14T07:59:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html#actions-create-cross-region-cfn\nCE is my answers"
      },
      {
        "date": "2023-11-17T18:00:00.000Z",
        "voteCount": 1,
        "content": "For CloudFormation you need to add the Region parameter: https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html#actions-create-cross-region-cfn"
      },
      {
        "date": "2023-11-13T04:46:00.000Z",
        "voteCount": 3,
        "content": "Answers: C E\nScenario:\n- We have Pipeline in RegionA (eu-west-1 Region)\n- We have Deploy action in RegionA (eu-west-1 Region)\nRequirements:\n- Need to have Deploy to RegionB (us-east-1 Region)\n- And still use RegionA pipeline above (eu-west-1 Region)"
      },
      {
        "date": "2023-11-13T04:47:00.000Z",
        "voteCount": 2,
        "content": "Which combination of additional steps to meet requirements:\n- Create bucket in RegionB (us-east-1 Region) [artifact store] This will be the OutputArtifact bucket for Deploy action in RegionB (us-east-1 Region)\n- Add a cross-Region action to a pipeline (CLI)\n-- Described in step #3 \"Modify the pipeline to include the S3 bucket for us-east-1 as an artifact store.\" \n-- Described in step #2 \"Create a new CloudFormation deploy action for us-east-1 in the pipeline.\" and \"Configure the new deploy action to use the CloudFormation template from the us-east-1 output artifact.\"\n\nREF: https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html#actions-cross-region-cli\n\nINCORRECT\n- A B cannot be a \"combination of steps\". They both have \"Create a new CloudFormation deploy action for us-east-1 in the pipeline.\"\n- D - we do not need S3 Cross-Region Replication (CRR) in the solution."
      },
      {
        "date": "2023-11-03T21:46:00.000Z",
        "voteCount": 1,
        "content": "A and B is the answer. Anything related to create S3 is wrong since Code Deploy have ability to automatically share artifacts to other regions"
      },
      {
        "date": "2023-12-19T22:48:00.000Z",
        "voteCount": 3,
        "content": "we are not using codedeploy"
      },
      {
        "date": "2023-11-02T13:28:00.000Z",
        "voteCount": 3,
        "content": "C &amp; E are correct options"
      },
      {
        "date": "2023-11-02T03:49:00.000Z",
        "voteCount": 1,
        "content": "Would go for CE"
      },
      {
        "date": "2023-10-31T03:13:00.000Z",
        "voteCount": 1,
        "content": "It should be BC! In order to do cross-region deployment, we should create two s3 for each region for ArtifactStore. Then CodeBuild should bundle the sam template and upload to each bucket. Finally CodePipeline should have two separate action for Cloudformation deployment, and each deployment has a region attribute to be defined and needs to pull the template from the ArtifactStore respectively."
      },
      {
        "date": "2023-10-18T08:35:00.000Z",
        "voteCount": 2,
        "content": "C, D and E answers are wrong. CodePipeline automatically creates an S3 bucket in the cross-region for the artifacts.  CodePipeline handles the copying of artifacts from one AWS Region to the other Regions when performing cross-region actions.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-create-cross-region.html"
      },
      {
        "date": "2024-05-20T12:59:00.000Z",
        "voteCount": 1,
        "content": "That link also says if you are using Cloudformation or CLI then you have to provide the buckets. So C and E"
      },
      {
        "date": "2023-10-07T23:23:00.000Z",
        "voteCount": 1,
        "content": "CE\nyou need to create s3 bucket to set it as output artifact for code build \ncode build can have one source and 5 output artifacts"
      },
      {
        "date": "2023-08-26T14:01:00.000Z",
        "voteCount": 2,
        "content": "CE seems correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/amazon/view/105569-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs an application on one Amazon EC2 instance. Application metadata is stored in Amazon S3 and must be retrieved if the instance is restarted. The instance must restart or relaunch automatically if the instance becomes unresponsive.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm for the StatusCheckFailed metric. Use the recover action to stop and start the instance. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS OpsWorks, and use the auto healing feature to stop and start the instance. Use a lifecycle event in OpsWorks to pull the metadata from Amazon S3 and update it on the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse EC2 Auto Recovery to automatically stop and start the instance in case of a failure. Use an S3 event notification to push the metadata to the instance when the instance is back up and running.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to create an EC2 instance that includes the UserData property for the EC2 resource. Add a command in UserData to retrieve the application metadata from Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-10T14:28:00.000Z",
        "voteCount": 9,
        "content": "Both Amazon CloudWatch's recover action and EC2 Auto Recovery are designed to respond to system status check failures, not instance status check failures. System status check failures indicate issues with the underlying hardware, while instance status check failures are often related to issues within your instance (like an OS-level issue).\n\nIf the requirement is to handle unresponsiveness due to both system-level and instance-level issues, neither option A nor C would fully meet the requirement. In that case, AWS OpsWorks with auto healing (Option B) could be a better fit since OpsWorks allows you to configure more complex health checks and could recover from both system-level and instance-level issues.\n\nSo, if you want to handle both types of unresponsiveness, Option B would be the most comprehensive solution."
      },
      {
        "date": "2024-08-16T07:38:00.000Z",
        "voteCount": 1,
        "content": "May I add that AWS Opswork offers lifecycle events which you can leverage to execute custom actions on the EC2 instance for example retrieving metadata from S3 as the question requested."
      },
      {
        "date": "2024-06-07T00:52:00.000Z",
        "voteCount": 1,
        "content": "B seem correct"
      },
      {
        "date": "2024-05-29T15:32:00.000Z",
        "voteCount": 1,
        "content": "Identical with Question #: 102"
      },
      {
        "date": "2024-04-10T19:15:00.000Z",
        "voteCount": 1,
        "content": "To automatic restart, must pull artifact for proactive"
      },
      {
        "date": "2024-01-29T00:40:00.000Z",
        "voteCount": 3,
        "content": "B: is correct: AWS opsworks auto healing will monitor the healthiness of EC2. If there is failure, restart EC2 and pull data from S3 to EC2\nA: incorrect because no mention of method to trigger S3 and S3 will not trigger by itself\nC: incorrect because no mention of method to trigger S3 and S3 will not trigger by itself\nD: Cloud formation only for deploy, this task is about opswork"
      },
      {
        "date": "2023-12-20T19:53:00.000Z",
        "voteCount": 3,
        "content": "OpWorks is deprecated now , So will it be part of exam ? What is the point of learning of service that are not , going to use."
      },
      {
        "date": "2023-11-06T11:40:00.000Z",
        "voteCount": 3,
        "content": "OpWorks is EOL now, however, I think this is the correct answer currenty."
      },
      {
        "date": "2023-12-10T01:11:00.000Z",
        "voteCount": 1,
        "content": "yes indeed. its EOL"
      },
      {
        "date": "2023-09-04T03:30:00.000Z",
        "voteCount": 4,
        "content": "A and C are wrong because S3 event notification destination is lambda, sqs and sns topic, you can't directly push metadata to EC2;\nD is wrong because although user data can retrieve s3 metadata, it can't restart automatically."
      },
      {
        "date": "2023-06-15T14:12:00.000Z",
        "voteCount": 2,
        "content": "B. It doesn't make sense for an S3 event notification to be triggered by an EC2 instance being restarted. The OpsWorks autohealing capability can detect failed instances and replace them. \nAfter the auto-healed instance is back online, OpsWorks triggers a Configure lifecycle event on the instance. The metadata from S3 could be retrieved by the lifecycle event with a recipe.\n\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workinginstances-autohealing.html\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html\nhttps://github.com/awsdocs/aws-opsworks-user-guide/blob/master/doc_source/create-custom-configure.md"
      },
      {
        "date": "2023-06-10T01:31:00.000Z",
        "voteCount": 2,
        "content": "B, not simplest one but the only that meets requirements.\nFor A - how can you push data from S3 to EC2? Data needs to be pulled from EC2."
      },
      {
        "date": "2023-05-23T01:17:00.000Z",
        "voteCount": 1,
        "content": "A for me\n By creating a CloudWatch alarm for the StatusCheckFailed metric, the system can detect if the instance becomes unresponsive. The recover action can then be triggered to automatically stop and start the instance, ensuring it restarts or relaunches when necessary.\n\nAdditionally, an S3 event notification can be set up to push the metadata to the instance once it is back up and running. This ensures that the application metadata is retrieved and available after the restart"
      },
      {
        "date": "2023-05-30T09:14:00.000Z",
        "voteCount": 3,
        "content": "The second part would not work. An  S3 notification event occurs only when actions occur on the object. When you restart the instance, nobody is overwriting the object to trigger the notification. IMHO."
      },
      {
        "date": "2023-04-28T23:59:00.000Z",
        "voteCount": 2,
        "content": "B seems to be more feasible in this case."
      },
      {
        "date": "2023-04-27T07:23:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/about-aws/whats-new/2022/03/amazon-ec2-default-automatic-recovery/"
      },
      {
        "date": "2023-04-15T11:25:00.000Z",
        "voteCount": 1,
        "content": "I'd say the answer is A ..you can configure Amazon CloudWatch to monitor the EC2 instance and trigger an automatic restart or relaunch if it becomes unresponsive. You can set up a CloudWatch alarm to monitor the instance's CPU utilization, network traffic, or other metrics, and define an action to take if the alarm is triggered, such as rebooting the instance or terminating and relaunching it."
      },
      {
        "date": "2023-07-29T23:58:00.000Z",
        "voteCount": 1,
        "content": "\"Use an S3 event notification to push the metadata to the instance when the instance is back up and running.\" makes no sense"
      },
      {
        "date": "2023-04-08T01:14:00.000Z",
        "voteCount": 1,
        "content": "A and C both is wrong cause recover is only for system status check failure. So if it's instance status check fails, it will not respond."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/amazon/view/105570-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has multiple AWS accounts. The company uses AWS IAM Identity Center (AWS Single Sign-On) that is integrated with AWS Toolkit for Microsoft Azure DevOps. The attributes for access control feature is enabled in IAM Identity Center.<br>The attribute mapping list contains two entries. The department key is mapped to ${path:enterprise.department}. The costCenter key is mapped to ${path:enterprise.costCenter}.<br>All existing Amazon EC2 instances have a department tag that corresponds to three company departments (d1, d2, d3). A DevOps engineer must create policies based on the matching attributes. The policies must minimize administrative effort and must grant each Azure AD user access to only the EC2 instances that are tagged with the user\u2019s respective department name.<br>Which condition key should the DevOps engineer include in the custom permissions policies to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image1\" src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image1.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image2\" src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image2.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image3\" src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image3.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image4\" src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image4.png\">"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-29T02:41:00.000Z",
        "voteCount": 5,
        "content": "C is correct: check the EC2's department tag, if it is the same as user(principaltag)'s department tag, allow access.\nA: wrong synxtax, should be StringEquals only\nB: we checking the tag of Ec2, not aws. \nD: if config like this, every cases will match and everyone can access every EC2, regardless of department"
      },
      {
        "date": "2024-07-25T01:47:00.000Z",
        "voteCount": 1,
        "content": "C, related with ABAC."
      },
      {
        "date": "2023-06-10T01:35:00.000Z",
        "voteCount": 4,
        "content": "C, see an example at\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/configure-abac.html"
      },
      {
        "date": "2023-04-14T15:03:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-04-08T01:25:00.000Z",
        "voteCount": 4,
        "content": "https://aws.amazon.com/blogs/aws/new-attributes-based-access-control-with-aws-single-sign-on/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/amazon/view/105335-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company hosts a security auditing application in an AWS account. The auditing application uses an IAM role to access other AWS accounts. All the accounts are in the same organization in AWS Organizations.<br>A recent security audit revealed that users in the audited AWS accounts could modify or delete the auditing application's IAM role. The company needs to prevent any modification to the auditing application's IAM role by any entity other than a trusted administrator IAM role.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that includes a Deny statement for changes to the auditing application's IAM role. Include a condition that allows the trusted administrator IAM role to make changes. Attach the SCP to the root of the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that includes an Allow statement for changes to the auditing application's IAM role by the trusted administrator IAM role. Include a Deny statement for changes by all other IAM principals. Attach the SCP to the IAM service in each AWS account where the auditing application has an IAM role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM permissions boundary that includes a Deny statement for changes to the auditing application's IAM role. Include a condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the audited AWS accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM permissions boundary that includes a Deny statement for changes to the auditing application\u2019s IAM role. Include a condition that allows the trusted administrator IAM role to make changes. Attach the permissions boundary to the auditing application's IAM role in the AWS accounts."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-14T11:41:00.000Z",
        "voteCount": 19,
        "content": "SCPs (Service Control Policies) are the best way to restrict permissions at the organizational level, which in this case would be used to restrict modifications to the IAM role used by the auditing application, while still allowing trusted administrators to make changes to it. Options C and D are not as effective because IAM permission boundaries are applied to IAM entities (users, groups, and roles), not the account itself, and must be applied to all IAM entities in the account."
      },
      {
        "date": "2024-03-08T06:05:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html?icmpid=docs_orgs_console"
      },
      {
        "date": "2024-02-29T12:39:00.000Z",
        "voteCount": 1,
        "content": "Service Control Policies (SCPs) in AWS Organizations can be used to enforce maximum permissions for member accounts. They don't directly grant permissions or create permission boundaries. So C &amp; D can be ruled out."
      },
      {
        "date": "2024-02-18T18:58:00.000Z",
        "voteCount": 3,
        "content": "SCPs are applied at the account or OU level and affect all IAM entities within that organization. IAM Permission boundaries are applied individually to specific IAM roles or users."
      },
      {
        "date": "2024-02-05T06:02:00.000Z",
        "voteCount": 1,
        "content": "A is correct: &lt; prevent any modification to the auditing application's IAM role&gt; means scp\nA: &lt;Include a condition that allows the trusted administrator IAM role&gt; this is not the same as allow statement. So this option still valid\nB: SCP does not have allow statement\nC and D: These options make modification to permission boundary of the auditing application's IAM role, which is irrelavant. Other accounts may or may not assume this role."
      },
      {
        "date": "2024-02-06T09:07:00.000Z",
        "voteCount": 1,
        "content": "B is not correct because can only attach scp to AWS org"
      },
      {
        "date": "2024-05-12T03:43:00.000Z",
        "voteCount": 1,
        "content": "B wrong because SCP not support principals"
      },
      {
        "date": "2023-09-27T06:19:00.000Z",
        "voteCount": 1,
        "content": "AWS supports permissions boundaries for IAM entities (users or roles)"
      },
      {
        "date": "2023-07-29T23:27:00.000Z",
        "voteCount": 2,
        "content": "in option A, shouldn't the first half override the second half. Explicitly deny everybody( will not matter if later it says Allow Admin )."
      },
      {
        "date": "2023-11-06T04:28:00.000Z",
        "voteCount": 4,
        "content": "I think its because its not two policies. Its only one policy which applies when condition is account not equal security admin account. So A should work"
      },
      {
        "date": "2023-07-18T04:41:00.000Z",
        "voteCount": 1,
        "content": "A seems ok.  For C its wrong as you don't use permission boundary to deny permission.  You use it to specify what and what can be done and not what cannot be done."
      },
      {
        "date": "2023-06-10T01:48:00.000Z",
        "voteCount": 1,
        "content": "For AWS Organizations the SCP is the way to go. So A."
      },
      {
        "date": "2023-05-31T06:07:00.000Z",
        "voteCount": 1,
        "content": "It is A without a question. SCP is far more efficient."
      },
      {
        "date": "2023-05-30T09:20:00.000Z",
        "voteCount": 2,
        "content": "An SCP would accomplish efficiently the task for all the accounts from a single place. A permission boundary is not for that, it would have to be configured in each account and for all the users IMHO."
      },
      {
        "date": "2023-05-25T23:30:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html?icmpid=docs_orgs_console"
      },
      {
        "date": "2023-05-08T04:27:00.000Z",
        "voteCount": 1,
        "content": "C is more suitable option here to restrict permission."
      },
      {
        "date": "2023-04-25T19:37:00.000Z",
        "voteCount": 1,
        "content": "A permissions boundary is designed to restrict permissions on IAM principals, such as roles, such that permissions don\u2019t exceed what was originally intended. The permissions boundary uses an AWS or customer managed policy to restrict access, and it\u2019s similar to other IAM policies you\u2019re familiar with because it has resource, action, and effect statements. A permissions boundary alone doesn\u2019t grant access to anything. Rather, it enforces a boundary that can\u2019t be exceeded, even if broader permissions are granted by some other policy attached to the role.\n\nhttps://aws.amazon.com/blogs/security/when-and-where-to-use-iam-permissions-boundaries/"
      },
      {
        "date": "2023-04-14T15:12:00.000Z",
        "voteCount": 1,
        "content": "mi vote is for C as the right answer"
      },
      {
        "date": "2023-04-09T11:47:00.000Z",
        "voteCount": 1,
        "content": "Only valid solution is A, for C or D you need to attach boundaries on all IAM roles/users not the account or the role itself."
      },
      {
        "date": "2023-04-08T01:36:00.000Z",
        "voteCount": 1,
        "content": "Between  A and C, A looks good, but \"SCPs affect only member accounts in the organization. They have no effect on users or roles in the management account.\" \nC would do the work for all accounts in the Organization."
      },
      {
        "date": "2023-05-25T23:29:00.000Z",
        "voteCount": 3,
        "content": "All the accounts are in the same organization in AWS Organizations."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/amazon/view/105572-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an on-premises application that is written in Go. A DevOps engineer must move the application to AWS. The company's development team wants to enable blue/green deployments and perform A/B testing.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on an Amazon EC2 instance, and create an AMI of the instance. Use the AMI to create an automatic scaling launch configuration that is used in an Auto Scaling group. Use Elastic Load Balancing to distribute traffic. When changes are made to the application, a new AMI will be created, which will initiate an EC2 instance refresh.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Lightsail to deploy the application. Store the application in a zipped format in an Amazon S3 bucket. Use this zipped version to deploy new versions of the application to Lightsail. Use Lightsail deployment options to manage the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeArtifact to store the application code. Use AWS CodeDeploy to deploy the application to a fleet of Amazon EC2 instances. Use Elastic Load Balancing to distribute the traffic to the EC2 instances. When making changes to the application, upload a new version to CodeArtifact and create a new CodeDeploy deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy new versions of the application. Use Elastic Beanstalk to manage the deployment options.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-29T14:27:00.000Z",
        "voteCount": 3,
        "content": "AWS Elastic Beanstalk deploy action can be used to deploy the application artifact from the S3 bucket to the green environment, which is the AWS cloud environment here."
      },
      {
        "date": "2024-02-18T20:44:00.000Z",
        "voteCount": 2,
        "content": "Lightsail does not have built-in Blue/Green deployment capabilities like Elastic Beanstalk."
      },
      {
        "date": "2023-12-20T21:08:00.000Z",
        "voteCount": 4,
        "content": "D is undoubtedly is most correct one .  But they shoud mention that , we are going to deploy application in different environment . Since deployming to the same envirnmont just overide the previous deployment . In order to meed the requirement of Blue / Green deployment we need two separate enviormnent . Then we have two separate version running simulateously and we can do DNS swapping to quicky shilf traffic ."
      },
      {
        "date": "2023-07-11T14:57:00.000Z",
        "voteCount": 3,
        "content": "I was about to discard  D because I was unsure if beanstalk supported GO (yes it does )\nhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/concepts.platforms.html\n\nSo D is undoubtedly the best option to quickly move to cloud and to do blue green with an testing"
      },
      {
        "date": "2023-06-10T01:57:00.000Z",
        "voteCount": 1,
        "content": "I guess D is easiest option to orchestrate blue/green deployments and A/B testing in this case."
      },
      {
        "date": "2023-05-31T06:08:00.000Z",
        "voteCount": 1,
        "content": "D elastic beanstalk due to deployment options"
      },
      {
        "date": "2023-05-15T09:20:00.000Z",
        "voteCount": 1,
        "content": "Maybe D, but it looks like an old approach. we need to use codebuild and codepipelines and Elastic beanstalk, but elastic beanstalk could be changed by AWS cloudformation."
      },
      {
        "date": "2023-05-01T19:08:00.000Z",
        "voteCount": 4,
        "content": "D. Use AWS Elastic Beanstalk to host the application. Store a zipped version of the application in Amazon S3. Use that location to deploy new versions of the application. Use Elastic Beanstalk to manage the deployment options.\n\nAWS Elastic Beanstalk provides a platform for deploying web applications, which is well-suited for use cases that require blue/green deployments and A/B testing. Elastic Beanstalk can deploy applications written in a variety of programming languages and frameworks, including Go. Elastic Beanstalk supports blue/green deployments, which allow you to deploy a new version of your application to a separate environment before switching traffic to it. This enables you to perform A/B testing before fully rolling out a new version of your application. Elastic Beanstalk also allows you to manage the deployment options, including the deployment strategy, instance types, and autoscaling options."
      },
      {
        "date": "2023-04-14T15:13:00.000Z",
        "voteCount": 1,
        "content": "D it is"
      },
      {
        "date": "2023-04-08T01:51:00.000Z",
        "voteCount": 1,
        "content": "Elastic Beanstalk"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/amazon/view/106206-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A developer is maintaining a fleet of 50 Amazon EC2 Linux servers. The servers are part of an Amazon EC2 Auto Scaling group, and also use Elastic Load Balancing for load balancing.<br>Occasionally, some application servers are being terminated after failing ELB HTTP health checks. The developer would like to perform a root cause analysis on the issue, but before being able to access application logs, the server is terminated.<br>How can log collection be automated?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling lifecycle hooks to put instances in a Pending:Wait state. Create an Amazon CloudWatch alarm for EC2 Instance Terminate Successful and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an AWS Config rule for EC2 Instance-terminate Lifecycle Action and trigger a step function that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon CloudWatch subscription filter for EC2 Instance Terminate Successful and trigger a CloudWatch agent that invokes a script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-10T01:59:00.000Z",
        "voteCount": 10,
        "content": "D is the easiest solution."
      },
      {
        "date": "2024-08-01T01:10:00.000Z",
        "voteCount": 1,
        "content": "D as the EC2 is Terminating and Cloudwatch Agent should be not running and cannot collect the logs"
      },
      {
        "date": "2024-06-20T22:32:00.000Z",
        "voteCount": 1,
        "content": "It must be 'C' as CloudWatch Agent will push the logs to a particular CloudWatch log group."
      },
      {
        "date": "2024-02-19T00:44:00.000Z",
        "voteCount": 2,
        "content": "Terminating:Wait refers to a state in which an instance is determined to be terminated by the Auto Scaling group as part of the termination process and is temporarily put on hold before it is actually terminated. This state pauses the termination process and provides an opportunity to perform custom actions (logging, graceful shutdown, data backup, etc)."
      },
      {
        "date": "2024-04-10T19:33:00.000Z",
        "voteCount": 1,
        "content": "why not C bro"
      },
      {
        "date": "2024-01-29T06:51:00.000Z",
        "voteCount": 4,
        "content": "D is correct: Using Eventbridge in combination with lambda is a common practice.\nA: Cloudwatch alarm only alert, no action so it cannot trigger lambda (when this question came out, it could not)\nB: AWS config rule cannot triger a script. \nC: cloudwatch agent itself does not have any direct action on the host but collecting logs"
      },
      {
        "date": "2024-01-03T03:54:00.000Z",
        "voteCount": 1,
        "content": "C is also a good choice in this question. Why? you need to have a CW agent installed on the hosts to be able to collect logs from the servers before termination."
      },
      {
        "date": "2024-01-06T02:36:00.000Z",
        "voteCount": 2,
        "content": "I think we can't select C because it says that it invokes the cloudwatch agent after the EC2 instance is terminated. It can't collect the logs from terminated EC2 Instance."
      },
      {
        "date": "2023-05-30T09:25:00.000Z",
        "voteCount": 3,
        "content": "D is the correct one IMHO.\n\nASG actions are not logged to cloudwatch logs to use a filter, and if so it would be complicated to extract the data. The canonical way is to rely in an EventBridge event."
      },
      {
        "date": "2023-05-23T01:03:00.000Z",
        "voteCount": 4,
        "content": "D\n\n\"When a scale-in event occurs, a lifecycle hook pauses the instance before it is terminated and sends you a notification using Amazon EventBridge. While the instance is in the wait state, you can invoke an AWS Lambda function or connect to the instance to download logs or other data before the instance is fully terminated. \"\n\nhttps://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/"
      },
      {
        "date": "2023-05-03T08:46:00.000Z",
        "voteCount": 1,
        "content": "D for sure 100%"
      },
      {
        "date": "2023-05-01T19:11:00.000Z",
        "voteCount": 4,
        "content": "D. Use Auto Scaling lifecycle hooks to put instances in a Terminating:Wait state. Create an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action and trigger an AWS Lambda function that invokes an SSM Run Command script to collect logs, push them to Amazon S3, and complete the lifecycle action once logs are collected.\n\nWith this solution, you can use an Auto Scaling lifecycle hook to put instances in a wait state before termination. This provides an opportunity to collect logs before the instance is terminated. The solution can use an Amazon EventBridge rule for EC2 Instance-terminate Lifecycle Action to trigger an AWS Lambda function that will execute an SSM Run Command script. The script can collect logs and push them to Amazon S3 before completing the lifecycle action and allowing the instance to terminate. This solution provides a way to collect logs before instances are terminated, allowing for root cause analysis of issues."
      },
      {
        "date": "2023-04-29T00:11:00.000Z",
        "voteCount": 1,
        "content": "D seems to be more relevant for this scenario"
      },
      {
        "date": "2023-04-23T18:17:00.000Z",
        "voteCount": 1,
        "content": "Note that there is a similar question on Tutorial Dojo and the answer is to \"trigger cloudwatch agent\""
      },
      {
        "date": "2023-05-13T10:09:00.000Z",
        "voteCount": 1,
        "content": "read this link and you will understand that C is wrong option- https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-an-ec2-auto-scaling-instance/"
      },
      {
        "date": "2023-04-23T18:05:00.000Z",
        "voteCount": 2,
        "content": "should be C"
      },
      {
        "date": "2023-09-04T03:42:00.000Z",
        "voteCount": 2,
        "content": "No way. Cloudwatch subscription filter is normally used to send cloudwatch log to kinesis firehose stream so that it can be consumed by other tools such as Splunk. If you need to invoke a lambda, the easiest way is to use event rule."
      },
      {
        "date": "2023-04-15T03:28:00.000Z",
        "voteCount": 1,
        "content": "D sure"
      },
      {
        "date": "2023-04-14T15:25:00.000Z",
        "voteCount": 1,
        "content": "I think is D"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/amazon/view/106179-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an organization in AWS Organizations. The organization includes workload accounts that contain enterprise applications. The company centrally manages users from an operations account. No users can be created in the workload accounts. The company recently added an operations team and must provide the operations team members with administrator access to each workload account.<br>Which combination of actions will provide this access? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a SysAdmin role in the operations account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the workload accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a SysAdmin role in each workload account. Attach the AdministratorAccess policy to the role. Modify the trust relationship to allow the sts:AssumeRole action from the operations account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Cognito identity pool in the operations account. Attach the SysAdmin role as an authenticated role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the operations account, create an IAM user for each operations team member.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the operations account, create an IAM user group that is named SysAdmins. Add an IAM policy that allows the sts:AssumeRole action for the SysAdmin role in each workload account. Add all operations team members to the group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Cognito user pool in the operations account. Create an Amazon Cognito user for each operations team member."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "BEF",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "ABE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-08T03:04:00.000Z",
        "voteCount": 7,
        "content": "Any thing Cognito, safe to remove (it is only used for application identity management)\nStep 1: Create each role in each workload account. Set trust relationship to only sts:AssumeRole via the operations user in operations account\nStep 2: Self explanatory: whatever permission you needs once the user assumed the role\nStep 3: Voila"
      },
      {
        "date": "2024-07-25T02:29:00.000Z",
        "voteCount": 1,
        "content": "BDE\nNot A - Create SysAdmin role for workload accounts. \nNot C F - No Cognito require."
      },
      {
        "date": "2024-06-07T01:01:00.000Z",
        "voteCount": 1,
        "content": "Operation account:\n - Need to create a role to assume role in workload account --&gt; E\n- Create a group of users can perform assume role --&gt; D\nworkload account\n- Need to create a role with have admin perssion for operation account assume --&gt;B"
      },
      {
        "date": "2024-04-12T01:20:00.000Z",
        "voteCount": 2,
        "content": "Not sure why everyone is saying BDE. Why would you create an IAM user for each member and also create for the group? Make it make sense"
      },
      {
        "date": "2024-03-08T06:09:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html"
      },
      {
        "date": "2024-02-28T05:59:00.000Z",
        "voteCount": 1,
        "content": "EBD looks like the best choice"
      },
      {
        "date": "2024-02-19T00:56:00.000Z",
        "voteCount": 1,
        "content": "sts:AssumeRole is one of the AWS Security Token Service (STS) actions used to obtain temporary security credentials and assume the role of another AWS account."
      },
      {
        "date": "2024-01-29T06:58:00.000Z",
        "voteCount": 3,
        "content": "BDE: No cognito here. \n-step 1: create role in workload accounts\n-step 2: create IAM user for each member\n-step 3: move all member to the group that has permission to assume the role in step 1"
      },
      {
        "date": "2023-06-10T02:12:00.000Z",
        "voteCount": 2,
        "content": "BDE seems to be right."
      },
      {
        "date": "2023-05-31T06:14:00.000Z",
        "voteCount": 1,
        "content": "def BDE cause role must be created in workload accounts and assumed by the operations account"
      },
      {
        "date": "2023-05-30T09:40:00.000Z",
        "voteCount": 1,
        "content": "Correct: BDE\nCognito has nothing to do with this, so C and F are wrong.\nThe roles must be created in the workload accounts and assumed from the operations account. So A is wrong."
      },
      {
        "date": "2023-04-29T00:17:00.000Z",
        "voteCount": 3,
        "content": "BDE seems the correct strategy"
      },
      {
        "date": "2023-04-25T22:15:00.000Z",
        "voteCount": 1,
        "content": "Why do we need option A when question is asking access to workload account?"
      },
      {
        "date": "2023-04-15T11:31:00.000Z",
        "voteCount": 1,
        "content": "A,B,E it is"
      },
      {
        "date": "2023-04-15T03:32:00.000Z",
        "voteCount": 2,
        "content": "BDE is right answer, nothing to do with cognito"
      },
      {
        "date": "2023-04-14T11:46:00.000Z",
        "voteCount": 1,
        "content": "Options C, D, and F are incorrect because they do not provide a way for the operations team members to assume a role in the workload accounts, which is necessary to access the resources in those accounts."
      },
      {
        "date": "2023-05-12T04:08:00.000Z",
        "voteCount": 1,
        "content": "Should be BDE, Why the need to create two roles?"
      },
      {
        "date": "2023-04-14T06:41:00.000Z",
        "voteCount": 1,
        "content": "BDE is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/amazon/view/105252-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has multiple accounts in an organization in AWS Organizations. The company's SecOps team needs to receive an Amazon Simple Notification Service (Amazon SNS) notification if any account in the organization turns off the Block Public Access feature on an Amazon S3 bucket. A DevOps engineer must implement this change without affecting the operation of any AWS accounts. The implementation must ensure that individual member accounts in the organization cannot turn off the notification.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesignate an account to be the delegated Amazon GuardDuty administrator account. Turn on GuardDuty for all accounts across the organization. In the GuardDuty administrator account, create an SNS topic. Subscribe the SecOps team's email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event pattern for GuardDuty findings and a target of the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that creates an SNS topic and subscribes the SecOps team\u2019s email address to the SNS topic. In the template, include an Amazon EventBridge rule that uses an event pattern of CloudTrail activity for s3:PutBucketPublicAccessBlock and a target of the SNS topic. Deploy the stack to every account in the organization by using CloudFormation StackSets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS Config across the organization. In the delegated administrator account, create an SNS topic. Subscribe the SecOps team's email address to the SNS topic. Deploy a conformance pack that uses the s3-bucket-level-public-access-prohibited AWS Config managed rule in each account and uses an AWS Systems Manager document to publish an event to the SNS topic to notify the SecOps team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on Amazon Inspector across the organization. In the Amazon Inspector delegated administrator account, create an SNS topic. Subscribe the SecOps team\u2019s email address to the SNS topic. In the same account, create an Amazon EventBridge rule that uses an event pattern for public network exposure of the S3 bucket and publishes an event to the SNS topic to notify the SecOps team."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 43,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-28T08:07:00.000Z",
        "voteCount": 10,
        "content": "Answer is C.\n* AWS AWS Systems Manager Automation provides predefined runbooks(ex. AWS-PublishSNSNotification ) for Amazon Simple Notification Service - https://docs.aws.amazon.com/systems-manager-automation-runbooks/latest/userguide/automation-aws-publishsnsnotification.html\n* Running automations in multiple AWS Regions and accounts (https://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-multiple-accounts-regions.html )\n\nB seems to be old approach. With cloudformation stackset, each account can still change resource config (ex. SNS)  that causes drift.... so I choose C because it utilize AWS organization fully with aws systems manager automation in multiple regions and multiple accounts with delegated administrator account( or management account )"
      },
      {
        "date": "2024-08-16T12:54:00.000Z",
        "voteCount": 1,
        "content": "With option B, you will get notifications when user accounts turn off the block public access feature but it doesn't stop them from doing it. The question requires that the implementation stops users from being able to carry out that operation successfully altogether."
      },
      {
        "date": "2024-09-05T15:13:00.000Z",
        "voteCount": 1,
        "content": "GuardDuty Policy\nPolicy:S3/BucketBlockPublicAccessDisabled\n\"An IAM entity invoked an API used to disable S3 Block Public Access on a bucket.\"\n\"Data source: CloudTrail management events\"\n\"This finding informs you that Block Public Access was disabled for the listed S3 bucket. When enabled, S3 Block Public Access settings are used to filter the policies or access control lists (ACLs) applied to buckets as a security measure to prevent inadvertent public exposure of data.\"\nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-s3.html#policy-s3-bucketblockpublicaccessdisabled"
      },
      {
        "date": "2024-07-25T02:39:00.000Z",
        "voteCount": 1,
        "content": "C\n\n\"A conformance pack is a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS Organizations.\"\nhttps://docs.aws.amazon.com/config/latest/developerguide/conformance-packs.html\nhttps://docs.aws.amazon.com/config/latest/developerguide/WhatIsConfig.html"
      },
      {
        "date": "2024-06-30T22:07:00.000Z",
        "voteCount": 1,
        "content": "It's A. GuardDuty echieves this with no effort."
      },
      {
        "date": "2024-06-28T23:16:00.000Z",
        "voteCount": 2,
        "content": "A DevOps engineer must implement this change without affecting the operation of any AWS accounts."
      },
      {
        "date": "2024-05-30T12:31:00.000Z",
        "voteCount": 1,
        "content": "I was sure the answer was \"C\" until I started reading through some of the requirements and comments.  The words \"implementation must ensure that individual member accounts in the organization cannot turn off the notification\" incline me to lean towards \"A\", because with \"C\", someone with admin privileges on a single account could turn off the notification in that account.  As pointed out by others, there are a number of GuardDuty findings associates with S3 public access.  Having GuardDuty and EventBridge pattern trigger SNS for some key words such as \"s3\" and \"Public\" seems to make sense in enforcing this across an organization.  I don't have enough experience with GuardDuty in an Organization to be 100% confident, but the emphasis on SNS requirement makes me think this could be a trick question."
      },
      {
        "date": "2024-09-05T15:12:00.000Z",
        "voteCount": 1,
        "content": "GuardDuty Policy\nPolicy:S3/BucketBlockPublicAccessDisabled\n\"An IAM entity invoked an API used to disable S3 Block Public Access on a bucket.\"\n\"Data source: CloudTrail management events\"\n\"This finding informs you that Block Public Access was disabled for the listed S3 bucket. When enabled, S3 Block Public Access settings are used to filter the policies or access control lists (ACLs) applied to buckets as a security measure to prevent inadvertent public exposure of data.\" \nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-s3.html#policy-s3-bucketblockpublicaccessdisabled"
      },
      {
        "date": "2024-05-01T09:59:00.000Z",
        "voteCount": 1,
        "content": "C is only correct option."
      },
      {
        "date": "2024-04-21T06:53:00.000Z",
        "voteCount": 3,
        "content": "Technically A would be sufficient here.\n\nThe question is only asking to be NOTIFIED when block public access gets disabled.\n\nSee the following GuardDuty finding: https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_finding-types-s3.html#policy-s3-bucketblockpublicaccessdisabled\n\nManaging multiple GuardDuty accounts is simplified using the AWS Organizations delegated administrator feature. With this feature, the AWS Organizations management account can designate a member account to be the GuardDuty administrator for the entire organization. The delegated GuardDuty administrator is then granted permission to enable and manage GuardDuty for all existing and future accounts in the organization."
      },
      {
        "date": "2024-03-13T03:51:00.000Z",
        "voteCount": 4,
        "content": "We can leverage AWS Organizations to enable Guarduty in all accounts. \nThere is an S3 finding called Policy:S3/AccountBlockPublicAccessDisabled\nThen we setup a single EventBrdige rule in the delegated account that publish the event to the SNS topic in the same account.\n\nThis is the easisest solution to be implemented and monitoring the public access seamlessly across all Organization's accounts\n\nThis is a common multi-account strategy for GuardDuty with AWS organizations, to collect such finding from hundred of accounts"
      },
      {
        "date": "2024-03-08T06:11:00.000Z",
        "voteCount": 3,
        "content": "Amazon GuardDuty is primarily on threat detection and response, not configuration monitoring. A conformance pack is a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS Organizations. https://docs.aws.amazon.com/config/latest/developerguide/conformance-\npacks.htmlhttps://docs.aws."
      },
      {
        "date": "2024-03-05T11:17:00.000Z",
        "voteCount": 3,
        "content": "Answer is C\nA conformance pack is a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a Region or across an organization in AWS Organizations. You can also use AWS Systems Manager documents (SSM documents) to store your conformance pack templates on AWS and directly deploy conformance packs using SSM document names."
      },
      {
        "date": "2024-02-21T16:59:00.000Z",
        "voteCount": 1,
        "content": "Hi can somebody with contributors access, would please forward all the questions pdf to me on telegram @rater250 , I'm willing to pay"
      },
      {
        "date": "2024-01-29T07:10:00.000Z",
        "voteCount": 1,
        "content": "C is correct: AWS config can only be modify by admin, not member accounts"
      },
      {
        "date": "2024-02-01T07:53:00.000Z",
        "voteCount": 2,
        "content": "Let me clarify: B cannot be correct because of this reason:  \"Deploy the stack to every account in the organization by using CloudFormation StackSets\" means in every accounts of this AWS org (canbe up to hundreds of account), we will deploy a SNS topic and an EventBridge rule. This would be an extremely expensive deployment"
      },
      {
        "date": "2024-01-27T04:16:00.000Z",
        "voteCount": 2,
        "content": "Option B is also not a valid case because we can direct use config with eventbrige why to go for clod trail we can use aws config rule s3-bucket-public-read-prohibited if rule changes eventbridge will trigger sns"
      },
      {
        "date": "2024-01-27T04:12:00.000Z",
        "voteCount": 1,
        "content": "I got confused with option B and C , but Lets think in C option when I will use system manager to trigger SNS I can simply use eventbridge run that checks for config rule compliance change , IF compliance changes then as a target we will specify SNS.\nYes , We can also specify system manager automation document to trigger sns but why I will use it I will directly use SNS.\n\nSo from above I still by looking words B is correct option. Main reason is you do not need system manager here to trigger SNS.\n\nPlus there is no mention for eventbridge rule that will trigger system manager , from config we cannot directly trigger it."
      },
      {
        "date": "2024-01-27T04:10:00.000Z",
        "voteCount": 1,
        "content": "I got  confused with option B and C , but Lets think in C option when I will use system manager to trigger SNS I can simply use eventbridge run that checks for config rule compliance change , IF compliance changes then as a target we will specify SNS.\nYes , We can also specify system manager automation document to trigger sns but why I will use it I will directly use SNS.\n\nSo from above I still by looking words B is correct option. Main reason is you do not need system manager here to trigger SNS."
      },
      {
        "date": "2023-12-31T04:49:00.000Z",
        "voteCount": 2,
        "content": "This is the type of thing that AWS Config is used for."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/amazon/view/105336-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has migrated its container-based applications to Amazon EKS and want to establish automated email notifications. The notifications sent to each email address are for specific activities related to EKS components. The solution will include Amazon SNS topics and an AWS Lambda function to evaluate incoming log events and publish messages to the correct SNS topic.<br>Which logging solution will support these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription filter for each component with Lambda as the subscription feed destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CloudWatch Logs to log the EKS components. Create CloudWatch Logs Insights queries linked to Amazon EventBridge events that invoke Lambda.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon S3 logging for the EKS components. Configure an Amazon CloudWatch subscription filter for each component with Lambda as the subscription feed destination.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon S3 logging for the EKS components. Configure S3 PUT Object event notifications with AWS Lambda as the destination."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-19T14:43:00.000Z",
        "voteCount": 14,
        "content": "Correct Answer is A. \nExplanation:\nAmazon EKS integrates with CloudWatch Logs to provide detailed logs of the state and execution of the services in the cluster. CloudWatch subscription filters can be used to route specific log events from a CloudWatch Logs group to a Lambda function. The Lambda function can then process the events and publish notifications to the appropriate Amazon SNS topic."
      },
      {
        "date": "2024-03-08T06:13:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#LambdaFunctionExample\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html"
      },
      {
        "date": "2024-03-05T11:49:00.000Z",
        "voteCount": 1,
        "content": "AWS EKS itself does not offer native S3 logging for container logs. CloudWatch Logs Insights queries cannot directly link to Amazon EventBridge events. So the answer here is A."
      },
      {
        "date": "2024-02-19T18:13:00.000Z",
        "voteCount": 1,
        "content": "CloudWatch Logs subscription filtering is a feature that allows capture log data in real time and forward it to other AWS services such as Kinesis Data Firehose, Kinesis Streams, and Lambda."
      },
      {
        "date": "2024-01-29T09:45:00.000Z",
        "voteCount": 2,
        "content": "A is correct: Use cloudwatch logs to collect logs from EKS. Use subcription filter to filter out logs and only send relevant logs to lambda to trigger it. \nB: CloudWatch Logs Insights is for data analysis. Additionally, using EventBridge events to trigger lambda incur costs\nC and D: Amazon S3 logging is used for monitoring actions on S3 itself, not EKS"
      },
      {
        "date": "2023-12-20T21:59:00.000Z",
        "voteCount": 4,
        "content": "A is right \nC, D are wrong , because there is not integration in EKS to send logs to s3. \nB is for log analysis , and aggreation"
      },
      {
        "date": "2023-11-19T10:59:00.000Z",
        "voteCount": 1,
        "content": "I don't have a technical reason but others dumps shows B as the Answer"
      },
      {
        "date": "2023-11-19T11:20:00.000Z",
        "voteCount": 2,
        "content": "No, sorry, this was for the previous questions"
      },
      {
        "date": "2023-06-15T06:07:00.000Z",
        "voteCount": 1,
        "content": "A, metric filter can call Lambda."
      },
      {
        "date": "2023-05-31T06:18:00.000Z",
        "voteCount": 1,
        "content": "certainly cloudwatch logs metric filter A"
      },
      {
        "date": "2023-05-01T19:24:00.000Z",
        "voteCount": 1,
        "content": "A. Enable Amazon CloudWatch Logs to log the EKS components. Create a CloudWatch subscription filter for each component with Lambda as the subscription feed destination.\n\nThis solution involves enabling Amazon CloudWatch Logs to log the EKS components and creating a CloudWatch subscription filter for each component with AWS Lambda as the subscription feed destination. This approach will allow the Lambda function to evaluate incoming log events and publish messages to the correct Amazon SNS topic. Amazon SNS can then send email notifications to each email address based on the messages it receives from the corresponding SNS topic."
      },
      {
        "date": "2023-04-15T08:15:00.000Z",
        "voteCount": 1,
        "content": "A, clear"
      },
      {
        "date": "2023-04-14T15:40:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2023-04-14T11:51:00.000Z",
        "voteCount": 1,
        "content": "Amazon CloudWatch Logs can log the EKS components, and subscription filters can be created for each component with AWS Lambda as the subscription feed destination. The Lambda function can evaluate incoming log events and publish messages to the appropriate Amazon SNS topic, enabling automated email notifications to be sent. Therefore, option A is the correct solution. Option C is incorrect because Amazon S3 logging is not designed for logging EKS components."
      },
      {
        "date": "2023-04-05T14:25:00.000Z",
        "voteCount": 3,
        "content": "A for sure"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/amazon/view/105337-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is implementing an Amazon Elastic Container Service (Amazon ECS) cluster to run its workload. The company architecture will run multiple ECS services on the cluster. The architecture includes an Application Load Balancer on the front end and uses multiple target groups to route traffic.<br>A DevOps engineer must collect application and access logs. The DevOps engineer then needs to send the logs to an Amazon S3 bucket for near-real-time analysis.<br>Which combination of steps must the DevOps engineer take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownload the Amazon CloudWatch Logs container instance from AWS. Configure this instance as a task. Update the application service definitions to include the logging task.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch Logs agent on the ECS instances. Change the logging driver in the ECS task definition to awslogs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EventBridge to schedule an AWS Lambda function that will run every 60 seconds and will run the Amazon CloudWatch Logs create-export-task command. Then point the output to the logging S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate access logging on the ALB. Then point the ALB directly to the logging S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate access logging on the target groups that the ECS services use. Then send the logs directly to the logging S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Firehose delivery stream that has a destination of the logging S3 bucket. Then create an Amazon CloudWatch Logs subscription filter for Kinesis Data Firehose.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BDF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDF",
        "count": 28,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-19T14:47:00.000Z",
        "voteCount": 11,
        "content": "Explanation:\n\nOption B is correct because you can change the logging driver in the ECS task definition to awslogs, which will direct the logs to Amazon CloudWatch Logs. Then, the logs can be forwarded to the Amazon S3 bucket.\n\nOption D is correct because enabling access logging on the Application Load Balancer (ALB) allows the collection of access logs that can be sent directly to an S3 bucket.\n\nOption F is correct because you can create an Amazon Kinesis Data Firehose delivery stream that can deliver logs from CloudWatch Logs directly to an Amazon S3 bucket in near-real-time."
      },
      {
        "date": "2024-03-08T06:14:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-logging-monitoring.html"
      },
      {
        "date": "2024-02-19T18:44:00.000Z",
        "voteCount": 1,
        "content": "Enable access logging using the ALB management console, CLI, or API. Specify the S3 bucket where the logs will be stored and, if necessary, set the log file prefix (e.g., production, staging.) to store the logs in different paths within the bucket."
      },
      {
        "date": "2024-01-29T10:00:00.000Z",
        "voteCount": 3,
        "content": "BDF: There are two types of log that needs to be collected\nB: push app log to Cloudwatch log\nD: push access log to S3\nF: using Kinesis to push app log from cloudwatch log to S3 in near real-time\n\nA: wrong - we need cloudwatch agent, not container instance\nC: No need to use event bridge and lambda to trigger cloudwatch log to push log to s3. \nE: access logs lie in ALB, not ECS services."
      },
      {
        "date": "2023-12-20T22:13:00.000Z",
        "voteCount": 2,
        "content": "BDF is the answer . \nbtw, can't we use cloudwatch ingists to collet the logs  from containers in ecs there days , and then usign the subscription filter we can sends those logs to s3. \nwithout having to install cloud watch agent."
      },
      {
        "date": "2023-12-10T20:19:00.000Z",
        "voteCount": 1,
        "content": "Real time. so not E"
      },
      {
        "date": "2023-06-15T06:44:00.000Z",
        "voteCount": 1,
        "content": "BDF makes sense. E is certainly wrong."
      },
      {
        "date": "2023-05-30T09:54:00.000Z",
        "voteCount": 2,
        "content": "BDF\n\nAccess logs cannot be configured by ALB target group\n\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-access-logs.html"
      },
      {
        "date": "2023-05-27T00:03:00.000Z",
        "voteCount": 1,
        "content": "Option B sends data to the Cloudwatch Log. This issue requires that logs be collected in S3."
      },
      {
        "date": "2023-05-01T19:57:00.000Z",
        "voteCount": 2,
        "content": "Answer is bdf"
      },
      {
        "date": "2023-04-15T08:20:00.000Z",
        "voteCount": 4,
        "content": "B - get application logs to CW\nD - get access logs to S3\nF - get application logs from CW to S3 in near-real time"
      },
      {
        "date": "2023-04-14T11:58:00.000Z",
        "voteCount": 1,
        "content": "Option BDE can be cumbersome to manage in a large environment and may not be ideal for applications that generate large amounts of logs. Option BDF, on the other hand, captures both application and access logs, and uses the CloudWatch Logs driver to stream logs directly to CloudWatch Logs. This solution is more scalable as it does not require the CloudWatch Logs agent to be installed on each instance, and it can capture logs from multiple ECS tasks running on the same instance. In addition, the logs can be sent to an S3 bucket using a Kinesis Data Firehose delivery stream, which provides near-real-time analysis capabilities."
      },
      {
        "date": "2023-04-05T14:32:00.000Z",
        "voteCount": 2,
        "content": "B D F for me"
      },
      {
        "date": "2023-04-05T15:12:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/using_cloudwatch_logs.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/amazon/view/106198-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company that uses electronic health records is running a fleet of Amazon EC2 instances with an Amazon Linux operating system. As part of patient privacy requirements, the company must ensure continuous compliance for patches for operating system and applications running on the EC2 instances.<br>How can the deployments of the operating system and application patches be automated using a default and custom repository?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to create a new patch baseline including the custom repository. Run the AWS-RunPatchBaseline document using the run command to verify and install patches.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Direct Connect to integrate the corporate repository and deploy the patches using Amazon CloudWatch scheduled events, then use the CloudWatch dashboard to create reports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse yum-config-manager to add the custom repository under /etc/yum.repos.d and run yum-config-manager-enable to activate the repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to create a new patch baseline including the corporate repository. Run the AWS-AmazonLinuxDefaultPatchBaseline document using the run command to verify and install patches."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-19T22:45:00.000Z",
        "voteCount": 5,
        "content": "AWS-AmazonLinuxDefaultPatchBaseline: defines which patches should be applied and which should be avoided.\nAWS-RunPatchBaseline: provides commands to actually run the patching process on the instance."
      },
      {
        "date": "2024-01-29T10:14:00.000Z",
        "voteCount": 4,
        "content": "A is correct: AWS system manager and AWS-RunPatchBaseline to utilize a default and custom repo\nB and C are irrelevant\nD: AWS-AmazonLinuxDefaultPatchBaseline: this baseline has \"default\" in its name, it is a predefined baseline and cannot work with a custom repo"
      },
      {
        "date": "2024-01-06T04:42:00.000Z",
        "voteCount": 2,
        "content": "Here are predefined documents that can not be modified (includes AWS-AmazonLinuxDefaultPatchBaseline)\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-predefined-and-custom-patch-baselines.html#patch-manager-baselines-custom\nAnd here is about the AWS-RunPatchBaseline\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/patch-manager-aws-runpatchbaseline.html"
      },
      {
        "date": "2024-01-06T04:42:00.000Z",
        "voteCount": 1,
        "content": "the Answer is A"
      },
      {
        "date": "2023-12-20T22:22:00.000Z",
        "voteCount": 3,
        "content": "I was confused between A and D , i choose A instictinvily, D statement sounds like it only going to install default package for linux not from custom repo we add . But not sure any one can clarify"
      },
      {
        "date": "2023-06-15T06:47:00.000Z",
        "voteCount": 4,
        "content": "A, SSM allows inclusion of custom repositories."
      },
      {
        "date": "2023-05-01T20:00:00.000Z",
        "voteCount": 1,
        "content": "A is it"
      },
      {
        "date": "2023-04-14T15:48:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-04-14T12:01:00.000Z",
        "voteCount": 1,
        "content": "To automate the deployment of operating system and application patches using a default and custom repository in Amazon EC2 instances with Amazon Linux operating systems, you can use AWS Systems Manager. You can create a new patch baseline in Systems Manager that includes the custom repository, then run the AWS-RunPatchBaseline document using the run command to verify and install patches. This allows you to ensure continuous compliance for patches while also automating the patch deployment process."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/amazon/view/106213-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is using AWS CodePipeline to automate its release pipeline. AWS CodeDeploy is being used in the pipeline to deploy an application to Amazon Elastic Container Service (Amazon ECS) using the blue/green deployment model. The company wants to implement scripts to test the green version of the application before shifting traffic. These scripts will complete in 5 minutes or less. If errors are discovered during these tests, the application must be rolled back.<br>Which strategy will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a stage to the CodePipeline pipeline between the source and deploy stages. Use AWS CodeBuild to create a runtime environment and build commands in the buildspec file to invoke test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a stage to the CodePipeline pipeline between the source and deploy stages. Use this stage to invoke an AWS Lambda function that will run the test scripts. If errors are found, use the aws deploy stop-deployment command to stop the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTestTraffic lifecycle event to invoke an AWS Lambda function to run the test scripts. If errors are found, exit the Lambda function with an error to initiate rollback.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a hooks section to the CodeDeploy AppSpec file. Use the AfterAllowTraffic lifecycle event to invoke the test scripts. If errors are found, use the aws deploy stop-deployment CLI command to stop the deployment."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-01T20:03:00.000Z",
        "voteCount": 10,
        "content": "Add a hooks section to the CodeDeploy AppSpec file. The AppSpec file is a YAML file that describes how to deploy an application to Amazon ECS using CodeDeploy. We can use the AfterAllowTestTraffic lifecycle event to run the test scripts. This event is triggered after the new version of the application is deployed, and before traffic is shifted to the new version.\n\nIn the AfterAllowTestTraffic lifecycle event, invoke an AWS Lambda function to run the test scripts. The Lambda function can be written in any programming language supported by Lambda, such as Python, Node.js, or Java.\n\nIf the test scripts detect any errors, exit the Lambda function with an error code. This will cause the deployment to fail, and CodeDeploy will initiate a rollback."
      },
      {
        "date": "2024-03-06T06:47:00.000Z",
        "voteCount": 1,
        "content": "AfterAllowTestTraffic lifecycle event in the hooks section will not shift the whole traffic to the green application but only a small percentage of traffic to the newly deployed version. C is the answer"
      },
      {
        "date": "2024-02-20T00:44:00.000Z",
        "voteCount": 2,
        "content": "CodeDeploy Blue/Green deployments, the AfterAllowTestTraffic hook is triggered after the test traffic redirection to the new version (Green) is set. Additional verification, testing, or other custom actions can be automated by executing Lambda functions at this time."
      },
      {
        "date": "2024-01-29T10:25:00.000Z",
        "voteCount": 3,
        "content": "C is correct: we can initiate the script using lambda for advanced features\nA and B are wrong: Both trigger the test script befor deploy stages\nD is wrong: It only stops the deployment, not rollback it"
      },
      {
        "date": "2023-08-13T02:28:00.000Z",
        "voteCount": 1,
        "content": "C is right."
      },
      {
        "date": "2023-06-15T07:03:00.000Z",
        "voteCount": 4,
        "content": "C is the right answer.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#appspec-hooks-ecs"
      },
      {
        "date": "2023-05-01T20:03:00.000Z",
        "voteCount": 1,
        "content": "The correct solution to meet these requirements is option C.\n\nExplanation:\n\nIn this scenario, the requirement is to add scripts to test the green version of the application before shifting traffic. These scripts should be executed quickly and, in case of errors, the application must be rolled back. To achieve this, we can use the following steps:"
      },
      {
        "date": "2023-04-15T08:28:00.000Z",
        "voteCount": 1,
        "content": "Lifecycle event hooks for an Amazon ECS deployment:\n\nAfterAllowTraffic \u2013 Use to run tasks after the second target group serves traffic to the replacement task set. The results of a hook function at this lifecycle event can trigger a rollback."
      },
      {
        "date": "2023-04-15T08:29:00.000Z",
        "voteCount": 1,
        "content": "Correction: \nAfterAllowTestTraffic \u2013 Use to run tasks after the test listener serves traffic to the replacement task set. The results of a hook function at this point can trigger a rollback."
      },
      {
        "date": "2023-04-14T15:53:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/amazon/view/106199-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Storage Gateway in file gateway mode in front of an Amazon S3 bucket that is used by multiple resources. In the morning when business begins, users do not see the objects processed by a third party the previous evening. When a DevOps engineer looks directly at the S3 bucket, the data is there, but it is missing in Storage Gateway.<br>Which solution ensures that all the updated third-party files are available in the morning?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a nightly Amazon EventBridge event to invoke an AWS Lambda function to run the RefreshCache command for Storage Gateway.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstruct the third party to put data into the S3 bucket using AWS Transfer for SFTP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify Storage Gateway to run in volume gateway mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse S3 Same-Region Replication to replicate any changes made directly in the S3 bucket to Storage Gateway."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-19T14:49:00.000Z",
        "voteCount": 15,
        "content": "Explanation:\n\nAWS Storage Gateway's file gateway mode provides a bridge between your on-premises servers and Amazon S3. File gateway caches frequently accessed files in your on-premises environment to provide low-latency access. However, if the S3 bucket's data is modified by another service, the cache does not automatically refresh. Thus, to ensure all the updated third-party files are available in the morning, you can use an AWS Lambda function triggered by Amazon EventBridge to run the RefreshCache command for Storage Gateway. This will ensure the cache is updated with the latest changes."
      },
      {
        "date": "2023-04-15T08:34:00.000Z",
        "voteCount": 12,
        "content": "A: refresh cache: https://repost.aws/knowledge-center/storage-gateway-s3-changes-not-showing"
      },
      {
        "date": "2023-11-15T06:11:00.000Z",
        "voteCount": 3,
        "content": "Thanks for this.\nAlso found https://repost.aws/knowledge-center/storage-gateway-automate-refreshcache\n\nStorage Gateway allows you to automate the RefreshCache operation based on a Time To Live (TTL) value. TTL is the length of time since the last refresh. When a user accesses the file directory after the TTL value, the file gateway refreshes the directory's contents from the S3 bucket. Valid TTL values for automating the RefreshCache operation range from 300 seconds to 2,592,000 seconds (5 minutes to 30 days)."
      },
      {
        "date": "2024-05-30T21:12:00.000Z",
        "voteCount": 1,
        "content": "Read and concede:\n\"Configure an automated cache refresh schedule using AWS Lambda with an Amazon CloudWatch rule\"\nhttps://docs.aws.amazon.com/filegateway/latest/files3/refresh-cache.html#auto-refresh-lambda-procedure"
      },
      {
        "date": "2023-08-15T07:59:00.000Z",
        "voteCount": 1,
        "content": "where is it saying files are written directly to s3 ?"
      },
      {
        "date": "2023-11-23T12:22:00.000Z",
        "voteCount": 3,
        "content": "You do make a point but if you read the phrase \" When a DevOps engineer looks directly at the S3 bucket \" it kinda implies besides you dont have any other better choice anyway. if you look at user \"ele\" comments and follow the link below it will get clear[hope that helps];\nhttps://repost.aws/knowledge-center/storage-gateway-s3-changes-not-showing"
      },
      {
        "date": "2023-08-13T03:02:00.000Z",
        "voteCount": 1,
        "content": "A is right.\nStorage Gateway updates the file share cache automatically when you write files to the cache\nlocally using the file share. However, Storage Gateway doesn't automatically update the cache\nwhen you upload a file directly to Amazon S3. When you do this, you must perform a\nRefreshCache operation to see the changes on the file share."
      },
      {
        "date": "2023-06-15T07:10:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2023-05-01T20:04:00.000Z",
        "voteCount": 2,
        "content": "The issue appears to be related to the Storage Gateway cache not being updated. To ensure that all the updated third-party files are available in the morning, you can use the RefreshCache API to manually refresh the cache or configure automatic cache refresh.\n\nOption A is a possible solution to configure automatic cache refresh, but it is not necessary to run the RefreshCache command every night if you can ensure that cache refresh occurs frequently enough to meet your requirements."
      },
      {
        "date": "2023-04-14T15:56:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-04-14T12:03:00.000Z",
        "voteCount": 1,
        "content": "Option B appears to be the correct choice. Configuring the third party to put data into the S3 bucket using AWS Transfer for SFTP would ensure that the data is immediately available in both the S3 bucket and Storage Gateway, avoiding any potential caching issues. Option A of configuring a nightly event to refresh the cache may not be an optimal solution as it could result in stale data being served during the day."
      },
      {
        "date": "2023-05-30T10:56:00.000Z",
        "voteCount": 1,
        "content": "Transfer SFTP has the same effect in this case as adding files to S3 with PutObject. The cache in the storage gateway would not be updated, requiring the same refresh as in option A."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/amazon/view/106261-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer needs to back up sensitive Amazon S3 objects that are stored within an S3 bucket with a private bucket policy using S3 cross-Region replication functionality. The objects need to be copied to a target bucket in a different AWS Region and account.<br>Which combination of actions should be performed to enable this replication? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replication IAM role in the source account\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replication I AM role in the target account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd statements to the source bucket policy allowing the replication IAM role to replicate objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd statements to the target bucket policy allowing the replication IAM role to replicate objects.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replication rule in the source bucket to enable the replication.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a replication rule in the target bucket to enable the replication."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T21:30:00.000Z",
        "voteCount": 7,
        "content": "S3 cross-Region replication (CRR) automatically replicates data between buckets across different AWS Regions. To enable CRR, you need to add a replication configuration to your source bucket that specifies the destination bucket, the IAM role, and the encryption type (optional). You also need to grant permissions to the IAM role to perform replication actions on both the source and destination buckets. Additionally, you can choose the destination storage class and enable additional replication options such as S3 Replication Time Control (S3 RTC) or S3 Batch Replication."
      },
      {
        "date": "2024-05-31T15:55:00.000Z",
        "voteCount": 2,
        "content": "Tricky question because they are trying to get one to confuse the \"enable\" replicaton \"role\"/policy (\"rule\") in source account with the \"allow\" replicaton role/\"policy\" in target account.  These references helped me work up some summary steps:\nSteps to configure S3 replication between different accounts\n1. Create source and destination buckets in different accounts and regions (acctA, acctB)\n2. Enable versioning on the buckets (acctA, acctB)\n3. Create IAM role and attach a policy granting S3 permission to replicate objects (acctA)\n4. Add the replication configuration to source bucket (acctA)\n5. Add bucket \"policy on the destination bucket to allow\" objects replication (acctB)(req. 2nd role)\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough1.html#enable-replication\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html"
      },
      {
        "date": "2024-05-31T16:09:00.000Z",
        "voteCount": 2,
        "content": "Source Account: (Source Bucket)(Versioning)(Role/Policy to \"Enable\" Replicaton)\nTarget Account: (Target Bucket)(Versioning)(Role/Policy to \"Allow\"  Replicaton)"
      },
      {
        "date": "2024-01-29T19:59:00.000Z",
        "voteCount": 2,
        "content": "ADF is correct: this task is done by S3 itself\nA: Create role in the source to allow S3 access permission\nD: add policy to allow repication in the target\nE: enable replication in the source"
      },
      {
        "date": "2023-09-20T08:27:00.000Z",
        "voteCount": 2,
        "content": "ADE make sense."
      },
      {
        "date": "2023-06-15T07:16:00.000Z",
        "voteCount": 2,
        "content": "ADE make sense."
      },
      {
        "date": "2023-05-01T20:10:00.000Z",
        "voteCount": 2,
        "content": "Confirmed"
      },
      {
        "date": "2023-04-15T19:50:00.000Z",
        "voteCount": 1,
        "content": "ADE confirmed!"
      },
      {
        "date": "2023-04-15T08:39:00.000Z",
        "voteCount": 3,
        "content": "ADE\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-walkthrough-2.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/amazon/view/106307-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has multiple member accounts that are part of an organization in AWS Organizations. The security team needs to review every Amazon EC2 security group and their inbound and outbound rules. The security team wants to programmatically retrieve this information from the member accounts using an AWS Lambda function in the management account of the organization.<br>Which combination of access changes will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trust relationship that allows users in the member accounts to assume the management account IAM role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trust relationship that allows users in the management account to assume the IAM roles of the member accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in each member account that has access to the AmazonEC2ReadOnlyAccess managed policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an I AM role in each member account to allow the sts:AssumeRole action against the management account IAM role's ARN.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an I AM role in the management account that allows the sts:AssumeRole action against the member account IAM role's ARN.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role in the management account that has access to the AmazonEC2ReadOnlyAccess managed policy."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "ACE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "ACD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-19T14:53:00.000Z",
        "voteCount": 8,
        "content": "Explanation:\n\n(B) The trust relationship enables an IAM entity (user, group, or role) to assume a role. In this case, the entities in the management account need to assume roles in the member accounts.\n\n(C) The IAM role in each member account should have a policy attached that grants read-only access to EC2 instances. The AmazonEC2ReadOnlyAccess managed policy provides this access.\n\n(E) An IAM role in the management account should be created that has the permission to perform the sts:AssumeRole action against the member account IAM role's ARN. This allows entities assuming this role to switch to the roles in the member accounts and perform actions according to the permissions of those roles."
      },
      {
        "date": "2024-01-29T20:34:00.000Z",
        "voteCount": 4,
        "content": "BCE are correct: \nB: create trust relationship for management to assume role in member accounts\nC: create role in member account that has access to AmazoneEC2\nE: Create IAM role in management account that allow access to member account IAM role"
      },
      {
        "date": "2023-11-26T09:21:00.000Z",
        "voteCount": 1,
        "content": "The security team wants to programmatically retrieve this information from the member accounts using an AWS Lambda function in the management account of the organization.\n\nReadOnlyAccess and option B grant the assumeRole\nBesides that the correct resource is \"IAM\" not \"I AM\" So BCF is correct"
      },
      {
        "date": "2023-08-28T00:48:00.000Z",
        "voteCount": 1,
        "content": "B- Member accounts should trust Management account\nC- Memeber accounts should have a Role athat has the necessary permission\nE- Managment account should have a IAM user account that has stsAssume role permission for the roles created in member accounts"
      },
      {
        "date": "2023-08-19T15:00:00.000Z",
        "voteCount": 2,
        "content": "BCE is wrong. They want to programmatically therefore B is definitenly wrong. The Lambda function IAM Role ARN in the management account needs to be able to assume a role in the member account that has the AmazonEC2ReadOnlyAccess attached to it. Therefore, I will go with C, D, E"
      },
      {
        "date": "2023-11-02T14:14:00.000Z",
        "voteCount": 1,
        "content": "D is clearly wrong. You are running your lambda function to get details in management account. The IAM role should be in management account with sts:AssumeRole permission to assume IAM roles in member accounts"
      },
      {
        "date": "2023-07-25T00:36:00.000Z",
        "voteCount": 1,
        "content": "BCE correct"
      },
      {
        "date": "2023-06-15T07:21:00.000Z",
        "voteCount": 1,
        "content": "BCE is right."
      },
      {
        "date": "2023-05-30T11:03:00.000Z",
        "voteCount": 1,
        "content": "B, C and E"
      },
      {
        "date": "2023-05-20T05:51:00.000Z",
        "voteCount": 2,
        "content": "B, C and E"
      },
      {
        "date": "2023-05-16T15:32:00.000Z",
        "voteCount": 2,
        "content": "A:By creating a trust relationship that allows users in the member accounts to assume the IAM role in the management account, they will have the necessary permissions to access resources and retrieve the required information.\n\nC:To grant the necessary permissions for retrieving information about EC2 security groups, an IAM role should be created in each member account. This role should have the AmazonEC2ReadOnlyAccess managed policy attached, which provides the required permissions.\n\nE:In the management account, an IAM role should be created that allows assuming the IAM role in the member accounts. This role should have the necessary permissions to perform the sts:AssumeRole action against the ARN of the IAM roles in the member accounts."
      },
      {
        "date": "2023-05-13T03:20:00.000Z",
        "voteCount": 1,
        "content": "BCE will create correct cross account permission"
      },
      {
        "date": "2023-05-04T02:44:00.000Z",
        "voteCount": 1,
        "content": "acd looks good)"
      },
      {
        "date": "2023-05-02T06:22:00.000Z",
        "voteCount": 1,
        "content": "ACE I guess\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html"
      },
      {
        "date": "2023-05-01T20:12:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2023-04-15T19:54:00.000Z",
        "voteCount": 2,
        "content": "Ill go with BCF"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/amazon/view/105438-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A space exploration company receives telemetry data from multiple satellites. Small packets of data are received through Amazon API Gateway and are placed directly into an Amazon Simple Queue Service (Amazon SQS) standard queue. A custom application is subscribed to the queue and transforms the data into a standard format.<br>Because of inconsistencies in the data that the satellites produce, the application is occasionally unable to transform the data. In these cases, the messages remain in the SQS queue. A DevOps engineer must develop a solution that retains the failed messages and makes them available to scientists for review and future processing.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Lambda to poll the SQS queue and invoke a Lambda function to check whether the queue messages are valid. If validation fails, send a copy of the data that is not valid to an Amazon S3 bucket so that the scientists can review and correct the data. When the data is corrected, amend the message in the SQS queue by using a replay Lambda function with the corrected data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the SQS standard queue to an SQS FIFO queue. Configure AWS Lambda to poll the SQS queue every 10 minutes by using an Amazon EventBridge schedule. Invoke the Lambda function to identify any messages with a SentTimestamp value that is older than 5 minutes, push the data to the same location as the application's output location, and remove the messages from the queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SQS dead-letter queue. Modify the existing queue by including a redrive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure API Gateway to send messages to different SQS virtual queues that are named for each of the satellites. Update the application to use a new virtual queue for any data that it cannot transform, and send the message to the new virtual queue. Instruct the scientists to use the virtual queue to review the data that is not valid. Reprocess this data at a later time."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-08T06:20:00.000Z",
        "voteCount": 2,
        "content": "Create an SQS dead-letter queue. Modify the existing queue by including a re-drive policy that sets the Maximum Receives setting to 1 and sets the dead-letter queue ARN to the ARN of the newly created queue. Instruct the scientists to use the dead-letter queue to review the data that is not valid. Reprocess this data at a later time."
      },
      {
        "date": "2024-03-07T10:30:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. Lambda function is required for automated fixing of the invalid message data and hence A is the right choice here."
      },
      {
        "date": "2024-03-08T00:03:00.000Z",
        "voteCount": 1,
        "content": "This is not a good approach because it requires unifying the validation logic of the custom application and Lambda function, requires updating both the custom application and Lambda when data specifications change, and requires that the timing of those updates be the same from the SQS perspective, making the deployment process more complex and devops cost expensive. BTW, failed messages are reviewed by scientists, and there is no requirement that they be automatically fix by the program."
      },
      {
        "date": "2024-02-21T04:15:00.000Z",
        "voteCount": 2,
        "content": "A Dead Letter Queue (DLQ) can be the destination queue for messages that cannot be successfully processed by other queues. DLQs are used to analyze why a message failed or to isolate problem messages."
      },
      {
        "date": "2024-01-29T20:37:00.000Z",
        "voteCount": 1,
        "content": "C is correct: Use dead letter queue and config maximum receives is the right way"
      },
      {
        "date": "2024-01-07T11:56:00.000Z",
        "voteCount": 1,
        "content": "definitely C"
      },
      {
        "date": "2023-12-10T04:03:00.000Z",
        "voteCount": 3,
        "content": "This is DLQ use case. So, its 100% C"
      },
      {
        "date": "2023-12-01T05:19:00.000Z",
        "voteCount": 4,
        "content": "everyone votes C but answer seems as A. which one correct? should we trust to voters or examtopic? :D"
      },
      {
        "date": "2023-10-29T09:38:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C . This is a use case for Dead Letter Queue"
      },
      {
        "date": "2023-09-20T08:41:00.000Z",
        "voteCount": 1,
        "content": "classic DLQ usecase"
      },
      {
        "date": "2023-08-27T05:07:00.000Z",
        "voteCount": 1,
        "content": "C - DLQ"
      },
      {
        "date": "2023-08-26T05:00:00.000Z",
        "voteCount": 1,
        "content": "C - DLQ"
      },
      {
        "date": "2023-07-27T07:22:00.000Z",
        "voteCount": 1,
        "content": "DevOps is about automation! Variant C says: \"Instruct scientists... \" :D\nSo variant A is the best among other"
      },
      {
        "date": "2023-07-12T00:27:00.000Z",
        "voteCount": 4,
        "content": "DLQ is the right solution. SQS is one to one! So Lambda make no sense."
      },
      {
        "date": "2023-07-08T03:27:00.000Z",
        "voteCount": 1,
        "content": "Always do with DLQ for failed deliveries. C all the way"
      },
      {
        "date": "2023-06-15T07:27:00.000Z",
        "voteCount": 1,
        "content": "C answer with DLQ is a right solution."
      },
      {
        "date": "2023-05-30T11:04:00.000Z",
        "voteCount": 1,
        "content": "The perfect case for a dead-letter queue"
      },
      {
        "date": "2023-05-23T23:52:00.000Z",
        "voteCount": 1,
        "content": "SQS DLQ needed"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/amazon/view/106215-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company wants to use AWS CloudFormation for infrastructure deployment. The company has strict tagging and resource requirements and wants to limit the deployment to two Regions. Developers will need to deploy multiple versions of the same application.<br>Which solution ensures resources are deployed in accordance with company policy?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS Trusted Advisor checks to find and remediate unapproved CloudFormation StackSets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Formation drift detection operation to find and remediate unapproved CloudFormation StackSets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate CloudFormation StackSets with approved CloudFormation templates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS Service Catalog products with approved CloudFormation templates.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-10T04:07:00.000Z",
        "voteCount": 9,
        "content": "100% D.\n\nAWS Service Catalog lets you centrally manage your cloud resources to achieve governance at scale of your infrastructure as code (IaC) templates, written in CloudFormation or Terraform configurations. With AWS Service Catalog, you can meet your compliance requirements while making sure your customers can quickly deploy the cloud resources they need.\nhttps://aws.amazon.com/servicecatalog/\n\nall other service in other answer is not related."
      },
      {
        "date": "2024-08-17T17:59:00.000Z",
        "voteCount": 1,
        "content": "Service Catalogue is a like a internal marketplace for an organization in that accounts in that organization are limited to using only the resources describe in the product catalogue. For the use case described the best choice is using Service Catalogue."
      },
      {
        "date": "2024-08-01T01:53:00.000Z",
        "voteCount": 1,
        "content": "keywords: strict tagging, resource requirements a, limit the deployment\nAWS Service Catalog"
      },
      {
        "date": "2024-07-26T23:56:00.000Z",
        "voteCount": 1,
        "content": "D would be a better option, especially for developers, to abstract configuring the CloudFormation StackSets when launching applications with diverse versions. In AWS Service Catalog, they would just pick up the version and deploy. Everything would be set for them in the background including the CloudFormation StackSet with the version parameter and the tagging enforcement."
      },
      {
        "date": "2024-05-31T19:00:00.000Z",
        "voteCount": 1,
        "content": "I'd argue that the correct answer is to use Service Catalog and StackSets.  Option \"D:\" doesn't preclude using StackSets, it just doesn't mention it as part of the solution.  ServiceCatalog is the formal method to distribute standard solutions (such as CloudFormation StackSets)"
      },
      {
        "date": "2024-05-31T19:55:00.000Z",
        "voteCount": 1,
        "content": "\"AWS Service Catalog enables you to launch a product in one or more accounts and AWS Regions. To do this, administrators must apply a stack set constraint to the product with the accounts and Regions, where it can launch as a stack set.\"\nhttps://docs.aws.amazon.com/servicecatalog/latest/userguide/launch-stacksets.html"
      },
      {
        "date": "2024-03-27T01:55:00.000Z",
        "voteCount": 1,
        "content": "\"A provisioned Service Catalog product is an AWS CloudFormation stack\"\nReally confusing. I go with D..."
      },
      {
        "date": "2024-05-02T02:17:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/servicecatalog/latest/adminguide/catalogs_constraints_template-constraints.html Please check topic this , correct answer is C"
      },
      {
        "date": "2024-03-07T10:46:00.000Z",
        "voteCount": 3,
        "content": "AWS Service Catalog can be used to deploy resources to two regions (or even more) with the help of AWS CloudFormation StackSets. So Answer is C"
      },
      {
        "date": "2024-02-26T03:58:00.000Z",
        "voteCount": 3,
        "content": "Answer C\nIf rules are applied across multiple accounts, the StackSets feature is more suitable. The service catalog is used for provisioning new accounts under the AWS control tower."
      },
      {
        "date": "2024-02-22T18:41:00.000Z",
        "voteCount": 2,
        "content": "It's D"
      },
      {
        "date": "2024-02-21T04:34:00.000Z",
        "voteCount": 1,
        "content": "Service Catalog cannot ensures that the same application can be deployed to multiple regions."
      },
      {
        "date": "2024-01-29T20:46:00.000Z",
        "voteCount": 1,
        "content": "D is correct: Catalog product impose strict requirements for app deloyment. If using stacksets, devs can deploy app to everywhere without any restrictions"
      },
      {
        "date": "2024-02-11T08:01:00.000Z",
        "voteCount": 2,
        "content": "Correction: should be C, not D. The question mentions &lt;deployment to two Regions&gt;. Only stacksets can do this. Even AWS Service Catalog products. It cannot be used cross-region unless it is deployed by stackset"
      },
      {
        "date": "2024-01-14T18:33:00.000Z",
        "voteCount": 1,
        "content": "To restrict regions or accounts where catalog products can be deployed, refer to AWS Service Catalog Stack Set Constraints (https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints-stackset.html),"
      },
      {
        "date": "2023-11-16T06:56:00.000Z",
        "voteCount": 2,
        "content": "I will go with D.\nReference here:\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/manage-aws-service-catalog-products-in-multiple-aws-accounts-and-aws-regions.html"
      },
      {
        "date": "2023-11-06T05:19:00.000Z",
        "voteCount": 4,
        "content": "While stacksets and catalog can use in this case catalog can restrict the strick policy than stacksets. \nThe company has strict tagging and resource requirements.. So it's product catalog. in stacksets developers can modify the resources  since they can get hold the template."
      },
      {
        "date": "2023-10-28T10:46:00.000Z",
        "voteCount": 3,
        "content": "Answer is D.\n\"Developers will need to deploy\" multiple versions of the same application. So service catalog products will be best for developers."
      },
      {
        "date": "2023-10-18T07:33:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is: C. Create CloudFormation StackSets with approved CloudFormation templates.\n\nCloudFormation StackSets enable you to deploy stacks across multiple accounts and Regions using a single template. This allows you to enforce company policy by only allowing developers to use approved templates.\n\nAWS Service Catalog products can be used to launch approved CloudFormation templates, but they do not enforce the use of approved templates."
      },
      {
        "date": "2023-09-22T02:05:00.000Z",
        "voteCount": 2,
        "content": "The best solution to ensure that resources are deployed in accordance with company policy is to create CloudFormation StackSets with approved CloudFormation templates.\n\nCloudFormation StackSets allow you to create and manage stacks across multiple AWS accounts and Regions. You can specify the template to use when creating a StackSet, as well as any parameters and capabilities that the template requires.\n\nBy using approved CloudFormation templates, you can ensure that all resources deployed by the StackSet meet your company's tagging and resource requirements. You can also use StackSets to limit the deployment to two Regions.\n\nThe other options are not as effective:"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/amazon/view/106309-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company requires that its internally facing web application be highly available. The architecture is made up of one Amazon EC2 web server instance and one NAT instance that provides outbound internet access for updates and accessing public data.<br>Which combination of architecture adjustments should the company implement to achieve high availability? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the NAT instance to an EC2 Auto Scaling group that spans multiple Availability Zones. Update the route tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate additional EC2 instances spanning multiple Availability Zones. Add an Application Load Balancer to split the load between them.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Application Load Balancer in front of the EC2 instance. Configure Amazon CloudWatch alarms to recover the EC2 instance upon host failure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the NAT instance with a NAT gateway in each Availability Zone. Update the route tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the NAT instance with a NAT gateway that spans multiple Availability Zones. Update the route tables."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-27T02:44:00.000Z",
        "voteCount": 9,
        "content": "BD \nE Is incorrect see NAT gateway basics in https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html"
      },
      {
        "date": "2024-03-10T04:22:00.000Z",
        "voteCount": 3,
        "content": "Quoting the above link: \"If you have resources in multiple Availability Zones and they share one NAT gateway, and if the NAT gateway\u2019s Availability Zone is down, resources in the other Availability Zones lose internet access. To improve resiliency, create a NAT gateway in each Availability Zone, and configure your routing to ensure that resources use the NAT gateway in the same Availability Zone.\""
      },
      {
        "date": "2023-10-26T05:38:00.000Z",
        "voteCount": 8,
        "content": "B&amp;D\n\nNAT Gateway does not span multiple AZ. you must create foreach AZ for HA"
      },
      {
        "date": "2024-05-05T14:25:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/networking-and-content-delivery/using-nat-gateways-with-multiple-amazon-vpcs-at-scale/\nNAT Gateways within an AZ are automatically implemented with redundancy. However, while Amazon VPCs can span multiple AZs, each NAT Gateway operates within a single AZ. If the NAT Gateway fails, then connections with resources using that NAT Gateway also fail. Therefore, it's recommended to deploy one NAT Gateway in each AZ and routing traffic locally within the same AZ."
      },
      {
        "date": "2024-03-07T11:49:00.000Z",
        "voteCount": 1,
        "content": "Both NAT Gateway and NAT instance are regional resources. But NAT Gateway offers automatic deployment across Availability Zones, you might need to manually configure redundancy across Availability Zones for NAT Instances."
      },
      {
        "date": "2024-01-29T23:32:00.000Z",
        "voteCount": 2,
        "content": "B and D are correct: We need to span EC2 to multiple avai zones and replace nat instance with nat gateway in each zone\nB: span EC2 to multiple avai zones\nD: replace nat instance with nat gateway"
      },
      {
        "date": "2024-01-07T11:49:00.000Z",
        "voteCount": 1,
        "content": "Answer is B and D"
      },
      {
        "date": "2023-12-10T04:12:00.000Z",
        "voteCount": 5,
        "content": "Asnwer is B and D, \nNAT gateways are regional services and do not span across Availability Zones. So, E is completely wrong."
      },
      {
        "date": "2023-11-20T12:49:00.000Z",
        "voteCount": 6,
        "content": "NAT Gateway can't spans in multiple regions, only in one subnet, I just tried it using the AWS Console"
      },
      {
        "date": "2023-12-10T04:10:00.000Z",
        "voteCount": 1,
        "content": "yes ure correct! i can confirm this. So BD is the correct answer"
      },
      {
        "date": "2023-09-30T14:08:00.000Z",
        "voteCount": 1,
        "content": "B&amp;D https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-comparison.html"
      },
      {
        "date": "2023-09-20T08:45:00.000Z",
        "voteCount": 3,
        "content": "B &amp; D is correct\nNAT GW does not span across AZ, And has to be created in multi AZ for HA."
      },
      {
        "date": "2023-09-10T06:53:00.000Z",
        "voteCount": 4,
        "content": "E is wrong. Natgatway cannot multiple subnets/zones"
      },
      {
        "date": "2023-08-30T17:21:00.000Z",
        "voteCount": 5,
        "content": "BD correct.\nE: incorrect because NAT Gateway does not span multi AZ, you need to deploy it to different AZs. Not like LB that spans multiAZ automatically."
      },
      {
        "date": "2023-08-26T05:05:00.000Z",
        "voteCount": 2,
        "content": "Defo BD - Cannot be E as Nat Gateways sit in one subnet"
      },
      {
        "date": "2023-08-14T03:55:00.000Z",
        "voteCount": 3,
        "content": "B,D.\nE is wrong because NAT Gateway is deployed to a single public subnet (Cannot span multiple AZs)"
      },
      {
        "date": "2023-07-30T10:54:00.000Z",
        "voteCount": 4,
        "content": "E is wrong, NAT Gateway is a zonal resource."
      },
      {
        "date": "2023-07-16T03:54:00.000Z",
        "voteCount": 2,
        "content": "A NAT Gateway is spun up in a single subnet that lives in a AZ. So you cannot build a NAT GW that spans mulitple AZ's. You will need to build a NAT GW in EACH AZ to succeed. BD are the correct answers."
      },
      {
        "date": "2023-07-13T14:13:00.000Z",
        "voteCount": 4,
        "content": "Being honest DE are really similar \nBut BE looks more correct due to use the same language"
      },
      {
        "date": "2024-08-17T18:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct because it  says you should create instances in multiple AZs and then set up a load balancer to split traffic between them. E is wrong because NAT gateways cannot span availability zones."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/amazon/view/106310-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is building a multistage pipeline with AWS CodePipeline to build, verify, stage, test, and deploy an application. A manual approval stage is required between the test stage and the deploy stage. The development team uses a custom chat tool with webhook support that requires near-real-time notifications.<br>How should the DevOps engineer configure status updates for pipeline activity and approval requests to post to the chat tool?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Logs subscription that filters on CodePipeline Pipeline Execution State Change. Publish subscription events to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the chat webhook URL to the SNS topic, and complete the subscription validation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that is invoked by AWS CloudTrail events. When a CodePipeline Pipeline Execution State Change event is detected, send the event details to the chat webhook URL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change. Publish the events to an Amazon Simple Notification Service (Amazon SNS) topic. Create an AWS Lambda function that sends event details to the chat webhook URL. Subscribe the function to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the pipeline code to send the event details to the chat webhook URL at the end of each stage. Parameterize the URL so that each pipeline can send to a different URL based on the pipeline environment."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-30T21:28:00.000Z",
        "voteCount": 14,
        "content": "The DevOps engineer should configure status updates for pipeline activity and approval requests to post to the chat tool by creating an Amazon EventBridge rule that filters on CodePipeline Pipeline Execution State Change. The events should be published to an Amazon Simple Notification Service (Amazon SNS) topic, and an AWS Lambda function should be created to send event details to the chat webhook URL. The function should be subscribed to the SNS topic. Option C is the correct answer.\n\nOption A is incorrect because it suggests using CloudWatch Logs instead of EventBridge, which is not the optimal solution for this use case. Option B is incorrect because it suggests using CloudTrail instead of CodePipeline events, which is not relevant. Option D is incorrect because modifying the pipeline code is not necessary and adds unnecessary complexity."
      },
      {
        "date": "2024-09-04T12:43:00.000Z",
        "voteCount": 2,
        "content": "Exam Tip : If you see that question is related to an Event or Action --&gt; EventBridge"
      },
      {
        "date": "2024-02-21T04:58:00.000Z",
        "voteCount": 1,
        "content": "API calls related to AWS CodePipeline are logged by CloudTrail. However, changes to the execution state of CodePipeline are events, not API calls. These events can be captured via Amazon EventBridge."
      },
      {
        "date": "2024-01-30T00:05:00.000Z",
        "voteCount": 2,
        "content": "C is correct: Use lambda to send event detail to the chat webhook url. Subcribe lambda to SNS topic\nA: Log subscription filer is for logging, not event\nB: Should not use lamda with cloudtrail events\nD: no need to modify pipeline code to send event at the end of each stage"
      },
      {
        "date": "2024-01-30T00:07:00.000Z",
        "voteCount": 1,
        "content": "cloudtrail event cannot trigger lambda"
      },
      {
        "date": "2024-02-11T08:08:00.000Z",
        "voteCount": 1,
        "content": "A: no way to collect log from code pipeline"
      },
      {
        "date": "2023-06-15T07:36:00.000Z",
        "voteCount": 1,
        "content": "C makes most sene."
      },
      {
        "date": "2023-05-13T03:38:00.000Z",
        "voteCount": 1,
        "content": "C right"
      },
      {
        "date": "2023-04-15T20:02:00.000Z",
        "voteCount": 2,
        "content": "C it is"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/amazon/view/106311-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's application development team uses Linux-based Amazon EC2 instances as bastion hosts. Inbound SSH access to the bastion hosts is restricted to specific IP addresses, as defined in the associated security groups. The company's security team wants to receive a notification if the security group rules are modified to allow SSH access from any IP address.<br>What should a DevOps engineer do to meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule with a source of aws.cloudtrail and the event name AuthorizeSecurityGroupIngress. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon GuardDuty and check the findings for security groups in AWS Security Hub. Configure an Amazon EventBridge rule with a custom pattern that matches GuardDuty events with an output of NON_COMPLIANT. Define an Amazon Simple Notification Service (Amazon SNS) topic as the target.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config rule by using the restricted-ssh managed rule to check whether security groups disallow unrestricted incoming SSH traffic. Configure automatic remediation to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon Inspector. Include the Common Vulnerabilities and Exposures-1.1 rules package to check the security groups that are associated with the bastion hosts. Configure Amazon Inspector to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 78,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 37,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-13T04:33:00.000Z",
        "voteCount": 18,
        "content": "A is right.\nThe Config rule restricted-ssh will not check the ingress rule that use the CIDR other than 0.0.0.0/0 and not notify anyone."
      },
      {
        "date": "2023-12-12T13:54:00.000Z",
        "voteCount": 9,
        "content": "A would send a notification for ANY change in the security group. The question clearly states that wants only when 0.0.0.0/0 is allowed. Therefore, should be C."
      },
      {
        "date": "2024-04-13T03:10:00.000Z",
        "voteCount": 1,
        "content": "\"a notification if the security group rules are modified to allow SSH access from any IP address\" \nfrom any IP address =&gt; so A is correct, any change in SG should send noti"
      },
      {
        "date": "2023-06-19T12:53:00.000Z",
        "voteCount": 8,
        "content": "I'm going to have to go with A on this one:\nhttps://aws.plainenglish.io/detecting-modifications-to-aws-ec2-security-groups-2ef8989a3350\n\nhttps://repost.aws/knowledge-center/monitor-security-group-changes-ec2"
      },
      {
        "date": "2024-10-08T16:47:00.000Z",
        "voteCount": 1,
        "content": "C is the answer : https://docs.aws.amazon.com/config/latest/developerguide/notifications-for-AWS-Config.html"
      },
      {
        "date": "2024-08-05T06:09:00.000Z",
        "voteCount": 1,
        "content": "keywords: Inbound SSH access\nC restricted for SSH port (22) only from ANY address"
      },
      {
        "date": "2024-07-27T00:07:00.000Z",
        "voteCount": 1,
        "content": "A! \"AWS Config provides rules such as restricted-ssh that can be used to detect Security Groups that have SSH access open for any IP\"."
      },
      {
        "date": "2024-07-04T11:11:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2024-05-21T01:50:00.000Z",
        "voteCount": 1,
        "content": "I think keyword for C must be \"ALL\".\nANY means when new IP is added to security group, so SNS will be triggered"
      },
      {
        "date": "2024-08-17T20:08:00.000Z",
        "voteCount": 1,
        "content": "In the context of AWS when you see ANY IP address, it is probably referring to the 0.0.0.0/0 CIDR block which allows traffic from all or any IP address from the internet. When you use the restricteded-ssh managed rule, Security Groups will be labelled as NON_COMPLIANT when they allow unrestricted SSH traffic from anywhere or any IP address (0.0.0.0/0)."
      },
      {
        "date": "2024-05-13T02:06:00.000Z",
        "voteCount": 2,
        "content": "C makes way more sense from the way AWS wants us to do it"
      },
      {
        "date": "2024-05-01T10:24:00.000Z",
        "voteCount": 2,
        "content": "i vote for c"
      },
      {
        "date": "2024-04-12T08:05:00.000Z",
        "voteCount": 2,
        "content": "A. This is the correct solution because it leverages Amazon EventBridge to monitor for changes to the security group rules, specifically the AuthorizeSecurityGroupIngress event, which indicates that the security group rules have been modified to allow SSH access from any IP address.\n\nBy creating an EventBridge rule with the appropriate event pattern and defining an Amazon SNS topic as the target, the DevOps engineer can ensure that the security team receives a notification whenever the security group rules are modified in an undesirable way."
      },
      {
        "date": "2024-03-20T13:01:00.000Z",
        "voteCount": 2,
        "content": "Answer is C \nThe restricted-ssh managed rule in AWS Config helps ensure your bastion host security groups are locked down for SSH access. It specifically checks if incoming SSH traffic is accessible for the security groups.\nThe rule is considered COMPLIANT if:\nSSH access is not open to the public (meaning the rule doesn't find a security group allowing 0.0.0.0/0 for port 22).\nSSH access is restricted to specific IP addresses or security groups using CIDR notation (e.g., 10.0.0.0/16).\nIf the rule detects a security group allowing SSH access from anywhere (0.0.0.0/0), it triggers a NON_COMPLIANT status."
      },
      {
        "date": "2024-04-12T08:04:00.000Z",
        "voteCount": 1,
        "content": "Yeah, but has nothing to do with anyone changing it. A is your answer because it detects changes and sends out an email notification"
      },
      {
        "date": "2024-03-19T14:51:00.000Z",
        "voteCount": 3,
        "content": "restricted-ssh : The rule is COMPLIANT if the IP addresses of the incoming SSH traffic in the security groups are restricted (CIDR other than 0.0.0.0/0 or ::/0). Otherwise, NON_COMPLIANT.\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/restricted-ssh.html\n\nThat addresses exactly the requirement !"
      },
      {
        "date": "2024-03-14T20:31:00.000Z",
        "voteCount": 5,
        "content": "A is not meet the following requirements:\n`if the security group rules are modified to allow ssh access FROM ANY IP ADDRESS`"
      },
      {
        "date": "2024-03-08T06:33:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/premiumsupport/knowledge-center/monitor-security-group-changes-ec2/"
      },
      {
        "date": "2024-02-29T12:19:00.000Z",
        "voteCount": 5,
        "content": "Answer : C\nKeyword \"allow SSH access from any IP address\"\nA will send notification for any change made to the SG not just SSH"
      },
      {
        "date": "2024-02-29T08:27:00.000Z",
        "voteCount": 4,
        "content": "I'll go with C. AWS config can help with that SG change detection. Then, we can just send a notification."
      },
      {
        "date": "2024-02-19T21:30:00.000Z",
        "voteCount": 2,
        "content": "a is right~~~~\nhttps://aws.amazon.com/premiumsupport/knowlege-center/monitor-security-group-cahnge-ec2/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/amazon/view/106312-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps team manages an API running on-premises that serves as a backend for an Amazon API Gateway endpoint. Customers have been complaining about high response latencies, which the development team has verified using the API Gateway latency metrics in Amazon CloudWatch. To identify the cause, the team needs to collect relevant data without introducing additional latency.<br>Which actions should be taken to accomplish this? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the CloudWatch agent server side and configure the agent to upload relevant logs to CloudWatch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and upload those segments to X-Ray during each request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS X-Ray tracing in API Gateway, modify the application to capture request segments, and use the X-Ray daemon to upload segments to X-Ray.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the on-premises application to send log information back to API Gateway with each request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the on-premises application to calculate and upload statistical data relevant to the API service requests to CloudWatch metrics."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-15T07:50:00.000Z",
        "voteCount": 8,
        "content": "AC is using standard parts of the solution."
      },
      {
        "date": "2024-01-30T02:01:00.000Z",
        "voteCount": 5,
        "content": "A and C: use cloudwatch log agent to collect app log and use AWS X-ray to collect information about requests (traces). \nB is incorrect because modifying app to send message directly to X-RAY introduces more latency to the app. Use X-RAY daemon to do that task is a better idea"
      },
      {
        "date": "2024-02-03T20:33:00.000Z",
        "voteCount": 2,
        "content": "- &lt;API Gateway latency metrics&gt; means dev team have collect APM. They need to collect app log as well, which indicates option A. \nD and E: modifing the app introduces latencies"
      },
      {
        "date": "2024-07-25T20:48:00.000Z",
        "voteCount": 1,
        "content": "AC is less impact to Application Latencies. \nKeywords: without additional latencies, cloudwatch\n\nB will provide more latencies\nDE will require modify the app and give more latencies."
      },
      {
        "date": "2024-04-25T10:51:00.000Z",
        "voteCount": 3,
        "content": "The X-Ray daemon batches and uploads the data in the background, which helps to avoid introducing additional latency."
      },
      {
        "date": "2023-11-24T03:07:00.000Z",
        "voteCount": 2,
        "content": "the reason i am not so sure about is that API Gateway have built-in integration with X-Ray. This means that they automatically send trace data to X-Ray without needing a separate X-Ray daemon. and i dont think we have the option of installing one or using one, unless someone shows me the official link."
      },
      {
        "date": "2023-06-02T06:24:00.000Z",
        "voteCount": 3,
        "content": "Installing the CloudWatch agent server-side (option A) is not directly related to collecting latency data from API Gateway. The CloudWatch agent is typically used to collect and monitor system-level metrics from the server itself.\n\nEnabling AWS X-Ray tracing in API Gateway and using the X-Ray daemon (option C) is not necessary in this scenario. The X-Ray daemon is primarily used when you have applications running on EC2 instances or on-premises servers that need to send trace data to X-Ray.\n\nModifying the on-premises application to send log information back to API Gateway with each request (option D) is not an optimal solution for collecting latency data. It may introduce additional latency and overhead to the API requests and could be challenging to implement efficiently and accurately."
      },
      {
        "date": "2023-06-13T08:16:00.000Z",
        "voteCount": 2,
        "content": "Do you think that doing B or E doesn't bring any latency?\nI think C is necessary because you could trace the performance of the application.\nAnd even the team can look into app logs on its server, but sending logs to Cloudwatch logs and then making a further investigation with AWS tools is not too bad."
      },
      {
        "date": "2023-05-20T00:10:00.000Z",
        "voteCount": 2,
        "content": "Why A? The team still can check logs without uploading to CloudWatch? I'd prefer E over A."
      },
      {
        "date": "2023-07-28T17:18:00.000Z",
        "voteCount": 3,
        "content": "I thought the same but E might cause additional latency which is NOT what we want."
      },
      {
        "date": "2023-05-13T03:48:00.000Z",
        "voteCount": 4,
        "content": "AC less impact on app"
      },
      {
        "date": "2023-04-15T20:16:00.000Z",
        "voteCount": 2,
        "content": "A, C, correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/amazon/view/106274-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an application that is using a MySQL-compatible Amazon Aurora Multi-AZ DB cluster as the database. A cross-Region read replica has been created for disaster recovery purposes. A DevOps engineer wants to automate the promotion of the replica so it becomes the primary database instance in the event of a failure.<br>Which solution will accomplish this?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a latency-based Amazon Route 53 CNAME with health checks so it points to both the primary and replica endpoints. Subscribe an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail and use that topic to invoke an AWS Lambda function that will promote the replica instance as the primary.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Aurora custom endpoint to point to the primary database instance. Configure the application to use this endpoint. Configure AWS CloudTrail to run an AWS Lambda function to promote the replica instance and modify the custom endpoint to point to the newly promoted instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to modify the application's AWS CloudFormation template to promote the replica, apply the template to update the stack, and point the application to the newly promoted instance. Create an Amazon CloudWatch alarm to invoke this Lambda function after the failure event occurs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the Aurora endpoint in AWS Systems Manager Parameter Store. Create an Amazon EventBridge event that detects the database failure and runs an AWS Lambda function to promote the replica instance and update the endpoint URL stored in AWS Systems Manager Parameter Store. Code the application to reload the endpoint from Parameter Store if a database connection fails.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-02T19:42:00.000Z",
        "voteCount": 8,
        "content": "D is the correct answer.\n\nExplanation:\n\nTo automate the promotion of a read replica to the primary instance in the event of a failure, we need to detect the failure and then invoke an AWS Lambda function to promote the replica instance. This can be achieved using Amazon EventBridge.\n\nOption A is incorrect because using a CNAME with health checks doesn't provide an automated way to promote the read replica. Additionally, subscribing an Amazon SNS topic to Amazon RDS failure notifications from AWS CloudTrail doesn't help to promote the replica.\n\nOption B is incorrect because a custom endpoint is not required to promote the read replica. Additionally, using AWS CloudTrail to run an AWS Lambda function to promote the replica instance doesn't provide an automated way to update the application endpoint to point to the newly promoted instance."
      },
      {
        "date": "2023-10-28T08:10:00.000Z",
        "voteCount": 7,
        "content": "doesnt failover happen automatically in aurora?"
      },
      {
        "date": "2024-02-15T02:26:00.000Z",
        "voteCount": 2,
        "content": "Aurora  supports automated failover for a single cluster. [Be it a global Aurora cluster or a multi AZ/region deployment]\nIn this case it's implied that the read-replica is not part of the cluster.\n\nthat's my best guess."
      },
      {
        "date": "2024-07-25T20:57:00.000Z",
        "voteCount": 1,
        "content": "D is correct. \n\nOption B is wrong as AWS CloudTrail to run an AWS Lambda function to promote the replica instance doesn't provide an automated way."
      },
      {
        "date": "2024-08-01T02:19:00.000Z",
        "voteCount": 1,
        "content": "Option B is wrong also due to:  \n- Custom Endpoint Management: Extra complexity in managing and updating endpoints dynamically.\n- Lag in Promotion: Possible delays due to CloudTrail event delivery and Lambda invocation.\n- Reliance on CloudTrail: Lag in event processing can cause potential downtime or data inconsistency."
      },
      {
        "date": "2024-07-10T22:45:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B\nHere is why.\nPreviously, you might have used the CNAMES mechanism to set up Domain Name Service (DNS) aliases from your own domain to achieve similar results. By using custom endpoints, you can avoid updating CNAME records when your cluster grows or shrinks. Custom endpoints also mean that you can use encrypted Transport Layer Security/Secure Sockets Layer (TLS/SSL) connections.\nhttps://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html#Aurora.Endpoints.Custom"
      },
      {
        "date": "2024-01-30T02:38:00.000Z",
        "voteCount": 1,
        "content": "A is correct: Using Amazon Route 53 CNAME with health checks is the way for failover recommended by AWS: https://aws.amazon.com/blogs/database/cross-region-disaster-recovery-using-amazon-aurora-global-database-for-amazon-aurora-postgresql/"
      },
      {
        "date": "2023-10-01T10:26:00.000Z",
        "voteCount": 1,
        "content": "option D is not either providing seemless solution, in option D application needed to be reload and that will cause downtime."
      },
      {
        "date": "2023-06-15T08:00:00.000Z",
        "voteCount": 2,
        "content": "D make most sense."
      },
      {
        "date": "2023-05-02T19:44:00.000Z",
        "voteCount": 3,
        "content": "Option D is the correct solution\n\nOption C is incorrect because modifying the AWS CloudFormation template requires manual intervention and cannot be automated. Additionally, creating an Amazon CloudWatch alarm to invoke the Lambda function after the failure event occurs doesn't provide an automated way to promote the replica instance.\n\nTherefore, Option D is the correct solution."
      },
      {
        "date": "2023-04-30T21:42:00.000Z",
        "voteCount": 2,
        "content": "D is the answer"
      },
      {
        "date": "2023-04-28T09:30:00.000Z",
        "voteCount": 2,
        "content": "D: Refference:https://aws.amazon.com/es/blogs/database/cross-region-cross-account-disaster-recovery-using-amazon-aurora-global-database/"
      },
      {
        "date": "2023-04-15T10:31:00.000Z",
        "voteCount": 2,
        "content": "D it is"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/amazon/view/105584-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company hosts its staging website using an Amazon EC2 instance backed with Amazon EBS storage. The company wants to recover quickly with minimal data losses in the event of network connectivity issues or power failures on the EC2 instance.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the instance to an EC2 Auto Scaling group with the minimum, maximum, and desired capacity set to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the instance to an EC2 Auto Scaling group with a lifecycle hook to detach the EBS volume when the EC2 instance shuts down or terminates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm for the StatusCheckFailed System metric and select the EC2 action to recover the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm for the StatusCheckFailed Instance metric and select the EC2 action to reboot the instance."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-20T02:58:00.000Z",
        "voteCount": 10,
        "content": "C is the right answer.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html"
      },
      {
        "date": "2023-06-19T15:03:00.000Z",
        "voteCount": 9,
        "content": "Explanation:\n\nAmazon CloudWatch provides system-wide visibility into resource utilization, application performance, and operational health. If a system status check fails, this implies there's a problem with the underlying EC2 system that may require AWS involvement to repair. The \"Recover this instance\" action for the system status check automatically recovers the instance if it becomes impaired due to an underlying issue."
      },
      {
        "date": "2024-07-25T21:00:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\n\nIn the event that AWS determines an instance is unavailable due to an underlying hardware issue, there are two mechanisms that you can configure for instance resiliency which can restore availability\u2014simplified automatic recovery and Amazon CloudWatch action based recovery. This process is called instance recovery.\n\nThe following are examples of underlying hardware issues that might require instance recovery:\n- Loss of network connectivity\n- Loss of system power\n- Software issues on the physical host\n- Hardware issues on the physical host that impact network reachability"
      },
      {
        "date": "2024-04-12T09:33:00.000Z",
        "voteCount": 1,
        "content": "C. This is the correct solution. By creating a CloudWatch alarm for the StatusCheckFailed System metric and configuring the alarm to trigger the \"Recover this instance\" action, the EC2 instance will be automatically recovered in the event of a system failure or power outage. This ensures the instance can be quickly recovered with minimal data loss, as the EBS volume remains attached during the recovery process.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html"
      },
      {
        "date": "2024-03-08T06:47:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html"
      },
      {
        "date": "2024-01-30T02:42:00.000Z",
        "voteCount": 1,
        "content": "C is correct: recover is the right way\nA and B are irrelevant\nD: should not reboot in case of power failures"
      },
      {
        "date": "2023-11-24T03:23:00.000Z",
        "voteCount": 3,
        "content": "My Reason:\nStatusCheckFailed_System: This check monitors the AWS systems on which your instance runs1. For Example loss of network connectivity, loss of system power, software issues on the physical host, and hardware issues on the physical host that impact network reachability\n\nStatusCheckFailed_Instance: This check monitors the software and network configuration of your individual instance. These checks detect problems that require your involvement to repair. If an instance status check fails, it typically means that there\u2019s an issue with the instance, such as a misconfigured network or a problem with the instance\u2019s file system."
      },
      {
        "date": "2023-06-16T07:47:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is C"
      },
      {
        "date": "2023-05-28T19:23:00.000Z",
        "voteCount": 3,
        "content": "A is incorrect. \nSimplified automatic recovery is not initiated for instances in an Auto Scaling group. If your instance is part of an Auto Scaling group with health checks enabled, then the instance is replaced when it becomes impaired.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html"
      },
      {
        "date": "2023-05-13T04:02:00.000Z",
        "voteCount": 1,
        "content": "C with recover action creates identical instance"
      },
      {
        "date": "2023-05-03T16:47:00.000Z",
        "voteCount": 3,
        "content": "C\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-instance-recover.html\n\nThere are only 2 ways to recover EC2 instances. Since this instance has EBS volumes, only the CloudWatch action based recovery is applicable."
      },
      {
        "date": "2023-05-02T19:43:00.000Z",
        "voteCount": 1,
        "content": "Option D is the correct solution\nOption C is incorrect because modifying the AWS CloudFormation template requires manual intervention and cannot be automated. Additionally, creating an Amazon CloudWatch alarm to invoke the Lambda function after the failure event occurs doesn't provide an automated way to promote the replica instance.\n\nTherefore, Option D is the correct solution."
      },
      {
        "date": "2023-04-15T10:25:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-04-14T14:03:00.000Z",
        "voteCount": 1,
        "content": "option A is a better choice for this scenario because it ensures that there is always an EC2 instance running to serve the staging website, and the new instance will have the same configuration as the original instance, including the EBS volume, so there will be minimal data loss. Option C may result in some data loss since a new EBS volume will be created, and it may take longer to recover the instance since the EC2 action to recover the instance will need to be triggered by the Amazon CloudWatch alarm."
      },
      {
        "date": "2023-05-30T23:30:00.000Z",
        "voteCount": 1,
        "content": "You would lose the contents on the EBS volume."
      },
      {
        "date": "2023-07-18T07:28:00.000Z",
        "voteCount": 1,
        "content": "How would you launch a new instance if they is a power outage?  So, C is correct as you will have to recover and hopefully quickly."
      },
      {
        "date": "2023-12-11T22:59:00.000Z",
        "voteCount": 1,
        "content": "If there is power outage than it is considered as instance failure and cloudwatch alarm can recover from system failure but can't recover from instance failure.\nhttps://repost.aws/knowledge-center/automatic-recovery-ec2-cloudwatch\n\nI believe option A is quickest recovery, and if Data needed to be backup then option B"
      },
      {
        "date": "2023-04-08T05:43:00.000Z",
        "voteCount": 3,
        "content": "C for me"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/amazon/view/106272-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company wants to use AWS development tools to replace its current bash deployment scripts. The company currently deploys a LAMP application to a group of Amazon EC2 instances behind an Application Load Balancer (ALB). During the deployments, the company unit tests the committed application, stops and starts services, unregisters and re-registers instances with the load balancer, and updates file permissions. The company wants to maintain the same deployment functionality through the shift to using AWS services.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services, and deregister and register instances with the ALB. Use the appspec.yml file to update file permissions without a custom script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to move the application from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy's deployment group to test the application, unregister and re-register instances with the ALand restart services. Use the appspec.yml file to update file permissions without a custom script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to move the application source code from the AWS CodeCommit repository to AWS CodeDeploy. Use CodeDeploy to test the application. Use CodeDeploy's appspec.yml file to restart services and update permissions without a custom script. Use AWS CodeBuild to unregister and re-register instances with the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to trigger AWS CodeBuild to test the application. Use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services. Unregister and re-register the instances in the AWS CodeDeploy deployment group with the ALB. Update the appspec.yml file to update file permissions without a custom script.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-20T03:04:00.000Z",
        "voteCount": 9,
        "content": "D is better than A. You need to include CodePipeline to move execution from CodeBuild to CodeDeploy."
      },
      {
        "date": "2023-05-02T19:55:00.000Z",
        "voteCount": 5,
        "content": "Option D is also a viable solution. It suggests using AWS CodePipeline to trigger AWS CodeBuild to test the application, and then use bash scripts invoked by AWS CodeDeploy's appspec.yml file to restart services, unregister and re-register instances with the ALB, and update file permissions. This approach also covers all the deployment functionality required by the company"
      },
      {
        "date": "2024-08-14T21:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. company want to replace its bash deployment scripts so option D is not suitable"
      },
      {
        "date": "2024-08-14T21:50:00.000Z",
        "voteCount": 2,
        "content": "Answer is B. company want to replace its bash deployment scripts so option D is not suitable"
      },
      {
        "date": "2024-07-25T21:09:00.000Z",
        "voteCount": 2,
        "content": "Should be D\n\nCodePipeline - execute from CodeBuild to CodeDeploy\nCodeBuild -  test the application\nCodeDeploy - deploy app, restart services, Unregister and re-register instance\n\nNot Option A: not using CodePipeline\nNot Option BC: using CodeCommit repo, not relevant with question."
      },
      {
        "date": "2024-03-21T08:48:00.000Z",
        "voteCount": 1,
        "content": "codebuild to test not codedeploy D is correct"
      },
      {
        "date": "2024-01-30T02:58:00.000Z",
        "voteCount": 4,
        "content": "D: is correct: need codepipeline for a seamless deployment. Need codebuild to test and codedeploy to deploy the app on EC2\nA: no mention of codepipeline\nB and C both mention AWS CodeCommit repository, which is irrelevant"
      },
      {
        "date": "2024-02-12T21:05:00.000Z",
        "voteCount": 1,
        "content": "A: &lt;deregister and register instances with the ALB&gt;: we need to unregister, not deregister it"
      },
      {
        "date": "2024-02-03T20:27:00.000Z",
        "voteCount": 1,
        "content": "B and C: The question doesnt mention the need for a source code repository. \nB: move the application from the AWS CodeCommit repository to AWS CodeDeploy -&gt; Cannot do this, codecommit does not store apps, only code\nC: move the application source code from the AWS CodeCommit repository to AWS CodeDeploy -&gt; cannot do this, code deploy does not store code"
      },
      {
        "date": "2023-06-19T15:12:00.000Z",
        "voteCount": 2,
        "content": "Explanation:\n\nAWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy, which is perfect for unit testing the application.\n\nAWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances. You can specify scripts to be run at set points during a deployment lifecycle, such as deregistering and registering instances with a load balancer, stopping and starting services, or changing file permissions, by defining them in the appspec.yml file."
      },
      {
        "date": "2024-04-12T10:10:00.000Z",
        "voteCount": 1,
        "content": "but it says to deregister"
      },
      {
        "date": "2023-06-16T07:51:00.000Z",
        "voteCount": 2,
        "content": "D is the correct bubamon"
      },
      {
        "date": "2023-05-24T01:22:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      },
      {
        "date": "2023-04-15T10:22:00.000Z",
        "voteCount": 2,
        "content": "D it is"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/amazon/view/106271-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs an application with an Amazon EC2 and on-premises configuration. A DevOps engineer needs to standardize patching across both environments. Company policy dictates that patching only happens during non-business hours.<br>Which combination of actions will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the physical machines into AWS Systems Manager using Systems Manager Hybrid Activations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an IAM role to the EC2 instances, allowing them to be managed by AWS Systems Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate IAM access keys for the on-premises machines to interact with AWS Systems Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun an AWS Systems Manager Automation document to patch the systems every hour",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EventBridge scheduled events to schedule a patch window.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Maintenance Windows to schedule a patch window.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ABF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABF",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "AF",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-30T03:04:00.000Z",
        "voteCount": 7,
        "content": "ABF are the right answers:\nA: enable hybrid on AWS system manager\nB: create IAM role for System manager to manage EC2 instances\nF: use maintenance windows to schedule patching on non-business hours\n\nC: incorrect because there is no IAM access keys for on-prem \nD: should not run patching every hour\nE: should not use Eventbridge because AWS has its own service to schedule patching"
      },
      {
        "date": "2023-07-25T01:40:00.000Z",
        "voteCount": 5,
        "content": "ABF is correct"
      },
      {
        "date": "2024-07-25T21:12:00.000Z",
        "voteCount": 2,
        "content": "ABF are correct\n\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/activations.html\n\nTo configure non-EC2 machines for use with AWS Systems Manager in a hybrid and multicloud environment, you create a hybrid activation. Non-EC2 machine types supported as managed nodes include the following:\n- Servers on your own premises (on-premises servers)\n- AWS IoT Greengrass core devices\n- AWS IoT and non-AWS edge devices\n- Virtual machines (VMs), including VMs in other cloud environments"
      },
      {
        "date": "2024-06-07T03:31:00.000Z",
        "voteCount": 1,
        "content": "ABF is correct"
      },
      {
        "date": "2023-07-16T13:44:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-managedinstances.html\n\nAF are right but the letter B is wrong the role is for non EC2 instances"
      },
      {
        "date": "2023-06-20T03:07:00.000Z",
        "voteCount": 4,
        "content": "ABF is correct.\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/activations.html"
      },
      {
        "date": "2023-05-02T19:56:00.000Z",
        "voteCount": 3,
        "content": "ABF is right"
      },
      {
        "date": "2023-04-15T10:17:00.000Z",
        "voteCount": 3,
        "content": "ABF it is"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/amazon/view/106270-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has chosen AWS to host a new application. The company needs to implement a multi-account strategy. A DevOps engineer creates a new AWS account and an organization in AWS Organizations. The DevOps engineer also creates the OU structure for the organization and sets up a landing zone by using AWS Control Tower.<br>The DevOps engineer must implement a solution that automatically deploys resources for new accounts that users create through AWS Control Tower Account Factory. When a user creates a new account, the solution must apply AWS CloudFormation templates and SCPs that are customized for the OU or the account to automatically deploy all the resources that are attached to the account. All the OUs are enrolled in AWS Control Tower.<br>Which solution will meet these requirements in the MOST automated way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Service Catalog with AWS Control Tower. Create portfolios and products in AWS Service Catalog. Grant granular permissions to provision these resources. Deploy SCPs by using the AWS CLI and JSON documents.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy CloudFormation stack sets by using the required templates. Enable automatic deployment. Deploy stack instances to the required accounts. Deploy a CloudFormation stack set to the organization\u2019s management account to deploy SCPs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule to detect the CreateManagedAccount event. Configure AWS Service Catalog as the target to deploy resources to any new accounts. Deploy SCPs by using the AWS CLI and JSON documents.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Customizations for AWS Control Tower (CfCT) solution. Use an AWS CodeCommit repository as the source. In the repository, create a custom package that includes the CloudFormation templates and the SCP JSON documents.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-20T03:13:00.000Z",
        "voteCount": 6,
        "content": "CfCT is designed for the purpose stated in the question. So D.\nhttps://docs.aws.amazon.com/controltower/latest/userguide/cfct-overview.html"
      },
      {
        "date": "2023-06-19T15:14:00.000Z",
        "voteCount": 6,
        "content": "The CfCT solution is designed for the exact purpose stated in the question. It extends the capabilities of AWS Control Tower by providing you with a way to automate resource provisioning and apply custom configurations across all AWS accounts created in the Control Tower environment. This enables the company to implement additional account customizations when new accounts are provisioned via the Control Tower Account Factory.\n\nThe CloudFormation templates and SCPs can be added to a CodeCommit repository and will be automatically deployed to new accounts when they are created. This provides a highly automated solution that does not require manual intervention to deploy resources and SCPs to new accounts."
      },
      {
        "date": "2024-07-25T21:15:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://docs.aws.amazon.com/controltower/latest/userguide/cfct-overview.html\nCustomizations for AWS Control Tower (CfCT) helps you customize your AWS Control Tower landing zone and stay aligned with AWS best practices. Customizations are implemented with AWS CloudFormation templates and service control policies (SCPs)."
      },
      {
        "date": "2024-07-25T21:17:00.000Z",
        "voteCount": 1,
        "content": "keywords: \"sets up a landing zone by using AWS Control Tower\""
      },
      {
        "date": "2024-06-03T19:00:00.000Z",
        "voteCount": 1,
        "content": "\"This CfCT capability is integrated with AWS Control Tower lifecycle events, so that your resource deployments remain synchronized with your landing zone.\"\n\"For example, when a new account is created through account factory, all resources attached to the account are deployed automatically.\"\n\"You can deploy the custom templates and policies to individual accounts and organizational units (OUs) within your organization.\"\nhttps://docs.aws.amazon.com/controltower/latest/userguide/cfct-overview.html"
      },
      {
        "date": "2024-01-30T08:32:00.000Z",
        "voteCount": 3,
        "content": "D is correct:  Use CfCT is the correct solution: it utilizes both CloudFormation template and SCP\nA and C: no mention of AWS CloudFormation\nB: No mention of AWS control tower"
      },
      {
        "date": "2024-01-15T08:35:00.000Z",
        "voteCount": 3,
        "content": "D. B is wrong because StackSets doesn't deploy stack instances to the organization management account."
      },
      {
        "date": "2023-06-02T07:11:00.000Z",
        "voteCount": 2,
        "content": "B. Deploying CloudFormation stack sets is the most automated way to deploy resources for new accounts created through AWS Control Tower Account Factory. With stack sets, you can define a CloudFormation template and deploy it to multiple accounts automatically. By enabling automatic deployment and deploying stack instances to the required accounts, you can ensure that the resources specified in the CloudFormation templates are automatically provisioned for each account. Additionally, by deploying a CloudFormation stack set to the organization's management account, you can deploy Service Control Policies (SCPs) across all accounts in the organization."
      },
      {
        "date": "2023-05-29T05:19:00.000Z",
        "voteCount": 2,
        "content": "Customizations for AWS Control Tower combines AWS Control Tower and other highly-available, trusted AWS services to help customers more quickly set up a secure, multi-account AWS environment using AWS best practices. You can easily add customizations to your AWS Control Tower landing zone using an AWS CloudFormation template and service control policies (SCPs). You can deploy the custom template and policies to individual accounts and organizational units (OUs) within your organization. It also integrates with AWS Control Tower lifecycle events to ensure that resource deployments stay in sync with your landing zone. For example, when a new account is created using the AWS Control Tower account factory, Customizations for AWS Control Tower ensures that all resources attached to the account's OUs will be automatically deployed."
      },
      {
        "date": "2023-04-30T21:59:00.000Z",
        "voteCount": 2,
        "content": "D is it"
      },
      {
        "date": "2023-04-15T10:11:00.000Z",
        "voteCount": 2,
        "content": "D it is"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/amazon/view/106269-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "An online retail company based in the United States plans to expand its operations to Europe and Asia in the next six months. Its product currently runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. All data is stored in an Amazon Aurora database instance.<br>When the product is deployed in multiple regions, the company wants a single product catalog across all regions, but for compliance purposes, its customer information and purchases must be kept in each region.<br>How should the company meet these requirements with the LEAST amount of application changes?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift for the product catalog and Amazon DynamoDB tables for the customer information and purchases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB global tables for the product catalog and regional tables for the customer information and purchases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Aurora for the product catalog and Amazon DynamoDB global tables for the customer information and purchases."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-02T07:15:00.000Z",
        "voteCount": 9,
        "content": "C. Using Aurora with read replicas for the product catalog allows for a single product catalog across all regions. Aurora read replicas can be set up in different regions to provide low-latency access to the product catalog from each region. Additionally, by deploying additional local Aurora instances in each region for customer information and purchases, the company can comply with the requirement of keeping customer data and purchases in each region."
      },
      {
        "date": "2024-07-25T21:20:00.000Z",
        "voteCount": 2,
        "content": "C\nkeywords: ''the LEAST amount of application changes\""
      },
      {
        "date": "2024-02-21T20:00:00.000Z",
        "voteCount": 3,
        "content": "How should the company meet these requirements with the LEAST amount of application changes? Anything option with DynamoDB is out since the all the data is stored Aurora(relational database)."
      },
      {
        "date": "2024-01-30T08:56:00.000Z",
        "voteCount": 3,
        "content": "C is correct: data is kept in each region and one product catalog for all regions \nA: Redshift is for data analysis, not for the need in the question\nB: DynamoDB is primarily used for session data in a web app\nD: Amazon DynamoDB global tables for the customer information is against the policy"
      },
      {
        "date": "2023-12-24T17:00:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-06-20T03:16:00.000Z",
        "voteCount": 2,
        "content": "C makes most sense and minimizes application changes."
      },
      {
        "date": "2023-04-30T22:02:00.000Z",
        "voteCount": 3,
        "content": "The best solution to meet the company's requirements with the LEAST amount of application changes is to use Aurora with read replicas for the product catalog and additional local Aurora instances in each region for the customer information and purchases. This will allow for a single product catalog across all regions, while still keeping customer information and purchases in each region for compliance purposes. Amazon Redshift is a data warehousing solution and is not appropriate for this use case. Amazon DynamoDB global tables may be used, but they require application changes to support them. Using local Aurora instances in each region for customer information and purchases could also work, but this would require more configuration and management than using Aurora with read replicas. Therefore, option C is the best solution."
      },
      {
        "date": "2023-04-15T10:06:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/amazon/view/106268-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is implementing a well-architected design for its globally accessible API stack. The design needs to ensure both high reliability and fast response times for users located in North America and Europe.<br>The API stack contains the following three tiers:<br><br>Amazon API Gateway -<br><br>AWS Lambda -<br><br>Amazon DynamoDB -<br>Which solution will meet the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Route 53 to point to API Gateway APIs in North America and Europe using health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB table in the same Region as the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Route 53 to point to API Gateway APIs in North America and Europe using latency-based routing and health checks. Configure the APIs to forward requests to a Lambda function in that Region. Configure the Lambda functions to retrieve and update the data in a DynamoDB global table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Route 53 to point to API Gateway in North America, create a disaster recovery API in Europe, and configure both APIs to forward requests to the Lambda functions in that Region. Retrieve the data from a DynamoDB global table. Deploy a Lambda function to check the North America API health every 5 minutes. In the event of a failure, update Route 53 to point to the disaster recovery API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Route 53 to point to API Gateway API in North America using latency-based routing. Configure the API to forward requests to the Lambda function in the Region nearest to the user. Configure the Lambda function to retrieve and update the data in a DynamoDB table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-02T20:22:00.000Z",
        "voteCount": 12,
        "content": "B is the correct solution.\n\nThe requirement is to ensure both high reliability and fast response times for users located in North America and Europe. To meet this requirement, we can use Amazon Route 53 with latency-based routing to direct users to the closest API Gateway endpoint. Additionally, we can use health checks to monitor the health of each endpoint and direct traffic away from unhealthy endpoints.\n\nTo maintain high reliability, we can use AWS Lambda to handle the API requests. Since Lambda scales automatically, we don't need to worry about provisioning or maintaining infrastructure. We can also use DynamoDB as the database since it provides low latency access and automatic scaling."
      },
      {
        "date": "2024-01-30T09:01:00.000Z",
        "voteCount": 3,
        "content": "B is correct: using both latency-based routing and health checks ensures high reliability and fast response\nA: only health check doesnt ensure fast response\nC: All traffic would be routed to one location only (either NA or Europe if NA failed)\nD: All traffic would be routed to one NA only. There would be no entry point which is near Europe users."
      },
      {
        "date": "2023-12-24T17:05:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-12-22T20:34:00.000Z",
        "voteCount": 2,
        "content": "B , \nUsing latency based routing for better response time . Having api gateway in each region reduce requrest fligh time. Lambda and Dynamo being a managed serivce scale automatically and having them in same region just reduce latency."
      },
      {
        "date": "2023-07-11T20:44:00.000Z",
        "voteCount": 3,
        "content": "Reliability is different that resiliency, hence A and C are out as they are focussing on health checks which is required for the resiliency. DR again for the resiliency"
      },
      {
        "date": "2023-06-20T03:19:00.000Z",
        "voteCount": 4,
        "content": "B is the best solution."
      },
      {
        "date": "2023-04-15T10:04:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/amazon/view/106436-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A rapidly growing company wants to scale for developer demand for AWS development environments. Development environments are created manually in the AWS Management Console. The networking team uses AWS CloudFormation to manage the networking infrastructure, exporting stack output values for the Amazon VPC and all subnets. The development environments have common standards, such as Application Load Balancers, Amazon EC2 Auto Scaling groups, security groups, and Amazon DynamoDB tables.<br>To keep up with demand, the DevOps engineer wants to automate the creation of development environments. Because the infrastructure required to support the application is expected to grow, there must be a way to easily update the deployed infrastructure. CloudFormation will be used to create a template for the development environments.<br>Which approach will meet these requirements and quickly provide consistent AWS environments for developers?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Fn::ImportValue intrinsic functions in the Resources section of the template to retrieve Virtual Private Cloud (VPC) and subnet values. Use CloudFormation StackSets for the development environments, using the Count input parameter to indicate the number of environments needed. Use the UpdateStackSet command to update existing development environments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team\u2019s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse nested stacks to define common infrastructure components. Use Fn::ImportValue intrinsic functions with the resources of the nested stack to retrieve Virtual Private Cloud (VPC) and subnet values. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Fn::ImportValue intrinsic functions in the Parameters section of the root template to retrieve Virtual Private Cloud (VPC) and subnet values. Define the development resources in the order they need to be created in the CloudFormation nested stacks. Use the CreateChangeSet. and ExecuteChangeSet commands to update existing development environments."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-15T09:56:00.000Z",
        "voteCount": 13,
        "content": "C is Correct.\nB is WRONG because intrinsic functions can't be used in Parameter as per AWS documentation. \nhttps://repost.aws/knowledge-center/cloudformation-template-validation"
      },
      {
        "date": "2023-09-14T20:11:00.000Z",
        "voteCount": 2,
        "content": "You can use intrinsic functions only in specific parts of a template. Currently, you can use intrinsic functions in resource properties, outputs, metadata attributes, and update policy attributes\nRefer \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference.html"
      },
      {
        "date": "2024-05-01T10:38:00.000Z",
        "voteCount": 1,
        "content": "C seems right"
      },
      {
        "date": "2024-01-30T23:17:00.000Z",
        "voteCount": 4,
        "content": "C is correct: use nested stacks and Fn::ImportValue intrinsic functions with the resources of the nested stack\nA: no mention of nested stack\nB and D: Fn::ImportValue intrinsic function is used on child template to import values from parent template. So it should not be used on root template, which is the universal parent tempalte of all other templates"
      },
      {
        "date": "2023-06-20T03:25:00.000Z",
        "voteCount": 2,
        "content": "C is the best answer. B is wrong as you need to use Fn::ImportValue in Resource section to import CFN template outputs."
      },
      {
        "date": "2023-06-20T01:15:00.000Z",
        "voteCount": 2,
        "content": "I will go with C"
      },
      {
        "date": "2023-06-19T15:19:00.000Z",
        "voteCount": 2,
        "content": "B. Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team\u2019s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.\n\nNested stacks allow you to modularize and reuse CloudFormation code. For this case, this is helpful because you have common infrastructure components that are shared across environments.\n\nThe Fn::ImportValue function is used to import values that have been exported in another stack. Since the networking team exports the VPC and subnet information, this can be used in the CloudFormation stack to reference those values."
      },
      {
        "date": "2023-07-28T22:32:00.000Z",
        "voteCount": 2,
        "content": "B is incorrect. One of the reasons is that intrinsic functions are not allowed in the Parameters section. https://repost.aws/knowledge-center/cloudformation-template-validation"
      },
      {
        "date": "2023-08-21T04:09:00.000Z",
        "voteCount": 4,
        "content": "B is WRONG, you cannot use `TemplateURL` to retrieve Network Stack export values."
      },
      {
        "date": "2023-06-16T07:43:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-05-23T10:47:00.000Z",
        "voteCount": 2,
        "content": "C ipsingh is absolutely correct"
      },
      {
        "date": "2023-05-15T13:34:00.000Z",
        "voteCount": 2,
        "content": "Im 50/50 C or B, but B doesn't provide a clear approach for retrieving the exported values  and placing position of Parameters section of the root template, which is not required to place it there, it must declare inside resource. So i think Answer C make sense."
      },
      {
        "date": "2023-05-30T23:54:00.000Z",
        "voteCount": 1,
        "content": "The template URL in B makes it wrong IMHO. You import the values from an exiting template importing the parameter exported by it."
      },
      {
        "date": "2023-05-14T00:39:00.000Z",
        "voteCount": 2,
        "content": "C makes more sense"
      },
      {
        "date": "2023-05-09T09:43:00.000Z",
        "voteCount": 2,
        "content": "c is correct"
      },
      {
        "date": "2023-04-30T22:12:00.000Z",
        "voteCount": 1,
        "content": "B. Use nested stacks to define common infrastructure components. To access the exported values, use TemplateURL to reference the networking team\u2019s template. To retrieve Virtual Private Cloud (VPC) and subnet values, use Fn::ImportValue intrinsic functions in the Parameters section of the root template. Use the CreateChangeSet and ExecuteChangeSet commands to update existing development environments.\n\nThis approach is a good fit because it allows the developer to define reusable infrastructure components as nested stacks. To retrieve VPC and subnet values, the intrinsic function Fn::ImportValue is used in the Parameters section of the root template, which retrieves the values from the output of the networking team\u2019s CloudFormation stack. To update existing environments, the CreateChangeSet and ExecuteChangeSet commands are used, which provides a way to easily update the deployed infrastructure. Additionally, the use of nested stacks helps to ensure consistency across environments."
      },
      {
        "date": "2023-07-28T22:33:00.000Z",
        "voteCount": 2,
        "content": "B is incorrect. One of the reasons is that intrinsic functions are not allowed in the Parameters section. https://repost.aws/knowledge-center/cloudformation-template-validation"
      },
      {
        "date": "2023-04-22T16:03:00.000Z",
        "voteCount": 2,
        "content": "Option B is correct. Using nested stacks, the common infrastructure components can be defined in separate templates that can be referenced by the root template. This allows for easy updates and maintenance of the common components. The networking team\u2019s CloudFormation template can be used to export the VPC and subnet values, which can be referenced in the root template using Fn::ImportValue intrinsic functions in the Parameters section. The CreateChangeSet and ExecuteChangeSet commands can be used to update the existing development environments.\n\nOption C is not the best choice because using Fn::ImportValue intrinsic functions with the resources of the nested stack can lead to circular dependencies and make it difficult to manage the infrastructure."
      },
      {
        "date": "2023-04-16T14:20:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/amazon/view/106267-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Organizations to manage multiple accounts. Information security policies require that all unencrypted Amazon EBS volumes be marked as non-compliant. A DevOps engineer needs to automatically deploy the solution and ensure that this compliance check is always present.<br>Which solution will accomplish this?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that defines an AWS Inspector rule to check whether EBS encryption is enabled. Save the template to an Amazon S3 bucket that has been shared with all accounts within the company. Update the account creation script pointing to the CloudFormation template in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP in Organizations. Set the policy to prevent the launch of Amazon EC2 instances without encryption on the EBS volumes using a conditional expression. Apply the SCP to all AWS accounts. Use Amazon Athena to analyze the AWS CloudTrail output, looking for events that deny an ec2:RunInstances action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an IAM role to all accounts from a single trusted account. Build a pipeline with AWS CodePipeline with a stage in AWS Lambda to assume the IAM role, and list all EBS volumes in the account. Publish a report to Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-04T13:06:00.000Z",
        "voteCount": 1,
        "content": "Exam Tip -&gt; Compliance = AWS Config"
      },
      {
        "date": "2024-02-22T00:11:00.000Z",
        "voteCount": 4,
        "content": "Deploy CloudFormation template with encrypted-volumes in the ConfigRuleName property, AWS Config will automatically scan the environment and check for unencrypted EBS volumes."
      },
      {
        "date": "2024-01-31T01:04:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-06-20T03:28:00.000Z",
        "voteCount": 3,
        "content": "B is the only solution meeting the criteria."
      },
      {
        "date": "2023-05-02T20:34:00.000Z",
        "voteCount": 3,
        "content": "B. Create an AWS Config organizational rule to check whether EBS encryption is enabled and deploy the rule using the AWS CLI. Create and apply an SCP to prohibit stopping and deleting AWS Config across the organization, will accomplish the compliance check on all accounts.\n\nOption A is incorrect because an AWS Inspector rule is used to analyze the behavior of the application on the EC2 instance, not to check the encryption of the EBS volume."
      },
      {
        "date": "2023-04-30T22:13:00.000Z",
        "voteCount": 2,
        "content": "B is right"
      },
      {
        "date": "2023-04-15T09:53:00.000Z",
        "voteCount": 2,
        "content": "B is the answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/amazon/view/105446-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is performing vulnerability scanning for all Amazon EC2 instances across many accounts. The accounts are in an organization in AWS Organizations. Each account's VPCs are attached to a shared transit gateway. The VPCs send traffic to the internet through a central egress VPC. The company has enabled Amazon Inspector in a delegated administrator account and has enabled scanning for all member accounts.<br>A DevOps engineer discovers that some EC2 instances are listed in the \"not scanning\" tab in Amazon Inspector.<br>Which combination of actions should the DevOps engineer take to resolve this issue? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that AWS Systems Manager Agent is installed and is running on the EC2 instances that Amazon Inspector is not scanning.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the target EC2 instances with security groups that allow outbound communication on port 443 to the AWS Systems Manager service endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant inspector:StartAssessmentRun permissions to the IAM role that the DevOps engineer is using.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure EC2 Instance Connect for the EC2 instances that Amazon Inspector is not scanning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the target EC2 instances with instance profiles that grant permissions to communicate with AWS Systems Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a managed-instance activation. Use the Activation Code and the Activation ID to register the EC2 instances."
    ],
    "answer": "ABE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABE",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-06T12:03:00.000Z",
        "voteCount": 6,
        "content": "A b e https://docs.aws.amazon.com/inspector/latest/user/scanning-ec2.html"
      },
      {
        "date": "2024-02-22T00:54:00.000Z",
        "voteCount": 3,
        "content": "C is not a fundamental solution. Because Inspector is actually able to run, and it is not the same IAM role that DevOps uses."
      },
      {
        "date": "2024-01-31T02:02:00.000Z",
        "voteCount": 3,
        "content": "ABE are correct: Check if SSM agent is installed, check connection and permission of Ec2 that allows access to SSM\nC: no need to grant  inspector:StartAssessmentRun permissions because the dev has already finish the scanning task\nD: There is not EC2 instance Connect, only need SSM agent\nF: there is no managed-instance activation"
      },
      {
        "date": "2023-12-09T05:37:00.000Z",
        "voteCount": 3,
        "content": "the following link explains it all;\nhttps://repost.aws/knowledge-center/systems-manager-ec2-instance-not-appear"
      },
      {
        "date": "2023-06-20T03:57:00.000Z",
        "voteCount": 2,
        "content": "ABE seem to be prerequisites to work with SSM and Inspector."
      },
      {
        "date": "2023-05-31T00:02:00.000Z",
        "voteCount": 2,
        "content": "A B E is the correct one IMHO"
      },
      {
        "date": "2023-05-08T07:53:00.000Z",
        "voteCount": 2,
        "content": "ABE makes more sense."
      },
      {
        "date": "2023-04-16T14:44:00.000Z",
        "voteCount": 3,
        "content": "A,B,E are correct  https://docs.aws.amazon.com/inspector/latest/user/scanning-ec2.html"
      },
      {
        "date": "2023-04-14T14:16:00.000Z",
        "voteCount": 4,
        "content": "Option C suggests granting inspector:StartAssessmentRun permissions to the IAM role being used by the DevOps engineer. However, this may not be relevant to the issue of instances not being scanned by Amazon Inspector, as the IAM role may already have the necessary permissions by default.\n\nTherefore, A, B, E is a better choice in this case as it includes the necessary steps to ensure that the instances can communicate with AWS Systems Manager, which is required for Amazon Inspector to scan the instances."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/amazon/view/105492-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A development team uses AWS CodeCommit for version control for applications. The development team uses AWS CodePipeline, AWS CodeBuild. and AWS CodeDeploy for CI/CD infrastructure. In CodeCommit, the development team recently merged pull requests that did not pass long-running tests in the code base. The development team needed to perform rollbacks to branches in the codebase, resulting in lost time and wasted effort.<br>A DevOps engineer must automate testing of pull requests in CodeCommit to ensure that reviewers more easily see the results of automated tests as part of the pull request review.<br>What should the DevOps engineer do to meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to the pullRequestCreated event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to pullRequestCreated and pullRequestSourceBranchUpdated events. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild badge as a comment on the pull request so that developers will see the badge in their code review.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-29T15:08:00.000Z",
        "voteCount": 16,
        "content": "C. Look at #3 in the below. \nhttps://container-devsecops.awssecworkshops.com/04-testing/"
      },
      {
        "date": "2024-06-05T17:13:00.000Z",
        "voteCount": 1,
        "content": "Link is dead"
      },
      {
        "date": "2023-06-20T04:03:00.000Z",
        "voteCount": 10,
        "content": "B, we need to run tests only when pull request is created and we need to publish test results, not only badge."
      },
      {
        "date": "2024-06-05T17:12:00.000Z",
        "voteCount": 2,
        "content": "\"Automated Code Review on Pull Requests using AWS CodeCommit and AWS CodeBuild\"\n\"The solution comprises of the following components:\"\n\"Amazon EventBridge: AWS service to receive pullRequestCreated and pullRequestSourceBranchUpdated events and trigger Amazon EventBridge rule.\"\nhttps://aws.amazon.com/blogs/devops/automated-code-review-on-pull-requests-using-aws-codecommit-and-aws-codebuild/"
      },
      {
        "date": "2024-05-01T10:42:00.000Z",
        "voteCount": 1,
        "content": "i go with C"
      },
      {
        "date": "2024-03-22T04:33:00.000Z",
        "voteCount": 4,
        "content": "C is the answer to ensure code reviewers more easily see the results of automated tests as part of the pull request review\npullRequestStatusChanged event is triggered whenever the status of a pull request changes. This could include transitions like: Open to Closed (pull request is merged or marked as closed)\nClosed to Open (pull request is reopened)\npullRequestCreated event is triggered whenever a new pull request is created in a CodeCommit repository.\npullRequestSourceBranchUpdated event is triggered whenever there are updates (new commits) pushed to the source branch of an open pull request"
      },
      {
        "date": "2024-07-27T22:57:00.000Z",
        "voteCount": 1,
        "content": "I agree the C is the closed correct answer but it doesn't mention pullRequestStatusChanged (not sure why you mention it in your comment). \n\"The primary events in AWS CodeCommit that can trigger the pipeline are:\n- pullRequestCreated: This event occurs when a new pull request is created.\n- pullRequestSourceBranchUpdated: This event occurs when the source branch of an existing pull request is updated (e.g. when new commits are pushed to the branch).\"\nThe other events that might be considered but I would exclude are: \n- pullRequestStatusChanged: This event occurs when the status of a pull request changes, from closed to open (which we can consider), or from open to close (which we shouldn't consider in our case).\n- pullRequestMerged: This event occurs when a pull request is merged. I would exclude it because we are looking to test before merging."
      },
      {
        "date": "2024-01-31T02:34:00.000Z",
        "voteCount": 2,
        "content": "B is correct: we need to react when there is merge request (pullRequestCreated event)\nA: we need to react when there is merge request, not when the status of merge request is changed (pullRequestStatusChanged event)\nC: we only need to react when there is merge request, not when a sourcebranch is updated (pullRequestSourceBranchUpdated events)\nD: we need to react when there is merge request, not when the status of merge request is changed (pullRequestStatusChanged event)"
      },
      {
        "date": "2024-07-05T01:10:00.000Z",
        "voteCount": 3,
        "content": "If the source is updated after the PR is created, you don't run any tests against those changes."
      },
      {
        "date": "2023-12-20T08:23:00.000Z",
        "voteCount": 2,
        "content": "Why not B?"
      },
      {
        "date": "2023-11-21T05:38:00.000Z",
        "voteCount": 2,
        "content": "Answer is C: the pullRequestStatusChanged only has two values (OPEN|CLOSED) so If there is any update in the code the tests will not run.\n\nhttps://docs.aws.amazon.com/codecommit/latest/APIReference/API_PullRequestStatusChangedEventMetadata.html"
      },
      {
        "date": "2023-11-02T00:34:00.000Z",
        "voteCount": 2,
        "content": "I'll go for C. Tbh, I don't think we will need a lambda here as the event rule can definitely trigger the code pipeline &amp; code build."
      },
      {
        "date": "2023-09-10T18:25:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/devops/automated-code-review-on-pull-requests-using-aws-codecommit-and-aws-codebuild/"
      },
      {
        "date": "2023-08-14T06:09:00.000Z",
        "voteCount": 2,
        "content": "Correct C."
      },
      {
        "date": "2023-08-01T23:39:00.000Z",
        "voteCount": 2,
        "content": "\u0421 \nrun tests on pull requests created and when source branch receives new commits to re-run tests"
      },
      {
        "date": "2023-07-29T09:06:00.000Z",
        "voteCount": 2,
        "content": "Not sure why so much discussion. Triggers Rule: A CloudWatch Event Rule is triggered based on the following events: pullRequestSourceBranchUpdated or pullRequestCreated.\nC is only viable option. I mean it even tells you the answer in the question \"development team needed to perform rollbacks to branches in the codebase\".\nAns is C."
      },
      {
        "date": "2023-06-19T15:23:00.000Z",
        "voteCount": 3,
        "content": "This approach allows testing whenever a pull request is created or the source branch of a pull request is updated. When the tests are complete, the AWS Lambda function posts the test status badge as a comment on the pull request, providing visual feedback to reviewers directly in the context of the pull request review.\n\nIt's important to note that CodeBuild creates a build badge that provides status about the last build, which might not directly reflect the test results of the specific pull request. Posting the test results would provide more accurate and relevant information but doing so might require additional scripting or tooling not described in the available options."
      },
      {
        "date": "2023-05-31T00:09:00.000Z",
        "voteCount": 3,
        "content": "C is the correct IMHO.\n\nA pull request is just a branch that the requestor is asking to be merged in master/main. When you create a pull request you set the branch, that is the start, you have to use the current contents of the branch to execute the tests. When time passes developers add commits to that branch or force-push it, changing the contents of the PR's branch. That is the moment in which you have to trigger the tests. The PR comments and discussions may change, but that does not change the code so no need to perform new tests.\nYou only test when the PR is created and every time the branch is pushed (updated)."
      },
      {
        "date": "2023-05-29T05:47:00.000Z",
        "voteCount": 2,
        "content": "Why not B?"
      },
      {
        "date": "2023-05-28T20:27:00.000Z",
        "voteCount": 2,
        "content": "D is incorrect.\n\npullRequestStatusChanged event\nIn this example event, a user who assumed a role named Admin with a session name of Mary_Major closed a pull request with the ID of 1. The pull request was not merged.\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/monitoring-events.html#pullRequestMergeStatusUpdated"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/amazon/view/106266-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has deployed an application in a production VPC in a single AWS account. The application is popular and is experiencing heavy usage. The company\u2019s security team wants to add additional security, such as AWS WAF, to the application deployment. However, the application's product manager is concerned about cost and does not want to approve the change unless the security team can prove that additional security is necessary.<br>The security team believes that some of the application's demand might come from users that have IP addresses that are on a deny list. The security team provides the deny list to a DevOps engineer. If any of the IP addresses on the deny list access the application, the security team wants to receive automated notification in near real time so that the security team can document that the application needs additional security. The DevOps engineer creates a VPC flow log for the production VPC.<br>Which set of additional steps should the DevOps engineer take to meet these requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a log group in Amazon CloudWatch Logs. Configure the VPC flow log to capture accepted traffic and to send the data to the log group. Create an Amazon CloudWatch metric filter for IP addresses on the deny list. Create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Use an Amazon Simple Notification Service (Amazon SNS) topic to send alarm notices to the security team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket for log files. Configure the VPC flow log to capture all traffic and to send the data to the S3 bucket. Configure Amazon Athena to return all log files in the S3 bucket for IP addresses on the deny list. Configure Amazon QuickSight to accept data from Athena and to publish the data as a dashboard that the security team can access. Create a threshold alert of 1 for successful access. Configure the alert to automatically notify the security team as frequently as possible when the alert threshold is met.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket for log files. Configure the VPC flow log to capture accepted traffic and to send the data to the S3 bucket. Configure an Amazon OpenSearch Service cluster and domain for the log files. Create an AWS Lambda function to retrieve the logs from the S3 bucket, format the logs, and load the logs into the OpenSearch Service cluster. Schedule the Lambda function to run every 5 minutes. Configure an alert and condition in OpenSearch Service to send alerts to the security team through an Amazon Simple Notification Service (Amazon SNS) topic when access from the IP addresses on the deny list is detected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a log group in Amazon CloudWatch Logs. Create an Amazon S3 bucket to hold query results. Configure the VPC flow log to capture all traffic and to send the data to the log group. Deploy an Amazon Athena CloudWatch connector in AWS Lambda. Connect the connector to the log group. Configure Athena to periodically query for all accepted traffic from the IP addresses on the deny list and to store the results in the S3 bucket. Configure an S3 event notification to automatically notify the security team through an Amazon Simple Notification Service (Amazon SNS) topic when new objects are added to the S3 bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-20T04:09:00.000Z",
        "voteCount": 7,
        "content": "A meets the requirements at the lowest cost."
      },
      {
        "date": "2023-07-08T04:52:00.000Z",
        "voteCount": 5,
        "content": "opensearch cost a lot of $$$. Athena got tons of things to do afterwards. It's used purely for interactive query. \n\nnatively, vpcflow sends logs to s3 or cloudwatch logs. no brainer answer"
      },
      {
        "date": "2024-01-31T06:37:00.000Z",
        "voteCount": 2,
        "content": "A is correct: push all VPC flow log to cloudwatch logs. Create metric filter to find denied IP addresses. Create cloudwatch alarm with the metric filter as input. Alarm's action is send noti to Security team via SNS\nB: \"Configure the alert to automatically notify the security team\": alert cannot notify by itself. Must use SNS\nC: This option uses both S3 bucket and \"Amazon OpenSearch Service cluster\" to store log files, which would cost a lot of money and unnecessary \nD: This option uses both S3 bucket and VPC flow log to store log files, which is costly"
      },
      {
        "date": "2023-05-13T07:22:00.000Z",
        "voteCount": 2,
        "content": "A most cost effective"
      },
      {
        "date": "2023-05-02T20:55:00.000Z",
        "voteCount": 3,
        "content": "To meet the requirements most cost-effectively, the DevOps engineer should create a log group in Amazon CloudWatch Logs and configure the VPC flow log to capture accepted traffic and to send the data to the log group. Then, create an Amazon CloudWatch metric filter for IP addresses on the deny list and create a CloudWatch alarm with the metric filter as input. Set the period to 5 minutes and the datapoints to alarm to 1. Finally, use an Amazon Simple Notification Service (Amazon SNS) topic to send alarm notices to the security team.\n\nOption A is the correct answer. It provides a cost-effective solution that meets the requirements. The CloudWatch alarm notifies the security team in near real-time when traffic from an IP address on the deny list is detected. This will help the security team document that the application needs additional security. This solution only requires the use of AWS services that the company is already using, and does not require any additional services or tools."
      },
      {
        "date": "2023-04-30T22:22:00.000Z",
        "voteCount": 1,
        "content": "D. Create an Amazon EventBridge rule that reacts to the pullRequestStatusChanged event. Create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete."
      },
      {
        "date": "2023-09-05T12:32:00.000Z",
        "voteCount": 1,
        "content": "Option D does not mention an event bridge rule"
      },
      {
        "date": "2023-11-22T13:02:00.000Z",
        "voteCount": 1,
        "content": "This answer is for a previous question"
      },
      {
        "date": "2023-04-30T22:22:00.000Z",
        "voteCount": 2,
        "content": "The best way to automate testing of pull requests in CodeCommit is to use Amazon EventBridge rules to detect pullRequestStatusChanged events, which are triggered when a pull request's status changes. When this event occurs, the DevOps engineer can create an AWS Lambda function that invokes a CodePipeline pipeline with a CodeBuild action that runs the tests for the application. Finally, program the Lambda function to post the CodeBuild test results as a comment on the pull request when the test results are complete. This approach ensures that reviewers more easily see the results of automated tests as part of the pull request review, without the need to perform manual testing or rollbacks in the codebase."
      },
      {
        "date": "2023-04-15T09:24:00.000Z",
        "voteCount": 2,
        "content": "A seems correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/amazon/view/106265-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer has automated a web service deployment by using AWS CodePipeline with the following steps:<br>1. An AWS CodeBuild project compiles the deployment artifact and runs unit tests.<br>2. An AWS CodeDeploy deployment group deploys the web service to Amazon EC2 instances in the staging environment.<br>3. A CodeDeploy deployment group deploys the web service to EC2 instances in the production environment.<br>The quality assurance (QA) team requests permission to inspect the build artifact before the deployment to the production environment occurs. The QA team wants to run an internal penetration testing tool to conduct manual tests. The tool will be invoked by a REST API call.<br>Which combination of actions should the DevOps engineer take to fulfill this request? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert a manual approval action between the test actions and deployment actions of the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the buildspec.yml file for the compilation stage to require manual approval before completion.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeDeploy deployment groups so that they require manual approval to proceed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the pipeline to directly call the REST API for the penetration testing tool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the pipeline to invoke an AWS Lambda function that calls the REST API for the penetration testing tool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-19T15:26:00.000Z",
        "voteCount": 6,
        "content": "Explanation:\nThe manual approval action (A) will allow the QA team to inspect the build artifact and run their internal penetration testing tool before the deployment to the production environment proceeds.\n\nUsing an AWS Lambda function (E) would provide an automated way to call the REST API of the penetration testing tool. This would allow for the tests to be conducted automatically within the pipeline. This is beneficial because it ensures consistency in the testing process and could be run programmatically, reducing manual steps."
      },
      {
        "date": "2024-08-06T03:02:00.000Z",
        "voteCount": 1,
        "content": "Option D (updating the pipeline to directly call the REST API for the penetration testing tool) is not recommended because it tightly couples the pipeline with the QA team's tool, making it less flexible and harder to maintain. Using a Lambda function as an intermediary provides better separation of concerns and easier maintainability."
      },
      {
        "date": "2024-07-25T22:57:00.000Z",
        "voteCount": 1,
        "content": "Should be AE\nAlthough there are limitation 15mins of Lambda function. \nBut Option D is wrong as CodePipeline does not have the ability to execute HTTP requests \"directly\".\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html"
      },
      {
        "date": "2024-08-01T03:02:00.000Z",
        "voteCount": 1,
        "content": "For option A, keywords: conduct manual tests"
      },
      {
        "date": "2024-03-22T08:51:00.000Z",
        "voteCount": 1,
        "content": "This is tricky but AD should be a better choice because of the 15 min timeout of Lambda functions. To call REST API in CodePipeline these are the two options\nFor complex API calls, security requirements, and access to external resources, an AWS Lambda function is the recommended approach.\nFor simple API calls with limited requirements, consider the inline script approach within CodeBuild, but with caution due to security and maintainability limitations."
      },
      {
        "date": "2024-02-28T04:14:00.000Z",
        "voteCount": 3,
        "content": "AE there is no way to call REST API directly in the code pipeline, it is possible invoke via Lambda function only"
      },
      {
        "date": "2024-02-22T02:37:00.000Z",
        "voteCount": 3,
        "content": "CodePipeline does not have the ability to execute HTTP requests \"directly\"."
      },
      {
        "date": "2024-01-31T06:48:00.000Z",
        "voteCount": 1,
        "content": "A and D are correct: a manual approval action between the test actions and deployment actions allows tester to verify and test built artifacts before allowing deploying to production \nB and C: no mentions of test and deployment env\nE: manual test take more than 15 minutes, which is the maximum execution time of lambda"
      },
      {
        "date": "2024-01-15T08:21:00.000Z",
        "voteCount": 2,
        "content": "D is wrong,  alternative option ( not using Lambda, for example, if the pen testing will take more than 15 minutes) is using codebuild, either add a new codebuild for pen testing, or update existing unit testing codebuild to include pen testing.  You should never run Pen testing inside codepipeline directly ,  it lacks the hooks to collect test result, inform result, etc"
      },
      {
        "date": "2023-11-06T04:10:00.000Z",
        "voteCount": 4,
        "content": "Tricky one:\nCodeDeploy can't do actions directly like invoke REST API but code Build can.\ne.g. it's mentioned to test build artifacts.. So after the build artifact is created This means the solution uses Code Build even not from Code Build you can setup a python script and run it directly using Code Build Command: \nI'd not use Lambda as an alternative due to the time taken for penetration tests would take more than 15 mins. and the pipeline would failed with Lambda execution timeout."
      },
      {
        "date": "2024-07-27T23:58:00.000Z",
        "voteCount": 1,
        "content": "The lambda function would just invoke the REST API, it won't execute the pen test itself. An asynchronous mechanism involving SQS could handle the waiting time between the requesting sending and the response receiving, which can indeed last more than 15mn."
      },
      {
        "date": "2023-08-14T02:08:00.000Z",
        "voteCount": 1,
        "content": "conducting manual tests might takes more than 15m."
      },
      {
        "date": "2023-08-01T22:43:00.000Z",
        "voteCount": 1,
        "content": "A, \nE in practice is a cheap and handy off-switch, recommended, for some contributors to CI/CD that we don't control directly. However, no idea what the writer of the question wanted."
      },
      {
        "date": "2023-07-25T02:25:00.000Z",
        "voteCount": 1,
        "content": "why don't you choose D"
      },
      {
        "date": "2023-07-11T17:58:00.000Z",
        "voteCount": 2,
        "content": "I\u2019ll choose AE. I can tie up multiple REST calls in a Lambda and customize it as I wish. A web hook is not flexible in this aspect I feel."
      },
      {
        "date": "2023-06-20T04:12:00.000Z",
        "voteCount": 1,
        "content": "AD, Lambda is not needed, a webhook can call REST API directly."
      },
      {
        "date": "2023-08-31T17:45:00.000Z",
        "voteCount": 1,
        "content": "But there is no option to invoke call an API directly = https://docs.aws.amazon.com/codepipeline/latest/userguide/integrations-action-type.html#integrations-invoke"
      },
      {
        "date": "2023-05-31T00:23:00.000Z",
        "voteCount": 3,
        "content": "\"AWS Lambda is a compute service that lets you run code without provisioning or managing servers. You can create Lambda functions and add them as actions in your pipelines. Because Lambda allows you to write functions to perform almost any task, you can customize the way your pipeline works. \"\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html"
      },
      {
        "date": "2023-05-29T02:21:00.000Z",
        "voteCount": 2,
        "content": "A &amp; D, lambda (E) is extra and not needed."
      },
      {
        "date": "2023-05-24T02:19:00.000Z",
        "voteCount": 2,
        "content": "Yepp A, E for me"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/amazon/view/106264-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is hosting a web application in an AWS Region. For disaster recovery purposes, a second region is being used as a standby. Disaster recovery requirements state that session data must be replicated between regions in near-real time and 1% of requests should route to the secondary region to continuously verify system functionality. Additionally, if there is a disruption in service in the main region, traffic should be automatically routed to the secondary region, and the secondary region must be able to scale up to handle all traffic.<br>How should a DevOps engineer meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn both regions, deploy the application on AWS Elastic Beanstalk and use Amazon DynamoDB global tables for session data. Use an Amazon Route 53 weighted routing policy with health checks to distribute the traffic across the regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn both regions, launch the application in Auto Scaling groups and use DynamoDB for session data. Use a Route 53 failover routing policy with health checks to distribute the traffic across the regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn both regions, deploy the application in AWS Lambda, exposed by Amazon API Gateway, and use Amazon RDS for PostgreSQL with cross-region replication for session data. Deploy the web application with client-side logic to call the API Gateway directly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn both regions, launch the application in Auto Scaling groups and use DynamoDB global tables for session data. Enable an Amazon CloudFront weighted distribution across regions. Point the Amazon Route 53 DNS record at the CloudFront distribution."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T09:57:00.000Z",
        "voteCount": 7,
        "content": "I think it is A, We can have  failover with CloudFront, but it can't have weighted routing, Here is the link of how auromatic failover works in the CloudFront\n\nhttps://disaster-recovery.workshop.aws/en/services/networking/cloudfront/cloudfront-origin-group.html"
      },
      {
        "date": "2024-07-25T23:12:00.000Z",
        "voteCount": 1,
        "content": "A\nKeywords: web application - ElasticBeanstalk, weighted routing required. \nDynamoDB Global Table required.\n\nAs understand, Cloudfront not support weighted routing."
      },
      {
        "date": "2024-07-23T19:57:00.000Z",
        "voteCount": 1,
        "content": "A - correct - Elasticbeanstalk - option of ALB to register route53 with active-active alias with health checks and weighted routing\n\"In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. Route 53 can respond to a DNS query using any healthy record.\"\n\n\nD - incorrect because no ALB in front of ASG."
      },
      {
        "date": "2024-06-06T14:54:00.000Z",
        "voteCount": 1,
        "content": "If the requirement is for \"1% of requests should route to the secondary region to continuously\", then that means the secondary region is in continuously in an Active state (Active/Active).  A \"request\" is not a health check.  You also have to have auto-scaling to dynamically pick up any extra traffic.  The question is a little weird, in I don't know you you dynamically adjust the weighted routing policy to steer all traffice to the secondary region.  I just know that \"D\" is the closest choice to meeting the specified requirements.  This is absolutely and \"Active-Active\" design using weighted routing at some level, and auto-scaling just meets the demaind wherever it comes from.  I think \"latency-based\" routing would make more sence, but the requirements are clearly describing \"weighted routing\" and Active-Active design.\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/latency-based-routing-leveraging-amazon-cloudfront-for-a-multi-region-active-active-architecture/"
      },
      {
        "date": "2024-06-06T15:08:00.000Z",
        "voteCount": 1,
        "content": "Just to clarify, the scenarios is absolutely desrivinb \"weighted routing\" with 99% to 1% traffic split between regions for normal operation (unbalanced Active/Active)."
      },
      {
        "date": "2024-05-01T10:55:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2024-03-17T03:58:00.000Z",
        "voteCount": 2,
        "content": "A is correct\nB + C no DynamoDB Global Tables\nD - does not use Route53"
      },
      {
        "date": "2024-01-31T06:50:00.000Z",
        "voteCount": 2,
        "content": "A is correct: beanstalk is literally designed for this specific purpose"
      },
      {
        "date": "2024-01-04T04:53:00.000Z",
        "voteCount": 2,
        "content": "It is A, 1% of the traffic should be going to the 2ndary site. so that's weighted routing."
      },
      {
        "date": "2023-12-21T08:53:00.000Z",
        "voteCount": 1,
        "content": "Why not B?"
      },
      {
        "date": "2024-01-07T10:00:00.000Z",
        "voteCount": 1,
        "content": "We have to use DynamoDB Global tables for make db acessable from 2 regions."
      },
      {
        "date": "2023-11-03T02:08:00.000Z",
        "voteCount": 2,
        "content": "It's A"
      },
      {
        "date": "2023-11-01T14:34:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2023-10-22T15:31:00.000Z",
        "voteCount": 1,
        "content": "Answer is D.\n\"Testing Regional failover\"\nhttps://aws.amazon.com/blogs/networking-and-content-delivery/latency-based-routing-leveraging-amazon-cloudfront-for-a-multi-region-active-active-architecture/"
      },
      {
        "date": "2024-03-22T09:01:00.000Z",
        "voteCount": 1,
        "content": "The title of this page mentions Active-Active scenario and not Active-Passive as mentioned in this question."
      },
      {
        "date": "2023-09-05T12:43:00.000Z",
        "voteCount": 1,
        "content": "Option A uses Elastic Beanstalk, which is not as scalable as Auto Scaling groups.\nD is correct"
      },
      {
        "date": "2023-10-19T07:41:00.000Z",
        "voteCount": 2,
        "content": "A- Route 53 does offer the capability to automatically route traffic to the secondary region in case of a disruption. In the context of the requirements specified, option A seems to be a feasible solution as it involves the use of AWS Elastic Beanstalk for deployment, DynamoDB global tables for session data replication, and a weighted routing policy in Route 53 for traffic distribution across regions. The health checks can ensure that traffic is routed to the secondary region automatically in case of a disruption in the main region.\n\nTherefore, considering the ability of Route 53 to automatically reroute traffic, option A appears to be the most appropriate solution for meeting the specified disaster recovery requirements."
      },
      {
        "date": "2023-07-30T12:01:00.000Z",
        "voteCount": 1,
        "content": "The answer is NONE of them, none of them specified both weighted and failover routing policies."
      },
      {
        "date": "2023-07-16T04:46:00.000Z",
        "voteCount": 2,
        "content": "D is not offering scaling on DRP.\u202fA offers scaling by using BeanStalk."
      },
      {
        "date": "2023-06-30T09:49:00.000Z",
        "voteCount": 4,
        "content": "Going for A given that DynamoDB global tables can replicate data across selected regions in near real-time. Clearly weighting and failover, thus Route53 should be selected. \n\nIt's not D because Cloudfront cannot do weighted routing."
      },
      {
        "date": "2023-06-20T22:45:00.000Z",
        "voteCount": 3,
        "content": "A and D are very similar but with using different services (BeanStalk vs CloudFront). However A is using R53 traffic distribution and D is using CF traffic distribution. I think D is better in this case. Note that not all applications will run easily on BeanStalk too."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/amazon/view/106441-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs an application on Amazon EC2 instances. The company uses a series of AWS CloudFormation stacks to define the application resources. A developer performs updates by building and testing the application on a laptop and then uploading the build output and CloudFormation stack templates to Amazon S3. The developer's peers review the changes before the developer performs the CloudFormation stack update and installs a new version of the application onto the EC2 instances.<br>The deployment process is prone to errors and is time-consuming when the developer updates each EC2 instance with the new application. The company wants to automate as much of the application deployment process as possible while retaining a final manual approval step before the modification of the application or resources.<br>The company already has moved the source code for the application and the CloudFormation templates to AWS CodeCommit. The company also has created an AWS CodeBuild project to build and test the application.<br>Which combination of steps will meet the company\u2019s requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application group and a deployment group in AWS CodeDeploy. Install the CodeDeploy agent on the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application revision and a deployment group in AWS CodeDeploy. Create an environment in CodeDeploy. Register the EC2 instances to the CodeDeploy environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to invoke the CodeBuild job, run the CloudFormation update, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, run the CloudFormation change sets and start the AWS CodeDeploy deployment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline to invoke the CodeBuild job, create CloudFormation change sets for each of the application stacks, and pause for a manual approval step. After approval, start the AWS CodeDeploy deployment."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-19T15:29:00.000Z",
        "voteCount": 13,
        "content": "(A) This step sets up the environment to use AWS CodeDeploy for application deployments. CodeDeploy uses an agent installed on the EC2 instances to perform the deployment tasks.\n\n(D) This option uses CodePipeline to orchestrate the process. CodeBuild is used to build and test the application. CloudFormation is used to prepare the infrastructure updates as change sets. A manual approval step is inserted before applying the changes. After approval, the CloudFormation change sets are applied, and then CodeDeploy is invoked to deploy the new version of the application to the EC2 instances."
      },
      {
        "date": "2023-08-19T04:00:00.000Z",
        "voteCount": 5,
        "content": "There is no application group in CodeDeploy"
      },
      {
        "date": "2023-08-22T03:51:00.000Z",
        "voteCount": 6,
        "content": "- EC2 needs to install the CodeDeploy agent.\n- CodeDeploy does not need to register EC2 instances, instead of it uses tag filter.\nTherefore, B is incorrect, A is correct. Final answer: AD"
      },
      {
        "date": "2023-10-26T19:01:00.000Z",
        "voteCount": 3,
        "content": "@fanq10 \nyou are right.\nCodeDeploy doesn't require registering EC2 instances, it fillters by tag"
      },
      {
        "date": "2024-07-25T23:20:00.000Z",
        "voteCount": 2,
        "content": "AD\nA - To run CodeDeploy on EC2, need agent. \nD - The approval step will trigger both CloudFormation and CodeDeploy\n\nB incorrect as no mention of installing agent on EC2 and CodeDeploy doesn't require registering EC2 instances, it filters by tag. \nC incorrect as The approval step does not affect CloudFormation updates, which is not accepted\nE incorrect as The approval step only allows CodeDeploy but no have CloudFormation updates"
      },
      {
        "date": "2024-07-13T00:30:00.000Z",
        "voteCount": 2,
        "content": "B:\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html\nYou can configure automatic installation and updates of the CodeDeploy agent when you create your deployment group in the console.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/applications.html\nAfter you configure instances, but before you can deploy a revision, you must create an application in CodeDeploy. An application is simply a name or container used by CodeDeploy to ensure the correct revision, deployment configuration, and deployment group are referenced during a deployment.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/application-revisions.html\nIn CodeDeploy, a revision contains a version of the source files CodeDeploy will deploy to your instances or scripts CodeDeploy will run on your instances.\nhttps://aws.amazon.com/ru/blogs/devops/using-codedeploy-environment-variables/"
      },
      {
        "date": "2024-06-30T10:26:00.000Z",
        "voteCount": 1,
        "content": "for those who vote B: what is creating environment in code deploy?\n\nI think application group means application and registering instances with code deploy is basically creating deployment group, and instance is not registered manually it has to be tagged"
      },
      {
        "date": "2024-05-01T10:57:00.000Z",
        "voteCount": 1,
        "content": "AD is ok"
      },
      {
        "date": "2024-03-08T07:31:00.000Z",
        "voteCount": 3,
        "content": "A- https://docs.aws.amazon.com/codedeploy/latest/userguide/codedeploy-agent.html \nD - This option correctly utilizes AWS CodePipeline to invoke the CodeBuild job and create CloudFormationchange sets. It adds a manual approval step before executing the change sets and starting the AWSCodeDeploy deployment. This ensures that the deployment process is automated while retaining the\nfinal manual approval step."
      },
      {
        "date": "2024-02-28T05:40:00.000Z",
        "voteCount": 1,
        "content": "AD\nNeeds to install code deploy agent and give necessary permission for access S3 bucket where it  will be stored the application revision.  then EC2 instance will download the application revision from the S3 bucket. Therefore Answer A is correct. if we use System manager only the EC2 instance can be installed and updated automatically."
      },
      {
        "date": "2024-01-31T07:01:00.000Z",
        "voteCount": 2,
        "content": "A and D are correct: A - To run codedploy on EC2, need agent. D - The approval step will trigger both cloudformation and codedeploy\nB: no mention of installing agent on EC2\nC: The approval step doesnot affect CloudFormation updates, which is not accepted\nE: The approval step only allows codedeploy but not CloudFormation updates"
      },
      {
        "date": "2024-01-04T05:08:00.000Z",
        "voteCount": 1,
        "content": "AD is the correct answer, you need the CD agent in order to use code deploy.  And D is correct also because you can do your testing during codebuild and finally do a change set review and then approval."
      },
      {
        "date": "2023-11-06T03:53:00.000Z",
        "voteCount": 4,
        "content": "Code deploy agent installation can be skipped when you setting up Code Deploy group .. e.g.\nYou can configure automatic installation and updates of the CodeDeploy agent when you create your deployment group in the console.\nWhy A is wrong - cause there is no application group only deployment group and when setting up deployment group you can setup agent installation automatically.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-ec2-create.html#:~:text=Note-,You%20can%20configure%20automatic%20installation%20and%20updates%20of%20the%20CodeDeploy%20agent%20when%20you%20create%20your%20deployment%20group%20in%20the%20console.,-Did%20this%20page"
      },
      {
        "date": "2024-01-04T05:11:00.000Z",
        "voteCount": 1,
        "content": "i agree with you on the explanation that there are no application group. but how can you use code deploy to push code to the EC2's if they have no agents on them?"
      },
      {
        "date": "2023-06-20T22:54:00.000Z",
        "voteCount": 2,
        "content": "AD is right."
      },
      {
        "date": "2023-06-06T02:19:00.000Z",
        "voteCount": 1,
        "content": "The CodeDeploy agent must be installed on your Amazon EC2 instance before using it in CodeDeploy deployments"
      },
      {
        "date": "2023-05-29T21:24:00.000Z",
        "voteCount": 2,
        "content": "There is nothing like application group in code deploy answer is BD"
      },
      {
        "date": "2023-05-13T07:45:00.000Z",
        "voteCount": 2,
        "content": "AD need codedeploy agent, and review CFN changes set, before run update"
      },
      {
        "date": "2023-05-07T01:32:00.000Z",
        "voteCount": 2,
        "content": "A and D are correct"
      },
      {
        "date": "2023-04-16T14:57:00.000Z",
        "voteCount": 1,
        "content": "B and D are correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/amazon/view/106262-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances run in an EC2 Auto Scaling group across multiple Availability Zones. The engineer needs to implement a deployment strategy that:<br>Launches a second fleet of instances with the same capacity as the original fleet.<br>Maintains the original fleet unchanged while the second fleet is launched.<br>Transitions traffic to the second fleet when the second fleet is fully deployed.<br>Terminates the original fleet automatically 1 hour after transition.<br>Which solution will satisfy these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS CloudFormation template with a retention policy for the ALB set to 1 hour. Update the Amazon Route 53 record to reflect the new ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse two AWS Elastic Beanstalk environments to perform a blue/green deployment from the original environment to the new one. Create an application version lifecycle policy to terminate the original environment in 1 hour.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy with a deployment group configured with a blue/green deployment configuration Select the option Terminate the original instances in the deployment group with a waiting period of 1 hour.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk with the configuration set to Immutable. Create an .ebextension using the Resources key that sets the deletion policy of the ALB to 1 hour, and deploy the application."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-30T22:41:00.000Z",
        "voteCount": 10,
        "content": "Option B, using two AWS Elastic Beanstalk environments to perform a blue/green deployment from the original environment to the new one, would not launch a second fleet of instances. Instead, it would create a new environment and deploy the application version to it. It also requires the use of an application version lifecycle policy to terminate the original environment in 1 hour.\n\nOption D, using AWS Elastic Beanstalk with the configuration set to Immutable and creating an .ebextension to set the deletion policy of the ALB to 1 hour, would not launch a second fleet of instances, and it would not maintain the original fleet unchanged while the second fleet is launched. Additionally, the .ebextension approach is not the recommended way to delete resources in AWS.\n\nTherefore, the correct option is C, using AWS CodeDeploy with a deployment group configured with a blue/green deployment configuration and selecting the option to Terminate the original instances in the deployment group with a waiting period of 1 hour."
      },
      {
        "date": "2024-10-03T00:31:00.000Z",
        "voteCount": 1,
        "content": "Option B would require more manual intervention and configuration, making it a less optimal solution compared to the seamless blue/green deployment and automatic fleet termination provided by Option C.  \n\nOption D involves unnecessary complexity around setting ALB deletion policies, and while immutable deployments offer zero-downtime updates, they don't fully meet the core requirements of automatic traffic shifting and fleet termination"
      },
      {
        "date": "2024-06-01T14:30:00.000Z",
        "voteCount": 1,
        "content": "Immutable strategy with Elastic Beanstalk involves deploying additional instance while Blue/Green strategy involves deploying another environment. The key difference is environment vs instances"
      },
      {
        "date": "2024-05-01T10:58:00.000Z",
        "voteCount": 1,
        "content": "C is ok"
      },
      {
        "date": "2024-03-02T11:28:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups-create-blue-green.html"
      },
      {
        "date": "2024-01-31T07:09:00.000Z",
        "voteCount": 1,
        "content": "A is correct: The question ask for a solution to automatic deployment of EC2 instances, which is the job of cloudFormation\n- B and D is irrelevant because it is use to deploy webapps only, not EC2 instances\n- C is also irrelevant because codedeploy (literlly by the name: CODEdeploy) is only used for deploying code, not EC2 instances, which is not code. Dont know why ChatGPT recommend this, but it is wrong definitely"
      },
      {
        "date": "2024-02-18T05:24:00.000Z",
        "voteCount": 1,
        "content": "No C is the more accurate and logical"
      },
      {
        "date": "2023-06-20T23:03:00.000Z",
        "voteCount": 3,
        "content": "C looks like the best solution."
      },
      {
        "date": "2023-04-30T22:40:00.000Z",
        "voteCount": 3,
        "content": "To satisfy the requirements of launching a second fleet of instances with the same capacity as the original fleet, maintaining the original fleet unchanged while the second fleet is launched, transitioning traffic to the second fleet when the second fleet is fully deployed, and terminating the original fleet automatically 1 hour after the transition, the best solution is to use AWS CodeDeploy with a blue/green deployment configuration, and selecting the option to Terminate the original instances in the deployment group with a waiting period of 1 hour."
      },
      {
        "date": "2023-04-15T08:45:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/amazon/view/106260-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A video-sharing company stores its videos in Amazon S3. The company has observed a sudden increase in video access requests, but the company does not know which videos are most popular. The company needs to identify the general access pattern for the video files. This pattern includes the number of users who access a certain file on a given day, as well as the number of pull requests for certain files.<br>How can the company meet these requirements with the LEAST amount of effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate S3 server access logging. Import the access logs into an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate S3 server access logging. Use Amazon Athena to create an external table with the log files. Use Athena to create a SQL query to analyze the access patterns.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke an AWS Lambda function for every S3 object access event. Configure the Lambda function to write the file access information, such as user. S3 bucket, and file key, to an Amazon Aurora database. Use an Aurora SQL query to analyze the access patterns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecord an Amazon CloudWatch Logs log message for every S3 object access event. Configure a CloudWatch Logs log stream to write the file access information, such as user, S3 bucket, and file key, to an Amazon Kinesis Data Analytics for SQL application. Perform a sliding window analysis."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-22T10:54:00.000Z",
        "voteCount": 2,
        "content": "Bis the Answer because Athena is designed for these type of use cases\nAWS Athena is a serverless interactive query service that lets you analyze data stored in Amazon Simple Storage Service (S3) using standard SQL."
      },
      {
        "date": "2024-01-31T07:14:00.000Z",
        "voteCount": 2,
        "content": "B is correct:Use S3 in combination with Athena is the recommended way to analyze data\nA: setups of Aurora is complex and unnecessary. It also more costly than B\nC and D are both too complicated."
      },
      {
        "date": "2023-07-08T18:30:00.000Z",
        "voteCount": 4,
        "content": "B is so much simpler. Athena can do interactive queries on S3 data."
      },
      {
        "date": "2023-06-20T23:06:00.000Z",
        "voteCount": 2,
        "content": "B is correct and simplest."
      },
      {
        "date": "2023-06-01T01:38:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2023-05-31T00:37:00.000Z",
        "voteCount": 1,
        "content": "B is the natural way to do it"
      },
      {
        "date": "2023-05-11T15:24:00.000Z",
        "voteCount": 1,
        "content": "https://repost.aws/ja/knowledge-center/analyze-logs-athena"
      },
      {
        "date": "2023-05-02T21:13:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2023-04-20T14:07:00.000Z",
        "voteCount": 1,
        "content": "I would go with B"
      },
      {
        "date": "2023-04-15T08:38:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/amazon/view/106259-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A development team wants to use AWS CloudFormation stacks to deploy an application. However, the developer IAM role does not have the required permissions to provision the resources that are specified in the AWS CloudFormation template. A DevOps engineer needs to implement a solution that allows the developers to deploy the stacks. The solution must follow the principle of least privilege.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy that allows the developers to provision the required resources. Attach the policy to the developer IAM role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy that allows full access to AWS CloudFormation. Attach the policy to the developer IAM role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role a cloudformation:* action. Use the new service role during stack deployments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role the iam:PassRole permission. Use the new service role during stack deployments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T10:35:00.000Z",
        "voteCount": 1,
        "content": "D is totally correct"
      },
      {
        "date": "2024-03-08T07:33:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html"
      },
      {
        "date": "2024-02-01T03:34:00.000Z",
        "voteCount": 3,
        "content": "D is correct: Need to create a role for Cloud formation that has the required permissions. Then adding iam:PassRole permission to the dev IAM role to allow them to pass this role to CF\nA: no mention of creating the required permissions for ACF. Additionally, should not grant permissions for dev. \nB: grant full access is against the least privilege policy\nC: no mention of granting iam:PassRole permission to the dev"
      },
      {
        "date": "2023-12-11T18:19:00.000Z",
        "voteCount": 1,
        "content": "A is incorrect; A would also allow resources to be used from outside of cfn.\nTherefore, D is correct."
      },
      {
        "date": "2023-08-26T02:51:00.000Z",
        "voteCount": 2,
        "content": "Option D allows you to create a dedicated AWS CloudFormation service role with the precise permissions required for stack deployments. Then, you grant the developer IAM role the iam:PassRole permission, which enables it to pass the service role to AWS CloudFormation without granting it broad IAM permissions. This approach aligns best with the principle of least privilege and ensures developers can deploy stacks while maintaining control over their permissions."
      },
      {
        "date": "2023-07-15T04:00:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.  DC wrong - Nothing like CloudFormation service-role."
      },
      {
        "date": "2023-08-22T03:58:00.000Z",
        "voteCount": 1,
        "content": "B is not best practice of using CloudFormation. \nD is correct, 100% sure. `iam:PassRole` to a CloudFormation Service Role (take a look at this: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html#:~:text=you%20can%20use.-,Important,-When%20you%20specify)"
      },
      {
        "date": "2023-11-02T02:48:00.000Z",
        "voteCount": 1,
        "content": "Should be D! See here https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html"
      },
      {
        "date": "2023-06-20T23:19:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer."
      },
      {
        "date": "2023-06-20T14:29:00.000Z",
        "voteCount": 1,
        "content": "This solution follows the principle of least privilege by creating a specific AWS CloudFormation service role that only has the permissions required for the resources in the AWS CloudFormation stack. The developers are then granted permission to pass this role (iam:PassRole) to the AWS CloudFormation service when they initiate stack deployments, which allows the service to act on behalf of the developer to provision the specified resources."
      },
      {
        "date": "2023-05-31T00:51:00.000Z",
        "voteCount": 1,
        "content": "D, you pass the role that can create the resources, the user does not have the right to create the resources himself but can pass the role to CloudFormation so CloudFormation assumes it. IMHO."
      },
      {
        "date": "2023-05-15T06:07:00.000Z",
        "voteCount": 1,
        "content": "This allows them to provision the required resources specified in the CloudFormation template without granting them full access to AWS CloudFormation or the underlying resources."
      },
      {
        "date": "2023-05-13T08:04:00.000Z",
        "voteCount": 1,
        "content": "D , passrole is right action"
      },
      {
        "date": "2023-05-11T15:35:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/ja_jp/AWSCloudFormation/latest/UserGuide/using-iam-servicerole.html"
      },
      {
        "date": "2023-05-08T09:27:00.000Z",
        "voteCount": 1,
        "content": "D is more suitable in this case."
      },
      {
        "date": "2023-05-07T15:36:00.000Z",
        "voteCount": 1,
        "content": "C. Create an AWS CloudFormation service role that has the required permissions. Grant the developer IAM role a cloudformation:* action. Use the new service role during stack deployments.\nBy creating an AWS CloudFormation service role with the required permissions, the DevOps engineer can control the resources that the developers can access. This approach ensures that the developers have only the necessary permissions to deploy the stacks, without granting them excessive permissions that could be exploited by malicious actors. The IAM policy granting a cloudformation:* action to the developer IAM role allows the developers to use the AWS CloudFormation service role and deploy the stacks with the required resources.\nOption A, creating an IAM policy that allows the developers to provision the required resources, is not a good solution because it could potentially grant the developers too much access to resources they don't need. This violates the principle of least privilege."
      },
      {
        "date": "2023-05-07T15:37:00.000Z",
        "voteCount": 1,
        "content": "Option B, creating an IAM policy that allows full access to AWS CloudFormation, is not a good solution either, as it grants excessive permissions to the developers.\n\nOption D, creating an AWS CloudFormation service role with the required permissions and granting the developer IAM role the iam:PassRole permission, allows the developers to assume the service role and deploy the stacks with the required resources. However, this option grants additional permissions to the developer IAM role, which could be abused by malicious actors\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iam-template.html"
      },
      {
        "date": "2023-05-05T13:50:00.000Z",
        "voteCount": 1,
        "content": "D it is"
      },
      {
        "date": "2023-04-30T22:44:00.000Z",
        "voteCount": 1,
        "content": "Option D is the recommended solution to meet the requirements because it follows the principle of least privilege. The IAM policy that allows the developers to provision the required resources should be created and associated with the IAM role, which should be assigned the iam:PassRole permission for the AWS CloudFormation service role. By doing so, the IAM role can only assume the specific AWS CloudFormation service role and deploy the stack with the required permissions, and not have full access to all resources or full access to AWS CloudFormation."
      },
      {
        "date": "2023-04-16T15:03:00.000Z",
        "voteCount": 2,
        "content": "B it is"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/amazon/view/106442-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A production account has a requirement that any Amazon EC2 instance that has been logged in to manually must be terminated within 24 hours. All applications in the production account are using Auto Scaling groups with the Amazon CloudWatch Logs agent configured.<br>How can this process be automated?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch Logs subscription to an AWS Step Functions application. Configure an AWS Lambda function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a second Lambda function once a day that will terminate all instances with this tag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm that will be invoked by the login event. Send the notification to an Amazon Simple Notification Service (Amazon SNS) topic that the operations team is subscribed to, and have them terminate the EC2 instance within 24 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm that will be invoked by the login event. Configure the alarm to send to an Amazon Simple Queue Service (Amazon SQS) queue. Use a group of worker instances to process messages from the queue, which then schedules an Amazon EventBridge rule to be invoked.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch Logs subscription to an AWS Lambda function. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a daily Lambda function that terminates all instances with this tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-11T07:31:00.000Z",
        "voteCount": 1,
        "content": "Opion D: \"with this tag\"\nSo, there will be one tag, like ShoudTerminate: true. But by doing so we will terminate ALL instances with a tag - even those created 10 minutes ago. It doesn't seem correct, or am I missing something?"
      },
      {
        "date": "2024-06-27T10:41:00.000Z",
        "voteCount": 1,
        "content": "D is best answer.\nhint: question includes \"~~Amazon CloudWatch Logs agent configured\"\n         Lambda function is keyword."
      },
      {
        "date": "2024-02-01T07:26:00.000Z",
        "voteCount": 3,
        "content": "D is correct:\nA: If using step function, no need to include \"Amazon EventBridge rule to invoke a second Lambda function\" \nB: With this method, policy-breaching Ec2 would be terminated manually, which cannot ensure that they are terminated within 24 hours\nC: no mention of terminating the instances"
      },
      {
        "date": "2023-12-11T18:27:00.000Z",
        "voteCount": 2,
        "content": "D is correct; with B, SNS can cause delays."
      },
      {
        "date": "2023-06-20T23:25:00.000Z",
        "voteCount": 1,
        "content": "D is the best answer."
      },
      {
        "date": "2023-04-30T22:52:00.000Z",
        "voteCount": 3,
        "content": "D. Create a CloudWatch Logs subscription to an AWS Lambda function. Configure the function to add a tag to the EC2 instance that produced the login event and mark the instance to be decommissioned. Create an Amazon EventBridge rule to invoke a daily Lambda function that terminates all instances with this tag."
      },
      {
        "date": "2023-04-16T15:05:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/amazon/view/106258-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has enabled all features for its organization in AWS Organizations. The organization contains 10 AWS accounts. The company has turned on AWS CloudTrail in all the accounts. The company expects the number of AWS accounts in the organization to increase to 500 during the next year. The company plans to use multiple OUs for these accounts.<br>The company has enabled AWS Config in each existing AWS account in the organization. A DevOps engineer must implement a solution that enables AWS Config automatically for all future AWS accounts that are created in the organization.<br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Configure the rule to invoke an AWS Lambda function that enables trusted access to AWS Config for the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, create an AWS CloudFormation stack set to enable AWS Config. Configure the stack set to deploy automatically when an account is created through Organizations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, create an SCP that allows the appropriate AWS Config API calls to enable AWS Config. Apply the SCP to the root-level OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, create an Amazon EventBridge rule that reacts to a CreateAccount API call. Configure the rule to invoke an AWS Systems Manager Automation runbook to enable AWS Config for the account."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-01T08:10:00.000Z",
        "voteCount": 1,
        "content": "B is correct: The question ask a solution to \"enables AWS Config automatically\" for all future accounts. In AWS org, to provision or configure resources on other accounts, we use ACF\nA, C and D: no mention of ACF"
      },
      {
        "date": "2024-02-04T07:56:00.000Z",
        "voteCount": 1,
        "content": "A: trusted access to AWS Config: this is used by other services to access to AWS config, not for account\nD: enable AWS Config for the account: this means we only activate AWS config for the management account, not the newly created ones"
      },
      {
        "date": "2023-11-06T03:15:00.000Z",
        "voteCount": 1,
        "content": "B:\nDetails the new feature with enable trusted access to new accounts in any region\nhttps://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-cloudformation.html"
      },
      {
        "date": "2023-08-21T21:58:00.000Z",
        "voteCount": 1,
        "content": "B  \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-sampletemplates.html"
      },
      {
        "date": "2023-06-21T01:27:00.000Z",
        "voteCount": 1,
        "content": "B is the best solution."
      },
      {
        "date": "2023-05-21T18:58:00.000Z",
        "voteCount": 2,
        "content": "B\nhttps://aws.amazon.com/about-aws/whats-new/2020/02/aws-cloudformation-stacksets-introduces-automatic-deployments-across-accounts-and-regions-through-aws-organizations/"
      },
      {
        "date": "2023-04-30T22:54:00.000Z",
        "voteCount": 4,
        "content": "The correct solution to enable AWS Config automatically for all future AWS accounts created in the organization is Option B: In the organization's management account, create an AWS CloudFormation stack set to enable AWS Config. Configure the stack set to deploy automatically when an account is created through Organizations.\n\nOption C is incorrect because although it suggests creating an SCP that allows the appropriate AWS Config API calls to enable AWS Config and applying the SCP to the root-level OU, it does not specifically enable AWS Config automatically for all future AWS accounts that are created in the organization."
      },
      {
        "date": "2023-12-19T17:53:00.000Z",
        "voteCount": 3,
        "content": "In terms of Option C: SCP can only Deny access, not Allow"
      },
      {
        "date": "2023-04-15T08:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/amazon/view/108077-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has many applications. Different teams in the company developed the applications by using multiple languages and frameworks. The applications run on premises and on different servers with different operating systems. Each team has its own release protocol and process. The company wants to reduce the complexity of the release and maintenance of these applications.<br>The company is migrating its technology stacks, including these applications, to AWS. The company wants centralized control of source code, a consistent and automatic delivery pipeline, and as few maintenance tasks as possible on the underlying infrastructure.<br>What should a DevOps engineer do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one AWS CodeCommit repository for all applications. Put each application's code in a different branch. Merge the branches, and use AWS CodeBuild to build the applications. Use AWS CodeDeploy to deploy the applications to one centralized application server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time. Use AWS CodeDeploy to deploy the applications to one centralized application server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build the applications one at a time and to create one AMI for each server. Use AWS CloudFormation StackSets to automatically provision and decommission Amazon EC2 fleets by using these AMIs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one AWS CodeCommit repository for each of the applications. Use AWS CodeBuild to build one Docker image for each application in Amazon Elastic Container Registry (Amazon ECR). Use AWS CodeDeploy to deploy the applications to Amazon Elastic Container Service (Amazon ECS) on infrastructure that AWS Fargate manages.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-30T22:57:00.000Z",
        "voteCount": 11,
        "content": "Option D is the best choice to meet the requirements of centralized control of source code, a consistent and automatic delivery pipeline, and minimal maintenance tasks.\nOption D is the best choice because it allows each application to have its own repository and build process, but uses containerization to create a consistent and automatic delivery pipeline that can be easily deployed to Amazon ECS on infrastructure that AWS Fargate manages. This approach also provides scalability and ease of maintenance."
      },
      {
        "date": "2024-02-01T08:16:00.000Z",
        "voteCount": 1,
        "content": "D is correct: \"centralized control of source code\" = CodeCommit. \"Consistent and automatic delivery pipeline\" = codepipeline/codebuide/codedeploy. \"as few maintenance tasks as possible on the underlying infrastructur\" = containerization\nA: \"CodeCommit repository for all applications\": should not, need separate repos for each app\nB and C: no mention of containerization (fargate, ECS)"
      },
      {
        "date": "2023-10-02T04:54:00.000Z",
        "voteCount": 1,
        "content": "D is the Answer: https://aws.amazon.com/blogs/compute/building-deploying-and-operating-containerized-applications-with-aws-fargate/"
      },
      {
        "date": "2023-07-22T04:54:00.000Z",
        "voteCount": 1,
        "content": "A ISNT CORRECT?"
      },
      {
        "date": "2024-01-07T14:17:00.000Z",
        "voteCount": 2,
        "content": "Of course no, it is the most wrong one, It saying to have all the applications code in one repo (bad thing to do), separated in branches, and after that merge them (second very bad thing to do)."
      },
      {
        "date": "2023-06-21T09:13:00.000Z",
        "voteCount": 3,
        "content": "Option D. \nI was torn between C and D, but the requirement for ease of maintenance on the underlying infrastructure clearly points to ECS."
      },
      {
        "date": "2023-06-21T01:31:00.000Z",
        "voteCount": 1,
        "content": "D is the best option, there is virtually no infrastructure to manage."
      },
      {
        "date": "2023-05-13T08:21:00.000Z",
        "voteCount": 1,
        "content": "D is best option."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/amazon/view/106444-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's application is currently deployed to a single AWS Region. Recently, the company opened a new office on a different continent. The users in the new office are experiencing high latency. The company's application runs on Amazon EC2 instances behind an Application Load Balancer (ALB) and uses Amazon DynamoDB as the database layer. The instances run in an EC2 Auto Scaling group across multiple Availability Zones. A DevOps engineer is tasked with minimizing application response times and improving availability for users in both Regions.<br>Which combination of actions should be taken to address the latency issues? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new DynamoDB table in the new Region with cross-Region replication enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new ALB and Auto Scaling group global resources and configure the new ALB to direct traffic to the new Auto Scaling group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon Route 53 aliases, health checks, and failover routing policies to route to the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the DynamoDB table to a global table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CDF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CDF",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-30T23:02:00.000Z",
        "voteCount": 10,
        "content": "C. Create new ALB and Auto Scaling group resources in the new Region and configure the new ALB to direct traffic to the new Auto Scaling group. This will allow users in the new Region to access the application with lower latency by reducing the network hops between the user and the application servers.\n\nD. Create Amazon Route 53 records, health checks, and latency-based routing policies to route to the ALB. This will enable Route 53 to route user traffic to the nearest healthy ALB, based on the latency between the user and the ALBs.\n\nF. Convert the DynamoDB table to a global table. This will enable reads and writes to the table in both Regions with low latency, improving the overall response time of the application"
      },
      {
        "date": "2024-04-21T01:07:00.000Z",
        "voteCount": 2,
        "content": "Technically converting dynamodb table to global table requires creating replica in another region with cross-region replication and you don't \"convert\" you add a replica in \"global tables\" in specified region so this answers are a little bit misleading.\n\nProbably F is better than A since they name this operation as \"converting\" e.g. here https://aws.amazon.com/blogs/aws/new-convert-your-single-region-amazon-dynamodb-tables-to-global-tables/"
      },
      {
        "date": "2024-02-01T08:35:00.000Z",
        "voteCount": 1,
        "content": "D is, of course, correct: &lt;apply a core set of security controls to an existing set of AWS accounts&gt; and &lt;The accounts are in an organization in AWS Organizations&gt; means we need ACF template to deploy these set of security controls. &lt;Individual account administrators must not be able to edit or delete any of the baseline resources&gt; means we need scp to deny permission\nA and B: no mention of SCP\nC: this option deploy the rules by AWS Config management account, which is not correct because we need ACF. Additionally, no mention of denying modification to CloudTrail trails"
      },
      {
        "date": "2024-02-01T08:27:00.000Z",
        "voteCount": 1,
        "content": "CDF: &lt;opened a new office on a different continent&gt; and &lt;The users in the new office are experiencing high latency&gt; means they need to replicate their existing site to the new region. &lt;Amazon EC2 instances behind an Application Load Balancer (ALB)&gt; means they need to replicate both these. &lt;address the latency issues&gt; means they need route53 with latency-based and health check\nA: &lt;cross-Region replication&gt; is used for backup only, not a live site. It would introduce a lot of latency\nB: &lt;Auto Scaling group global resources&gt;: there is no such thing\nE: No mention of latency-based."
      },
      {
        "date": "2023-12-23T20:24:00.000Z",
        "voteCount": 1,
        "content": "CDF very easy"
      },
      {
        "date": "2023-10-23T06:15:00.000Z",
        "voteCount": 1,
        "content": "CDF is collect"
      },
      {
        "date": "2023-04-16T15:12:00.000Z",
        "voteCount": 2,
        "content": "C,D,F are correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/amazon/view/106219-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer needs to apply a core set of security controls to an existing set of AWS accounts. The accounts are in an organization in AWS Organizations. Individual teams will administer individual accounts by using the AdministratorAccess AWS managed policy. For all accounts. AWS CloudTrail and AWS Config must be turned on in all available AWS Regions. Individual account administrators must not be able to edit or delete any of the baseline resources. However, individual account administrators must be able to edit or delete their own CloudTrail trails and AWS Config rules.<br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the organization's management account by using CloudFormation StackSets. Set the stack policy to deny Update:Delete actions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Control Tower. Enroll the existing accounts in AWS Control Tower. Grant the individual account administrators access to CloudTrail and AWS Config.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesignate an AWS Config management account. Create AWS Config recorders in all accounts by using AWS CloudFormation StackSets. Deploy AWS Config rules to the organization by using the AWS Config management account. Create a CloudTrail organization trail in the organization\u2019s management account. Deny modification or deletion of the AWS Config recorders by using an SCP.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that defines the standard account resources. Deploy the template to all accounts from the organization's management account by using Cloud Formation StackSets Create an SCP that prevents updates or deletions to CloudTrail resources or AWS Config resources unless the principal is an administrator of the organization's management account."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-30T23:07:00.000Z",
        "voteCount": 16,
        "content": "C\nThis solution meets the requirements in the most operationally efficient way. It uses AWS CloudFormation StackSets to deploy AWS Config recorders in all accounts and AWS Config rules to the organization, which can be centrally managed from an AWS Config management account. A CloudTrail organization trail can also be created in the organization\u2019s management account to collect logs from all accounts. An SCP can be used to deny modification or deletion of the AWS Config recorders, ensuring that the baseline resources cannot be modified or deleted by individual account administrators. However, individual account administrators can still edit or delete their own CloudTrail trails and AWS Config rules."
      },
      {
        "date": "2023-11-21T22:21:00.000Z",
        "voteCount": 2,
        "content": "this solution lacks clarity on allowing individual account administrators control over their CloudTrail trails."
      },
      {
        "date": "2023-12-14T15:52:00.000Z",
        "voteCount": 1,
        "content": "C is good. \nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
      },
      {
        "date": "2023-12-14T15:52:00.000Z",
        "voteCount": 1,
        "content": "An SCP restricts permissions for IAM users and roles in member accounts, including the member account's root user. Any account has only those permissions permitted by every parent above it. If a permission is blocked at any level above the account, either implicitly (by not being included in an Allow policy statement) or explicitly (by being included in a Deny policy statement), a user or role in the affected account can't use that permission, even if the account administrator attaches the AdministratorAccess IAM policy with */* permissions to the user.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
      },
      {
        "date": "2023-11-18T07:29:00.000Z",
        "voteCount": 4,
        "content": "Why C? If you deny modification or deletion of the AWS Config recorders by using an SCP, how do individual account administrators edit or delete their own CloudTrail trails and AWS Config rules?"
      },
      {
        "date": "2024-08-15T00:58:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-07-26T01:06:00.000Z",
        "voteCount": 1,
        "content": "I think should be C\nKeywords: \"an existing set of AWS accounts\""
      },
      {
        "date": "2024-07-16T00:04:00.000Z",
        "voteCount": 1,
        "content": "must be D"
      },
      {
        "date": "2024-05-02T04:26:00.000Z",
        "voteCount": 1,
        "content": "I agree with C"
      },
      {
        "date": "2024-03-23T02:20:00.000Z",
        "voteCount": 2,
        "content": "Option C is the most operationally efficient and meets all the requirements: ensuring CloudTrail and AWS Config are enabled in all regions, preventing the deletion or editing of baseline resources by individual account administrators, while still allowing them the flexibility to manage their own specific resources. This approach uses centralized control mechanisms (AWS Config management account and organization trail for CloudTrail) and leverages SCPs for enforcement, aligning with best practices for security and governance in AWS Organizations."
      },
      {
        "date": "2024-03-05T07:15:00.000Z",
        "voteCount": 1,
        "content": "Im going with D. SCPs is what helps us here"
      },
      {
        "date": "2024-05-06T18:55:00.000Z",
        "voteCount": 1,
        "content": "but SCP not support direct principal."
      },
      {
        "date": "2024-03-03T03:42:00.000Z",
        "voteCount": 3,
        "content": "D for me. \nI think C is incorrect because \"However, individual account administrators must be able to edit or delete their own CloudTrail trails and AWS Config rules.\" requirement is not satisfied because this answer has nothing about individual account administrators are able to edit their own CloudTrail trails. Organisational trail can be edited only from management or delegated administrator account."
      },
      {
        "date": "2024-02-28T19:58:00.000Z",
        "voteCount": 2,
        "content": "C for sure"
      },
      {
        "date": "2024-02-24T00:22:00.000Z",
        "voteCount": 3,
        "content": "When Control Tower is enabled, AWS-GR_CLOUDTRAIL_ENABLED and AWS-GR_CONFIG_ENABLED will enable CloudTrail and Config in all available regions. The guardrails are automatically set to disallow changes to baseline resources.\n\nA, C, D - No mention about baseline resource."
      },
      {
        "date": "2024-02-13T02:07:00.000Z",
        "voteCount": 1,
        "content": "D is correct: This denies modifications to AWS config or cloudtrail unless the principal is the management account\nA: No explicitly mention of denying modifications to Config or cloudtrail\nB: No explicitly mention of denying modifications to Config or cloudtrail\nC: &lt; Create a CloudTrail organization trail in the organization\u2019s management account&gt;: This means the deny rule only affects the management account"
      },
      {
        "date": "2024-02-09T08:38:00.000Z",
        "voteCount": 1,
        "content": "C is using AWS Config Recorder, AWS Config uses the configuration recorder to detect changes in your resource configurations and capture these changes as configuration items.\n\nIt is not used for prevent you doing something, it is detecting something"
      },
      {
        "date": "2024-02-06T08:08:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
      },
      {
        "date": "2024-02-06T16:28:00.000Z",
        "voteCount": 1,
        "content": "how many questions are there in DOP-C02. It says 217, but i dont see that many"
      },
      {
        "date": "2024-02-12T09:23:00.000Z",
        "voteCount": 1,
        "content": "i only see 209 questions i think even though it says 217 not sure if its something that they have to wait 2 weeks to release the rest since they update it maybe"
      },
      {
        "date": "2024-02-01T08:35:00.000Z",
        "voteCount": 1,
        "content": "CloudTrail trails"
      },
      {
        "date": "2024-01-28T05:31:00.000Z",
        "voteCount": 1,
        "content": "CloudTrail resources we want todeny this not config recorder"
      },
      {
        "date": "2024-01-15T09:29:00.000Z",
        "voteCount": 1,
        "content": "the common practice is using stacksets to enable AWS config, so D make sense"
      },
      {
        "date": "2024-01-15T09:27:00.000Z",
        "voteCount": 1,
        "content": "C mentioned using AWS Config recorders, which is for drift detection and has nothing to do with enable AWS config"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/amazon/view/105450-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has its AWS accounts in an organization in AWS Organizations. AWS Config is manually configured in each AWS account. The company needs to implement a solution to centrally configure AWS Config for all accounts in the organization The solution also must record resource changes to a central account.<br>Which combination of actions should a DevOps engineer perform to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a delegated administrator account for AWS Config. Enable trusted access for AWS Config in the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a delegated administrator account for AWS Config. Create a service-linked role for AWS Config in the organization\u2019s management account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template to create an AWS Config aggregator. Configure a CloudFormation stack set to deploy the template to all accounts in the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config organization aggregator in the organization's management account. Configure data collection from all AWS accounts in the organization and from all AWS Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config organization aggregator in the delegated administrator account. Configure data collection from all AWS accounts in the organization and from all AWS Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-18T13:17:00.000Z",
        "voteCount": 15,
        "content": "AE \nhttps://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/\nA - When enabling trust - the service-linked role will be created but not the other way around.\nE - the delegated account will be the account that manages AWS config so it should collect all data centrally."
      },
      {
        "date": "2024-07-26T01:12:00.000Z",
        "voteCount": 1,
        "content": "A - You can enable trusted access using either the AWS Config console or the AWS Organizations console.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html"
      },
      {
        "date": "2024-03-25T08:48:00.000Z",
        "voteCount": 1,
        "content": "AE is the answer\nAWS Config offers an organization-wide data aggregation capability called the Config organization aggregator. It allows you to collect and view configuration data from all member accounts within your AWS Organization in a single location. This centralizes your view of resource configurations and compliance posture across your entire AWS environment."
      },
      {
        "date": "2024-02-01T09:03:00.000Z",
        "voteCount": 1,
        "content": "A and E are correct: &lt;AWS Config is manually configured in each AWS account&gt; means we dont need ACF (only used for the deployment of AWS config). &lt;centrally configure AWS Config for all accounts&gt; means we need to allow a central account to control AWS config in all member accounts.\n- &lt;record resource changes to a central account&gt; means we need to collect data from all member accounts and push to the central account\nB: service-linked role only used for interacting with other AWS services\nC: no need ACF\nD: we need AWS Config organization aggregator in the delegated administrator account, not  the organization's management account"
      },
      {
        "date": "2023-09-26T01:03:00.000Z",
        "voteCount": 2,
        "content": "AE is most correct"
      },
      {
        "date": "2023-07-12T01:11:00.000Z",
        "voteCount": 3,
        "content": "Here you have the Tutorial :) \nhttps://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/"
      },
      {
        "date": "2023-06-14T09:44:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/mt/org-aggregator-delegated-admin/\nhttps://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html"
      },
      {
        "date": "2023-05-23T14:28:00.000Z",
        "voteCount": 3,
        "content": "BE is the most efficient"
      },
      {
        "date": "2023-05-08T10:13:00.000Z",
        "voteCount": 3,
        "content": "BD is most suitable in this case"
      },
      {
        "date": "2023-11-06T03:01:00.000Z",
        "voteCount": 1,
        "content": "Why ? it says setup service linked role in management account not in Delegated account?"
      },
      {
        "date": "2023-04-14T14:39:00.000Z",
        "voteCount": 2,
        "content": "The correct answers are B and D. Option B is correct because it suggests configuring a delegated administrator account for AWS Config and creating a service-linked role for AWS Config in the organization\u2019s management account. This allows AWS Config to perform supported operations within the accounts in the organization, and enables trusted access. Option D is correct because it suggests creating an AWS Config organization aggregator in the organization's management account and configuring data collection from all AWS accounts in the organization and from all AWS Regions, which enables multi-account, multi-region data aggregation. Options A and E are not correct because they do not suggest using a service-linked role for AWS Config or creating an AWS Config organization aggregator in the organization's management account."
      },
      {
        "date": "2023-04-06T12:30:00.000Z",
        "voteCount": 3,
        "content": "AE . https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-config.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/amazon/view/108354-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company wants to migrate its content sharing web application hosted on Amazon EC2 to a serverless architecture. The company currently deploys changes to its application by creating a new Auto Scaling group of EC2 instances and a new Elastic Load Balancer, and then shifting the traffic away using an Amazon Route 53 weighted routing policy.<br>For its new serverless application, the company is planning to use Amazon API Gateway and AWS Lambda. The company will need to update its deployment processes to work with the new application. It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base.<br>Which deployment strategy will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CDK to deploy API Gateway and Lambda functions. When code needs to be changed, update the AWS CloudFormation stack and deploy the new version of the APIs and Lambda functions. Use a Route 53 failover routing policy for the canary release strategy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk to deploy API Gateway and Lambda functions. When code needs to be changed, deploy a new version of the API and Lambda functions. Shift traffic gradually using an Elastic Beanstalk blue/green deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS OpsWorks to deploy API Gateway in the service layer and Lambda functions in a custom layer. When code needs to be changed, use OpsWorks to perform a blue/green deployment and shift traffic gradually."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-02T22:20:00.000Z",
        "voteCount": 11,
        "content": "The deployment strategy that will meet the company's requirements is B. Use AWS CloudFormation to deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Promote the new version when testing is complete.\n\nExplanation:\n\nOption B provides a deployment strategy for the company's new serverless architecture, allowing the company to retain the ability to test new features on a small number of users before rolling the features out to the entire user base. Using AWS CloudFormation, the company can deploy API Gateway and Lambda functions using Lambda function versions. When code needs to be changed, the company can update the CloudFormation stack with the new Lambda code and update the API versions using a canary release strategy. Once testing is complete, the new version can be promoted."
      },
      {
        "date": "2023-07-12T20:31:00.000Z",
        "voteCount": 7,
        "content": "A Wrong: not using Canary, or Blue/green\nC Wrong: Beanstalk is not serverless deployment platform\nD Wrong: Irrelevant, OpsWork is configuration management platform and situation is requesting application deployment /AWS resource provisioning platform"
      },
      {
        "date": "2023-09-07T04:11:00.000Z",
        "voteCount": 3,
        "content": "Your answer is correct but the explanation is not.\nA is wrong because we can't use Route 53 failover routing for canary release. If it says Route53 weighted routing, then it is a possible option.\nD is wrong because when you use blue/green mode, the switch from blue to green is all done at once, not like a granular canary change"
      },
      {
        "date": "2024-02-01T09:15:00.000Z",
        "voteCount": 1,
        "content": "B is correct: &lt;serverless architecture&gt; means ECS, lambda, Beanstalk. &lt; It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base&gt; means canary deployment\nA: &lt;Route 53 failover routing policy for the canary release strategy&gt;: there is no such thing\nC and D: no mention of canary deployment"
      },
      {
        "date": "2024-02-01T09:14:00.000Z",
        "voteCount": 1,
        "content": "B is correct: &lt;serverless architecture&gt; means ECS, lambda, Beanstalk. &lt; It will also need to retain the ability to test new features on a small number of users before rolling the features out to the entire user base&gt; means canary deployment\nA: &lt;Route 53 failover routing policy for the canary release strategy&gt;: there is no such thing\nC: no mention of canary deployment"
      },
      {
        "date": "2023-05-03T08:37:00.000Z",
        "voteCount": 1,
        "content": "ONly B is possible"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/amazon/view/108078-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A development team uses AWS CodeCommit, AWS CodePipeline, and AWS CodeBuild to develop and deploy an application. Changes to the code are submitted by pull requests. The development team reviews and merges the pull requests, and then the pipeline builds and tests the application.<br>Over time, the number of pull requests has increased. The pipeline is frequently blocked because of failing tests. To prevent this blockage, the development team wants to run the unit and integration tests on each pull request before it is merged.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodeBuild project to run the unit and integration tests. Create a CodeCommit approval rule template. Configure the template to require the successful invocation of the CodeBuild project. Attach the approval rule to the project's CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit Create a CodeBuild project to run the unit and integration tests. Configure the CodeBuild project as a target of the EventBridge rule that includes a custom event payload with the CodeCommit repository and branch information from the event.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit. Modify the existing CodePipeline pipeline to not run the deploy steps if the build is started from a pull request. Configure the EventBridge rule to run the pipeline with a custom payload that contains the CodeCommit repository and branch information from the event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodeBuild project to run the unit and integration tests. Create a CodeCommit notification rule that matches when a pull request is created or updated. Configure the notification rule to invoke the CodeBuild project."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-30T23:23:00.000Z",
        "voteCount": 12,
        "content": "To run the unit and integration tests on each pull request before it is merged, a solution that listens to pullRequestCreated events and runs a CodeBuild project to execute tests would be the most appropriate option.\n\nOption B describes a solution that creates an Amazon EventBridge rule to match pullRequestCreated events from CodeCommit and configures a CodeBuild project to run the unit and integration tests, passing the CodeCommit repository and branch information from the event as a custom payload.\n\nTherefore, option B is the correct answer."
      },
      {
        "date": "2024-05-13T10:15:00.000Z",
        "voteCount": 1,
        "content": "These days it would be C instead of B, it's very common to reuse the same pipeline but with conditions to skip certain steps depending on the branch.\n\nhttps://aws.amazon.com/blogs/devops/aws-codepipeline-adds-support-for-branch-based-development-and-monorepos/"
      },
      {
        "date": "2024-02-18T13:49:00.000Z",
        "voteCount": 1,
        "content": "The Answer  should Be A because this option is allows test the code and the approval is depending on test's result"
      },
      {
        "date": "2024-05-20T05:17:00.000Z",
        "voteCount": 1,
        "content": "The development team reviews and merges the pull requests, and then the pipeline builds and tests the application."
      },
      {
        "date": "2024-02-01T09:27:00.000Z",
        "voteCount": 2,
        "content": "A is definitely correct:  &lt;The development team reviews and merges the pull requests&gt; and &lt;the development team wants to run the unit and integration tests on each pull request before it is merged&gt; means the dev team always review all pull requests and they need a solution to test committed code before merging to main. option A allow them to do tests and manually approve it before allow merging\nB C and D: no mention of the step that allow the dev team to manually approve the merge."
      },
      {
        "date": "2023-11-10T13:31:00.000Z",
        "voteCount": 2,
        "content": "B is the answer . D is wrong. Code commit only can setup notification rule to SNS topics or Chatbot."
      },
      {
        "date": "2023-09-08T08:06:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/devops/validating-aws-codecommit-pull-requests-with-aws-codebuild-and-aws-lambda/"
      },
      {
        "date": "2023-08-21T02:02:00.000Z",
        "voteCount": 1,
        "content": "Only D covers create pull request and update pull request"
      },
      {
        "date": "2023-11-23T08:01:00.000Z",
        "voteCount": 2,
        "content": "Can't be D because you can trigger a Codebuild project with CodeCommit notification rules"
      },
      {
        "date": "2023-07-27T01:09:00.000Z",
        "voteCount": 1,
        "content": "Option B is the most appropriate solution as it uses Amazon EventBridge rules to automatically trigger a CodeBuild project for running tests on each pull request, enabling early testing and preventing pipeline blockages due to failing tests after merging."
      },
      {
        "date": "2023-07-16T06:12:00.000Z",
        "voteCount": 3,
        "content": "Nothing in requirements says that the team wants a blockage of the pull request merge. \nAnd the B solution talks only about the \"PullRequestCreated\" which is not enough, it has to be reexecuted at any event on the pull request."
      },
      {
        "date": "2023-07-12T20:44:00.000Z",
        "voteCount": 3,
        "content": "A wrong: Dev team will need to manually approve each pull request before merging, this can be time consuming and error-prone.\nC Wrong: Modifying the exisiting codepipeline is not necessary\nD wrong: No preventiong from pipeline being blocked by failing tests"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/amazon/view/108414-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an application that runs on a fleet of Amazon EC2 instances. The application requires frequent restarts. The application logs contain error messages when a restart is required. The application logs are published to a log group in Amazon CloudWatch Logs.<br>An Amazon CloudWatch alarm notifies an application engineer through an Amazon Simple Notification Service (Amazon SNS) topic when the logs contain a large number of restart-related error messages. The application engineer manually restarts the application on the instances after the application engineer receives a notification from the SNS topic.<br>A DevOps engineer needs to implement a solution to automate the application restart on the instances without restarting the instances.<br>Which solution will meet these requirements in the MOST operationally efficient manner?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure the SNS topic to invoke the runbook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that restarts the application on the instances. Configure the Lambda function as an event destination of the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Create an AWS Lambda function to invoke the runbook. Configure the Lambda function as an event destination of the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS Systems Manager Automation runbook that runs a script to restart the application on the instances. Configure an Amazon EventBridge rule that reacts when the CloudWatch alarm enters ALARM state. Specify the runbook as a target of the rule.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 54,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-17T21:35:00.000Z",
        "voteCount": 19,
        "content": "It is debatable, as both C and D are correct and simple in their own ways, however, take a look at the number of components in each approach:\n\nC: CW -&gt; SNS -&gt; LAMBDA -&gt; SSM (4)\nD: CW -&gt; EVENTBRIDGE -&gt; SSM (3)\n\nThere is an extra component (SNS) to maintain on C, also, there is some coding involved on this option, which also needs to be maintained.\nEven if we already have the SNS created on option C, we still have to go there to remove the notification and configure the lambda invocation.\n\nOption D has fewer components, and require less customization."
      },
      {
        "date": "2023-05-08T22:41:00.000Z",
        "voteCount": 10,
        "content": "C makes more sense here"
      },
      {
        "date": "2024-07-26T01:38:00.000Z",
        "voteCount": 1,
        "content": "D is more simpler solution than C."
      },
      {
        "date": "2024-06-30T12:09:00.000Z",
        "voteCount": 1,
        "content": "D)\n\nB is wrong since it's way easier to use SSM automation runbook to execute logic inside instance using \"run command\" action within automation runbook than doing this with lambda"
      },
      {
        "date": "2024-03-25T10:26:00.000Z",
        "voteCount": 1,
        "content": "A is not possible - AWS Systems Manager (SSM) Run Command or Automation runbooks cannot be directly triggered by an Amazon SNS topic.\nThen C and D are the next best options. C is flexible but D is the most simple solution"
      },
      {
        "date": "2024-02-18T14:21:00.000Z",
        "voteCount": 3,
        "content": "Both C and D are valid answers. However, D is less complicated."
      },
      {
        "date": "2024-02-18T13:59:00.000Z",
        "voteCount": 2,
        "content": "C is correct , But D is more easy to implement , cost saving, managed services by AWS ^_^"
      },
      {
        "date": "2024-02-01T09:39:00.000Z",
        "voteCount": 1,
        "content": "B is correct: &lt;implement a solution to automate the application restart on the instances&gt; means we need to automate the restart step. We can use lambda, AWS system manager. &lt;CloudWatch alarm notifies an application engineer through an Amazon Simple Notification Service&gt; means we already have the alarm. We just need to simply trigger the restart process with lambda\nA, C and D are all too complicated compared to B. They ask for \"the MOST operationally efficient manner\", not the most complicated one"
      },
      {
        "date": "2024-05-13T02:02:00.000Z",
        "voteCount": 1,
        "content": "Option B not correct.\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/running-automations-event-bridge.html"
      },
      {
        "date": "2024-02-01T09:39:00.000Z",
        "voteCount": 1,
        "content": "B is correct: &lt;implement a solution to automate the application restart on the instances&gt; means we need to automate the restart step. We can use lambda, AWS system manager. &lt;CloudWatch alarm notifies an application engineer through an Amazon Simple Notification Service&gt; means we already have the alarm. We just need to simply trigger the restart process with lambda\nA, C and D are all too complicated compared to A. They ask for \"the MOST operationally efficient manner\", not the most complicated one"
      },
      {
        "date": "2023-12-23T21:08:00.000Z",
        "voteCount": 2,
        "content": "For me D is the answer , because we use lamba for the custom operations , if we already have SSM automation to perform that same action then why writing our custom logic in lambda ?"
      },
      {
        "date": "2023-12-17T12:24:00.000Z",
        "voteCount": 4,
        "content": "It\u2019s D. Here is a reference:\n\nhttps://aws.amazon.com/blogs/mt/use-amazon-eventbridge-rules-to-run-aws-systems-manager-automation-in-response-to-cloudwatch-alarms/"
      },
      {
        "date": "2023-11-28T09:13:00.000Z",
        "voteCount": 3,
        "content": "D It's the most simples approach. But C its also a solution, but why build and mantain a lambda?"
      },
      {
        "date": "2023-11-23T08:18:00.000Z",
        "voteCount": 2,
        "content": "I think is B, you only need to create the lambda and update the SNS to the lambda,"
      },
      {
        "date": "2023-11-07T03:05:00.000Z",
        "voteCount": 2,
        "content": "B seems like the shortest number of steps given that SNS already exists"
      },
      {
        "date": "2023-09-29T02:05:00.000Z",
        "voteCount": 3,
        "content": "Ill go with D too, less components, less configurations"
      },
      {
        "date": "2023-09-07T04:49:00.000Z",
        "voteCount": 2,
        "content": "B is wrong because SSM document is used to run on managed instances so definitely more efficient than lambda.\n\nC is wrong because although this solution should work,  we need to write a lambda script to invoke the runbook, while in D we don't need to do it"
      },
      {
        "date": "2023-08-22T03:55:00.000Z",
        "voteCount": 2,
        "content": "I think both C and D will work, but the question is to chose the most efficient way, so I pickup D."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/amazon/view/108355-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer at a company is supporting an AWS environment in which all users use AWS IAM Identity Center (AWS Single Sign-On). The company wants to immediately disable credentials of any new IAM user and wants the security team to receive a notification.<br>Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to an IAM CreateUser API call in AWS CloudTrail.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to an IAM GetLoginProfile API call in AWS CloudTrail.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to disable any access keys and delete the login profiles that are associated with the IAM user.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that is a target of the EventBridge rule. Configure the Lambda function to delete the login profiles that are associated with the IAM user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Notification Service (Amazon SNS) topic that is a target of the EventBridge rule. Subscribe the security team's group email address to the topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SQS) queue that is a target of the Lambda function. Subscribe the security team's group email address to the queue."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-17T13:53:00.000Z",
        "voteCount": 1,
        "content": "Took the test 4/15 and passed. Almost all of the questions appeared. \nACE is correct."
      },
      {
        "date": "2024-02-02T02:18:00.000Z",
        "voteCount": 4,
        "content": "ACE are correct: &lt;disable credentials of any new IAM user&gt; means disable all access key and profile related to the user. &lt;the security team to receive a notification&gt; means SNS\nB: GetLoginProfile API is not equal to creating new user\nD: we should delete all access key and profile related to the user, not just profile\nF: we need SNS, not SQS"
      },
      {
        "date": "2024-01-17T08:15:00.000Z",
        "voteCount": 2,
        "content": "Answer ACE"
      },
      {
        "date": "2024-01-09T03:48:00.000Z",
        "voteCount": 1,
        "content": "Answer ACE"
      },
      {
        "date": "2023-07-12T22:08:00.000Z",
        "voteCount": 3,
        "content": "No Brainer"
      },
      {
        "date": "2023-05-03T08:22:00.000Z",
        "voteCount": 4,
        "content": "My answer ACE"
      },
      {
        "date": "2023-05-02T22:32:00.000Z",
        "voteCount": 3,
        "content": "ACE is the right answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/amazon/view/108356-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company wants to set up a continuous delivery pipeline. The company stores application code in a private GitHub repository. The company needs to deploy the application components to Amazon Elastic Container Service (Amazon ECS). Amazon EC2, and AWS Lambda. The pipeline must support manual approval actions.<br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline with Amazon ECS. Amazon EC2, and Lambda as deploy providers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline with AWS CodeDeploy as the deploy provider.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodePipeline with AWS Elastic Beanstalk as the deploy provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy with GitHub integration to deploy the application."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-02T22:33:00.000Z",
        "voteCount": 7,
        "content": "B is correct"
      },
      {
        "date": "2023-07-12T02:16:00.000Z",
        "voteCount": 6,
        "content": "Because the Term \"The pipeline must support manual approval actions.\" \nThat is not possible without a pipeline :)"
      },
      {
        "date": "2024-09-01T06:00:00.000Z",
        "voteCount": 1,
        "content": "A  is correct, CodeDeploy can't deploy the ECS"
      },
      {
        "date": "2024-02-02T02:40:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-02-12T03:50:00.000Z",
        "voteCount": 1,
        "content": "Correction: B is correct"
      },
      {
        "date": "2023-11-14T00:58:00.000Z",
        "voteCount": 4,
        "content": "The solution for deploy ECS by codePipeline and codeDeploy\n\nCreate your CodeDeploy application and deployment group (ECS compute platform\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/tutorials-ecs-ecr-codedeploy.html#tutorials-ecs-ecr-codedeploy-deployment"
      },
      {
        "date": "2023-09-19T02:00:00.000Z",
        "voteCount": 1,
        "content": "Why not A ?\nB (Code depoly providr does not support ECS)\nD does not have \"codepipeleine\"  and the question says \"\"The pipeline must support manual approval actions.\" \n\nSo A is the only feasible option"
      },
      {
        "date": "2023-12-23T21:19:00.000Z",
        "voteCount": 3,
        "content": "using codedeploy we can deploy to ecs and even can perform blue / green deployment. Codedeploy support all there of the deployment strategies"
      },
      {
        "date": "2023-08-20T05:27:00.000Z",
        "voteCount": 1,
        "content": "Answer D.\nSource: https://docs.aws.amazon.com/codedeploy/latest/userguide/integrations-partners-github.html\nThe question is asking for a application code stored in a GitHub repository."
      },
      {
        "date": "2023-08-25T19:59:00.000Z",
        "voteCount": 1,
        "content": "My bad, Above only support EC2 and OnPrem."
      },
      {
        "date": "2023-07-27T04:07:00.000Z",
        "voteCount": 2,
        "content": "option A with AWS CodePipeline and individual deployment actions for Amazon ECS, Amazon EC2, and AWS Lambda, along with support for manual approval actions, is the most suitable solution to meet the requirements of the continuous delivery pipeline.\nB mentions using AWS CodeDeploy as the deploy provider, but it does not explicitly mention support for deploying to Amazon ECS, Amazon EC2, and AWS Lambda. AWS CodeDeploy primarily focuses on deploying applications to Amazon EC2 instances, and while it does have support for AWS Lambda, it might not be as straightforward to use for deploying to Amazon ECS."
      },
      {
        "date": "2023-07-27T06:00:00.000Z",
        "voteCount": 1,
        "content": "i think B is correct"
      },
      {
        "date": "2023-07-08T19:47:00.000Z",
        "voteCount": 5,
        "content": "You will need a deployment tool (CodeDeploy) for this. You cannot directly deploy via CodePipeline. Hence, B."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/amazon/view/105586-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an application that runs on Amazon EC2 instances that are in an Auto Scaling group. When the application starts up. the application needs to process data from an Amazon S3 bucket before the application can start to serve requests.<br>The size of the data that is stored in the S3 bucket is growing. When the Auto Scaling group adds new instances, the application now takes several minutes to download and process the data before the application can serve requests. The company must reduce the time that elapses before new EC2 instances are ready to serve requests.<br>Which solution is the MOST cost-effective way to reduce the application startup time?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a warm pool for the Auto Scaling group with warmed EC2 instances in the Stopped state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a warm pool for the Auto Scaling group with warmed EC2 instances in the Running state. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook when the application is ready to serve requests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the maximum instance count of the Auto Scaling group. Configure an autoscaling:EC2_INSTANCE_LAUNCHING lifecycle hook on the Auto Scaling group. Modify the application to complete the lifecycle hook and to place the new instance in the Standby state when the application is ready to serve requests."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-30T23:37:00.000Z",
        "voteCount": 17,
        "content": "Option A is the most cost-effective solution. By configuring a warm pool of EC2 instances in the Stopped state, the company can reduce the time it takes for new instances to be ready to serve requests. When the Auto Scaling group launches a new instance, it can attach the stopped EC2 instance from the warm pool. The instance can then be started up immediately, rather than having to wait for the data to be downloaded and processed. This reduces the overall startup time for the application.\n\nOption C is also a solution that involves a warm pool of EC2 instances, but the instances are in the Running state. This means that they are already running and incurring costs, even though they are not currently serving requests. This is not a cost-effective solution."
      },
      {
        "date": "2024-07-26T01:45:00.000Z",
        "voteCount": 2,
        "content": "keywords: MOST cost-effective way to reduce the application startup time"
      },
      {
        "date": "2024-03-25T06:24:00.000Z",
        "voteCount": 1,
        "content": "C \" The company must reduce the time that elapses before new EC2 instances are ready to serve requests.\"!!!!!!!!!! this cannot happen with a stopped instance as it will still need to read the data from S3 upon startup,"
      },
      {
        "date": "2024-05-01T02:50:00.000Z",
        "voteCount": 1,
        "content": "I thought this, as well, but A appears to be correct. See https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/"
      },
      {
        "date": "2024-02-29T06:54:00.000Z",
        "voteCount": 1,
        "content": "A \nfor warm pool in the hibernated or stop status we will pay only for the attached EBS volume, therefore its much cost effective rather than running instance"
      },
      {
        "date": "2024-02-24T20:19:00.000Z",
        "voteCount": 1,
        "content": "Warm Pool allows instances to be set to a stopped state after performing any process (e.g., running initialization scripts, warm-up tasks, etc.)."
      },
      {
        "date": "2024-02-02T02:45:00.000Z",
        "voteCount": 1,
        "content": "A is correct: the question says &lt;the application needs to process data from an Amazon S3 bucket before the application can start to serve requests&gt; but &lt;The size of the data that is stored in the S3 bucket is growing&gt;. This means we should maintain a warm pool for EC2 so that they are always ready to process data (reduce the time that elapses before new EC2 instances are ready)\nB and D: no mention of warmpool\nC: If the instance is up and running, no need to configure warm pool"
      },
      {
        "date": "2023-11-23T08:31:00.000Z",
        "voteCount": 2,
        "content": "Answer is A, the question is cost-effective, and even with A you will have less wait time to download the S3 data, it will download the delta from the warm up process to ready to join to ASG"
      },
      {
        "date": "2024-01-05T20:53:00.000Z",
        "voteCount": 1,
        "content": "A&amp;C are both good in terms of solutions, however, the caveat here is the \"cost-effective\" solution and that's why I agree with A. \nhttps://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/"
      },
      {
        "date": "2023-09-12T02:29:00.000Z",
        "voteCount": 1,
        "content": "excerpt from the url: https://aws.amazon.com/blogs/compute/scaling-your-applications-faster-with-ec2-auto-scaling-warm-pools/\nEC2 Auto Scaling Warm Pools works by launching a configured number of EC2 instances in the background, allowing any lengthy application initialization processes to run as necessary, and then stopping those instances until they are needed"
      },
      {
        "date": "2023-08-22T04:06:00.000Z",
        "voteCount": 1,
        "content": "A is the most cost-effective solution. Besides, when the warm EC2 was created, it already downloaded the contents from S3 so the next time when it started, it would just download any new files from S3.  ( e.g s3 sync )"
      },
      {
        "date": "2023-08-14T20:36:00.000Z",
        "voteCount": 2,
        "content": "C is right.\nplease carefully check the question:\nThe company must reduce the time that elapses before new EC2 instances are ready to serve requests.\nWhen the application starts up. the application needs to process data from an Amazon S3 bucket before the application can start to serve requests."
      },
      {
        "date": "2023-08-06T09:15:00.000Z",
        "voteCount": 3,
        "content": "I understand the question is asking for the most cost-effective. keeping it stopped state is most cost efficient but it would not work because in the question it also states that, \"When the application starts up. the application needs to process data\" and to process that data takes time. If the Ec2 instance is stopped then started at the time of need, then again it will take time to process the data, right? so in this scenario, the EC2 instance need to be running."
      },
      {
        "date": "2023-07-25T11:38:00.000Z",
        "voteCount": 2,
        "content": "I think it should be C, as the A option would not be effective. Coming from the instance stop state the application will start up again and need to process the data from S3 bucket."
      },
      {
        "date": "2023-07-12T22:30:00.000Z",
        "voteCount": 3,
        "content": "Warm pool with stopped state is most cost efficient option"
      },
      {
        "date": "2023-07-02T07:41:00.000Z",
        "voteCount": 1,
        "content": "A - Keep it stopped until you need it to save money"
      },
      {
        "date": "2023-04-14T14:49:00.000Z",
        "voteCount": 1,
        "content": "While A can also be a cost-effective solution, C is the MOST cost-effective solution because it utilizes Amazon S3 Transfer Acceleration, which is a feature that enables fast, easy, and secure transfers of files over the internet between Amazon S3 buckets and EC2 instances located in different regions or across the internet. By using S3 Transfer Acceleration, the data transfer speed can be increased significantly, which can reduce the time that elapses before new EC2 instances are ready to serve requests.\n\nIn contrast, A suggests using a larger instance size with more CPU and network capacity, which can be more expensive than the current instance size. Moreover, this approach may not be scalable in the long run since as the data in the S3 bucket continues to grow, the instance size may need to be further increased, which can incur more costs. Therefore, while A can also be a viable solution, C is the most cost-effective and scalable solution."
      },
      {
        "date": "2023-04-13T00:54:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-warm-pools.html\nKeeping instances in a Stopped state is an effective way to minimize costs."
      },
      {
        "date": "2023-04-08T06:33:00.000Z",
        "voteCount": 2,
        "content": "A for me to decrease costs"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/amazon/view/108079-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is using an AWS CodeBuild project to build and package an application. The packages are copied to a shared Amazon S3 bucket before being deployed across multiple AWS accounts.<br>The buildspec.yml file contains the following:<br><img title=\"image5\" src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image5.png\"><br>The DevOps engineer has noticed that anybody with an AWS account is able to download the artifacts.<br>What steps should the DevOps engineer take to stop this?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the post_build command to use --acl public-read and configure a bucket policy that grants read access to the relevant AWS accounts only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a default ACL for the S3 bucket that defines the set of authenticated users as the relevant AWS accounts only and grants read-only access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket policy that grants read access to the relevant AWS accounts and denies read access to the principal \u201c*\u201d.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the post_build command to remove --acl authenticated-read and configure a bucket policy that allows read access to the relevant AWS accounts only.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-30T23:39:00.000Z",
        "voteCount": 13,
        "content": "D is correct"
      },
      {
        "date": "2023-08-22T04:31:00.000Z",
        "voteCount": 5,
        "content": "--acl authenticated-read means any authenticated users can read the S3 bucket. We should remove it and configure the bucket policy to explicitly grant access"
      },
      {
        "date": "2023-08-22T04:31:00.000Z",
        "voteCount": 6,
        "content": "I mean D..."
      },
      {
        "date": "2024-07-26T01:48:00.000Z",
        "voteCount": 2,
        "content": "\"--acl authenticated-read\" means any authenticated users can read the S3 bucket. We should remove it and configure the bucket policy to explicitly grant access"
      },
      {
        "date": "2024-04-16T13:00:00.000Z",
        "voteCount": 2,
        "content": "D is the answer\n\nACL-authenticated users: This refers to any user who has successfully authenticated with AWS credentials, including IAM users and federated users. It does not include anonymous users (public access).\nIt's generally recommended to use bucket policies for access control in S3 rather than ACLs. Bucket policies offer more granular control and better security practices. You can achieve \"acl-authenticated reads\" access using a bucket policy as well."
      },
      {
        "date": "2024-02-24T20:58:00.000Z",
        "voteCount": 4,
        "content": "`remove --acl authenticated-read` is required to fulfill the requirement."
      },
      {
        "date": "2024-02-02T03:26:00.000Z",
        "voteCount": 1,
        "content": "B is correct: In the \"buildspec.yml file\", we see that there is \"--acl authenticated-read\". This allow all aws users who successfully authen to AWS can download the file. To restrict access, we need to modify ACL that only grant access to some specific users. \nNote that we should not use bucket policy because it will affect all ojbects in the bucket (that is why it is called BUCKET policy). We only need to restrict acess to an object, then ACL is the right choice.\nA is incorrect: Use use --acl public-read means we allow all user to access the object \nC and D: Use bucket policy, which is incorrect"
      },
      {
        "date": "2023-11-23T08:33:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/amazon/view/108080-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has developed a serverless web application that is hosted on AWS. The application consists of Amazon S3. Amazon API Gateway, several AWS Lambda functions, and an Amazon RDS for MySQL database. The company is using AWS CodeCommit to store the source code. The source code is a combination of AWS Serverless Application Model (AWS SAM) templates and Python code.<br>A security audit and penetration test reveal that user names and passwords for authentication to the database are hardcoded within CodeCommit repositories. A DevOps engineer must implement a solution to automatically detect and prevent hardcoded secrets.<br>What is the MOST secure solution that meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Write the secret to AWS Systems Manager Parameter Store as a secure string. Update the SAM templates and the Python code to pull the secret from Parameter Store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CodeGuru Profiler. Decorate the handler function with @with_lambda_profiler(). Manually review the recommendation report. Choose the option to protect the secret. Update the SAM templates and the Python code to pull the secret from AWS Secrets Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the CodeCommit repository with Amazon CodeGuru Reviewer. Manually check the code review for any recommendations. Write the secret to AWS Systems Manager Parameter Store as a string. Update the SAM templates and the Python code to pull the secret from Parameter Store."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-30T23:42:00.000Z",
        "voteCount": 11,
        "content": "B\nThe MOST secure solution that meets the requirement of automatically detecting and preventing hardcoded secrets is to use AWS CodeGuru Reviewer to check the code for any hardcoded secrets, and then update the SAM templates and Python code to retrieve the secrets from AWS Secrets Manager.\n\nOption B is the correct answer. By associating the CodeCommit repository with Amazon CodeGuru Reviewer, the code can be checked for any hardcoded secrets during code reviews. When a hardcoded secret is detected, CodeGuru Reviewer will recommend updating the code to retrieve the secret from a secure storage service like AWS Secrets Manager. The DevOps engineer can choose the option to protect the secret and then update the SAM templates and Python code to retrieve the secret from AWS Secrets Manager instead of hardcoding it in the code."
      },
      {
        "date": "2023-06-15T06:57:00.000Z",
        "voteCount": 7,
        "content": "B is correct.\nCodeGuru Reviewer for security problems.\nAmazon CodeGuru Profiler is for performance."
      },
      {
        "date": "2024-02-02T04:03:00.000Z",
        "voteCount": 1,
        "content": "B is correct: &lt;implement a solution to automatically detect and prevent hardcoded secrets&gt; means we need CodeGuru reviewer to analyze the code and uncover hardcoded credentials. \nA and C: no mention of CodeGuru reviewer\nD: using System Manager Parameter store is a good method to avoid hardcoded credentials. However, the question requires &lt;the MOST secure solution&gt;, so we should use AWS secret manager (option B). It costs more than Para store, but more secure."
      },
      {
        "date": "2023-12-24T23:48:00.000Z",
        "voteCount": 1,
        "content": "I'd say it's C, because the system to examine includes Python code and CodeGuru profiles for Python needs the decorator: https://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda.html"
      },
      {
        "date": "2023-07-27T05:32:00.000Z",
        "voteCount": 1,
        "content": "option C\nhttps://docs.aws.amazon.com/codeguru/latest/profiler-ug/python-lambda-command-line.html"
      },
      {
        "date": "2023-08-08T09:12:00.000Z",
        "voteCount": 6,
        "content": "Sorry B\n\nAmazon CodeGuru Reviewer and Amazon CodeGuru Profiler are both tools that can be used to improve the quality and security of your code. However, they have different strengths and weaknesses.\n\nCodeGuru Reviewer is a static code analysis tool that can be used to find potential defects in your code. It can scan your code for hardcoded secrets, security vulnerabilities, and other potential problems. CodeGuru Reviewer can also provide recommendations on how to fix the problems that it finds.\n\nCodeGuru Profiler is a dynamic code analysis tool that can be used to understand how your code performs. It can track the performance of your code, identify bottlenecks, and suggest ways to improve performance. CodeGuru Profiler can also be used to find potential memory leaks and other performance problems."
      },
      {
        "date": "2023-06-19T15:07:00.000Z",
        "voteCount": 2,
        "content": "Definitely B."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/amazon/view/108608-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is using Amazon S3 buckets to store important documents. The company discovers that some S3 buckets are not encrypted. Currently, the company\u2019s IAM users can create new S3 buckets without encryption. The company is implementing a new requirement that all S3 buckets must be encrypted.<br><br>A DevOps engineer must implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3 buckets. The encryption must be enabled on new S3 buckets as soon as the S3 buckets are created. The default encryption type must be 256-bit Advanced Encryption Standard (AES-256).<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that is invoked periodically by an Amazon EventBridge scheduled rule. Program the Lambda function to scan all current S3 buckets for encryption status and to set AES-256 as the default encryption for any S3 bucket that does not have an encryption configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up and activate the s3-bucket-server-side-encryption-enabled AWS Config managed rule. Configure the rule to use the AWS-EnableS3BucketEncryption AWS Systems Manager Automation runbook as the remediation action. Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that is invoked by an Amazon EventBridge event rule. Define the rule with an event pattern that matches the creation of new S3 buckets. Program the Lambda function to parse the EventBridge event, check the configuration of the S3 buckets from the event, and set AES-256 as the default encryption.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an IAM policy that denies the s3:CreateBucket action if the s3:x-amz-server-side-encryption condition key has a value that is not AES-256. Create an IAM group for all the company\u2019s IAM users. Associate the IAM policy with the IAM group."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-27T07:32:00.000Z",
        "voteCount": 13,
        "content": "B caters to both existing and new buckets. \nC is triggered on when new bucket is created, existing buckets are not handled by the event."
      },
      {
        "date": "2023-05-05T23:00:00.000Z",
        "voteCount": 10,
        "content": "B to me"
      },
      {
        "date": "2024-06-11T20:22:00.000Z",
        "voteCount": 1,
        "content": "I think neither \"B\" or \"C\" is complete solution.  They both need to be done to deal with both existing and new buckets.\nA carefull reading of the question doesn't preclude the need to do both.\nHowever, the specific and emphasized criteria of enabling encryption \"as soon as the S3 buckets are created\" can only be done by \"C\" (event driven action)\nI think this may be a trick question.  I'm very confident they are defining an event driven action as part of the solution, and only \"C\" provides that.\n\nB: (NO)  \"Manually run the re-evaluation process to ensure that existing S3 buckets are compliant.\"\nComment: Doesn't achieve \"encryption must be enabled on new S3 buckets as soon as the S3 buckets are created.\""
      },
      {
        "date": "2024-02-24T21:16:00.000Z",
        "voteCount": 1,
        "content": "`s3-bucket-server-side-encryption-enabled` checks if your Amazon S3 bucket either has the Amazon S3 default encryption enabled or that the Amazon S3 bucket policy explicitly denies put-object requests without server side encryption that uses AES-256 or AWS Key Management Service."
      },
      {
        "date": "2024-02-02T04:25:00.000Z",
        "voteCount": 2,
        "content": "A is correct: &lt;implement a solution to ensure that server-side encryption is enabled on all existing S3 buckets and all new S3 buckets&gt;: We can use lambda to configure all S3. Use Eventbridge to schedule-run lambda. \nB: This option uses AWS config rule to activate AWS-EnableS3BucketEncryption AWS Systems Manager Automation runbook, which is incorrect. Remember that AWS config have no action and cannot trigger anything. It only collect data and report. Additionally, this option does not mention actions to new S3 bucket\nC: &lt;define the rule with an event pattern that matches the creation of new S3 buckets&gt; means that this only affect newly-created bucket, not existing ones. \nD: No mention of enforcing encryption on S3\n\nNote: Should not use chatgpt for this exam, its answers are mostly wrong"
      },
      {
        "date": "2024-02-13T02:33:00.000Z",
        "voteCount": 1,
        "content": "Correct: D"
      },
      {
        "date": "2024-01-08T05:04:00.000Z",
        "voteCount": 5,
        "content": "Answer is B\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-server-side-encryption-enabled.html"
      },
      {
        "date": "2024-01-06T05:51:00.000Z",
        "voteCount": 1,
        "content": "I would have chose B over D because aws config can do this with lambda."
      },
      {
        "date": "2024-01-06T05:50:00.000Z",
        "voteCount": 1,
        "content": "A has automation. I didn't like B: because of this statement: Manually run the re-evaluation process to ensure that existing S3 buckets are compliant."
      },
      {
        "date": "2023-12-25T09:18:00.000Z",
        "voteCount": 1,
        "content": "Amazon S3 Encrypts New Objects By Default\nhttps://aws.amazon.com/blogs/aws/amazon-s3-encrypts-new-objects-by-default/#:~:text=At%20AWS%2C%20security%20is%20the,specify%20a%20different%20encryption%20option."
      },
      {
        "date": "2023-10-26T04:46:00.000Z",
        "voteCount": 1,
        "content": "B to me.\nAWS Config can monitor resource compliance against desired configurations. The managed rule s3-bucket-server-side-encryption-enabled checks whether Amazon S3 buckets have server-side encryption enabled. The AWS Systems Manager Automation runbook, AWS-EnableS3BucketEncryption, can be used as a remediation action to enable default encryption. This solution would also work for new buckets as soon as they're created, making it an effective solution."
      },
      {
        "date": "2023-06-15T07:01:00.000Z",
        "voteCount": 4,
        "content": "B is right.\nDoable solution for new buckets as well as existing buckets."
      },
      {
        "date": "2023-05-07T02:56:00.000Z",
        "voteCount": 1,
        "content": "Option C meets the requirement of modifying the policy immediately after creating the bucket."
      },
      {
        "date": "2023-06-15T07:01:00.000Z",
        "voteCount": 1,
        "content": "What about existing buckets?"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/amazon/view/108676-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is architecting a continuous development strategy for a company\u2019s software as a service (SaaS) web application running on AWS. For application and security reasons, users subscribing to this application are distributed across multiple Application Load Balancers (ALBs), each of which has a dedicated Auto Scaling group and fleet of Amazon EC2 instances. The application does not require a build stage, and when it is committed to AWS CodeCommit, the application must trigger a simultaneous deployment to all ALBs, Auto Scaling groups, and EC2 fleets.<br><br>Which architecture will meet these requirements with the LEAST amount of configuration?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single AWS CodePipeline pipeline that deploys the application in parallel using unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single AWS CodePipeline pipeline that deploys the application using a single AWS CodeDeploy application and single deployment group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single AWS CodePipeline pipeline that deploys the application in parallel using a single AWS CodeDeploy application and unique deployment group for each ALB-Auto Scaling group pair.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodePipeline pipeline for each ALB-Auto Scaling group pair that deploys the application using an AWS CodeDeploy application and deployment group created for the same ALB-Auto Scaling group pair."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-15T07:06:00.000Z",
        "voteCount": 12,
        "content": "You can just use one CodeDeploy application and multiple deployment groups in this case.\nso C."
      },
      {
        "date": "2024-07-08T18:25:00.000Z",
        "voteCount": 1,
        "content": "Option B not feasible as it assumes a single deployment group can manage deployments across multiple ALBs and Auto Scaling groups simultaneously, which is not supported."
      },
      {
        "date": "2024-06-18T11:37:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/about-aws/whats-new/2023/10/aws-codedeploy-multiple-load-balancers-amazon-ec2-applications/"
      },
      {
        "date": "2024-04-21T05:53:00.000Z",
        "voteCount": 3,
        "content": "B is the simplest :)\n\nDuring creation of deployment group:\n1. select \"Amazon EC2 Auto Scaling groups\"\n2. tip appears: \"You can select up to 10 Amazon EC2 Auto Scaling groups to deploy your application revision to.\""
      },
      {
        "date": "2024-04-24T11:18:00.000Z",
        "voteCount": 2,
        "content": "Also B for me, you can target multiple ASGs as part of one deployment: https://docs.aws.amazon.com/codedeploy/latest/APIReference/API_TargetInstances.html#CodeDeploy-Type-TargetInstances-autoScalingGroups"
      },
      {
        "date": "2024-02-02T08:56:00.000Z",
        "voteCount": 3,
        "content": "C is correct: &lt;the application must trigger a simultaneous deployment&gt; means deployment in parallel \nB and D: no mention of deployment in parallel\nA: &lt;unique AWS CodeDeploy applications and deployment groups created for each ALB-Auto Scaling group pair&gt; means there are multiple AWS CodeDeploy applications and deployment groups for each site, which is unnecessary"
      },
      {
        "date": "2023-07-27T06:05:00.000Z",
        "voteCount": 1,
        "content": "Option C\ndeployed in parallel to all ALB-Auto Scaling group pairs simultaneously. This means that the deployment process is efficient and fast, and all ALBs and Auto Scaling groups receive updates at the same time."
      },
      {
        "date": "2023-05-13T23:03:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer."
      },
      {
        "date": "2023-05-09T06:56:00.000Z",
        "voteCount": 3,
        "content": "C is correct."
      },
      {
        "date": "2023-05-07T03:30:00.000Z",
        "voteCount": 1,
        "content": "A\nAWS CodePipeline can target multiple AWS CodeDeploy applications."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/amazon/view/108678-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is hosting a static website from an Amazon S3 bucket. The website is available to customers at example.com. The company uses an Amazon Route 53 weighted routing policy with a TTL of 1 day. The company has decided to replace the existing static website with a dynamic web application. The dynamic web application uses an Application Load Balancer (ALB) in front of a fleet of Amazon EC2 instances.<br><br>On the day of production launch to customers, the company creates an additional Route 53 weighted DNS record entry that points to the ALB with a weight of 255 and a TTL of 1 hour. Two days later, a DevOps engineer notices that the previous static website is displayed sometimes when customers navigate to example.com.<br><br>How can the DevOps engineer ensure that the company serves only dynamic content for example.com?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete all objects, including previous versions, from the S3 bucket that contains the static website content.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the weighted DNS record entry that points to the S3 bucket. Apply a weight of 0. Specify the domain reset option to propagate changes immediately.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure webpage redirect requests on the S3 bucket with a hostname that redirects to the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the weighted DNS record entry that points to the S3 bucket from the example.com hosted zone. Wait for DNS propagation to become complete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-06T01:56:00.000Z",
        "voteCount": 6,
        "content": "D is the answer \nB wrong because \nRoute 53 initially considers only the nonzero weighted records, if any.\n\nIf all the records that have a weight greater than 0 are unhealthy, then Route 53 considers the zero-weighted records."
      },
      {
        "date": "2024-08-01T07:53:00.000Z",
        "voteCount": 2,
        "content": "D is correct\n\nNot B as \n- Route 53 initially considers only the nonzero weighted records, if any.\n- If all the records that have a weight greater than 0 are unhealthy, then Route 53 considers the zero-weighted records.\nhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-weighted.html\n\nBesides, in B, &lt;domain reset option to propagate changes immediately&gt;, i don't think DNS record will update immediately"
      },
      {
        "date": "2024-02-18T14:32:00.000Z",
        "voteCount": 1,
        "content": "D is correct but I will go with B because is more save to the customers, some clients have the old record (TTL 1 day) so after 1 day I can confirm that all the clients have the the new DNs record so I can delete the record"
      },
      {
        "date": "2024-02-02T09:15:00.000Z",
        "voteCount": 3,
        "content": "D is correct: \nA: should not delete all objects from S3, they change nothing\nC: should not do this, we have a more efficient method\nB: &lt; domain reset option to propagate changes immediately&gt;: there is no such thing. DNS record will expire after TTL. Cannot force DNS resolvers to query for DNS record before TTL expire"
      },
      {
        "date": "2023-09-12T04:38:00.000Z",
        "voteCount": 3,
        "content": "B- is incorrect ,  as  gigi_devops  has metioned setting 0 weight is not enough. Also reset domain option is not available."
      },
      {
        "date": "2023-08-15T04:11:00.000Z",
        "voteCount": 2,
        "content": "D is right.\nJust setting the weight to 0 does not ensure that traffic will not go to example.com."
      },
      {
        "date": "2023-08-01T19:50:00.000Z",
        "voteCount": 3,
        "content": "Setting the weight to 0 is not enough. So C is the best answer. https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-weighted.html#:~:text=Si%20tous%20les%20enregistrements%20dont%20le%20poids%20est%20sup%C3%A9rieur%20%C3%A0%200%20ne%20sont%20pas%20sains%2C%20Route%2053%20prend%20en%20compte%20les%20enregistrements%20pond%C3%A9r%C3%A9s%20%C3%A0%20z%C3%A9ro"
      },
      {
        "date": "2023-08-01T19:47:00.000Z",
        "voteCount": 1,
        "content": "Setting the weight to 0 is not enough. So C is the best answer.                                             https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-weighted.html#:~:text=Si%20tous%20les%20enregistrements%20dont%20le%20poids%20est%20sup%C3%A9rieur%20%C3%A0%200%20ne%20sont%20pas%20sains%2C%20Route%2053%20prend%20en%20compte%20les%20enregistrements%20pond%C3%A9r%C3%A9s%20%C3%A0%20z%C3%A9ro"
      },
      {
        "date": "2023-08-01T19:50:00.000Z",
        "voteCount": 4,
        "content": "Sorry I wanted to say \"D\""
      },
      {
        "date": "2023-07-27T06:10:00.000Z",
        "voteCount": 1,
        "content": "option B is the most appropriate choice as it immediately redirects all traffic away from the S3 bucket and ensures that only the dynamic content from the ALB is served for example.com."
      },
      {
        "date": "2023-08-08T21:21:00.000Z",
        "voteCount": 2,
        "content": "D.\n\nWe can remove the Weighted DNS as it is not necessary"
      },
      {
        "date": "2023-07-08T20:28:00.000Z",
        "voteCount": 1,
        "content": "D. Also, do check the weight of the CNAME record of the ALB. It might be conflicting."
      },
      {
        "date": "2023-07-05T03:02:00.000Z",
        "voteCount": 1,
        "content": "agree with D"
      },
      {
        "date": "2023-06-29T15:23:00.000Z",
        "voteCount": 4,
        "content": "In reference to B, I don't think the \"domain reset option\" exists. So, it's D."
      },
      {
        "date": "2023-06-25T05:13:00.000Z",
        "voteCount": 2,
        "content": "Option D."
      },
      {
        "date": "2023-06-16T20:07:00.000Z",
        "voteCount": 4,
        "content": "To ensure that the company only serves dynamic content for example.com, the DevOps engineer should remove the weighted DNS record entry that points to the S3 bucket from the example.com hosted zone. This will immediately remove the static website from the DNS resolution pool.\nSolution B would only update the weight of the record entry, but it would still take 24 hours for the changes to propagate."
      },
      {
        "date": "2023-06-15T07:13:00.000Z",
        "voteCount": 2,
        "content": "B\nTo disable routing to a resource, set Weight to 0"
      },
      {
        "date": "2023-06-06T01:13:00.000Z",
        "voteCount": 2,
        "content": "nocinfra 0 minutes ago Awaiting moderator approval\nB for me , You can gradually change the balance by changing the weights. If you want to stop sending traffic to a resource, you can change the weight for that record to 0."
      },
      {
        "date": "2023-06-06T01:12:00.000Z",
        "voteCount": 2,
        "content": "B for me , You can gradually change the balance by changing the weights. If you want to stop sending traffic to a resource, you can change the weight for that record to 0."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/amazon/view/108679-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is implementing AWS CodePipeline to automate its testing process. The company wants to be notified when the execution state fails and used the following custom event pattern in Amazon EventBridge:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image6.png\"><br><br>Which type of events will match this event pattern?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFailed deploy and build actions across all the pipelines",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll rejected or failed approval actions across all the pipelines\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll the events across all pipelines",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApproval actions across all the pipelines"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-26T19:14:00.000Z",
        "voteCount": 13,
        "content": "Use this sample event pattern to capture all rejected or failed approval actions across all the pipelines.\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html"
      },
      {
        "date": "2024-02-02T09:25:00.000Z",
        "voteCount": 3,
        "content": "B is correct:  &lt;state:failed and category:approval&gt; means failed approval\nA, C and D: no mention of approval"
      },
      {
        "date": "2023-05-23T15:44:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/ja_jp/codepipeline/latest/userguide/detect-state-changes-cloudwatch-events.html"
      },
      {
        "date": "2023-05-07T03:54:00.000Z",
        "voteCount": 1,
        "content": "category: approval"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/amazon/view/108586-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "An application running on a set of Amazon EC2 instances in an Auto Scaling group requires a configuration file to operate. The instances are created and maintained with AWS CloudFormation. A DevOps engineer wants the instances to have the latest configuration file when launched, and wants changes to the configuration file to be reflected on all the instances with a minimal delay when the CloudFormation template is updated. Company policy requires that application configuration files be maintained along with AWS infrastructure configuration files in source control.<br><br>Which solution will accomplish this?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template, add an AWS Config rule. Place the configuration file content in the rule\u2019s InputParameters property, and set the Scope property to the EC2 Auto Scaling group. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template, add an EC2 launch template resource. Place the configuration file content in the launch template. Add an AWS Systems Manager Resource Data Sync resource to the template to poll for updates to the configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template, add CloudFormation init metadata. Place the configuration file content in the metadata. Configure the cfn-init script to run when the instance is launched, and configure the cfn-hup script to poll for updates to the configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-08T09:18:00.000Z",
        "voteCount": 7,
        "content": "Use the AWS::CloudFormation::Init type to include metadata on an Amazon EC2 instance for the cfn-init helper script. If your template calls the cfn-init script, the script looks for resource metadata rooted in the AWS::CloudFormation::Init metadata key. Reference:\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-init.html"
      },
      {
        "date": "2024-07-26T02:45:00.000Z",
        "voteCount": 2,
        "content": "Require CloudFormation init for cfn-init and cfn-hup\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-init.html"
      },
      {
        "date": "2024-03-03T23:05:00.000Z",
        "voteCount": 2,
        "content": "In this scenario, CloudFormation init metadata is the most suitable approach for ensuring that instances launched by the Auto Scaling group have the latest configuration file."
      },
      {
        "date": "2024-02-02T09:37:00.000Z",
        "voteCount": 3,
        "content": "D is correct: &lt;The instances are created and maintained with AWS CloudFormation&gt; means we will only use ACF to satisfy the requirements of this question. &lt;changes to the configuration file to be reflected on all the instances with a minimal delay when the CloudFormation template is updated&gt; means we nead cfn-init, which is a daemon that check for updates and update the changes\nA and C: no mention of cfn-init\nB: no mention of CloudFormation init. we need CloudFormation init because cfn-init is specified in CloudFormation init key."
      },
      {
        "date": "2024-01-17T08:40:00.000Z",
        "voteCount": 4,
        "content": "D. cfn-hup poll for cloudformation metadata. B is wrong because putting the config content in launch template instead of metadata, where cfn-hub is not able to poll."
      },
      {
        "date": "2024-01-12T05:47:00.000Z",
        "voteCount": 2,
        "content": "cfn-init is defined inside AWS::CloudFormation::Init"
      },
      {
        "date": "2024-01-09T04:11:00.000Z",
        "voteCount": 1,
        "content": "I vote for B"
      },
      {
        "date": "2024-01-06T06:07:00.000Z",
        "voteCount": 3,
        "content": "But what happened to the aspect of using source control?"
      },
      {
        "date": "2023-12-25T00:12:00.000Z",
        "voteCount": 2,
        "content": "Google Bart says it is B. \nBy using an EC2 launch template resource, the configuration file will be installed and configured on all instances when they are launched. The cfn-init script will also poll for updates to the configuration, so that all instances will have the latest configuration file as soon as it is updated.\n\nIn addition, the solution will comply with company policy by storing the configuration file in source control along with the AWS infrastructure configuration files. This will ensure that changes to the configuration file are tracked and managed in a consistent way.\nOption C: Using an AWS Systems Manager Resource Data Sync resource alone is not enough to ensure that all instances have the latest configuration file. The cfn-init script is needed to install and configure the configuration file on the instance, and the cfn-hup script is needed to poll for updates to the configuration."
      },
      {
        "date": "2023-12-03T10:25:00.000Z",
        "voteCount": 3,
        "content": "Selected B:\nYou can have cfn on Launch Config and Launch Template\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html\nhttps://stackoverflow.com/questions/54691327/cfn-init-for-cloudformation-launchtemplate"
      },
      {
        "date": "2023-11-28T21:48:00.000Z",
        "voteCount": 1,
        "content": "I choos B because the config file must be mantained along teh insfrastructure configuration files in source control"
      },
      {
        "date": "2023-10-26T04:59:00.000Z",
        "voteCount": 1,
        "content": "B and D are similar.  I will go for B, because D doesn't involve EC2 launch templates"
      },
      {
        "date": "2023-09-12T05:04:00.000Z",
        "voteCount": 2,
        "content": "cfn-init and cfn-hup are used  to  update metadata. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html"
      },
      {
        "date": "2023-07-28T03:42:00.000Z",
        "voteCount": 1,
        "content": "option C provides a reliable and scalable solution to manage the configuration file for the application running on the EC2 instances, while also adhering to company policies regarding source control for configuration files."
      },
      {
        "date": "2023-08-08T21:35:00.000Z",
        "voteCount": 4,
        "content": "D\n\ncfn-hup is a daemon that detects changes in resource metadata and runs user-specified actions when a change is detected. This allows you to automatically update the configuration of your Amazon EC2 instances when you make changes to your AWS CloudFormation stacks."
      },
      {
        "date": "2023-06-15T07:27:00.000Z",
        "voteCount": 4,
        "content": "D\nMetadata:\n      \"AWS::CloudFormation::Init\":"
      },
      {
        "date": "2023-05-15T05:50:00.000Z",
        "voteCount": 3,
        "content": "By using an EC2 launch template, you can include the configuration file content directly in the template. The cfn-init script can be configured to run when the instance is launched.\n\nas CloudFormation init metadata is more suitable for configuring instances during stack creation rather than for dynamically updating configuration files."
      },
      {
        "date": "2023-05-07T05:52:00.000Z",
        "voteCount": 2,
        "content": "D it is"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/amazon/view/109258-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company manages an application that stores logs in Amazon CloudWatch Logs. The company wants to archive the logs to an Amazon S3 bucket. Logs are rarely accessed after 90 days and must be retained for 10 years.<br><br>Which combination of steps should a DevOps engineer take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a CloudWatch Logs subscription filter to use AWS Glue to transfer all logs to an S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a CloudWatch Logs subscription filter to use Amazon Kinesis Data Firehose to stream all logs to an S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a CloudWatch Logs subscription filter to stream all logs to an S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the S3 bucket lifecycle policy to transition logs to S3 Glacier after 90 days and to expire logs after 3.650 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the S3 bucket lifecycle policy to transition logs to Reduced Redundancy after 90 days and to expire logs after 3.650 days."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-15T01:07:00.000Z",
        "voteCount": 6,
        "content": "Amazon Kinesis Data Firehose simplifies the process of loading streaming data into S3 and provides automatic scaling, buffering, and retries."
      },
      {
        "date": "2024-08-01T18:42:00.000Z",
        "voteCount": 1,
        "content": "B - keywords: continue stream but not one time task\nD - keywords: S3 Glacier"
      },
      {
        "date": "2024-07-28T04:19:00.000Z",
        "voteCount": 2,
        "content": "vote B and D. \nI initially thought the C is better than B, because Amazon Kinesis Data Firehose is primarily used for time-sensitive tasks, which is not suitable for this case, But when I read the C. I found the Directly streaming logs from cloudwatch log to s3 is not a feature provided by Cloudwatch. \nSo, I will go with B and D."
      },
      {
        "date": "2024-06-12T22:42:00.000Z",
        "voteCount": 3,
        "content": "You can absolutly directly \"export log data from your log groups to an Amazon S3 bucket\"\nHowever, this is a one time export, and NOT an ongoing stream.\nIf you want to steam continuously you have to use \"subscription filter with Kinesis Data Streams, Lambda, or Firehose.\"\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\nhttps://dev.to/aws-builders/automate-export-of-cloudwatch-logs-to-s3-bucket-using-lambda-with-eventbridge-trigger-2ieg"
      },
      {
        "date": "2024-05-17T07:56:00.000Z",
        "voteCount": 1,
        "content": "Looks like creating subscription filters in AWS cloudwatch logs, there are only limited destination options. There is no S3 as a direct destination. You have to either create Elasticsearch or Kinesis or Kinesis Firehose or Lambda subscription filters. Given the choices we have, we need to pick B &amp; D"
      },
      {
        "date": "2024-05-02T02:31:00.000Z",
        "voteCount": 1,
        "content": "C &amp; D for the reasons that thanhnv142 mentioned."
      },
      {
        "date": "2024-05-13T04:11:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html\nPls check link. You can use a subscription filter with Kinesis Data Streams, Lambda, or Firehose. Logs that are sent to a receiving service through a subscription filter are base64 encoded and compressed with the gzip format. correct is B and D"
      },
      {
        "date": "2024-02-27T03:29:00.000Z",
        "voteCount": 2,
        "content": "CD\uff0cThe question does not mention trying to switch to S3 in real time. C is more cost-effective.\nhttps://docs.aws.amazon.com/zh_cn/AmazonCloudWatch/latest/logs/S3ExportTasksConsole.html"
      },
      {
        "date": "2024-02-24T22:16:00.000Z",
        "voteCount": 3,
        "content": "Amazon S3 Glacier is a secure, durable, and very low-cost cloud storage service that can be used for data archiving and long-term backup."
      },
      {
        "date": "2024-02-18T22:22:00.000Z",
        "voteCount": 2,
        "content": "You can use a subscription filter with Kinesis Data Streams, Lambda, or Firehose.\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html"
      },
      {
        "date": "2024-02-02T10:03:00.000Z",
        "voteCount": 4,
        "content": "C and D is correct: &lt;archive the logs to an Amazon S3 bucket&gt; means we need to transport logs from CloudWatch Logs to S3. CloudWatch Logs can directly transport log data to S3. Logs are rarely accessed after 90 days means we need S3 bucket lifecycle policy"
      },
      {
        "date": "2024-02-02T10:03:00.000Z",
        "voteCount": 1,
        "content": "A: AWS Glue is used primarily to integrate data from multiple data sources (up to 70) for data analysis. Of course it works well with one data source only (CW logs in this case). But it costs a lot of money and using it with only one data source is a waste of corporate budget. Should not use this. \nB: Amazon Kinesis Data Firehose is primarily used for time-sensitive tasks, such as video streaming. It is very powerful that it can handle data in near realtime. However, this premium feature comes with a big expense. We only need to archive data, not video streaming it.  \nE: we need to transition it to S3 Glacier not Reduced Redundancy after 90 days"
      },
      {
        "date": "2023-07-08T20:33:00.000Z",
        "voteCount": 4,
        "content": "B to shift logs out using Kinesis Firehose to S3. Then D to set S3 bucket storage class to Glacier Flexible."
      },
      {
        "date": "2023-06-28T09:02:00.000Z",
        "voteCount": 4,
        "content": "C would make sense, but subscription filters don't go to S3 directly\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html"
      },
      {
        "date": "2023-08-08T21:48:00.000Z",
        "voteCount": 2,
        "content": "Option C is incorrect because streaming all logs to an S3 bucket is not a good solution for archiving logs"
      },
      {
        "date": "2023-06-20T22:01:00.000Z",
        "voteCount": 1,
        "content": "BD is right"
      },
      {
        "date": "2023-05-16T05:51:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: BD"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/amazon/view/109261-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is developing a new application. The application uses AWS Lambda functions for its compute tier. The company must use a canary deployment for any changes to the Lambda functions. Automated rollback must occur if any failures are reported.<br><br>The company\u2019s DevOps team needs to create the infrastructure as code (IaC) and the CI/CD pipeline for this solution.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template for the application. Define each Lambda function in the template by using the AWS::Lambda::Function resource type. In the template, include a version for the Lambda function by using the AWS::Lambda::Version resource type. Declare the CodeSha256 property. Configure an AWS::Lambda::Alias resource that references the latest version of the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Serverless Application Model (AWS SAM) template for the application. Define each Lambda function in the template by using the AWS::Serverless::Function resource type. For each function, include configurations for the AutoPublishAlias property and the DeploymentPreference property. Configure the deployment configuration type to LambdaCanary10Percent10Minutes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage that starts the pipeline. Create an AWS CodeBuild project to deploy the AWS Serverless Application Model (AWS SAM) template. Upload the template and source code to the CodeCommit repository. In the CodeCommit repository, create a buildspec.yml file that includes the commands to build and deploy the SAM application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeCommit repository. Create an AWS CodePipeline pipeline. Use the CodeCommit repository in a new source stage that starts the pipeline. Create an AWS CodeDeploy deployment group that is configured for canary deployments with a DeploymentPreference type of Canary10Percent10Minutes. Upload the AWS CloudFormation template and source code to the CodeCommit repository. In the CodeCommit repository, create an appspec.yml file that includes the commands to deploy the CloudFormation template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch composite alarm for all the Lambda functions. Configure an evaluation period and dimensions for Lambda. Configure the alarm to enter the ALARM state if any errors are detected or if there is insufficient data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm for each Lambda function. Configure the alarms to enter the ALARM state if any errors are detected. Configure an evaluation period, dimensions for each Lambda function and version, and the namespace as AWS/Lambda on the Errors metric.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T20:45:00.000Z",
        "voteCount": 2,
        "content": "BCF is my choice"
      },
      {
        "date": "2024-02-02T20:44:00.000Z",
        "voteCount": 2,
        "content": "BCF are correct:\nA is not correct: &lt;needs to create the infrastructure as code (IaC)&gt; means we prefer AWS SAM over ACF. ACF is used to deploy AWS instances, not for IaC\nD is wrong: no mention of AWS SAM\nE is wrong: &lt;Amazon CloudWatch composite alarm for all the Lambda functions&gt;, but we need alarm for each lambda func, not one alarm for all of them"
      },
      {
        "date": "2023-12-29T21:33:00.000Z",
        "voteCount": 1,
        "content": "it should be BDF because code deploy can be configured for canary\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html"
      },
      {
        "date": "2023-11-28T22:05:00.000Z",
        "voteCount": 2,
        "content": "BCF, E is not correct you need to monitor each lambda to do a rollback of a particular deploy"
      },
      {
        "date": "2023-11-26T12:34:00.000Z",
        "voteCount": 1,
        "content": "BCF is right"
      },
      {
        "date": "2023-10-30T07:11:00.000Z",
        "voteCount": 1,
        "content": "Answer is BCF.\nhttps://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html"
      },
      {
        "date": "2023-10-21T10:51:00.000Z",
        "voteCount": 3,
        "content": "Can someone please explain why not A and D? seem the same of B C without using SAM"
      },
      {
        "date": "2023-09-12T05:45:00.000Z",
        "voteCount": 2,
        "content": "A is wrong beacuse of AWS::Lambda::Function\nB - Can work\nC: is correct\nD:  SAM or Lambda deployment in Codedeploy  cannot be canary deployment.  canaray deployment should be included in the Lambda code as mentioned in option B.\nE:  Composite Alram is not required.  if any Lambda fails , it should generate alarm\nF: works\nBasically  select B from AB which is for lambda coding,  Select C from CD for deploying and F from EF for monitoring and alerting"
      },
      {
        "date": "2024-01-30T00:07:00.000Z",
        "voteCount": 1,
        "content": "Why we cannot use code deploy for canary there are already few deployment percentage for codedeploy BDF is correct"
      },
      {
        "date": "2023-07-08T20:38:00.000Z",
        "voteCount": 2,
        "content": "Leaning to BCF. Lambda errors are standard although BCE is possible too.\n\nFor server less it is giveaway question. Stick with SAM if possible"
      },
      {
        "date": "2023-07-08T20:40:00.000Z",
        "voteCount": 2,
        "content": "I\u2019ll stick with BCF still. Composite alarms does not apply in this context. https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Create_Composite_Alarm.html"
      },
      {
        "date": "2023-07-08T13:41:00.000Z",
        "voteCount": 2,
        "content": "BCF is correct."
      },
      {
        "date": "2023-07-05T03:08:00.000Z",
        "voteCount": 3,
        "content": "BC and F"
      },
      {
        "date": "2023-06-29T13:34:00.000Z",
        "voteCount": 3,
        "content": "Composite Alarm requires underlying metric alarms which requires one CloudWatch alarm for each lambda functions and then tie them back to a composite alarm. So BC and F makes sense."
      },
      {
        "date": "2023-06-25T05:18:00.000Z",
        "voteCount": 1,
        "content": "I think BCE makes more sense."
      },
      {
        "date": "2023-06-16T23:17:00.000Z",
        "voteCount": 1,
        "content": "F creates an Amazon CloudWatch alarm for each Lambda function. However, it is not necessary to create an alarm for each Lambda function. A single composite alarm can be used to monitor all the Lambda functions."
      },
      {
        "date": "2023-07-08T13:41:00.000Z",
        "voteCount": 1,
        "content": "The issue with Answer E is that the alarm will also trigger on \"insufficient data\", which is not what you want. Answer F is correct."
      },
      {
        "date": "2023-05-16T05:59:00.000Z",
        "voteCount": 1,
        "content": "BCF are correct"
      },
      {
        "date": "2023-05-15T01:30:00.000Z",
        "voteCount": 1,
        "content": "Cannot deploy the canary deployemnt in a pipeline for lambda creation, it has to be created in lambda resource file."
      },
      {
        "date": "2023-05-15T01:27:00.000Z",
        "voteCount": 4,
        "content": "BCF correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/amazon/view/108810-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is deploying a new version of a company\u2019s application in an AWS CodeDeploy deployment group associated with its Amazon EC2 instances. After some time, the deployment fails. The engineer realizes that all the events associated with the specific deployment ID are in a Skipped status, and code was not deployed in the instances associated with the deployment group.<br><br>What are valid reasons for this failure? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe networking configuration does not allow the EC2 instances to reach the internet via a NAT gateway or internet gateway, and the CodeDeploy endpoint cannot be reached.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IAM user who triggered the application deployment does not have permission to interact with the CodeDeploy endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe target EC2 instances were not properly registered with the CodeDeploy endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn instance profile with proper permissions was not attached to the target EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe appspec.yml file was not included in the application revision."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-25T10:50:00.000Z",
        "voteCount": 10,
        "content": "A.\n\nExplanation: For CodeDeploy to work, the EC2 instances need to reach the CodeDeploy endpoint to download the deployment artifacts. If the networking configuration of the EC2 instances does not allow them to access the internet via a NAT gateway or internet gateway, they won't be able to reach the CodeDeploy endpoint, leading to deployment failure.\nD\n\nExplanation: When EC2 instances are part of a CodeDeploy deployment group, they need to have an associated IAM instance profile with the necessary permissions to interact with CodeDeploy and download the deployment artifacts. If the instance profile with proper permissions is not attached to the target EC2 instances, the deployment will fail as the instances won't have the required permissions to complete the deployment process."
      },
      {
        "date": "2023-06-15T07:47:00.000Z",
        "voteCount": 7,
        "content": "AD\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html\nSearch with: Troubleshooting all lifecycle events skipped errors"
      },
      {
        "date": "2023-12-14T20:39:00.000Z",
        "voteCount": 1,
        "content": "C is correct. \nthe first reason for skipped errors on the link. \n The CodeDeploy agent might not be installed or running on the instance. To determine if the CodeDeploy agent is running:"
      },
      {
        "date": "2024-03-11T05:54:00.000Z",
        "voteCount": 1,
        "content": "No registration required, once agent is installed it should be sufficient. However permissions and network connectivity to S3 or code deploy would be must. Since that takes priority, A&amp;D should be right."
      },
      {
        "date": "2024-09-06T04:12:00.000Z",
        "voteCount": 1,
        "content": "My question is \"skipped\" situation doesn't sound like a network error. There is no 4xx error or fail status"
      },
      {
        "date": "2024-05-17T09:46:00.000Z",
        "voteCount": 1,
        "content": "The user needs to create a service role and attach the AWSCodeDeployRole policy to it to grant the correct permissions for CodeDeploy to access EC2 instances. The role chosen should allow access to start and stop EC2 instances.\nIf the IAM role used by CodeDeploy doesn't have the necessary permissions to access the deployment artifacts or interact with the EC2 instances, the deployment may be skipped.\nSo it is not the IAM permissions of the user invoking the CodeDeploy."
      },
      {
        "date": "2024-02-02T21:05:00.000Z",
        "voteCount": 1,
        "content": "A and D are correct: the deployment process might be skipped because of codedeploy agent\nA: no connection means skipped deployment\nD: insufficient permission means skipped deployment"
      },
      {
        "date": "2024-01-17T08:52:00.000Z",
        "voteCount": 1,
        "content": "A and D. See https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-skipped-lifecycle-events"
      },
      {
        "date": "2024-01-01T19:57:00.000Z",
        "voteCount": 1,
        "content": "Do you really have to have internet connectivity to use CodeDeploy? Why not use VPC endpoint in such cases? I go for CD."
      },
      {
        "date": "2023-12-10T03:29:00.000Z",
        "voteCount": 2,
        "content": "Some of the other options could cause a deployment to fail, but not specifically result in a \"Skipped\" status:\n\nA    Networking issues may prevent the deployment from reaching instances, but this would likely cause the deployment to fail, not be skipped.\nB    Lack of permissions for the IAM user would cause the deployment job itself to fail authorization.\nE    Missing appspec.yml would cause validation errors prior to the deployment attempt.\n\nAnyone has diffrent views?"
      },
      {
        "date": "2023-12-10T03:32:00.000Z",
        "voteCount": 1,
        "content": "oh yeah;\nwhy;\nC -&gt; CodeDeploy needs to be able to communicate with the instances in order to deploy revisions to them. If the instances are not registered, CodeDeploy will skip deploying to them.\nD - &gt; i think everyone know that point. i guess dont need explaintion. \nPeace :)"
      },
      {
        "date": "2023-09-12T06:15:00.000Z",
        "voteCount": 1,
        "content": "I agree with rhinozD"
      },
      {
        "date": "2023-07-05T03:08:00.000Z",
        "voteCount": 1,
        "content": "AD is correct"
      },
      {
        "date": "2023-05-13T23:28:00.000Z",
        "voteCount": 1,
        "content": "Its AD"
      },
      {
        "date": "2023-05-09T07:56:00.000Z",
        "voteCount": 1,
        "content": "AD is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/amazon/view/108811-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has a guideline that every Amazon EC2 instance must be launched from an AMI that the company\u2019s security team produces. Every month, the security team sends an email message with the latest approved AMIs to all the development teams.<br><br>The development teams use AWS CloudFormation to deploy their applications. When developers launch a new service, they have to search their email for the latest AMIs that the security department sent. A DevOps engineer wants to automate the process that the security team uses to provide the AMI IDs to the development teams.<br><br>What is the MOST scalable solution that meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirect the security team to use CloudFormation to create new versions of the AMIs and to list the AMI ARNs in an encrypted Amazon S3 object as part of the stack\u2019s Outputs section. Instruct the developers to use a cross-stack reference to load the encrypted S3 object and obtain the most recent AMI ARNs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirect the security team to use a CloudFormation stack to create an AWS CodePipeline pipeline that builds new AMIs and places the latest AMI ARNs in an encrypted Amazon S3 object as part of the pipeline output. Instruct the developers to use a cross-stack reference within their own CloudFormation template to obtain the S3 object location and the most recent AMI ARNs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirect the security team to use Amazon EC2 Image Builder to create new AMIs and to place the AMI ARNs as parameters in AWS Systems Manager Parameter Store. Instruct the developers to specify a parameter of type SSM in their CloudFormation stack to obtain the most recent AMI ARNs from Parameter Store.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirect the security team to use Amazon EC2 Image Builder to create new AMIs and to create an Amazon Simple Notification Service (Amazon SNS) topic so that every development team can receive notifications. When the development teams receive a notification, instruct them to write an AWS Lambda function that will update their CloudFormation stack with the most recent AMI ARNs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T21:19:00.000Z",
        "voteCount": 6,
        "content": "C is correct: &lt;automate the process that the security team uses to provide the AMI IDs to the development teams&gt; and &lt;MOST scalable solution&gt; means we need a pipeline (imange builder) to build AMI and to automate sharing\nA and B: no mention of EC2 Imange builder, which is better than codepipeline in building Ec2 image\nD: They have to do this manually"
      },
      {
        "date": "2024-01-09T07:53:00.000Z",
        "voteCount": 2,
        "content": "C is the best option"
      },
      {
        "date": "2023-10-31T19:27:00.000Z",
        "voteCount": 2,
        "content": "Answer is C.\nhttps://aws.amazon.com/ko/blogs/compute/tracking-the-latest-server-images-in-amazon-ec2-image-builder-pipelines/"
      },
      {
        "date": "2023-07-08T20:53:00.000Z",
        "voteCount": 2,
        "content": "Use SSM Parameter Store or Secret Manager as the lookup K/V store for all the related AMIs. ANother way is also for security team to constantly update and share the images cross-account and grant them KMS keys to the encrypted AMIs. (not in question)"
      },
      {
        "date": "2023-05-13T23:32:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2023-05-09T07:58:00.000Z",
        "voteCount": 4,
        "content": "C make more sense"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/amazon/view/109204-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "An application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). A DevOps engineer is using AWS CodeDeploy to release a new version. The deployment fails during the AllowTraffic lifecycle event, but a cause for the failure is not indicated in the deployment logs.<br><br>What would cause this?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe appspec.yml file contains an invalid script that runs in the AllowTraffic lifecycle hook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe user who initiated the deployment does not have the necessary permissions to interact with the ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe health checks specified for the ALB target group are misconfigured.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CodeDeploy agent was not installed in the EC2 instances that are part of the ALB target group."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-15T07:54:00.000Z",
        "voteCount": 16,
        "content": "C is the answer\nrefer this: https://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-deployments.html#troubleshooting-deployments-allowtraffic-no-logs"
      },
      {
        "date": "2024-02-02T21:23:00.000Z",
        "voteCount": 4,
        "content": "C is correct: &lt;deployment fails during the AllowTraffic lifecycle event&gt; means there are problems with ALB. \nA: no mention of the ALB\nB: The user who init the deployment does not need necessary permission\nD: If agent was not installed, it would fail from the start"
      },
      {
        "date": "2023-05-16T06:39:00.000Z",
        "voteCount": 2,
        "content": "C is the answer"
      },
      {
        "date": "2023-05-13T23:36:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/amazon/view/108587-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has 20 service teams. Each service team is responsible for its own microservice. Each service team uses a separate AWS account for its microservice and a VPC with the 192.168.0.0/22 CIDR block. The company manages the AWS accounts with AWS Organizations.<br><br>Each service team hosts its microservice on multiple Amazon EC2 instances behind an Application Load Balancer. The microservices communicate with each other across the public internet. The company\u2019s security team has issued a new guideline that all communication between microservices must use HTTPS over private network connections and cannot traverse the public internet.<br><br>A DevOps engineer must implement a solution that fulfills these obligations and minimizes the number of changes for each service team.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS account in AWS Organizations. Create a VPC in this account, and use AWS Resource Access Manager to share the private subnets of this VPC with the organization. Instruct the service teams to launch a new Network Load Balancer (NLB) and EC2 instances that use the shared private subnets. Use the NLB DNS names for communication between microservices.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB) in each of the microservice VPCs. Use AWS PrivateLink to create VPC endpoints in each AWS account for the NLBs. Create subscriptions to each VPC endpoint in each of the other AWS accounts. Use the VPC endpoint DNS names for communication between microservices.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Network Load Balancer (NLB) in each of the microservice VPCs. Create VPC peering connections between each of the microservice VPCs. Update the route tables for each VPC to use the peering links. Use the NLB DNS names for communication between microservices.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS account in AWS Organizations. Create a transit gateway in this account, and use AWS Resource Access Manager to share the transit gateway with the organization. In each of the microservice VPCs, create a transit gateway attachment to the shared transit gateway. Update the route tables of each VPC to use the transit gateway. Create a Network Load Balancer (NLB) in each of the microservice VPCs. Use the NLB DNS names for communication between microservices."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-05T03:18:00.000Z",
        "voteCount": 7,
        "content": "B is correct because all 20 services team in different separate AWS accounts are using the same CIDR block, which means they are overlapping CIDR. \n\nD state that to update the route tables of each VPC to use the transit gateway but they are all having the same CIDR block so this cannot proceed, as shared by Arnaud92 link the pre-requisite of using the transit gateway is \"No-overlapping CIDR block between VPCs.\""
      },
      {
        "date": "2024-05-21T11:43:00.000Z",
        "voteCount": 2,
        "content": "B is the answer\nWhen VPCs have overlapping CIDR blocks, AWS PrivateLink still ensures secure and private connectivity by using Interface Endpoints (ENIs) and Network Load Balancers (NLBs) to route traffic, bypassing the need for direct IP routing between the VPCs."
      },
      {
        "date": "2024-02-02T21:43:00.000Z",
        "voteCount": 2,
        "content": "B is correct: &lt;all communication between microservices must use HTTPS over private network connections and cannot traverse the public internet&gt; means privatelink\nA and C: no mention of privatelink\nD: Using transite gateway. But this solution need IP to route traffic and cannot be used for overlapped VPC CIDR block (every team uses 192.168.0.0/22)"
      },
      {
        "date": "2023-11-23T11:16:00.000Z",
        "voteCount": 4,
        "content": "Answer is B, Transit gateway can't route overlapping networks, the solution for this is privatelink: https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-privatelink.html"
      },
      {
        "date": "2023-09-12T06:40:00.000Z",
        "voteCount": 2,
        "content": "Thanks to rhinozD.  Please check the side by sode comparision at the bottom of this page https://tomgregory.com/cross-account-vpc-access-in-aws"
      },
      {
        "date": "2023-09-24T13:51:00.000Z",
        "voteCount": 5,
        "content": "In that same document you shared it says:\nNo-overlapping CIDR block between VPCs possible for Transit Gateway.\nSo it cannot be D."
      },
      {
        "date": "2023-08-15T05:15:00.000Z",
        "voteCount": 2,
        "content": "B is right.,"
      },
      {
        "date": "2023-07-12T06:42:00.000Z",
        "voteCount": 3,
        "content": "B. is the right Solution! \nDue to AWS's Transit Gateway not supporting same CIDRs (https://aws.amazon.com \u203a transit-gateway \u203a faqs), the most viable solution is the deployment of a Network Load Balancer (NLB) in each VPC. However, it's crucial to note that NLB operates similar to a NAT Gateway, allowing only incoming requests. After an incoming request is accepted, the NLB can then provide a response."
      },
      {
        "date": "2023-07-11T11:42:00.000Z",
        "voteCount": 1,
        "content": "AWS Transit Gateway doesn\u2019t support routing between Amazon VPCs with identical CIDRs. If you attach a new Amazon VPC that has a CIDR which is identical to an already attached Amazon VPC, AWS Transit Gateway will not propagate the new Amazon VPC route into the AWS Transit Gateway route table."
      },
      {
        "date": "2023-07-08T21:05:00.000Z",
        "voteCount": 1,
        "content": "I\u2019ll lean towards B. For D, transit gateway is really expensive and does get the job done. There is also a need for NAT gateway as by default all AWS API traffic passes through the public internet. Hence, PrivateLink endpoints are for."
      },
      {
        "date": "2023-06-24T14:34:00.000Z",
        "voteCount": 1,
        "content": "I go with option D. It makes more sense to me."
      },
      {
        "date": "2023-06-19T02:35:00.000Z",
        "voteCount": 3,
        "content": "I think the correct answer is B. Please note all service team is using the same cidr block for their vpc. It's impossible to add them in the same network mesh using vpc peering and transit gateway."
      },
      {
        "date": "2023-06-06T00:09:00.000Z",
        "voteCount": 2,
        "content": "see https://tomgregory.com/cross-account-vpc-access-in-aws/ , Option 3\nThe use of a central hub reduce the complexity for 20 accounts\nneed an additional account to avoid cidr block collision, in the link they put the transit gateway in one of existing account"
      },
      {
        "date": "2023-06-15T08:01:00.000Z",
        "voteCount": 2,
        "content": "Please read the \"Side-by-side comparison\" part at the end of the post.\nD is wrong.\nB is correct."
      },
      {
        "date": "2023-06-05T06:57:00.000Z",
        "voteCount": 1,
        "content": "Answer is D.\nOption B is incorrect because it requires creating a Network Load Balancer in each of the microservice VPCs and using AWS PrivateLink to create VPC endpoints. This would result in a lot of configuration changes for each service team and increased complexity."
      },
      {
        "date": "2023-05-13T23:40:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2023-05-10T02:13:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      },
      {
        "date": "2023-05-07T20:50:00.000Z",
        "voteCount": 3,
        "content": "Option D is correct to me."
      },
      {
        "date": "2023-05-05T14:20:00.000Z",
        "voteCount": 3,
        "content": "Option D is the correct solution to meet the requirements."
      },
      {
        "date": "2023-06-04T01:34:00.000Z",
        "voteCount": 1,
        "content": "see https://tomgregory.com/cross-account-vpc-access-in-aws/  , Option 3\nThe use of a central hub reduce the complexity for 20 accounts"
      },
      {
        "date": "2023-06-06T00:08:00.000Z",
        "voteCount": 1,
        "content": "D is correct , central hub reduce complexity , need an additional account to avoid cidr block collision, in the link they put the transit gateway in one of existing account"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/amazon/view/109205-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "An Amazon EC2 instance is running in a VPC and needs to download an object from a restricted Amazon S3 bucket. When the DevOps engineer tries to download the object, an AccessDenied error is received.<br><br>What are the possible causes for this error? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe S3 bucket default encryption is enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is an error in the S3 bucket policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe object has been moved to S3 Glacier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is an error in the IAM role configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tS3 Versioning is enabled."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T08:01:00.000Z",
        "voteCount": 3,
        "content": "I think B and D"
      },
      {
        "date": "2023-12-25T10:32:00.000Z",
        "voteCount": 2,
        "content": "ACCESS DENIED - you got it"
      },
      {
        "date": "2023-09-12T06:47:00.000Z",
        "voteCount": 3,
        "content": "IMHO it is BD"
      },
      {
        "date": "2023-08-02T06:52:00.000Z",
        "voteCount": 4,
        "content": "BD\nNot an error though. Misconfiguration."
      },
      {
        "date": "2023-06-25T05:24:00.000Z",
        "voteCount": 2,
        "content": "B and D for sure."
      },
      {
        "date": "2023-05-16T06:43:00.000Z",
        "voteCount": 1,
        "content": "BD are the answers I got"
      },
      {
        "date": "2023-05-13T23:44:00.000Z",
        "voteCount": 1,
        "content": "BD are correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/amazon/view/109206-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company wants to use a grid system for a proprietary enterprise in-memory data store on top of AWS. This system can run in multiple server nodes in any Linux-based distribution. The system must be able to reconfigure the entire cluster every time a node is added or removed. When adding or removing nodes, an /etc/cluster/nodes.config file must be updated, listing the IP addresses of the current node members of that cluster.<br><br>The company wants to automate the task of adding new nodes to a cluster.<br><br>What can a DevOps engineer do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS OpsWorks Stacks to layer the server nodes of that cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPut the file nodes.config in version control. Create an AWS CodeDeploy deployment configuration and deployment group based on an Amazon EC2 tag value for the cluster nodes. When adding a new node to the cluster, update the file with all tagged instances, and make a commit in version control. Deploy the new file and restart the services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket and upload a version of the /etc/cluster/nodes.config file. Create a crontab script that will poll for that S3 file and download it frequently. Use a process manager, such as Monit or systemd, to restart the cluster services when it detects that the new file was modified. When adding a node to the cluster, edit the file\u2019s most recent members. Upload the new file to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a user data script that lists all members of the current security group of the cluster and automatically updates the /etc/cluster/nodes.config file whenever a new instance is added to the cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T01:23:00.000Z",
        "voteCount": 5,
        "content": "A is correct: &lt;wants to use a grid system&gt; means opswork stacks\nB, C and D: no mention of opswork stack"
      },
      {
        "date": "2023-07-08T21:10:00.000Z",
        "voteCount": 5,
        "content": "I\u2019ll use config management tool as well. In this case Opsworks (Chef/Puppet)."
      },
      {
        "date": "2024-07-24T09:59:00.000Z",
        "voteCount": 1,
        "content": "But how the files content (get actual nodes list) is updated in that case?"
      },
      {
        "date": "2024-09-02T02:26:00.000Z",
        "voteCount": 1,
        "content": "D is the best"
      },
      {
        "date": "2023-09-22T12:13:00.000Z",
        "voteCount": 2,
        "content": "1\nThe best solution to meet the company's requirements is to use AWS OpsWorks Stacks to layer the server nodes of the cluster. Create a Chef recipe that populates the content of the /etc/cluster/nodes.config file and restarts the service by using the current members of the layer. Assign that recipe to the Configure lifecycle event."
      },
      {
        "date": "2023-06-15T08:17:00.000Z",
        "voteCount": 2,
        "content": "A is correct.\nThis event occurs on all of the stack's instances when one of the following occurs:\nAn instance enters or leaves the online state.\nYou associate an Elastic IP address with an instance or disassociate one from an instance.\nYou attach an Elastic Load Balancing load balancer to a layer, or detach one from a layer.\nhttps://docs.aws.amazon.com/opsworks/latest/userguide/workingcookbook-events.html"
      },
      {
        "date": "2023-05-13T23:46:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/amazon/view/109207-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is working on a data archival project that requires the migration of on-premises data to an Amazon S3 bucket. The DevOps engineer develops a script that incrementally archives on-premises data that is older than 1 month to Amazon S3. Data that is transferred to Amazon S3 is deleted from the on-premises location. The script uses the S3 PutObject operation.<br><br>During a code review, the DevOps engineer notices that the script does not verify whether the data was successfully copied to Amazon S3. The DevOps engineer must update the script to ensure that data is not corrupted during transmission. The script must use MD5 checksums to verify data integrity before the on-premises data is deleted.<br><br>Which solutions for the script will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the returned response for the VersionId. Compare the returned VersionId against the MD5 checksum.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the MD5 checksum within the Content-MD5 parameter. Check the operation call\u2019s return status to find out if an error was returned.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the checksum digest within the tagging parameter as a URL query parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the returned response for the ETag. Compare the returned ETag against the MD5 checksum.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the checksum digest within the Metadata parameter as a name-value pair. After upload, use the S3 HeadObject operation to retrieve metadata from the object."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-25T10:56:00.000Z",
        "voteCount": 9,
        "content": "B. Explanation: When using the S3 PutObject operation, you can include the MD5 checksum of the object in the Content-MD5 parameter of the request. Amazon S3 will calculate the MD5 checksum of the object and compare it to the provided checksum. If the checksums do not match, Amazon S3 will return an error response, indicating that the data integrity check failed. This way, you can ensure that the data was successfully copied to Amazon S3 without corruption.\n\nD. Explanation: When you use the S3 PutObject operation, it returns an ETag in the response, which is the MD5 checksum of the object that was stored in Amazon S3. After performing the upload, you can check the returned ETag against the MD5 checksum you have locally calculated. If they match, it means the data was transferred successfully without corruption. If they don't match, it indicates a data integrity issue, and you can take appropriate actions."
      },
      {
        "date": "2024-02-25T21:56:00.000Z",
        "voteCount": 3,
        "content": "If the object was created by a PutObject, PostObject, or Copy operation, or via the AWS Management Console, and the object is either plain text or encrypted with server-side encryption using the Amazon S3 managed key ( SSE-S3), the object's ETag is the MD5 digest of the object data."
      },
      {
        "date": "2024-02-03T01:30:00.000Z",
        "voteCount": 3,
        "content": "B and D are correct: &lt;verify whether the data was successfully copied to Amazon S3&gt; means we need to check &lt;operation call\u2019s return status&gt; code. &lt;use MD5 checksums to verify data integrity&gt; means we need to check ETag\nA: no mention of ETag\nC and E: no mention of ETag or return status code"
      },
      {
        "date": "2023-06-15T08:24:00.000Z",
        "voteCount": 4,
        "content": "BD\nrefer this link: https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html"
      },
      {
        "date": "2023-05-13T23:49:00.000Z",
        "voteCount": 3,
        "content": "BD are correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/amazon/view/109209-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company deploys updates to its Amazon API Gateway API several times a week by using an AWS CodePipeline pipeline. As part of the update process, the company exports the JavaScript SDK for the API from the API Gateway console and uploads the SDK to an Amazon S3 bucket.<br><br>The company has configured an Amazon CloudFront distribution that uses the S3 bucket as an origin. Web clients then download the SDK by using the CloudFront distribution\u2019s endpoint. A DevOps engineer needs to implement a solution to make the new SDK available automatically during new API deployments.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodePipeline action immediately after the deployment stage of the API. Configure the action to invoke an AWS Lambda function. Configure the Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and create a CloudFront invalidation for the SDK path.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodePipeline action immediately after the deployment stage of the API. Configure the action to use the CodePipeline integration with API Gateway to export the SDK to Amazon S3. Create another action that uses the CodePipeline integration with Amazon S3 to invalidate the cache for the SDK path.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to UpdateStage events from aws.apigateway. Configure the rule to invoke an AWS Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the CloudFront API to create an invalidation for the SDK path.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to CreateDeployment events from aws.apigateway. Configure the rule to invoke an AWS Lambda function to download the SDK from API Gateway, upload the SDK to the S3 bucket, and call the S3 API to invalidate the cache for the SDK path."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T02:15:00.000Z",
        "voteCount": 5,
        "content": "A is correct: &lt;by using an AWS CodePipeline pipeline&gt; means we need CodePipeline. \nC and D: no mention of CodePipeline.\nB: &lt; Configure the action to use the CodePipeline integration with API Gateway to export the SDK to Amazon S3&gt;: codepipeline need to invoke other tools to do its task. There is not integration with API gateway"
      },
      {
        "date": "2023-10-30T08:04:00.000Z",
        "voteCount": 3,
        "content": "Vote A. Reasons:\nC: No. \"aws.apigateway needs API Gateway AWS integration to send events to EventBridge without using compute service, such as Lambda or Amazon EC2.\"\nhttps://aws.amazon.com/blogs/compute/capturing-client-events-using-amazon-api-gateway-and-amazon-eventbridge/\nB&amp;D: No. S3 API doesn't contain invalidate cache call, whereas CloudFront does. Search \"invalidat\" in\nhttps://docs.aws.amazon.com/cli/latest/reference/s3api/\nhttps://docs.aws.amazon.com/cli/latest/reference/cloudfront/"
      },
      {
        "date": "2023-09-12T08:00:00.000Z",
        "voteCount": 2,
        "content": "B &amp; D are wrong as it suggests to invalidate the cache fro S3\nD uses event bridge rule to invoke lambada however we know Codepipeline is available and that can be used to perform the action"
      },
      {
        "date": "2023-08-29T10:22:00.000Z",
        "voteCount": 1,
        "content": "I pick C due to it stating during the deployment rather than at the end."
      },
      {
        "date": "2023-07-02T19:42:00.000Z",
        "voteCount": 4,
        "content": "Lambda is king"
      },
      {
        "date": "2023-07-08T21:46:00.000Z",
        "voteCount": 4,
        "content": "??????"
      },
      {
        "date": "2023-05-13T23:53:00.000Z",
        "voteCount": 3,
        "content": "A is the right answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/amazon/view/109215-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has developed an AWS Lambda function that handles orders received through an API. The company is using AWS CodeDeploy to deploy the Lambda function as the final stage of a CI/CD pipeline.<br><br>A DevOps engineer has noticed there are intermittent failures of the ordering API for a few seconds after deployment. After some investigation, the DevOps engineer believes the failures are due to database changes not having fully propagated before the Lambda function is invoked.<br><br>How should the DevOps engineer overcome this?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a BeforeAllowTraffic hook to the AppSpec file that tests and waits for any necessary database changes before traffic can flow to the new version of the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an AfterAllowTraffic hook to the AppSpec file that forces traffic to wait for any pending database changes before allowing the new version of the Lambda function to respond.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a BeforeInstall hook to the AppSpec file that tests and waits for any necessary database changes before deploying the new version of the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a ValidateService hook to the AppSpec file that inspects incoming traffic and rejects the payload if dependent services, such as the database, are not yet ready."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-14T14:38:00.000Z",
        "voteCount": 3,
        "content": "In my research, there are only TWO CodeDeploy AppSpec ifecycle event hooks for Lambda deployment:\n  BeforeAllowTraffic\t# Use to run tasks before traffic is shifted to the deployed Lambda function version.\n  AfterAllowTraffic\t# Use to run tasks after all traffic is shifted to the deployed Lambda function version.\n\nA: (YES) Don't redirect traffic untill ready\nB: (NO)  Block traffic until ready\nC: (NO)  Event hook N/A for Lambda\nD: (NO)  Event hook N/A for Lambda"
      },
      {
        "date": "2024-05-02T04:33:00.000Z",
        "voteCount": 1,
        "content": "I think A"
      },
      {
        "date": "2024-02-19T01:49:00.000Z",
        "voteCount": 3,
        "content": "D can be correct if there is a wait to database to be ready so I will go with A"
      },
      {
        "date": "2024-02-03T02:25:00.000Z",
        "voteCount": 4,
        "content": "A is correct: &lt;using AWS CodeDeploy to deploy&gt; and &lt;a CI/CD pipeline&gt; means lifecycle event hook\nB: AfterAllowTraffic wont solve the problem, we need to hook before traffic is allowed, as in &lt;not having fully propagated before the Lambda function is invoked&gt;\nC: beforeInstall is used to prepare for the installation process, so it is not relevant\nD: there is no ValidateService hook"
      },
      {
        "date": "2023-07-08T21:51:00.000Z",
        "voteCount": 3,
        "content": "\"Hooks\": [\n\t\t{\n\t\t\t\"BeforeInstall\": \"BeforeInstallHookFunctionName\"\n\t\t},\n\t\t{\n\t\t\t\"AfterInstall\": \"AfterInstallHookFunctionName\"\n\t\t},\n\t\t{\n\t\t\t\"AfterAllowTestTraffic\": \"AfterAllowTestTrafficHookFunctionName\"\n\t\t},\n\t\t{\n\t\t\t\"BeforeAllowTraffic\": \"BeforeAllowTrafficHookFunctionName\"\n\t\t},\n\t\t{\n\t\t\t\"AfterAllowTraffic\": \"AfterAllowTrafficHookFunctionName\"\n\t\t}\n\t]\n}"
      },
      {
        "date": "2023-07-08T21:52:00.000Z",
        "voteCount": 2,
        "content": "Opting for A based on this"
      },
      {
        "date": "2023-11-28T02:56:00.000Z",
        "voteCount": 6,
        "content": "Those hooks are not valid for a Lambda, see the doc.\nLambda only supports BeforeAllowTraffic and AfterAllowTraffic. Anyway the answer is A"
      },
      {
        "date": "2023-06-18T06:51:00.000Z",
        "voteCount": 3,
        "content": "A is make sense"
      },
      {
        "date": "2023-05-16T06:57:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer"
      },
      {
        "date": "2023-05-14T02:22:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/amazon/view/109216-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses a single AWS account to test applications on Amazon EC2 instances. The company has turned on AWS Config in the AWS account and has activated the restricted-ssh AWS Config managed rule.<br><br>The company needs an automated monitoring solution that will provide a customized notification in real time if any security group in the account is not compliant with the restricted-ssh rule. The customized notification must contain the name and ID of the noncompliant security group.<br><br>A DevOps engineer creates an Amazon Simple Notification Service (Amazon SNS) topic in the account and subscribes the appropriate personnel to the topic.<br><br>What should the DevOps engineer do next to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure an input transformer for the EventBridge rule. Configure the EventBridge rule to publish a notification to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Config to send all evaluation results for the restricted-ssh rule to the SNS topic. Configure a filter policy on the SNS topic to send only notifications that contain the text of NON_COMPLIANT in the notification to subscribers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that matches an AWS Config evaluation result of NON_COMPLIANT for the restricted-ssh rule. Configure the EventBridge rule to invoke AWS Systems Manager Run Command on the SNS topic to customize a notification and to publish the notification to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that matches all AWS Config evaluation results of NON_COMPLIANT. Configure an input transformer for the restricted-ssh rule. Configure the EventBridge rule to publish a notification to the SNS topic."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-06T11:13:00.000Z",
        "voteCount": 2,
        "content": "AWS Config can send notifications to an SNS topic directly but here you need a customized notification which is only possible with the input transformer in Amazon EventBridge. So I think A is the better choice."
      },
      {
        "date": "2024-05-07T08:07:00.000Z",
        "voteCount": 2,
        "content": "B\nAWS Config can send notifications directly to SNS."
      },
      {
        "date": "2024-02-28T00:29:00.000Z",
        "voteCount": 1,
        "content": "A\uff0cAbout strict-ssh   https://docs.aws.amazon.com/zh_cn/config/latest/developerguide/restricted-ssh.html"
      },
      {
        "date": "2024-02-03T03:17:00.000Z",
        "voteCount": 4,
        "content": "A is correct: &lt;needs an automated monitoring solution that will provide a customized notification&gt; and &lt;creates an Amazon Simple Notification Service (Amazon SNS) topic&gt; means they have already have SNS. we need to trigger alarm with eventbridge and send noti to SNS\nB: no mention of event bride\nC: AWS Systems Manager Run Command on the SNS topic to customize a notification: this step is unnecessary\nD: &lt;matches all AWS Config evaluation results of NON_COMPLIAN&gt;: we need to match NON_COMPLIANT for the restricted-ssh rule only"
      },
      {
        "date": "2023-08-24T04:20:00.000Z",
        "voteCount": 2,
        "content": "Here is an example\nhttps://repost.aws/knowledge-center/config-resource-non-compliant"
      },
      {
        "date": "2023-07-29T02:42:00.000Z",
        "voteCount": 1,
        "content": "Option C is the most appropriate solution for creating a customized SNS notification when the restricted-ssh AWS Config rule is evaluated as NON_COMPLIANT."
      },
      {
        "date": "2023-08-14T05:10:00.000Z",
        "voteCount": 3,
        "content": "Sorry A \n\nEventBridge input transformers are used to customize the data that is sent to a target of an EventBridge rule. They can be used to extract specific data from the event, to convert the data to a different format, or to filter the data."
      },
      {
        "date": "2024-01-06T14:01:00.000Z",
        "voteCount": 1,
        "content": "why would you want to customize anything to SNS. I chose C, but A makes more sense. no need for sns customization"
      },
      {
        "date": "2023-07-20T09:11:00.000Z",
        "voteCount": 3,
        "content": "A\nThe Amazon EventBridge rule should be set up to match AWS Config evaluation results specifically for the restricted-ssh rule.\nAn input transformer should be configured for the EventBridge rule to extract and format the required information (e.g., name and ID of the noncompliant security group) from the AWS Config evaluation result.\nThe EventBridge rule should be configured to publish a notification to the SNS topic once it detects a noncompliant result."
      },
      {
        "date": "2023-06-18T06:54:00.000Z",
        "voteCount": 4,
        "content": "A is good,\n- restrict trigger to only ssh sg non compliance\n- you need input trans*** for sending message to SNS"
      },
      {
        "date": "2023-05-14T02:28:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/amazon/view/108863-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company requires an RPO of 2 hours and an RTO of 10 minutes for its data and application at all times. An application uses a MySQL database and Amazon EC2 web servers. The development team needs a strategy for failover and disaster recovery.<br><br>Which combination of deployment strategies will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora cluster in one Availability Zone across multiple Regions as the data store. Use Aurora\u2019s automatic recovery capabilities in the event of a disaster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora global database in two Regions as the data store. In the event of a failure, promote the secondary Region as the primary for the application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora multi-master cluster across multiple Regions as the data store. Use a Network Load Balancer to balance the database traffic in different Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the application in two Regions and use Amazon Route 53 failover-based routing that points to the Application Load Balancers in both Regions. Use health checks to determine the availability in a given Region. Use Auto Scaling groups in each Region to adjust capacity based on demand.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the application in two Regions and use a multi-Region Auto Scaling group behind Application Load Balancers to manage the capacity based on demand. In the event of a disaster, adjust the Auto Scaling group\u2019s desired instance count to increase baseline capacity in the failover Region."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-06T12:14:00.000Z",
        "voteCount": 1,
        "content": "Amazon Aurora clusters are designed to be region-specific, meaning that an Aurora DB cluster is limited to a single AWS region. However, you can use Amazon Aurora Global Database to span multiple AWS regions. But Amazon Aurora clusters can span across multiple AZs.\nAn AWS Auto Scaling group cannot span multiple regions. Each Auto Scaling group is limited to a single AWS region. However, within that region, an Auto Scaling group can span multiple Availability Zones to ensure high availability and fault tolerance."
      },
      {
        "date": "2024-02-03T03:33:00.000Z",
        "voteCount": 4,
        "content": "B and D are correct: &lt;needs a strategy for failover and disaster recovery&gt; means global table or db cluster and route53 fail-over policy\nA and C: These options mention spanning an Amazon Aurora cluster across multiple region. This is not true. A cluster can span across multiple AZs, not regions. The only Aurora solution that can span multiple regions is global table, which includes multiple clusters.\nE: No mention of Route 53 failover-based routing"
      },
      {
        "date": "2023-10-14T07:36:00.000Z",
        "voteCount": 4,
        "content": "between ABC, choose B. A is wrong because \u201cAmazon Aurora cluster in one Availability Zone across multiple Regions\u201d is nonsense. C is incorrect too because Aurora multi-master cluster can't be across multiple regions\n\nbetween DE, choose D because using Route 53 failover-based routing makes sense. E is wrong  Auto Scaling group can't be multi-region"
      },
      {
        "date": "2023-07-16T14:15:00.000Z",
        "voteCount": 4,
        "content": "No brainer"
      },
      {
        "date": "2023-05-16T07:08:00.000Z",
        "voteCount": 4,
        "content": "Got BD as my answers"
      },
      {
        "date": "2023-05-14T02:33:00.000Z",
        "voteCount": 2,
        "content": "BD are correct"
      },
      {
        "date": "2023-05-10T06:22:00.000Z",
        "voteCount": 3,
        "content": "BD is the correct answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/amazon/view/108864-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A business has an application that consists of five independent AWS Lambda functions.<br><br>The DevOps engineer has built a CI/CD pipeline using AWS CodePipeline and AWS CodeBuild that builds, tests, packages, and deploys each Lambda function in sequence. The pipeline uses an Amazon EventBridge rule to ensure the pipeline starts as quickly as possible after a change is made to the application source code.<br><br>After working with the pipeline for a few months, the DevOps engineer has noticed the pipeline takes too long to complete.<br><br>What should the DevOps engineer implement to BEST improve the speed of the pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CodeBuild projects within the pipeline to use a compute type with more available network throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom CodeBuild execution environment that includes a symmetric multiprocessing configuration to run the builds in parallel.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the CodePipeline configuration to run actions for each Lambda function in parallel by specifying the same runOrder.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify each CodeBuild project to run within a VPC and use dedicated instances to increase throughput."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T12:49:00.000Z",
        "voteCount": 9,
        "content": "Parallel Execution:\nBy modifying the CodePipeline configuration to run actions for each Lambda function in parallel with the same runOrder, you allow multiple Lambda functions to be built and deployed simultaneously, which significantly improves the overall speed of the pipeline.\n\nRunOrder:\nThe runOrder parameter in CodePipeline allows you to specify the order in which actions run. If multiple actions have the same runOrder, they can run in parallel."
      },
      {
        "date": "2024-01-08T10:05:00.000Z",
        "voteCount": 1,
        "content": "Thanks for runOrder"
      },
      {
        "date": "2023-05-10T06:30:00.000Z",
        "voteCount": 7,
        "content": "Agree with C"
      },
      {
        "date": "2024-02-03T09:00:00.000Z",
        "voteCount": 2,
        "content": "C is correct: &lt;the pipeline takes too long to complete&gt; and &lt;consists of five independent AWS Lambda functions&gt; means we should run the lambda funcs in parallel by specifying the same runOrder\nA and D: no mention of running in parallel\nB: No mention of runOrder"
      },
      {
        "date": "2023-06-20T14:51:00.000Z",
        "voteCount": 4,
        "content": "Yeah, it's definitely C."
      },
      {
        "date": "2023-05-16T07:08:00.000Z",
        "voteCount": 3,
        "content": "Answer is C"
      },
      {
        "date": "2023-05-14T02:41:00.000Z",
        "voteCount": 3,
        "content": "C is right answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/amazon/view/109217-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS CloudFormation stacks to deploy updates to its application. The stacks consist of different resources. The resources include AWS Auto Scaling groups, Amazon EC2 instances, Application Load Balancers (ALBs), and other resources that are necessary to launch and maintain independent stacks. Changes to application resources outside of CloudFormation stack updates are not allowed.<br><br>The company recently attempted to update the application stack by using the AWS CLI. The stack failed to update and produced the following error message: \u201cERROR: both the deployment and the CloudFormation stack rollback failed. The deployment failed because the following resource(s) failed to update: [AutoScalingGroup].\u201d<br><br>The stack remains in a status of UPDATE_ROLLBACK_FAILED.<br><br>Which solution will resolve this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the subnet mappings that are configured for the ALBs. Run the aws cloudformation update-stack-set AWS CLI command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the IAM role by providing the necessary permissions to update the stack. Run the aws cloudformation continue-update-rollback AWS CLI command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubmit a request for a quota increase for the number of EC2 instances for the account. Run the aws cloudformation cancel-update-stack AWS CLI command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the Auto Scaling group resource. Run the aws cloudformation rollback-stack AWS CLI command."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-05T04:11:00.000Z",
        "voteCount": 11,
        "content": "https://repost.aws/knowledge-center/cloudformation-update-rollback-failed\nIf your stack is stuck in the UPDATE_ROLLBACK_FAILED state after a failed update, then the only actions that you can perform on the stack are the ContinueUpdateRollback or DeleteStack operations.\n\nSo only B has ContinueUpdateRollback"
      },
      {
        "date": "2024-06-06T13:29:00.000Z",
        "voteCount": 1,
        "content": "To update an AWS CloudFormation stack, you need an IAM role with permissions that allow you to perform the necessary actions on the resources defined in your CloudFormation template, as well as on the CloudFormation service itself. B is the answer"
      },
      {
        "date": "2024-02-03T09:08:00.000Z",
        "voteCount": 3,
        "content": "B is correct: &lt;UPDATE_ROLLBACK_FAILED&gt; means we are left with only two options: continue-update-rollback or delete-stack. We should provide necessary permissions to update the stack as well\nA, C and D: no mention of continue-update-rollback or adding necessary permissions"
      },
      {
        "date": "2023-09-22T12:55:00.000Z",
        "voteCount": 2,
        "content": "They should update the IAM role by providing the necessary permissions to update the stack and then run the aws cloudformation continue-update-rollback AWS CLI command"
      },
      {
        "date": "2023-07-05T04:06:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2023-05-16T07:11:00.000Z",
        "voteCount": 3,
        "content": "B is the answer I got"
      },
      {
        "date": "2023-05-14T02:48:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/amazon/view/109218-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is deploying a new application that uses Amazon EC2 instances. The company needs a solution to query application logs and AWS account API activity.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to Amazon S3. Use CloudWatch to query both sets of logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon CloudWatch Logs. Configure AWS CloudTrail to deliver the API logs to CloudWatch Logs. Use CloudWatch Logs Insights to query both sets of logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon Kinesis. Configure AWS CloudTrail to deliver the API logs to Kinesis. Use Kinesis to load the data into Amazon Redshift. Use Amazon Redshift to query both sets of logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to send logs from the EC2 instances to Amazon S3. Use AWS CloudTrail to deliver the API logs to Amazon S3. Use Amazon Athena to query both sets of logs in Amazon S3."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-16T01:27:00.000Z",
        "voteCount": 6,
        "content": "B is correct\nA - wrong because CloudWatch is not a query tool.\nC - Wrong because CloudWatch agent cant send logs directly to Kinesis.  Should be from CloudWatch log\nD - Wrong because CloudWatch agent cant send logs directly to S3. Should be from CloudWatch log to firehorse to S3"
      },
      {
        "date": "2024-02-03T09:15:00.000Z",
        "voteCount": 4,
        "content": "B is correct: &lt;query application logs and AWS account API activity&gt; means we need cloudwatch log and cloud trail\nC and D: cloudwatch agent cannot directly send logs to S3 or Kinesis. \nA: Cloudwatch query works only on cloudwatch, not S3"
      },
      {
        "date": "2023-07-26T10:28:00.000Z",
        "voteCount": 4,
        "content": "Explanation:\nOption B provides a comprehensive solution for querying application logs and AWS account API activity. The Amazon CloudWatch agent is used to send logs from the EC2 instances to Amazon CloudWatch Logs, allowing easy access to application logs. AWS CloudTrail is configured to deliver the API logs to CloudWatch Logs, enabling monitoring and analysis of AWS account activity. Finally, CloudWatch Logs Insights is utilized to query and analyze both sets of logs efficiently."
      },
      {
        "date": "2023-07-02T20:19:00.000Z",
        "voteCount": 4,
        "content": "Since Cloudwatch Insights can perform query, no need to use s3/athena."
      },
      {
        "date": "2023-05-16T07:13:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-05-14T02:51:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/amazon/view/109219-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company wants to ensure that their EC2 instances are secure. They want to be notified if any new vulnerabilities are discovered on their instances, and they also want an audit trail of all login activities on the instances.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Amazon Kinesis Agent to capture system logs and deliver them to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to detect vulnerabilities on the EC2 instances. Install the Systems Manager Agent to capture system logs and view login activity in the CloudTrail console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon CloudWatch to detect vulnerabilities on the EC2 instances. Install the AWS Config daemon to capture system logs and view them in the AWS Config console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon Inspector to detect vulnerabilities on the EC2 instances. Install the Amazon CloudWatch Agent to capture system logs and record them via Amazon CloudWatch Logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-02T20:20:00.000Z",
        "voteCount": 9,
        "content": "Aamzon Inspector detect software vulnerabilities and unintended network exposure in near real time in AWS workloads such as Amazon EC2, AWS Lambda functions, and Amazon ECR."
      },
      {
        "date": "2024-02-03T09:23:00.000Z",
        "voteCount": 5,
        "content": "D is correct: &lt;new vulnerabilities are discovered&gt; means AWS inspector\nA and B: AWS SSM does not support vulnerabilities scanning\nC: Amazon CloudWatch does not support vulnerabilities scanning"
      },
      {
        "date": "2024-09-06T04:42:00.000Z",
        "voteCount": 1,
        "content": "Exam tip : if you see \"vulnerabilities\" in the question -&gt; choose the answer covers \"Amazon Inspector\""
      },
      {
        "date": "2024-08-09T00:06:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2023-05-16T07:22:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-05-14T02:55:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/amazon/view/109220-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is running an application on Amazon EC2 instances in an Auto Scaling group. Recently, an issue occurred that prevented EC2 instances from launching successfully, and it took several hours for the support team to discover the issue. The support team wants to be notified by email whenever an EC2 instance does not start successfully.<br><br>Which action will accomplish this?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a health check to the Auto Scaling group to invoke an AWS Lambda function whenever an instance status is impaired.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Auto Scaling group to send a notification to an Amazon SNS topic whenever a failed instance launch occurs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm that invokes an AWS Lambda function when a failed AttachInstances Auto Scaling API call is made.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a status check alarm on Amazon EC2 to send a notification to an Amazon SNS topic whenever a status check fail occurs."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-20T15:07:00.000Z",
        "voteCount": 7,
        "content": "Likely B:\nhttps://aws.amazon.com/blogs/aws/auto-scaling-notifications-recurrence-and-more-control/\n\"EC2_INSTANCE_LAUNCH_ ERROR\""
      },
      {
        "date": "2024-02-03T09:26:00.000Z",
        "voteCount": 4,
        "content": "B is correct: &lt;wants to be notified by email&gt; means SNS. &lt;EC2 instances in an Auto Scaling group&gt; means this should be triggered by auto scaling group\nA, C and D: no mention of both SNS and auto scaling group"
      },
      {
        "date": "2023-06-16T01:00:00.000Z",
        "voteCount": 3,
        "content": "B is correct\nhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-sns-notifications.html"
      },
      {
        "date": "2023-05-16T07:24:00.000Z",
        "voteCount": 1,
        "content": "I got B as my answer"
      },
      {
        "date": "2023-05-14T15:31:00.000Z",
        "voteCount": 2,
        "content": "i think B is correct , but Option A is incorrect because,  this would only be triggered if an instance was already running and experiencing issues. It would not provide notification when an instance fails to launch."
      },
      {
        "date": "2023-05-14T02:58:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/amazon/view/108806-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is using AWS Organizations to centrally manage its AWS accounts. The company has turned on AWS Config in each member account by using AWS CloudFormation StackSets. The company has configured trusted access in Organizations for AWS Config and has configured a member account as a delegated administrator account for AWS Config.<br><br>A DevOps engineer needs to implement a new security policy. The policy must require all current and future AWS member accounts to use a common baseline of AWS Config rules that contain remediation actions that are managed from a central account. Non-administrator users who can access member accounts must not be able to modify this common baseline of AWS Config rules that are deployed into each member account.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation template that contains the AWS Config rules and remediation actions. Deploy the template from the Organizations management account by using CloudFormation StackSets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the Organizations management account by using CloudFormation StackSets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation template that contains the AWS Config rules and remediation actions. Deploy the template from the delegated administrator account by using AWS Config.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the delegated administrator account by using AWS Config.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-11T01:38:00.000Z",
        "voteCount": 8,
        "content": "Option D. Create an AWS Config conformance pack that contains the AWS Config rules and remediation actions. Deploy the pack from the delegated administrator account by using AWS Config.\n\nConformance packs are a collection of AWS Config rules and remediation actions that can be easily deployed as a single entity in an account and a region, and across an organization in AWS Organizations. These packs are created and managed from a central account, and help to establish a secure and compliant posture for your accounts. Non-administrator users can view the AWS Config rules within a conformance pack but they cannot modify them. AWS Config conformance packs are therefore a good fit for achieving the desired control and security policy.\n\nThe other options, while potentially viable for deploying Config rules, do not inherently protect the baseline AWS Config rules from being modified by non-administrator users in the member accounts."
      },
      {
        "date": "2023-05-09T06:25:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/config/latest/developerguide/conformance-packs.html"
      },
      {
        "date": "2024-05-07T08:39:00.000Z",
        "voteCount": 1,
        "content": "The question says\n'The policy must require all current and future AWS member accounts to use a common baseline of AWS Config rules'\nDoes D account for that?"
      },
      {
        "date": "2024-02-03T09:38:00.000Z",
        "voteCount": 4,
        "content": "D is correct: &lt;a common baseline of AWS Config rule&gt; means conformance pack. &lt;a member account as a delegated administrator account for AWS Config&gt; means delegated admin\nA and C: no mentionf of conformance pack\nB: should deploy this using AWS config and in the delegated account, not the management account"
      },
      {
        "date": "2023-07-05T10:19:00.000Z",
        "voteCount": 5,
        "content": "Not sure why some people are saying B.\nA= CFN cannot protect the config.\nB= Yes technically, where is the actual CONFIG management plane? Its in the delegated admin account, which is not the management account = delegated admin config account will have no idea of management account config.\nC= CFN cannot protect config.\nD= Yes. Delegated CONFIG account can config on orgz level &amp; protect the rules. Only logical option."
      },
      {
        "date": "2023-06-19T14:09:00.000Z",
        "voteCount": 1,
        "content": "D is correct. Deploying via Cloudformation StackSet cannot make sure that the aws config itself is not modified by the member accounts. Deploy aws organizational rule will achieve both permission restriction and auto deployment\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-account-deployment.html"
      },
      {
        "date": "2023-06-16T01:12:00.000Z",
        "voteCount": 3,
        "content": "D is correct\nhttps://aws.amazon.com/blogs/mt/deploying-conformance-packs-across-an-organization-with-automatic-remediation/"
      },
      {
        "date": "2023-05-29T07:02:00.000Z",
        "voteCount": 1,
        "content": "option B is the most appropriate solution for centrally managing and enforcing the common baseline of AWS Config rules across all member accounts while ensuring that non-administrator users cannot modify the rules."
      },
      {
        "date": "2023-05-28T01:22:00.000Z",
        "voteCount": 2,
        "content": "Can't you use D?"
      },
      {
        "date": "2023-05-14T15:38:00.000Z",
        "voteCount": 3,
        "content": "i think its B, because AWS Config conformance packs are a way to package AWS Config rules and remediation actions into a single, shareable entity. With AWS Organizations, you can use CloudFormation StackSets to deploy conformance packs across all member accounts in your organization. This allows you to centrally manage the deployment of AWS Config rules and remediation actions across multiple AWS accounts. By deploying the conformance pack from the Organizations management account, you can ensure that non-administrator users cannot modify the baseline rules deployed to each member account."
      },
      {
        "date": "2023-06-16T01:13:00.000Z",
        "voteCount": 1,
        "content": "No, you just need the manager account to deploy the comformance pack to all organization."
      },
      {
        "date": "2023-06-20T01:08:00.000Z",
        "voteCount": 1,
        "content": "It's B\nhttps://catalog.us-east-1.prod.workshops.aws/workshops/7bb9fd8f-d049-4163-98e3-5c0cbb211f0c/en-US/enable-custom-conformance-pack-using-stacksets"
      },
      {
        "date": "2024-01-06T14:20:00.000Z",
        "voteCount": 2,
        "content": "the question tells you there's a \"delegated account\". so your answer should be looking for that account in your answer choices as well."
      },
      {
        "date": "2023-05-14T03:03:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/amazon/view/108805-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer manages a large commercial website that runs on Amazon EC2. The website uses Amazon Kinesis Data Streams to collect and process web logs. The DevOps engineer manages the Kinesis consumer application, which also runs on Amazon EC2.<br><br>Sudden increases of data cause the Kinesis consumer application to fall behind, and the Kinesis data streams drop records before the records can be processed. The DevOps engineer must implement a solution to improve stream handling.<br><br>Which solution meets these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Kinesis consumer application to store the logs durably in Amazon S3. Use Amazon EMR to process the data directly on Amazon S3 to derive customer insights. Store the results in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHorizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis data streams.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the Kinesis consumer application to run as an AWS Lambda function. Configure the Kinesis data streams as the event source for the Lambda function to process the data streams.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards in the Kinesis data streams to increase the overall throughput so that the consumer application processes the data faster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-17T03:57:00.000Z",
        "voteCount": 39,
        "content": "The answer is B because:\nA few hours ago, I just finished the DOP-C02 exam.\nMy score is 1000 points.\nThis question has come up, I choose B."
      },
      {
        "date": "2023-09-20T03:40:00.000Z",
        "voteCount": 2,
        "content": "Were there any other questions from here in the exam?"
      },
      {
        "date": "2023-11-25T00:56:00.000Z",
        "voteCount": 1,
        "content": "First of all Congratulations.\nNow how do you know that this question was not from those 10 questions that do not count towards your score. and your answer to this question was Wrong but not counted towards your score. Just Saying!!\nPeace :)"
      },
      {
        "date": "2024-01-09T03:48:00.000Z",
        "voteCount": 2,
        "content": "B is the Answer, let him show off, it is ok."
      },
      {
        "date": "2023-11-25T01:03:00.000Z",
        "voteCount": 6,
        "content": "why not C?\nbecause we just replace ONE ec2 with ONE lambda here. And no mention of aws lambda reserved concurrency or provisioned concurrency.\nIn the question were are asked for 'MOST operational efficiency'. that's my two cents.\nCiao"
      },
      {
        "date": "2024-06-17T13:21:00.000Z",
        "voteCount": 2,
        "content": "I think B makes more sence, because \"the Kinesis data streams drop records before the records can be processed\".  It's not an intake throughput issue that needs more shards, but an outtake throughput issue.\n\"Consumer Record Processing Falling Behind\"\n\"After you identify how far behind your consumers are reading, look at the most common reasons why consumers fall behind.  Start with the GetRecords.IteratorAgeMilliseconds metric, which tracks the read position across all shards and consumers in the stream.  Note that if an iterator's age passes 50% of the retention period (by default, 24 hours...), there is risk for data loss due to record expiration.\"\n\"A quick stopgap solution is to increase the retention period.\"\n[...]\n\"An alternative approach is to increase your parallelism by increasing the number of shards.\"\n\"Finally, confirm you have an adequate amount of physical resources (memory, CPU utilization, etc.) on the underlying processing nodes during peak demand.\"\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html"
      },
      {
        "date": "2024-06-07T10:59:00.000Z",
        "voteCount": 1,
        "content": "B is a good choice\nThe GetRecords.IteratorAgeMilliseconds metric in Amazon CloudWatch for Amazon Kinesis Data Streams measures the age of the last record returned by the GetRecords operation. Specifically, it represents the time difference between the current time and the approximate arrival timestamp of the last record processed by a consumer in milliseconds.\n\nPurpose: Measures the delay in processing data records from the time they are added to the stream to the time they are processed by a consumer.\n\nScaling: If you observe high iterator age values, consider increasing the number of shards or enhancing the processing capacity of your consumers (e.g., adding more instances or increasing the processing power of existing instances)."
      },
      {
        "date": "2024-05-02T04:40:00.000Z",
        "voteCount": 1,
        "content": "I choose B"
      },
      {
        "date": "2024-04-18T10:44:00.000Z",
        "voteCount": 4,
        "content": "Consumer Record Processing Falling Behind\n\nFor most use cases, consumer applications are reading the latest data from the stream. In certain circumstances, consumer reads may fall behind, which may not be desired. After you identify how far behind your consumers are reading, look at the most common reasons why consumers fall behind.\n\nStart with the GetRecords.IteratorAgeMilliseconds metric, which tracks the read position across all shards and consumers in the stream. Note that if an iterator's age passes 50% of the retention period (by default, 24 hours, configurable up to 365 days), there is risk for data loss due to record expiration. A quick stopgap solution is to increase the retention period. This stops the loss of important data while you troubleshoot the issue further. https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#record-processing-falls-behind"
      },
      {
        "date": "2024-03-04T03:58:00.000Z",
        "voteCount": 2,
        "content": "B\n GetRecords.IteratorAgeMilliseconds metric : its for  track the progress of Kinese consumer, this cloud watch matric is use for the measuring  the difference between current time and when the last record of GetRecords calls written to the stream.  IteratorAgeMilliseconds metric is 0 means is processing fast enough and if its &gt;0 its slow in processing. Therefore, Horizontally scale the Kinesis consumer application by adding more EC2 instances based on the Amazon CloudWatch GetRecords.IteratorAgeMilliseconds metric. Increase the retention period of the Kinesis data streams."
      },
      {
        "date": "2024-02-16T05:47:00.000Z",
        "voteCount": 2,
        "content": "option C (Converting the Kinesis consumer application to run as an AWS Lambda function) is the most suitable solution. This approach automatically scales with the amount of incoming data, reduces the operational burden of managing EC2 instances, and leverages the serverless model to only incur costs for the actual compute time used for processing the data. This option provides a scalable, efficient, and cost-effective solution to the problem without the need for extensive infrastructure management."
      },
      {
        "date": "2024-02-03T09:46:00.000Z",
        "voteCount": 1,
        "content": "B is correct: &lt;consumer application to fall behind&gt; means we need to increase the power of the consumer. &lt;Kinesis data streams drop records&gt; means we should extend timeout. Only B match these requirements\nA: irrelevant\nC: Lambda is used only for short-lived tasks because its maximum execution time is 15 min. In this case, we need to process web logs. This is a time-consuming task, which is not suitable for Lambda\nD: No mention of increasing Consumer power"
      },
      {
        "date": "2023-12-03T17:08:00.000Z",
        "voteCount": 3,
        "content": "B &amp; D are correct. However, the question is looking for a solution for two issues\n\"application to fall behind, and the Kinesis data streams drop records before the records can be processed\"\nThen, B is the most appropriate solution"
      },
      {
        "date": "2023-11-05T06:41:00.000Z",
        "voteCount": 1,
        "content": "B is the answer: The data fall beind due to lack of pysical resources at consumer side. Icrease of more nodes will address this issue. https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html#record-processing-falls-behind"
      },
      {
        "date": "2023-09-30T05:44:00.000Z",
        "voteCount": 2,
        "content": "I would say B"
      },
      {
        "date": "2023-09-22T13:14:00.000Z",
        "voteCount": 1,
        "content": "D. Increase the number of shards in the Kinesis data streams to increase the overall throughput so that the consumer application processes the data faster.\n\nHere's the rationale for choosing this option:\n\nIncreasing Shards for Throughput:\nBy increasing the number of shards in the Kinesis data streams, you increase the overall throughput and the capacity to handle sudden increases in data. This directly addresses the issue of the consumer application falling behind during data spikes.\n\nOperational Efficiency:\nScaling the shards provides a more straightforward and efficient solution in terms of operation compared to modifying the consumer application, horizontally scaling instances, or converting the application to run as an AWS Lambda function."
      },
      {
        "date": "2023-08-31T02:37:00.000Z",
        "voteCount": 1,
        "content": "B is not correct. it manually scale EC2 instances.\nC is operationally efficiency ."
      },
      {
        "date": "2023-08-27T03:33:00.000Z",
        "voteCount": 2,
        "content": "The answer is B"
      },
      {
        "date": "2023-08-25T21:34:00.000Z",
        "voteCount": 2,
        "content": "B,  Scaling based on GetRecords.IteratorAgeMilliseconds is right. GetRecords.IteratorAgeMilliseconds - The age of the last record in all GetRecords calls made against a Kinesis stream, measured over the specified time period. Age is the difference between the current time and when the last record of the GetRecords call was written to the stream. The Minimum and Maximum statistics can be used to track the progress of Kinesis consumer applications. A value of zero indicates that the records being read are completely caught up with the stream."
      },
      {
        "date": "2023-08-01T21:02:00.000Z",
        "voteCount": 4,
        "content": "Lambda has some limitations like the execution time(15min), cocurrency limit."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/amazon/view/109130-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company recently created a new AWS Control Tower landing zone in a new organization in AWS Organizations. The landing zone must be able to demonstrate compliance with the Center for Internet Security (CIS) Benchmarks for AWS Foundations.<br><br>The company\u2019s security team wants to use AWS Security Hub to view compliance across all accounts. Only the security team can be allowed to view aggregated Security Hub findings. In addition, specific users must be able to view findings from their own accounts within the organization. All accounts must be enrolled in Security Hub after the accounts are created.<br><br>Which combination of steps will meet these requirements in the MOST automated way? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on trusted access for Security Hub in the organization\u2019s management account. Create a new security account by using AWS Control Tower. Configure the new security account as the delegated administrator account for Security Hub. In the new security account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on trusted access for Security Hub in the organization\u2019s management account. From the management account, provide Security Hub with the CIS Benchmarks for AWS Foundations standards.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS IAM Identity Center (AWS Single Sign-On) permission set that includes the required permissions. Use the CreateAccountAssignment API operation to associate the security team users with the permission set and with the delegated security account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that explicitly denies any user who is not on the security team from accessing Security Hub.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Security Hub, turn on automatic enablement.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization\u2019s management account, create an Amazon EventBridge rule that reacts to the CreateManagedAccount event. Create an AWS Lambda function that uses the Security Hub CreateMembers API operation to add new accounts to Security Hub. Configure the EventBridge rule to invoke the Lambda function."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "ADE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "ACF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-17T03:58:00.000Z",
        "voteCount": 17,
        "content": "The answer is ACE because:\nA few hours ago, I just finished the DOP-C02 exam.\nMy score is 1000 points.\nThis question has come up, I choose ACE."
      },
      {
        "date": "2023-09-09T00:55:00.000Z",
        "voteCount": 5,
        "content": "bot account, Pics or it didn't happen,"
      },
      {
        "date": "2024-06-17T13:51:00.000Z",
        "voteCount": 1,
        "content": "Either a bot or a bot for brains.  Same useless comments made on multiple questions."
      },
      {
        "date": "2024-07-24T18:00:00.000Z",
        "voteCount": 2,
        "content": "A - Only security team needs access to findings org wide - hence delegated account\nC - Allow security team members access to delegated account for Security hub using Identity center of control tower\nE - Each new account needs security hub for it's own users to access and also for aggregation  across org"
      },
      {
        "date": "2024-06-10T10:05:00.000Z",
        "voteCount": 2,
        "content": "Automatic enablement in AWS Security Hub refers to the feature that allows AWS Security Hub to be automatically enabled for new and existing AWS accounts that are part of an organization in AWS Organizations. This feature simplifies the process of onboarding multiple accounts into Security Hub, ensuring consistent security posture and compliance across the organization."
      },
      {
        "date": "2024-05-02T04:41:00.000Z",
        "voteCount": 2,
        "content": "ACE is correct"
      },
      {
        "date": "2024-04-16T02:31:00.000Z",
        "voteCount": 1,
        "content": "ACF\nE - ensures that all new accounts are automatically enrolled in Security Hub (same as F) but it does not address the requirement for specific users to view findings from their own accounts"
      },
      {
        "date": "2024-06-10T10:05:00.000Z",
        "voteCount": 1,
        "content": "I think C will take care of this. \"it does not address the requirement for specific users to view findings from their own accounts\""
      },
      {
        "date": "2024-04-16T02:30:00.000Z",
        "voteCount": 1,
        "content": "ACF\nE - ensures that all new accounts are automatically enrolled in Security Hub (same as F) but it does not address the requirement for specific users to view findings from their own accounts"
      },
      {
        "date": "2024-04-13T22:51:00.000Z",
        "voteCount": 2,
        "content": "ace are correct answer"
      },
      {
        "date": "2024-02-03T20:51:00.000Z",
        "voteCount": 2,
        "content": "ACE are correct: &lt;Only the security team can be allowed to view aggregated Security Hub findings&gt; means we need a delegated admin. &lt;All accounts must be enrolled in Security Hub after the accounts are created&gt; and &lt;in the MOST automated way&gt; means we need enable automatic enablement\nB: no mention of delegated admin\nD: This options denied access of the security team, which is irrelevant\nF: This option's result is the same as in option E, but more complicated"
      },
      {
        "date": "2023-11-05T06:27:00.000Z",
        "voteCount": 1,
        "content": "According to this article .. The Delegated account users have access in ANY account while the users under own account can view their own findings. So, there is no need to setup IAM policies for Security account users. https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-accounts-allowed-actions.html"
      },
      {
        "date": "2023-11-02T14:53:00.000Z",
        "voteCount": 3,
        "content": "A - Create delegate account for the security hub\nC - Give access to users to security using permissions sets\nE - Use auto enable so every new account will be monitored by security hub"
      },
      {
        "date": "2023-07-15T13:10:00.000Z",
        "voteCount": 2,
        "content": "ACE are the correct answers."
      },
      {
        "date": "2023-07-13T06:04:00.000Z",
        "voteCount": 2,
        "content": "ACE. Reason being, it is a landing zone and AWS SSO (IAM IC) is already part of the Control Tower product! Add security dept users as a SSO group and attach the security permission set to access security hub"
      },
      {
        "date": "2023-06-26T12:54:00.000Z",
        "voteCount": 3,
        "content": "with Control Tower comes the Identity Center implementation with default Identity Center directory."
      },
      {
        "date": "2023-05-25T09:32:00.000Z",
        "voteCount": 2,
        "content": "B is not the typical way AWS separates responsabilities in multi account (management, sec, audit)\nC is related with Active Directory \nE is more automated than F"
      },
      {
        "date": "2023-07-06T12:16:00.000Z",
        "voteCount": 1,
        "content": "Identity Center is not exclusively related to Active Directory\nAn SCP can only prevent access but doesnt enable any access, so D is not sufficient\nACE for me \nhttps://docs.aws.amazon.com/securityhub/latest/userguide/accounts-orgs-auto-enable.html"
      },
      {
        "date": "2023-05-23T14:54:00.000Z",
        "voteCount": 4,
        "content": "ACF IS MORE EFFICIENT"
      },
      {
        "date": "2023-05-13T01:36:00.000Z",
        "voteCount": 1,
        "content": "Ade for me"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/amazon/view/108803-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs applications in AWS accounts that are in an organization in AWS Organizations. The applications use Amazon EC2 instances and Amazon S3.<br><br>The company wants to detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity in its existing AWS accounts and in any AWS accounts that the company creates in the future. When the company detects one of these events, the company wants to use an existing Amazon Simple Notification Service (Amazon SNS) topic to send a notification to its operational support team for investigation and remediation.<br><br>Which solution will meet these requirements in accordance with AWS best practices?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization\u2019s management account, configure an AWS account as the Amazon GuardDuty administrator account. In the GuardDuty administrator account, add the company\u2019s existing AWS accounts to GuardDuty as members. In the GuardDuty administrator account, create an Amazon EventBridge rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization\u2019s management account, configure Amazon GuardDuty to add newly created AWS accounts by invitation and to send invitations to the existing AWS accounts. Create an AWS CloudFormation stack set that accepts the GuardDuty invitation and creates an Amazon EventBridge rule. Configure the rule with an event pattern to match GuardDuty events and to forward matching events to the SNS topic. Configure the CloudFormation stack set to deploy into all AWS accounts in the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization\u2019s management account, create an AWS CloudTrail organization trail. Activate the organization trail in all AWS accounts in the organization. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization\u2019s management account, configure an AWS account as the AWS CloudTrail administrator account. In the CloudTrail administrator account, create a CloudTrail organization trail. Add the company\u2019s existing AWS accounts to the organization trail. Create an SCP that enables VPC Flow Logs in each account in the organization. Configure AWS Security Hub for the organization. Create an Amazon EventBridge rule with an event pattern to match Security Hub events and to forward matching events to the SNS topic."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-12T11:24:00.000Z",
        "voteCount": 9,
        "content": "Dear Admin, Please Fix the Wrong response here! \nIt\u00b4s A:\nThis solution meets all the requirements:\n\nDetect potentially compromised EC2 instances, suspicious network activity, and unusual API activity: Amazon GuardDuty is a threat detection service that continuously monitors for malicious or unauthorized behavior. It analyzes events from AWS CloudTrail, Amazon VPC Flow Logs, and DNS logs to detect such activities.\n\nSend a notification to the operational support team: Creating an Amazon EventBridge rule that matches GuardDuty findings and then forwarding these to an SNS topic allows for the generation of notifications whenever suspicious activity is detected.\n\nCover future AWS accounts: By designating a GuardDuty administrator account in AWS Organizations, you can manage GuardDuty across all of your existing and future AWS accounts. This ensures that any new account created under the organization is automatically covered by GuardDuty."
      },
      {
        "date": "2024-08-05T08:22:00.000Z",
        "voteCount": 1,
        "content": "keywords: compromised EC2 instances, suspicious network activity, and unusual API activity\n= GuardDuty"
      },
      {
        "date": "2024-06-10T10:28:00.000Z",
        "voteCount": 2,
        "content": "When you use GuardDuty with an AWS organization, the management account of that organization can designate any account within the organization as the delegated GuardDuty administrator account. For this administrator account, GuardDuty gets enabled automatically only in the designated AWS Region. This account also has the permission to enable and manage GuardDuty for all of the accounts in the organization within that Region. The administrator account can view the members of and add members to this AWS organization.\n\nAWS GuardDuty can detect unusual API activity within existing AWS accounts in an AWS Organization. It monitors AWS CloudTrail event logs, which include records of all API calls made within your AWS environment. GuardDuty analyzes these logs to identify unusual or suspicious API activity that might indicate a potential security threat."
      },
      {
        "date": "2024-06-10T10:23:00.000Z",
        "voteCount": 1,
        "content": "A looks like a better choice.\nWhen you use GuardDuty with an AWS organization, the management account of that organization can designate any account within the organization as the delegated GuardDuty administrator account. For this administrator account, GuardDuty gets enabled automatically only in the designated AWS Region. This account also has the permission to enable and manage GuardDuty for all of the accounts in the organization within that Region. The administrator account can view the members of and add members to this AWS organization."
      },
      {
        "date": "2024-04-13T23:03:00.000Z",
        "voteCount": 2,
        "content": "answer A"
      },
      {
        "date": "2024-04-13T02:44:00.000Z",
        "voteCount": 1,
        "content": "If GuardDuty is indeed set up at the organization level (which is supported and encouraged by AWS for simplicity and coverage), then Option A becomes a very strong choice. It provides centralized management and automatic, seamless inclusion of all organization accounts in security monitoring without requiring manual intervention for each new account."
      },
      {
        "date": "2024-03-26T01:46:00.000Z",
        "voteCount": 1,
        "content": "Definitely B"
      },
      {
        "date": "2024-02-03T20:59:00.000Z",
        "voteCount": 3,
        "content": "A is correct: &lt;detect potentially compromised EC2 instances, suspicious network activity, and unusual API activity&gt; means AWS GuardDuty\nB: dont have to invite other accounts because all accounts are in an org in AWS org.\nC and D: no mention of GuardDuty"
      },
      {
        "date": "2024-01-12T06:46:00.000Z",
        "voteCount": 3,
        "content": "invitation is used to handle users OUTSIDE the organization."
      },
      {
        "date": "2024-01-08T12:44:00.000Z",
        "voteCount": 2,
        "content": "Go For A. \nhttps://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html"
      },
      {
        "date": "2023-11-12T03:12:00.000Z",
        "voteCount": 2,
        "content": "Member accounts must accept invite from the designated guard duty account  before its effect. I use AWS organisation at work and quite familiar with the workings. I lean towards B"
      },
      {
        "date": "2023-11-05T05:59:00.000Z",
        "voteCount": 1,
        "content": "It true it's missing auto enabled on. but Invitation is organization is not needed as Organization get precedence with account management when you have deligated Guardduty account. \"If you have already set up a GuardDuty administrator account with associated member accounts by invitation and the member accounts are part of the same organization, their Type changes from By Invitation to Via Organizations\" https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html"
      },
      {
        "date": "2023-08-15T07:21:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer.\nA could better if not for the fact that it doesn't handle automatic enablement on new AWS account.\nB handles this case with CloudFormation stacksets : https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-guardduty-master.html"
      },
      {
        "date": "2023-08-18T07:21:00.000Z",
        "voteCount": 2,
        "content": "A= does handle automatic enablement. If the GD delegated account is setup properly with automatic enablement check box ticked. As soon as the AWS account is created, GD auto enablement kicks into gear. B = How does CFN accept the GD invite? New AWS Account. CFN runs on new account &gt; accept GD invite...but when was this invite sent? I have to login to AWS Console &gt; GD &gt; create invite vs A = no invite = directly enabled for GB in new account. https://docs.aws.amazon.com/guardduty/latest/ug/guardduty_organizations.html"
      },
      {
        "date": "2023-08-13T10:38:00.000Z",
        "voteCount": 3,
        "content": "B is wrong. Newly created AWS accounts = you don't need to do this if the GD Orgz is configured properly, you can accept from delegated admin account. The point remains, you can add the accounts using option A. The misdirect here is that A does not state anything about new accounts vs B which does. Bearing in mind A + B still have to do something in GD, A is actually the better option.\nA is right."
      },
      {
        "date": "2023-08-07T12:16:00.000Z",
        "voteCount": 1,
        "content": "Option A is not the best choice because although it correctly configures GuardDuty as the administrator, it does not handle the automatic addition of new AWS accounts to GuardDuty and the forwarding of events to the SNS topic"
      },
      {
        "date": "2023-08-07T12:15:00.000Z",
        "voteCount": 1,
        "content": "Option B is the most suitable solution as it combines GuardDuty, AWS CloudFormation StackSets, and Amazon EventBridge to automatically monitor all existing and future AWS accounts and send notifications to the specified SNS topic when security events are detected."
      },
      {
        "date": "2023-08-02T07:14:00.000Z",
        "voteCount": 2,
        "content": "Cannot be A, it does not deal with future accounts at all."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/amazon/view/109224-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company\u2019s DevOps engineer is working in a multi-account environment. The company uses AWS Transit Gateway to route all outbound traffic through a network operations account. In the network operations account, all account traffic passes through a firewall appliance for inspection before the traffic goes to an internet gateway.<br><br>The firewall appliance sends logs to Amazon CloudWatch Logs and includes event severities of CRITICAL, HIGH, MEDIUM, LOW, and INFO. The security team wants to receive an alert if any CRITICAL events occur.<br><br>What should the DevOps engineer do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Synthetics canary to monitor the firewall state. If the firewall reaches a CRITICAL state or logs a CRITICAL event, use a CloudWatch alarm to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the security team\u2019s email address to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch metric filter by using a search for CRITICAL events. Publish a custom metric for the finding. Use a CloudWatch alarm based on the custom metric to publish a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the security team\u2019s email address to the topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon GuardDuty in the network operations account. Configure GuardDuty to monitor flow logs. Create an Amazon EventBridge event rule that is invoked by GuardDuty events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team\u2019s email address to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Firewall Manager to apply consistent policies across all accounts. Create an Amazon EventBridge event rule that is invoked by Firewall Manager events that are CRITICAL. Define an Amazon Simple Notification Service (Amazon SNS) topic as a target. Subscribe the security team\u2019s email address to the topic."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-14T08:50:00.000Z",
        "voteCount": 7,
        "content": "The logs from the firewall appliance are already being sent to Amazon CloudWatch Logs. So , The best approach to meet the given requirements is to create an Amazon CloudWatch metric filter by using a search for CRITICAL events"
      },
      {
        "date": "2024-05-02T04:42:00.000Z",
        "voteCount": 1,
        "content": "I think B"
      },
      {
        "date": "2024-04-13T23:05:00.000Z",
        "voteCount": 2,
        "content": "answer B"
      },
      {
        "date": "2024-02-03T21:02:00.000Z",
        "voteCount": 4,
        "content": "B is correct: &lt;firewall appliance sends logs to Amazon CloudWatch Logs&gt; means we already have the log in CW logs, only need to create alarm on these log files and send to sends\nA: No need to monitor the state of the firewall\nC and D: no mention of CloudWatch Logs"
      },
      {
        "date": "2023-07-09T06:27:00.000Z",
        "voteCount": 4,
        "content": "B. As the appliance pipes to CW Logs for consolidation. Define an alarm listening to the metric and should be okay.\n\nD is ONLY CORRECT IF YOU ARE USING AWS FIREWALL MANAGER."
      },
      {
        "date": "2023-05-16T07:36:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-05-14T03:52:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/amazon/view/109237-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is divided into teams. Each team has an AWS account, and all the accounts are in an organization in AWS Organizations. Each team must retain full administrative rights to its AWS account. Each team also must be allowed to access only AWS services that the company approves for use. AWS services must gain approval through a request and approval process.<br><br>How should a DevOps engineer configure the accounts to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation StackSets to provision IAM policies in each account to deny access to restricted AWS services. In each account, configure AWS Config rules that ensure that the policies are attached to IAM principals in the account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Control Tower to provision the accounts into OUs within the organization. Configure AWS Control Tower to enable AWS IAM Identity Center (AWS Single Sign-On). Configure IAM Identity Center to provide administrative access. Include deny policies on user roles for restricted AWS services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace all the accounts under a new top-level OU within the organization. Create an SCP that denies access to restricted AWS services. Attach the SCP to the OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that allows access to only approved AWS services. Attach the SCP to the root OU of the organization. Remove the FullAWSAccess SCP from the root OU of the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 27,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-26T10:19:00.000Z",
        "voteCount": 20,
        "content": "A=local account admin can change this.\nB=local admin has admin permissions. Complicated.\nC=implicit permit on everything else = breaks requirements.\nD= As they want to approve each service, its got to be white-list based SCP setup.\nAnswer is D."
      },
      {
        "date": "2024-10-13T08:05:00.000Z",
        "voteCount": 1,
        "content": "D is more straight forward"
      },
      {
        "date": "2024-08-25T08:38:00.000Z",
        "voteCount": 1,
        "content": "The answer is (D). The following SCP example from the AWS DOCUMENT allows accounts to create resource shares that share prefix lists\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_ram.html"
      },
      {
        "date": "2024-08-20T03:36:00.000Z",
        "voteCount": 1,
        "content": "Agree with D\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html"
      },
      {
        "date": "2024-08-01T23:28:00.000Z",
        "voteCount": 1,
        "content": "I prefer C than D.\nAs SCP more in Deny but not Allow\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
      },
      {
        "date": "2024-07-24T18:16:00.000Z",
        "voteCount": 2,
        "content": "SCP - only deny not allow - So answer is C"
      },
      {
        "date": "2024-06-19T18:36:00.000Z",
        "voteCount": 1,
        "content": "The question is looking to use the Allow List Strategy using SCP. So the answer that best fits is D."
      },
      {
        "date": "2024-06-10T13:13:00.000Z",
        "voteCount": 3,
        "content": "SCPs primary function is not grant permissions by themselves but restrict the permissions that IAM policies and other access control mechanisms can grant."
      },
      {
        "date": "2024-05-02T04:49:00.000Z",
        "voteCount": 2,
        "content": "D seems better."
      },
      {
        "date": "2024-04-13T23:26:00.000Z",
        "voteCount": 3,
        "content": "Ans D:\nIt is easier to allow approved services than deny all the other services, considering the vast amount of AWS services. it's easier to whitelist than blacklisting all the remaining services."
      },
      {
        "date": "2024-03-24T06:35:00.000Z",
        "voteCount": 2,
        "content": "Option C:\nPlace all the accounts under a new top-level OU within the organization: This allows for centralized management of the accounts.\nCreate an SCP that denies access to restricted AWS services: This ensures that only approved services are accessible. SCPs (Service Control Policies) are the best way to control permissions at the organizational level.\nAttach the SCP to the OU: By attaching the SCP to the OU, all accounts within the OU will inherit the restrictions set by the SCP.\nD is wrong: This option allows access only to approved AWS services by creating an SCP that allows access to only approved services and attaching it to the root OU of the organization. However, this would restrict all accounts, including those of other departments or teams within the organization. It doesn't meet the requirement of allowing each team to retain full administrative rights to its AWS account."
      },
      {
        "date": "2024-04-24T08:22:00.000Z",
        "voteCount": 1,
        "content": "I think Option C is wrong because the question says 'Each team also must be allowed to access only AWS services that the company approves for use'\nWhen you deny specific services they can still access services that have not been approved."
      },
      {
        "date": "2024-02-17T00:18:00.000Z",
        "voteCount": 3,
        "content": "Conclusion: Option C is the best solution to meet the requirements with operational efficiency and scalability. It allows teams to retain administrative rights while enforcing company-wide controls on service access through SCPs. This approach is straightforward to manage at scale, as adding or removing services from the SCP can adjust access permissions across all accounts within the OU. It directly aligns with the goal of allowing access only to approved AWS services and supports a governance model that can evolve with the organization's needs."
      },
      {
        "date": "2024-02-07T10:11:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
      },
      {
        "date": "2024-02-04T06:26:00.000Z",
        "voteCount": 4,
        "content": "C is correct: &lt;all the accounts are in an organization in AWS Organizations&gt; means we need scps\nA and B: no mention of scps\nD: SCP only denies access, not allow. Additionally, should not attack SCP to the root OU because this may inadvertently denies users' access to AWS services"
      },
      {
        "date": "2024-02-06T09:04:00.000Z",
        "voteCount": 2,
        "content": "correction: D: SCP has allow statement. D perfectly fits this question"
      },
      {
        "date": "2024-01-24T19:56:00.000Z",
        "voteCount": 2,
        "content": "C is correct; apart from SCP's only denying ... why would u want to add SCPs to the root org."
      },
      {
        "date": "2024-01-10T04:33:00.000Z",
        "voteCount": 2,
        "content": "D is wrong SCP can only deny, not approve. my answer is C"
      },
      {
        "date": "2023-12-16T02:53:00.000Z",
        "voteCount": 2,
        "content": "Option C is Correct \nOption D is wrong because AWS strongly recommends that you don't attach SCPs to the root of your organization without thoroughly testing the impact that the policy has on accounts. Instead, create an OU that you can move your accounts into one at a time, or at least in small numbers, to ensure that you don't inadvertently lock users out of key services.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/amazon/view/109225-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer used an AWS CloudFormation custom resource to set up AD Connector. The AWS Lambda function ran and created AD Connector, but CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLETE.<br><br>Which action should the engineer take to resolve this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the Lambda function code has exited successfully.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the Lambda function code returns a response to the pre-signed URL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the Lambda function IAM role has cloudformation:UpdateStack permissions for the stack ARN.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure the Lambda function IAM role has ds:ConnectDirectory permissions for the AWS account."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-29T09:25:00.000Z",
        "voteCount": 13,
        "content": "B. Ensure the Lambda function code returns a response to the pre-signed URL.\nExplanation:\nWhen using a custom resource in CloudFormation, the AWS Lambda function responsible for handling the resource creation should send a response to the pre-signed URL provided by CloudFormation. This response signals the completion status of the custom resource creation process to CloudFormation.\n\nIn this case, since the Lambda function successfully created the AD Connector, the engineer should ensure that the Lambda function code includes the logic to send a response to the pre-signed URL. This response should indicate the success status and any relevant data, such as the ARN or other details of the created AD Connector."
      },
      {
        "date": "2024-04-13T23:33:00.000Z",
        "voteCount": 1,
        "content": "its   B"
      },
      {
        "date": "2024-02-04T06:34:00.000Z",
        "voteCount": 2,
        "content": "B is correct: &lt;but CloudFormation is not transitioning from CREATE_IN_PROGRESS to CREATE_COMPLETE&gt; means ACF hasnot received a response code from Lambda\nA, C and D: no mention of response code"
      },
      {
        "date": "2023-10-27T04:13:00.000Z",
        "voteCount": 1,
        "content": "Lambda should send a cfnresponsse to presign url"
      },
      {
        "date": "2023-10-12T02:15:00.000Z",
        "voteCount": 3,
        "content": "B is correct \nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-custom-resources.html"
      },
      {
        "date": "2023-06-20T15:35:00.000Z",
        "voteCount": 2,
        "content": "It's B."
      },
      {
        "date": "2023-05-17T05:43:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-05-14T04:13:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/amazon/view/109226-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS CodeCommit for source code control. Developers apply their changes to various feature branches and create pull requests to move those changes to the main branch when the changes are ready for production.<br><br>The developers should not be able to push changes directly to the main branch. The company applied the AWSCodeCommitPowerUser managed policy to the developers\u2019 IAM role, and now these developers can push changes to the main branch directly on every repository in the AWS account.<br><br>What should the company do to restrict the developers\u2019 ability to push changes to the main branch directly?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an additional policy to include a Deny rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the main branch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the IAM policy, and add an AWSCodeCommitReadOnly managed policy. Add an Allow rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the IAM policy. Include a Deny rule for the GitPush and PutFile actions for the specific repositories in the policy statement with a condition that references the main branch.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an additional policy to include an Allow rule for the GitPush and PutFile actions. Include a restriction for the specific repositories in the policy statement with a condition that references the feature branches."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-12T11:40:00.000Z",
        "voteCount": 13,
        "content": "A is possible!\nIf you think C is correct, then you should know that a policy managed by AWS cannot be modified."
      },
      {
        "date": "2024-08-05T08:31:00.000Z",
        "voteCount": 1,
        "content": "Not C as AWS managed policy cannot be modified"
      },
      {
        "date": "2024-06-11T12:56:00.000Z",
        "voteCount": 2,
        "content": "AWS Managed Policies are read-only, meaning you cannot modify their contents. If you need a similar policy with slight modifications, you can copy the managed policy and create a customer-managed policy."
      },
      {
        "date": "2024-04-13T23:37:00.000Z",
        "voteCount": 1,
        "content": "it s A."
      },
      {
        "date": "2024-02-04T06:43:00.000Z",
        "voteCount": 3,
        "content": "A is correct: &lt;The developers should not be able to push changes directly to the main branch&gt; means we should deny these permissions in IAM policy. &lt;managed polic&gt; means we should add another policy, not modify this one. \nB: &lt;Remove the IAM policy&gt;: this is an managed policy, cannot remove it\nC: Cannot modify a managed policy. We can only create another policy\nD: This option would deny commiting code to every sub-branches, which is not correct"
      },
      {
        "date": "2023-12-26T05:48:00.000Z",
        "voteCount": 3,
        "content": "A, AWS managed policy cannot be modified, additional policy must be attached with a DENY"
      },
      {
        "date": "2023-07-05T05:07:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-06-16T07:42:00.000Z",
        "voteCount": 3,
        "content": "AWSCodeCommitPowerUser is an AWS-managed policy.\nSo you need to add an additional policy to deny push to the main branch directly."
      },
      {
        "date": "2023-05-23T14:57:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-05-22T00:02:00.000Z",
        "voteCount": 2,
        "content": "It`s A"
      },
      {
        "date": "2023-05-20T00:05:00.000Z",
        "voteCount": 2,
        "content": "C, why we need to create  an additional policy?"
      },
      {
        "date": "2023-05-24T02:51:00.000Z",
        "voteCount": 5,
        "content": "You can never modify a managed policy"
      },
      {
        "date": "2023-05-14T04:16:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/amazon/view/109227-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company manages a web application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances run in an Auto Scaling group across multiple Availability Zones. The application uses an Amazon RDS for MySQL DB instance to store the data. The company has configured Amazon Route 53 with an alias record that points to the ALB.<br><br>A new company guideline requires a geographically isolated disaster recovery (DR) site with an RTO of 4 hours and an RPO of 15 minutes.<br><br>Which DR strategy will meet these requirements with the LEAST change to the application stack?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a replica environment of everything except Amazon RDS in a different Availability Zone. Create an RDS read replica in the new Availability Zone, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new stack to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a latency routing policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a replica environment of everything except Amazon RDS in a different AWS Region. In the event of an outage, copy and restore the latest RDS snapshot from the primary Region to the DR Region. Adjust the Route 53 record set to point to the ALB in the DR Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a replica environment of everything except Amazon RDS in a different AWS Region. Create an RDS read replica in the new Region, and configure the new environment to point to the local RDS DB instance. Add the new stack to the Route 53 record set by using a health check to configure a failover routing policy. In the event of an outage, promote the read replica to primary.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-27T04:18:00.000Z",
        "voteCount": 5,
        "content": "D is correct. Failover policy will route traffic to the ALB in the backup region."
      },
      {
        "date": "2024-04-13T23:49:00.000Z",
        "voteCount": 2,
        "content": "geographically isolated location applies to option D"
      },
      {
        "date": "2024-03-31T02:46:00.000Z",
        "voteCount": 3,
        "content": "Answer is D.\n\nA. It did not cover the whole scenario. there is a need to promote the read replica otherwise the application in the region will not be able to write in that rds. Also, another Region means geographically isolated, while other Az will not solve the problem.\n\nDetails are everything during an investigation..."
      },
      {
        "date": "2024-03-31T02:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is D.\n\nA. It  did not cover the whole scenario. there is a need to promote the read replica otherwise the application in the region will not be able to write in that rds. Also, another Region means geographically isolated, while other Az will not solve the problem.\n\nDetails are everything during an investigation..."
      },
      {
        "date": "2024-02-19T04:08:00.000Z",
        "voteCount": 2,
        "content": "they mentioned geographically not regional , the cost will be more if we make it regional so we can go with the AZ DR"
      },
      {
        "date": "2024-03-31T02:44:00.000Z",
        "voteCount": 1,
        "content": "A did not cover the whole scenario. there is a need to promote the read replica otherwise the application in the region will not be able to write in that rds. Also, another Region means geographically isolated.\nDetails are everything during an investigation..."
      },
      {
        "date": "2024-02-04T06:51:00.000Z",
        "voteCount": 3,
        "content": "D is correct: &lt; configured Amazon Route 53&gt; and &lt;requires a geographically isolated disaster recovery (DR) site&gt; means fail-over routing and the DR site should be in another region\nA: &lt;Amazon RDS in a different Availability Zone&gt;: We need to setup the DB in a different region, not in a different AZ, which is still in the same region\nB and C: no mention of fail-over"
      },
      {
        "date": "2024-01-02T17:28:00.000Z",
        "voteCount": 2,
        "content": "D is the answer"
      },
      {
        "date": "2023-11-29T08:17:00.000Z",
        "voteCount": 3,
        "content": "What about C? I have an RTO of 4 hours I mean we got time we dont need a read replica we could take time to restore from a snapshot"
      },
      {
        "date": "2024-01-09T02:35:00.000Z",
        "voteCount": 2,
        "content": "Restore Time vs. RTO: While 4 hours might seem like a sufficient window for restoring a snapshot, the actual restore time can vary depending on several factors:\nSnapshot size: Larger snapshots take longer to restore.\nRDS instance type: High-performance instance types can handle restorations faster.\nNetwork bandwidth: Sufficient bandwidth is crucial for speedy data transfer during restoration.\nRDS engine version: Newer versions might have optimized restore processes.\nIn practice, restoring a large RDS snapshot, especially across Regions, could easily take more than 15 minutes, potentially exceeding the RPO and resulting in data loss."
      },
      {
        "date": "2023-06-29T09:49:00.000Z",
        "voteCount": 4,
        "content": "D is right"
      },
      {
        "date": "2023-05-23T14:57:00.000Z",
        "voteCount": 2,
        "content": "D is true"
      },
      {
        "date": "2023-05-14T04:20:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/amazon/view/109228-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A large enterprise is deploying a web application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The application stores data in an Amazon RDS for Oracle DB instance and Amazon DynamoDB. There are separate environments for development, testing, and production.<br><br>What is the MOST secure and flexible way to obtain password credentials during deployment?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetrieve an access key from an AWS Systems Manager SecureString parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the EC2 instances with an EC2 IAM role to access AWS services. Retrieve the database credentials from AWS Secrets Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetrieve an access key from an AWS Systems Manager plaintext parameter to access AWS services. Retrieve the database credentials from a Systems Manager SecureString parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch the EC2 instances with an EC2 IAM role to access AWS services. Store the database passwords in an encrypted config file with the application artifacts."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 25,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-28T22:39:00.000Z",
        "voteCount": 1,
        "content": "Keywords: MOST secure"
      },
      {
        "date": "2024-06-12T13:01:00.000Z",
        "voteCount": 2,
        "content": "This step is important for applications running on EC2 instances to retrieve passwords from  AWS Secrets Manager.\nCreate an IAM role with the necessary permissions to access AWS Secrets Manager.\nAttach this IAM role to your EC2 instance."
      },
      {
        "date": "2024-04-19T08:21:00.000Z",
        "voteCount": 3,
        "content": "The most secure and flexible way to obtain password credentials during deployment in the given scenario is to use AWS Secrets Manager. AWS Secrets Manager is a service that allows you to securely store, retrieve, and rotate credentials, such as passwords, API keys, and other sensitive data."
      },
      {
        "date": "2024-04-13T23:53:00.000Z",
        "voteCount": 2,
        "content": "B seems more relevant"
      },
      {
        "date": "2024-03-31T02:48:00.000Z",
        "voteCount": 2,
        "content": "B. EC2 Role + Secrets Mananger"
      },
      {
        "date": "2024-02-04T06:54:00.000Z",
        "voteCount": 4,
        "content": "B is correct: &lt;obtain password credentials&gt; means we should consider AWS SSM and secret manager. However, &lt;the MOST secure &gt; means we should opt for secret manager, which is more costly but more secure\nA, C and D: no mention of secret manager"
      },
      {
        "date": "2024-01-02T18:26:00.000Z",
        "voteCount": 1,
        "content": "why not A?"
      },
      {
        "date": "2024-02-04T06:55:00.000Z",
        "voteCount": 3,
        "content": "&lt;obtain password credentials&gt; means we should consider AWS SSM and secret manager. However, &lt;the MOST secure &gt; means we should opt for secret manager, which is more costly but more secure"
      },
      {
        "date": "2024-01-09T02:40:00.000Z",
        "voteCount": 1,
        "content": "We are not storing access keys for EC2 instances, instead we are using instance profile for that it is the best practice, and for database credentials it is correct to use Secret manager, it is more integrated with RDS, and other database services within AWS."
      },
      {
        "date": "2023-12-26T05:53:00.000Z",
        "voteCount": 2,
        "content": "I vote B"
      },
      {
        "date": "2023-07-16T16:06:00.000Z",
        "voteCount": 3,
        "content": "No Brainer"
      },
      {
        "date": "2023-06-29T09:56:00.000Z",
        "voteCount": 4,
        "content": "Most secure is B"
      },
      {
        "date": "2023-06-26T02:17:00.000Z",
        "voteCount": 2,
        "content": "Option B is the right answer."
      },
      {
        "date": "2023-05-14T04:22:00.000Z",
        "voteCount": 1,
        "content": "B sounds the right answer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/amazon/view/108802-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "The security team depends on AWS CloudTrail to detect sensitive security issues in the company\u2019s AWS account. The DevOps engineer needs a solution to auto-remediate CloudTrail being turned off in an AWS account.<br><br>What solution ensures the LEAST amount of downtime for the CloudTrail log deliveries?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule for the CloudTrail StopLogging event. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the AWS-managed CloudTrail-enabled AWS Config rule, set with a periodic interval of 1 hour. Create an Amazon EventBridge rule for AWS Config rules compliance change. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on the ARN of the resource in which StopLogging was called. Add the Lambda function ARN as a target to the EventBridge rule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule for a scheduled event every 5 minutes. Create an AWS Lambda function that uses the AWS SDK to call StartLogging on a CloudTrail trail in the AWS account. Add the Lambda function ARN as a target to the EventBridge rule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a t2.nano instance with a script running every 5 minutes that uses the AWS SDK to query CloudTrail in the current account. If the CloudTrail trail is disabled, have the script re-enable the trail."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-16T07:54:00.000Z",
        "voteCount": 14,
        "content": "A.\nold but gold link: \nhttps://aws.amazon.com/blogs/mt/monitor-changes-and-auto-enable-logging-in-aws-cloudtrail/"
      },
      {
        "date": "2023-09-18T18:17:00.000Z",
        "voteCount": 1,
        "content": "Good job, buddy"
      },
      {
        "date": "2024-04-19T08:36:00.000Z",
        "voteCount": 2,
        "content": "This solution ensures the least amount of downtime for CloudTrail log deliveries when auto-remediating CloudTrail being turned off. Here's why:\n\nEvent-Driven Automation: By creating an Amazon EventBridge rule for the CloudTrail StopLogging event, the remediation process is triggered immediately when CloudTrail logging is stopped, minimizing the downtime.\nTargeted Remediation: The Lambda function uses the AWS SDK to call StartLogging on the specific CloudTrail trail ARN where the StopLogging event occurred. This targeted approach ensures that logging is re-enabled for the affected trail without impacting other trails or introducing unnecessary overhead.\nLow Latency: EventBridge rules and Lambda functions are designed to be highly responsive, ensuring that the remediation action is initiated with minimal delay after the StopLogging event occurs."
      },
      {
        "date": "2024-03-31T02:53:00.000Z",
        "voteCount": 1,
        "content": "A. is correct\nDetails are everything during an investigation"
      },
      {
        "date": "2024-02-04T07:02:00.000Z",
        "voteCount": 2,
        "content": "A is correct: &lt;The DevOps engineer needs a solution to auto-remediate CloudTrail being turned off&gt; means we should turn on it again if we detect that it is turn-off. AWS config rule or Eventbridge would be considered. &lt; the LEAST amount of downtime&gt; means we should choose A because this minimizes downtime\nB: this option utilizes an AWS config rule, which is good. But it sets the rule with a periodic interval of 1 hours, which would introduce a lot of downtime\nC: this option utilizes evenbridge, but the event to trigger eventbridge is undetermined \nD: Should not use a custom script to do the task"
      },
      {
        "date": "2023-11-29T08:23:00.000Z",
        "voteCount": 2,
        "content": "A its quicker and the solution is asking for the leeast amount of downtime"
      },
      {
        "date": "2023-10-27T04:29:00.000Z",
        "voteCount": 1,
        "content": "\"LEAST amount of downtime\" = A\ncloudwatch event is near real time. Al the other options are not."
      },
      {
        "date": "2023-09-03T02:56:00.000Z",
        "voteCount": 1,
        "content": "Both A and B will work. However the question mentions leatst Cloutrial down time. Option A is correct  beacuse the remiation is triggred immeiately .\nOption B  can be delayed a it uns once in ever hour"
      },
      {
        "date": "2023-08-23T02:46:00.000Z",
        "voteCount": 1,
        "content": "I don't think stoplogging is CloudTrail being turned off.\nYou can stop logging anytime - https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-turning-off-logging.html\n\nBut it doesn't means CloudTrail being turned off"
      },
      {
        "date": "2023-08-16T01:39:00.000Z",
        "voteCount": 1,
        "content": "cloudtrail-enabled rule will check CloudTrail being turned off."
      },
      {
        "date": "2024-03-31T02:52:00.000Z",
        "voteCount": 2,
        "content": "Please notice that the question says  \"LEAST amount of downtime\" while B is possible it says \"set with a periodic interval of 1 hour.\" which can basically take 1h to enable CloudTrail again with causes a lot of downtime"
      },
      {
        "date": "2023-05-17T05:54:00.000Z",
        "voteCount": 2,
        "content": "I got A as my answer"
      },
      {
        "date": "2023-05-16T07:05:00.000Z",
        "voteCount": 2,
        "content": "The requirement is  - What solution ensures the LEAST amount of downtime for the CloudTrail log deliveries? For me that means reacting to AWS events. AWS config rule with 1 hr schedule does not meet the criteria in my opinion."
      },
      {
        "date": "2023-05-14T12:17:00.000Z",
        "voteCount": 3,
        "content": "Answer A, \nThis solution is the most appropriate as it listens to the StopLogging event and automatically starts logging immediately. This approach eliminates the need to wait for a scheduled interval, thereby reducing the amount of downtime and ensuring the security team can detect security issues in real-time.\n\nOption B is incorrect as it uses AWS Config rules to detect CloudTrail stoppage, which might not be an immediate solution to this issue"
      },
      {
        "date": "2023-05-14T04:26:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer"
      },
      {
        "date": "2023-05-11T23:50:00.000Z",
        "voteCount": 2,
        "content": "A is correct."
      },
      {
        "date": "2023-05-09T04:36:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/amazon/view/108749-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS CodeArtifact to centrally store Python packages. The CodeArtifact repository is configured with the following repository policy:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image7.png\"><br><br>A development team is building a new project in an account that is in an organization in AWS Organizations. The development team wants to use a Python library that has already been stored in the CodeArtifact repository in the organization. The development team uses AWS CodePipeline and AWS CodeBuild to build the new application. The CodeBuild job that the development team uses to build the application is configured to run in a VPC. Because of compliance requirements, the VPC has no internet connectivity.<br><br>The development team creates the VPC endpoints for CodeArtifact and updates the CodeBuild buildspec.yaml file. However, the development team cannot download the Python library from the repository.<br><br>Which combination of steps should a DevOps engineer take so that the development team can use CodeArtifact? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 gateway endpoint. Update the route tables for the subnets that are running the CodeBuild job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the repository policy\u2019s Principal statement to include the ARN of the role that the CodeBuild project uses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShare the CodeArtifact repository with the organization by using AWS Resource Access Manager (AWS RAM).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the role that the CodeBuild project uses so that the role has sufficient permissions to use the CodeArtifact repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify the account that hosts the repository as the delegated administrator for CodeArtifact in the organization."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 22,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-27T16:48:00.000Z",
        "voteCount": 13,
        "content": "I guess the answer is AD because of this:\n\"AWS CodeArtifact operates in multiple Availability Zones and stores artifact data and metadata in Amazon S3 and Amazon DynamoDB. Your encrypted data is redundantly stored across multiple facilities and multiple devices in each facility, making it highly available and highly durable.\"\nhttps://aws.amazon.com/codeartifact/features/\nWith no internet connectivity, a gateway endpoint becomes necessary to access S3."
      },
      {
        "date": "2023-06-04T00:41:00.000Z",
        "voteCount": 8,
        "content": "https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html\n\nIt clearly state that you need to create a S3 endpoint to use codeartifact in a private network."
      },
      {
        "date": "2024-02-07T10:38:00.000Z",
        "voteCount": 5,
        "content": "An Amazon S3 endpoint is not needed when using Python or Swift package formats."
      },
      {
        "date": "2024-07-10T18:55:00.000Z",
        "voteCount": 2,
        "content": "When this question was created, there was no exception for Python and Swift packages. You can check this using the Wayback machine: https://web.archive.org/web/20230521063821/https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html\n\nConsidering that it's very common to have outdated questions in the exam, I'd say this is one those cases. So yeah, I'll also go with AD (also because B is not needed since the repository policy is already allowing the entire org)."
      },
      {
        "date": "2023-09-14T03:08:00.000Z",
        "voteCount": 2,
        "content": "A- incorrect because the question says Devops engineers careted VPC endpoints for CodeArtifact"
      },
      {
        "date": "2023-09-19T17:31:00.000Z",
        "voteCount": 2,
        "content": "AD even though Devops engineer created a CodeArtifcat still a S3 end point is required"
      },
      {
        "date": "2024-06-18T10:56:00.000Z",
        "voteCount": 1,
        "content": "note here says \"An Amazon S3 endpoint is not needed when using Python or Swift package formats.\" \nhttps://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html"
      },
      {
        "date": "2023-05-08T07:43:00.000Z",
        "voteCount": 5,
        "content": "Codeartifact uses s3 gateway endpoints to store packages. The key word here are no internet access."
      },
      {
        "date": "2024-08-20T03:41:00.000Z",
        "voteCount": 1,
        "content": "AD for me"
      },
      {
        "date": "2024-08-07T01:41:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html"
      },
      {
        "date": "2024-07-24T18:45:00.000Z",
        "voteCount": 2,
        "content": "B - doesn't make any sense because aws:PrincipalOrgID condition key in repo policy already allows any principal within the org to access the repo"
      },
      {
        "date": "2024-06-18T11:03:00.000Z",
        "voteCount": 1,
        "content": "BD\nnote here says \"An Amazon S3 endpoint is not needed when using Python or Swift package formats.\"\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html"
      },
      {
        "date": "2024-06-13T09:13:00.000Z",
        "voteCount": 2,
        "content": "C is needed\nif the codeartifact and codebuild are in different organization accounts, AWS RAM is a service that allows you to share AWS resources with other AWS accounts within your organization. AWS RAM can be used to share CodeArtifact resources across different accounts.\n\nA is not needed\nyou do not need an S3 gateway as a VPC endpoint specifically for using AWS CodeArtifact with Python packages. AWS CodeArtifact itself manages the storage and retrieval of packages, and it uses its own service endpoints for these operations.\n\nD is needed for\nEnsure the IAM role used by CodeBuild has permissions to access CodeArtifact\n\nB is not needed \nHere it is not required because the CodeArtifact policy has Principal as *"
      },
      {
        "date": "2024-05-13T10:38:00.000Z",
        "voteCount": 1,
        "content": "C and D\n\nA - S3 gateway endpoint is not required for Python: https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html\n\nB - Principal is already \"*\"."
      },
      {
        "date": "2024-05-24T18:22:00.000Z",
        "voteCount": 1,
        "content": "Pls Read link https://docs.aws.amazon.com/ram/latest/userguide/shareable.html"
      },
      {
        "date": "2024-05-02T05:01:00.000Z",
        "voteCount": 2,
        "content": "AD because Principal is already \"*\"."
      },
      {
        "date": "2024-04-26T09:03:00.000Z",
        "voteCount": 2,
        "content": "as for A: \"To pull packages from CodeArtifact, you must create a gateway endpoint for Amazon S3.\" but... \"Note - An Amazon S3 endpoint is not needed when using Python or Swift package formats.\"\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html"
      },
      {
        "date": "2024-04-19T09:15:00.000Z",
        "voteCount": 4,
        "content": "The issue here is policy update as the developers have already enabled VPC endpoint (CodeArtifact uses Amazon Simple Storage Service (Amazon S3) to store package assets. To pull packages from CodeArtifact, you must create a gateway endpoint for Amazon S3. When your build or deployment process downloads packages from CodeArtifact, it must access CodeArtifact to get package metadata and Amazon S3 to download package assets (for example, Maven .jar files).\n\nNote\nAn Amazon S3 endpoint is not needed when using Python or Swift package formats.\n\nTo create the Amazon S3 gateway endpoint for CodeArtifact, use the Amazon EC2 create-vpc-endpoint AWS CLI command. When you create the endpoint, you must select the route tables for your VPC. For more information, see Gateway VPC Endpoints in the Amazon Virtual Private Cloud User Guide.)"
      },
      {
        "date": "2024-04-19T09:17:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html"
      },
      {
        "date": "2024-04-14T00:09:00.000Z",
        "voteCount": 4,
        "content": "ANS B&amp;D\nCodeArtifact uses Amazon Simple Storage Service (Amazon S3) to store package assets. To pull packages from CodeArtifact, you must create a gateway endpoint for Amazon S3. When your build or deployment process downloads packages from CodeArtifact, it must access CodeArtifact to get package metadata and Amazon S3 to download package assets (for example, Maven .jar files).\nNote\nAn Amazon S3 endpoint is not needed when using Python or Swift package formats."
      },
      {
        "date": "2024-03-31T02:57:00.000Z",
        "voteCount": 3,
        "content": "A,D are correct"
      },
      {
        "date": "2024-02-20T18:04:00.000Z",
        "voteCount": 3,
        "content": "'ad' correct  = 'AWS codeartiface' operates in multiple availability zones and stores artiface data and metadata in amazon s3 and amazon dynamoDB your encrypted data is redundanly stored across myltiple facilities and multiple devices in each facility, marking it highly availiable and highly durable..."
      },
      {
        "date": "2024-02-07T10:40:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html\nAn Amazon S3 endpoint is not needed when using Python or Swift package formats."
      },
      {
        "date": "2024-02-04T07:22:00.000Z",
        "voteCount": 3,
        "content": "C and D:&lt;the development team cannot download the Python library from the repository.&gt; indicates insufficient permission or network problem\nA: irrelevant\nB: principal is already * for everyone, including the ARN of the codebuild role\nE: irrelevant"
      },
      {
        "date": "2023-10-14T19:52:00.000Z",
        "voteCount": 2,
        "content": "AD\n\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/create-s3-gateway-endpoint.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/amazon/view/108801-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications. These templates must be deployed in a specific order. The company is making more changes to the templates than previously expected and wants to deploy new templates more efficiently. Additionally, the data engineering team must be notified of all changes to the templates.<br><br>What should the company do to accomplish these goals?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to deploy the CloudFormation templates in the required order. Use stack policies to alert the data engineering team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost the CloudFormation templates in Amazon S3. Use Amazon S3 events to directly trigger CloudFormation updates and Amazon SNS notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement CloudFormation StackSets and use drift detection to trigger update alerts to the data engineering team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage CloudFormation nested stacks and stack sets for deployments. Use Amazon SNS to notify the data engineering team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-17T04:03:00.000Z",
        "voteCount": 13,
        "content": "The answer is D because:\nA few hours ago, I just finished the DOP-C02 exam.\nMy score is 1000 points.\nThis question has come up, I choose D."
      },
      {
        "date": "2023-09-09T03:08:00.000Z",
        "voteCount": 8,
        "content": "pics or it did not happen, troll bot account"
      },
      {
        "date": "2024-04-19T09:33:00.000Z",
        "voteCount": 4,
        "content": "Here's why this solution is the best approach:\n\nNested Stacks: CloudFormation nested stacks allow you to break down complex templates into smaller, more manageable templates. You can create a root stack that references and manages multiple nested stacks. This approach simplifies the management and deployment of multiple interdependent templates in the correct order.\nStackSets: CloudFormation StackSets allow you to create, update, or delete stacks across multiple AWS accounts and regions with a single operation. This addresses the requirement of deploying applications across multiple regions efficiently.\nAmazon SNS: Amazon Simple Notification Service (SNS) can be used to send notifications to the data engineering team whenever changes are made to the CloudFormation templates or stacks."
      },
      {
        "date": "2024-04-14T00:15:00.000Z",
        "voteCount": 1,
        "content": "should be D"
      },
      {
        "date": "2024-03-31T02:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is D."
      },
      {
        "date": "2024-02-19T04:50:00.000Z",
        "voteCount": 2,
        "content": "the nested for order :\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-nested-stacks.html"
      },
      {
        "date": "2024-02-04T07:26:00.000Z",
        "voteCount": 2,
        "content": "D is correct: &lt;uses a series of individual Amazon CloudFormation templates to deploy its multi-Region applications&gt; and &lt;wants to deploy new templates more efficiently&gt; mean stacksets, which is template for multiple regions. &lt;the data engineering team must be notified of all changes&gt; means SNS\nA and B: no mention of stacksets\nC: no mention of SNS"
      },
      {
        "date": "2024-01-02T18:45:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2023-10-27T04:41:00.000Z",
        "voteCount": 1,
        "content": "It's D.\nC is not correct since according to this link:\nhttps://aws.amazon.com/blogs/mt/implementing-an-alarm-to-automatically-detect-drift-in-aws-cloudformation-stacks/\nWe need AWS config rule to detect drifts and to send event. There is no build in solution to notify drift detection like mentioned in C,"
      },
      {
        "date": "2023-09-18T18:24:00.000Z",
        "voteCount": 1,
        "content": "C is for notifying changes on what has been deployed by Cloud Formation\nD is for notifying changes made on the Cloud Formation template (the recipe) itself"
      },
      {
        "date": "2023-09-10T17:39:00.000Z",
        "voteCount": 1,
        "content": "C didn't mention how to deploy in a specific order.\nD mentioned nested stack, which you can configure the dependency ordering"
      },
      {
        "date": "2023-05-17T05:55:00.000Z",
        "voteCount": 1,
        "content": "I got D as my answer"
      },
      {
        "date": "2023-05-14T18:30:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-05-12T21:39:00.000Z",
        "voteCount": 1,
        "content": "it' s D, I think."
      },
      {
        "date": "2023-05-12T00:04:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-05-10T12:05:00.000Z",
        "voteCount": 1,
        "content": "The correct solution is D:\n\nThe solution works as follows:\n\nAWS Config triggers the evaluation when any resource that matches the rule\u2019s scope (currently set to \u201cAWS::CloudFormation::Stack\u201d) changes in configuration and at the frequency (\u201cMaximumExecutionFrequency\u201d parameter) that you specify at the time of this solution deployment.\nEventBridge receives the events from AWS Config, applies the EventBridge rule to match the compliance change event, and transforms the input (customize the text) as defined in the \u201cInputTransformer\u201d template.\nThe chosen customer-managed KMS Key is then accessed to encrypt the notification.\nThe encrypted notification is published to the target SNS topic.\n The endpoints subscribed to this topic start receiving the published messages."
      },
      {
        "date": "2023-05-09T04:26:00.000Z",
        "voteCount": 1,
        "content": "I think C: https://aws.amazon.com/blogs/mt/implementing-an-alarm-to-automatically-detect-drift-in-aws-cloudformation-stacks/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/amazon/view/109044-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer has implemented a CI/CD pipeline to deploy an AWS CloudFormation template that provisions a web application. The web application consists of an Application Load Balancer (ALB), a target group, a launch template that uses an Amazon Linux 2 AMI, an Auto Scaling group of Amazon EC2 instances, a security group, and an Amazon RDS for MySQL database. The launch template includes user data that specifies a script to install and start the application.<br><br>The initial deployment of the application was successful. The DevOps engineer made changes to update the version of the application with the user data. The CI/CD pipeline has deployed a new version of the template. However, the health checks on the ALB are now failing. The health checks have marked all targets as unhealthy.<br><br>During investigation, the DevOps engineer notices that the CloudFormation stack has a status of UPDATE_COMPLETE. However, when the DevOps engineer connects to one of the EC2 instances and checks /var/log/messages, the DevOps engineer notices that the Apache web server failed to start successfully because of a configuration error.<br><br>How can the DevOps engineer ensure that the CloudFormation deployment will fail if the user data fails to successfully finish running?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the cfn-signal helper script to signal success or failure to CloudFormation. Use the WaitOnResourceSignals update policy within the CloudFormation template. Set an appropriate timeout for the update policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm for the UnhealthyHostCount metric. Include an appropriate alarm threshold for the target group. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a lifecycle hook on the Auto Scaling group by using the AWS::AutoScaling::LifecycleHook resource. Create an Amazon Simple Notification Service (Amazon SNS) topic as the target to signal success or failure to CloudFormation. Set an appropriate timeout on the lifecycle hook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to stream the cloud-init logs. Create a subscription filter that includes an AWS Lambda function with an appropriate invocation timeout. Configure the Lambda function to use the SignalResource API operation to signal success or failure to CloudFormation."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-19T07:22:00.000Z",
        "voteCount": 6,
        "content": "A is correct"
      },
      {
        "date": "2024-06-13T13:09:00.000Z",
        "voteCount": 3,
        "content": "To ensure that the CloudFormation deployment fails if the user data script does not successfully finish, you can use a combination of AWS CloudFormation's CreationPolicy, cfn-signal, and wait condition resources. These mechanisms can signal CloudFormation about the success or failure of the instance creation process, including the execution of user data scripts."
      },
      {
        "date": "2024-04-14T00:21:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-02-04T07:34:00.000Z",
        "voteCount": 3,
        "content": "A is correct: &lt;ensure that the CloudFormation deployment will fail if the user data fails to successfully finish running&gt; means we need cfn-signal\nB, C and D: no mention of cfn-signal"
      },
      {
        "date": "2024-01-02T18:48:00.000Z",
        "voteCount": 3,
        "content": "yes A.\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-signal.html"
      },
      {
        "date": "2023-12-06T07:27:00.000Z",
        "voteCount": 1,
        "content": "The instance is running, and the logs are available. The configuration happens inside the instance by the userdata. How A is correct if the issue is beyond CF?"
      },
      {
        "date": "2023-12-06T07:29:00.000Z",
        "voteCount": 2,
        "content": "Ok now make sense: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-signal.html"
      },
      {
        "date": "2023-05-17T05:57:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-05-14T18:34:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer"
      },
      {
        "date": "2023-05-12T00:14:00.000Z",
        "voteCount": 2,
        "content": "Agree with A"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/amazon/view/109191-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has a data ingestion application that runs across multiple AWS accounts. The accounts are in an organization in AWS Organizations. The company needs to monitor the application and consolidate access to the application. Currently, the company is running the application on Amazon EC2 instances from several Auto Scaling groups. The EC2 instances have no access to the internet because the data is sensitive. Engineers have deployed the necessary VPC endpoints. The EC2 instances run a custom AMI that is built specifically for the application.<br><br>To maintain and troubleshoot the application, system administrators need the ability to log in to the EC2 instances. This access must be automated and controlled centrally. The company\u2019s security team must receive a notification whenever the instances are accessed.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule to send notifications to the security team whenever a user logs in to an EC2 instance. Use EC2 Instance Connect to log in to the instances. Deploy Auto Scaling groups by using AWS CloudFormation. Use the cfn-init helper script to deploy appropriate VPC routes for external access. Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a NAT gateway and a bastion host that has internet access. Create a security group that allows incoming traffic on all the EC2 instances from the bastion host. Install AWS Systems Manager Agent on all the EC2 instances. Use Auto Scaling group lifecycle hooks for monitoring and auditing access. Use Systems Manager Session Manager to log in to the instances. Send logs to a log group in Amazon CloudWatch Logs. Export data to Amazon S3 for auditing. Send notifications to the security team by using S3 event notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse EC2 Image Builder to rebuild the custom AMI. Include the most recent version of AWS Systems Manager Agent in the image. Configure the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to all the EC2 instances. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Automation to build Systems Manager Agent into the custom AMI. Configure AWS Config to attach an SCP to the root organization account to allow the EC2 instances to connect to Systems Manager. Use Systems Manager Session Manager to log in to the instances. Enable logging of session details to Amazon S3. Create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-05T05:45:00.000Z",
        "voteCount": 7,
        "content": "C and D are left over choice due to no internet access for EC2\n\nC is correct \nBy using EC2 Image Builder to rebuild the custom AMI and including the most recent version of AWS Systems Manager Agent in the image, you can configure the Auto Scaling group to attach the AmazonSSMManagedInstanceCore role to all the EC2 instances. This allows you to use Systems Manager Session Manager to log in to the instances. You can enable logging of session details to Amazon S3 and create an S3 event notification for new file uploads to send a message to the security team through an Amazon Simple Notification Service (Amazon SNS) topic2"
      },
      {
        "date": "2024-02-04T07:49:00.000Z",
        "voteCount": 6,
        "content": "C is correct: &lt;The company needs to monitor the application and consolidate access to the application&gt; means using SSM. We should install SSM agent on all EC2 instances. &lt;The EC2 instances run a custom AMI that is built specifically for the application&gt; means we should rebuild the image and integrate agent into the AMI. To rebuild, the best option is EC2 image builder. &lt;The company\u2019s security team must receive a notification whenever the instances are accessed.&gt; means SNS \nA: &lt;Rebuild the custom AMI so that the custom AMI includes AWS Systems Manager Agent.&gt;: no mention of using EC2 image builder and SNS\nB: no mention of integrating SSM agents into the AMI and we cannot just send S3 noti to users &lt;Send notifications to the security team by using S3 event notifications.&gt;\nD: no me ntion of using EC2 image builder to rebuild the AMI."
      },
      {
        "date": "2024-08-02T00:16:00.000Z",
        "voteCount": 1,
        "content": "- AWS Systems Manager Agent\n- Systems Manager Session Manager for login the instances\n- enable logging of session details to s3\n- s3 event notification to SNS."
      },
      {
        "date": "2024-04-14T01:17:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-07-26T19:02:00.000Z",
        "voteCount": 2,
        "content": "C\n\nOption C offers a well-architected approach to addressing the requirements, providing both centralized access and logging, and automated login to EC2 instances for system administrators. Additionally, it ensures that the security team receives notifications for auditing and monitoring purposes."
      },
      {
        "date": "2023-05-20T00:22:00.000Z",
        "voteCount": 3,
        "content": "D is not a good option for the following reasons:\n\n1. AWS Systems Manager Automation is not the ideal choice for building a custom AMI. Instead, EC2 Image Builder, as stated in option C, is an AWS service designed for building, testing, and maintaining Golden Amazon Machine Images (AMIs), making it a suitable choice for both building and managing custom AMIs.\n \n2. The option D suggests attaching an SCP (Service Control Policy) to the root organization to allow EC2 instances to connect to Systems Manager. This approach is incorrect because SCPs are used to define permissions on an organizational level, rather than allowing specific access between resources like EC2 instances and Systems Manager. Attaching the AmazonSSMManagedInstanceCore role to EC2 instances as mentioned in option C is the correct method, which allows instances to communicate with Systems Manager."
      },
      {
        "date": "2023-05-13T18:07:00.000Z",
        "voteCount": 1,
        "content": "if someone know why D is not correct , pls post"
      },
      {
        "date": "2023-06-20T15:56:00.000Z",
        "voteCount": 2,
        "content": "Because I don't think AWS Config can be used to attach an SCP."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/amazon/view/109533-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses Amazon S3 to store proprietary information. The development team creates buckets for new projects on a daily basis. The security team wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled. Additionally, no buckets should ever be publicly read or write accessible.<br><br>What should a DevOps engineer do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS CloudTrail and configure automatic remediation using AWS Lambda.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Config rules and configure automatic remediation using AWS Systems Manager documents.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Trusted Advisor and configure automatic remediation using Amazon EventBridge.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Systems Manager and configure automatic remediation using Systems Manager documents."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-14T01:19:00.000Z",
        "voteCount": 2,
        "content": "AWS config to remediate non-compliace"
      },
      {
        "date": "2024-02-04T20:45:00.000Z",
        "voteCount": 4,
        "content": "B is correct: &lt;wants to ensure that all existing and future buckets have encryption, logging, and versioning enabled&gt; means we need aws config. \nA, C and D: no mention of AWS config"
      },
      {
        "date": "2024-01-10T04:59:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2024-01-02T18:56:00.000Z",
        "voteCount": 1,
        "content": "yes B is correct"
      },
      {
        "date": "2023-06-04T01:40:00.000Z",
        "voteCount": 4,
        "content": "AWS Config allows you to remediate noncompliant resources that are evaluated by AWS Config Rules. AWS Config applies remediation using AWS Systems Manager Automation documents.\n\nsee https://docs.aws.amazon.com/config/latest/developerguide/remediation.html"
      },
      {
        "date": "2023-05-17T06:13:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/amazon/view/108799-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is researching the least expensive way to implement an image batch processing cluster on AWS. The application cannot run in Docker containers and must run on Amazon EC2. The batch job stores checkpoint data on an NFS volume and can tolerate interruptions. Configuring the cluster software from a generic EC2 Linux image takes 30 minutes.<br><br>What is the MOST cost-effective solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EFS for checkpoint data. To complete the job, use an EC2 Auto Scaling group and an On-Demand pricing model to provision EC2 instances temporarily.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse GlusterFS on EC2 instances for checkpoint data. To run the batch job, configure EC2 instances manually. When the job completes, shut down the instances manually.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances, and utilize user data to configure the EC2 Linux instance on startup.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EFS for checkpoint data. Use EC2 Fleet to launch EC2 Spot Instances. Create a custom AMI for the cluster and use the latest AMI when creating instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-12T01:33:00.000Z",
        "voteCount": 7,
        "content": "D is more suitable, as it says to avoid 30min launch time."
      },
      {
        "date": "2023-05-16T07:39:00.000Z",
        "voteCount": 2,
        "content": "I assume you are saying D over C as D will make the EC2 instances operational quicker, while C would require 30 minutes to install the software before it can start to be used. resulting in it being more cost effective."
      },
      {
        "date": "2024-07-28T23:08:00.000Z",
        "voteCount": 2,
        "content": "keywords: MOST cost-effective,  a generic EC2 Linux image takes 30 minutes.\nMean take 30mins or longer time for EC2 booting and will cost more."
      },
      {
        "date": "2024-03-31T04:52:00.000Z",
        "voteCount": 3,
        "content": "D is the correct answer.\nMake the calculations 30 min of bootstraping when you have multiple scale actions is a lot of time idle when you have many instances, so the money spent during a lot of spot request that wast time boostraping is larger than keeping a single AMI.\n\nDetails are everything during an investigation..."
      },
      {
        "date": "2024-02-21T14:32:00.000Z",
        "voteCount": 2,
        "content": "Answer is C.\nC is cheaper than D"
      },
      {
        "date": "2024-04-07T05:17:00.000Z",
        "voteCount": 3,
        "content": "\"utilize user data to configure the EC2 Linux instance on startup\" - it takes an addiction time to configure an instance.\nit's better to use a custom AMI and have everything preinstalled"
      },
      {
        "date": "2024-02-04T20:51:00.000Z",
        "voteCount": 3,
        "content": "C is correct: &lt;can tolerate interruptions&gt; means EC2 spot instances. \nA and B: no mention of spot instances\nD: Create a custom AMI for the cluster and use the latest AMI when creating instances: this incurs more cost than option C, which incurs no cost for the configuration step"
      },
      {
        "date": "2024-03-17T07:48:00.000Z",
        "voteCount": 3,
        "content": "As for me, extra 30 minutes for each EC2 launch seems like an extra cost comparing to one-time built AMI.\nSo D looks cheaper than C"
      },
      {
        "date": "2023-11-16T11:02:00.000Z",
        "voteCount": 2,
        "content": "It's D"
      },
      {
        "date": "2023-10-22T00:26:00.000Z",
        "voteCount": 2,
        "content": "D most suitable. why would you be willing to wait for 30 minutes for software installation?"
      },
      {
        "date": "2023-08-30T04:15:00.000Z",
        "voteCount": 4,
        "content": "D - Spot Instances are cheaper than Ec2 and the workload can tolerate interruptions. Also Custom AMI removes the need for 30 mins configuration which takes a resource that will need to be paid"
      },
      {
        "date": "2024-02-04T20:56:00.000Z",
        "voteCount": 2,
        "content": "I think you may be wrong. D - creating custom AMI costs you a lot. You would need to buy services from EC2 image builder to rebuild and pay for the one who rebuild the custom AMI. Meanwhile, configuring manually (option C)  costs you nothing but time. Of course, in corporate environment, time is money, but you would cost way less than option D - which suggests cost from creating and maintaining the AMI image (latest AMI image)"
      },
      {
        "date": "2023-07-16T05:49:00.000Z",
        "voteCount": 3,
        "content": "C is the answer.  Because it specifically mentioned EC2 Linux instance as requested in the question.  D only mentioned creating an AMI but is it Linux AMI?"
      },
      {
        "date": "2023-11-05T01:59:00.000Z",
        "voteCount": 2,
        "content": "Read the question properly. :D It says Linux EC2 instances.."
      },
      {
        "date": "2023-05-17T06:14:00.000Z",
        "voteCount": 4,
        "content": "D is correct"
      },
      {
        "date": "2023-05-15T18:02:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer"
      },
      {
        "date": "2023-05-10T07:59:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-05-09T04:00:00.000Z",
        "voteCount": 1,
        "content": "B because: \" Configuring the cluster software from a generic EC2 Linux image takes 30 minutes.\n\""
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/amazon/view/108798-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company recently migrated its legacy application from on-premises to AWS. The application is hosted on Amazon EC2 instances behind an Application Load Balancer, which is behind Amazon API Gateway. The company wants to ensure users experience minimal disruptions during any deployment of a new version of the application. The company also wants to ensure it can quickly roll back updates if there is an issue.<br><br>Which solution will meet these requirements with MINIMAL changes to the application?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce changes as a separate environment parallel to the existing one. Configure API Gateway to use a canary release deployment to send a small subset of user traffic to the new environment.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce changes as a separate environment parallel to the existing one. Update the application\u2019s DNS alias records to point to the new environment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route user traffic to the new target group in steps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce changes as a separate target group behind the existing Application Load Balancer. Configure API Gateway to route all traffic to the Application Load Balancer, which then sends the traffic to the new target group."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-26T19:14:00.000Z",
        "voteCount": 5,
        "content": "Option A is also a valid approach that can meet the requirements with MINIMAL changes to the application.\n\nIn Option A, the changes are introduced as a separate environment parallel to the existing one. This new environment can be used to deploy the new version of the application. By configuring API Gateway to use a canary release deployment, a small subset of user traffic is directed to the new environment, while the majority of traffic continues to be routed to the existing environment hosting the current version of the application."
      },
      {
        "date": "2024-07-28T23:11:00.000Z",
        "voteCount": 1,
        "content": "Keywords: minimal disruptions, MINIMAL changes"
      },
      {
        "date": "2024-06-14T10:46:00.000Z",
        "voteCount": 2,
        "content": "I chose A because the question says MINIMAL changes to the application. Canary deployment needs minimal changes to the application as it requires only adding canary settings to the deployment stage. AWS API Gateway supports canary deployments, allowing you to route a percentage of your traffic to a new stage or version of your API."
      },
      {
        "date": "2024-04-14T01:43:00.000Z",
        "voteCount": 2,
        "content": "C:\nSeparate Target Group: By introducing a new target group behind the existing Application Load Balancer, you can direct traffic to this new target group without affecting the existing environment. This means the new version of the application can be tested in isolation.\nStep-by-Step Traffic Routing: API Gateway allows you to gradually shift user traffic from one target group (existing version) to another (new version). This means you can start with a small percentage of traffic and gradually increase it, allowing you to monitor the new version's performance and stability.\nQuick Rollback: If the new version has any issues, you can quickly revert the traffic to the original target group, ensuring minimal disruption to users.\nSeparate Environment with Canary Deployment:  This introduces a completely separate environment, requiring additional infrastructure management and potentially more configuration changes depending on how the environments are set up."
      },
      {
        "date": "2024-03-31T05:03:00.000Z",
        "voteCount": 3,
        "content": "Agree with A. A parallel environment will allow the company to test the deployment, do smoke tests and all the basic stuff to check if the application is working fine. Then, they can start serving traffic to the users, allowing 10% of the users to go to the new environment and test. \nThe key is the word \"disruptions\" disruption can be considered a failure in the infrastructure, in the communications (networking) but NOT a Bug. Even do, switching 10% of the users to test is best than switching the entire loadbalancer to a new target group because in this scenarios is 100% of users affect against 10%\n\nDetails are everything during an investigation..."
      },
      {
        "date": "2024-02-27T19:29:00.000Z",
        "voteCount": 1,
        "content": "A is not meet the \"MINIMAL changes to the application\" requirement. If application receives requests from a different ALB, the application will receive a different request value, such as HTTP headers, and may need to be modified application. Since D is the same ALB, it is unlikely that changes will be necessary."
      },
      {
        "date": "2024-02-04T22:41:00.000Z",
        "voteCount": 3,
        "content": "A is correct: &lt;users experience minimal disruptions during any deployment of a new version of the application.&gt; and &lt;ensure it can quickly roll back updates if there is an issue&gt; means deploy in parallel: canary release or blue/green deployment\nB, C and D: If there was a large bug with the a new version, users would experience huge service disruptions"
      },
      {
        "date": "2024-01-31T05:47:00.000Z",
        "voteCount": 3,
        "content": "Canary deployment is used to stop disruption hence I have voted A"
      },
      {
        "date": "2023-12-27T12:34:00.000Z",
        "voteCount": 2,
        "content": "Answer is D. Even API supports canary deployment it is only if the API can redirect the traffic between two stages, in this case the API sends the traffic directly to the ALB, and from the ALB yo can choose to which environment redirect the traffic"
      },
      {
        "date": "2023-12-13T11:02:00.000Z",
        "voteCount": 1,
        "content": "Why not Option D, as the deployment is API gateway--&gt;ALB--&gt;target groups(EC2). and question is saying that: The company wants to ensure users experience minimal disruptions during any deployment of a new version of the application, with minimal changes to application (it is not saying that we shouldn't change deployment steps)"
      },
      {
        "date": "2023-05-17T06:15:00.000Z",
        "voteCount": 3,
        "content": "A is definitely correct"
      },
      {
        "date": "2023-05-15T18:05:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-05-12T01:38:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer"
      },
      {
        "date": "2023-05-10T12:17:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is A\nCorrect Answer is A. API Gateway supports canary deployment on a deployment stage before you direct all traffic to that stage. A parallel environment means we will create a new ALB and a target group that will target a new set of EC2 instances on which the newer version of the app will be deployed. So the canary setting associated to the new version of the API will connect with the new ALB instance which in turn will direct the traffic to the new EC2 instances on which the newer version of the application is deployed."
      },
      {
        "date": "2023-05-09T03:52:00.000Z",
        "voteCount": 3,
        "content": "I disagree with C: https://docs.aws.amazon.com/apigateway/latest/developerguide/canary-release.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/amazon/view/109048-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is storing 100 GB of log data in .csv format in an Amazon S3 bucket. SQL developers want to query this data and generate graphs to visualize it. The SQL developers also need an efficient, automated way to store metadata from the .csv file.<br><br>Which combination of steps will meet these requirements with the LEAST amount of effort? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter the data through AWS X-Ray to visualize the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter the data through Amazon QuickSight to visualize the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery the data with Amazon Athena.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery the data with Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS Glue Data Catalog as the persistent metadata store.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB as the persistent metadata store."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-09T07:34:00.000Z",
        "voteCount": 6,
        "content": "BCE. Glue Data Catalog can crawl S3 buckets to store table metadata. Then call the data catalog directly in Athena. It will show the partitions of the data. \n\nAthena does not deal with DynamoDB directly. Hence F is out."
      },
      {
        "date": "2023-07-09T07:35:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/glue/latest/dg/catalog-and-crawler.html"
      },
      {
        "date": "2024-02-04T22:48:00.000Z",
        "voteCount": 4,
        "content": "BCE are correct: &lt; query this data and generate graphs to visualize it&gt; means athena and quicksight\nA: irrelevant\nD: too expensive\nF: Dynamodb is primarily used for storing web session data and not for this purpose"
      },
      {
        "date": "2023-05-17T06:35:00.000Z",
        "voteCount": 3,
        "content": "BCE are correct"
      },
      {
        "date": "2023-05-15T18:09:00.000Z",
        "voteCount": 1,
        "content": "BCE are correct"
      },
      {
        "date": "2023-05-12T22:07:00.000Z",
        "voteCount": 1,
        "content": "yep, agree with B,C and E."
      },
      {
        "date": "2023-05-12T01:42:00.000Z",
        "voteCount": 2,
        "content": "Agree with BCE"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/amazon/view/108792-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company deploys its corporate infrastructure on AWS across multiple AWS Regions and Availability Zones. The infrastructure is deployed on Amazon EC2 instances and connects with AWS IoT Greengrass devices. The company deploys additional resources on on-premises servers that are located in the corporate headquarters.<br><br>The company wants to reduce the overhead involved in maintaining and updating its resources. The company\u2019s DevOps team plans to use AWS Systems Manager to implement automated management and application of patches. The DevOps team confirms that Systems Manager is available in the Regions that the resources are deployed in. Systems Manager also is available in a Region near the corporate headquarters.<br><br>Which combination of steps must the DevOps team take to implement automated patch and configuration management across the company\u2019s EC2 instances, IoT devices, and on-premises infrastructure? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply tags to all the EC2 instances, AWS IoT Greengrass devices, and on-premises servers. Use Systems Manager Session Manager to push patches to all the tagged devices.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Systems Manager Run Command to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Systems Manager Patch Manager to schedule patching for the EC2 instances, AWS IoT Greengrass devices, and on-premises servers as a Systems Manager maintenance window task.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon EventBridge to monitor Systems Manager Patch Manager for updates to patch baselines. Associate Systems Manager Run Command with the event to initiate a patch action for all EC2 instances, AWS IoT Greengrass devices, and on-premises servers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM instance profile for Systems Manager. Attach the instance profile to all the EC2 instances in the AWS account. For the AWS IoT Greengrass devices and on-premises servers, create an IAM service role for Systems Manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a managed-instance activation. Use the Activation Code and Activation ID to install Systems Manager Agent (SSM Agent) on each server in the on-premises environment. Update the AWS IoT Greengrass IAM token exchange role. Use the role to deploy SSM Agent on all the IoT devices.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CEF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CEF",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-23T07:04:00.000Z",
        "voteCount": 8,
        "content": "I also choose E instead of B.\n\nWhy E is correct: \"Previously in this post, you created and deployed the SSM Agent component which would have created an IAM service role. Suppose the AWS IoT Greengrass documentation was followed to deploy the SSM agent. In that case, the name of the IAM service role should be SSMServiceRole.\"\n\nWhy is B wrong: B is redundant given that answer C calls out Systems Manager Patch Manager which itself uses Systems Manager Run Command. Furthermore Run Command is described here to be used to run automated scripts and not to schedule patching: \"we\u2019ll demonstrate how to use Session Manager to open remote login to an edge device, patch them using Patch Manager, and run automated scripts through Run Command\"\n\nQuotes above are from: https://aws.amazon.com/blogs/mt/how-to-centrally-manage-aws-iot-greengrass-devices-using-aws-systems-manager/?force_isolation=true"
      },
      {
        "date": "2024-02-05T00:07:00.000Z",
        "voteCount": 7,
        "content": "CEF: \n- &lt; implement automated patch&gt; means Systems Manager Patch Manager\n- &lt; configuration management &gt; means we need install system manager agent\n- we need to configure sufficient permissions for SSM"
      },
      {
        "date": "2024-07-28T23:17:00.000Z",
        "voteCount": 1,
        "content": "Systems Manager Patch Manager, System Manager Agent, permission"
      },
      {
        "date": "2024-04-19T11:54:00.000Z",
        "voteCount": 3,
        "content": "By following the combination of steps C, E, and F, the DevOps team can effectively implement automated patch and configuration management across the company's EC2 instances, IoT Greengrass devices, and on-premises infrastructure using AWS Systems Manager's capabilities and best practices."
      },
      {
        "date": "2024-04-14T01:49:00.000Z",
        "voteCount": 1,
        "content": "ans is CEF"
      },
      {
        "date": "2024-03-16T09:30:00.000Z",
        "voteCount": 1,
        "content": "CEF are correct"
      },
      {
        "date": "2023-05-17T06:36:00.000Z",
        "voteCount": 1,
        "content": "CEF are correct"
      },
      {
        "date": "2023-05-13T18:29:00.000Z",
        "voteCount": 1,
        "content": "Agreed with Parag CEF"
      },
      {
        "date": "2023-05-12T01:46:00.000Z",
        "voteCount": 1,
        "content": "CEF make more sense."
      },
      {
        "date": "2023-05-09T02:15:00.000Z",
        "voteCount": 1,
        "content": "I disagree with the solution ... FEC for me"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/amazon/view/109194-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is testing a web application that runs on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Auto Scaling group across multiple Availability Zones. The company uses a blue/green deployment process with immutable instances when deploying new software.<br><br>During testing, users are being automatically logged out of the application at random times. Testers also report that, when a new version of the application is deployed, all users are logged out. The development team needs a solution to ensure users remain logged in across scaling events and application deployments.<br><br>What is the MOST operationally efficient way to ensure users remain logged in?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable smart sessions on the load balancer and modify the application to check for an existing session.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable session sharing on the load balancer and modify the application to read from the session store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore user session information in an Amazon S3 bucket and modify the application to read session information from the bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the application to store user session information in an Amazon ElastiCache cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T00:34:00.000Z",
        "voteCount": 5,
        "content": "D is correct: &lt;During testing, users are being automatically logged out of the application at random times&gt;: the cause is there is no data storage that stores user's session. We need a session data storage to store user session"
      },
      {
        "date": "2024-02-05T01:01:00.000Z",
        "voteCount": 1,
        "content": "A. Enable smart sessions on the load balance: there is no smart session on ALB\nB. Enable session sharing on the load balancer: load balancer does not store session data\nC. storing session data in S3 introduces latency"
      },
      {
        "date": "2024-08-05T09:08:00.000Z",
        "voteCount": 1,
        "content": "keywords: Amazon ElastiCache cluster"
      },
      {
        "date": "2024-04-14T02:01:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2024-03-31T05:24:00.000Z",
        "voteCount": 2,
        "content": "D is the correct one"
      },
      {
        "date": "2024-01-12T07:33:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/developer/elasticache-as-an-asp-net-session-store/"
      },
      {
        "date": "2023-05-21T23:15:00.000Z",
        "voteCount": 2,
        "content": "Why not C? Compared to D C is serverless thus more  operationally efficient."
      },
      {
        "date": "2024-02-28T22:36:00.000Z",
        "voteCount": 2,
        "content": "This is why many web application frameworks support Redis and Memcached session.  Also S3 is expensive to read, latency."
      },
      {
        "date": "2023-05-27T12:23:00.000Z",
        "voteCount": 1,
        "content": "like comment by accident. S3 is an object store, its not a mounted FS as such, potential performance issues &amp; consistency rule it out. Session data  - keep it local, caching system like redis or DB. C actually requires a lot more work as who is now managing the sessions, the bucket, keeping all in sync?"
      },
      {
        "date": "2023-05-17T06:37:00.000Z",
        "voteCount": 4,
        "content": "D is correct"
      },
      {
        "date": "2023-05-15T18:18:00.000Z",
        "voteCount": 1,
        "content": "agree with D"
      },
      {
        "date": "2023-05-13T18:32:00.000Z",
        "voteCount": 1,
        "content": "Yep D, session should be stored to share."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/amazon/view/109195-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer needs to configure a blue/green deployment for an existing three-tier application. The application runs on Amazon EC2 instances and uses an Amazon RDS database. The EC2 instances run behind an Application Load Balancer (ALB) and are in an Auto Scaling group.<br><br>The DevOps engineer has created a launch template and an Auto Scaling group for the blue environment. The DevOps engineer also has created a launch template and an Auto Scaling group for the green environment. Each Auto Scaling group deploys to a matching blue or green target group. The target group also specifies which software, blue or green, gets loaded on the EC2 instances. The ALB can be configured to send traffic to the blue environment\u2019s target group or the green environment\u2019s target group. An Amazon Route 53 record for www.example.com points to the ALB.<br><br>The deployment must move traffic all at once between the software on the blue environment\u2019s EC2 instances to the newly deployed software on the green environment\u2019s EC2 instances.<br><br>What should the DevOps engineer do to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment\u2019s EC2 instances. When the rolling restart is complete, use an AWS CLI command to update the ALB to send traffic to the green environment\u2019s target group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS CLI command to update the ALB to send traffic to the green environment\u2019s target group. Then start a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment\u2019s EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the launch template to deploy the green environment\u2019s software on the blue environment\u2019s EC2 instances. Keep the target groups and Auto Scaling groups unchanged in both environments. Perform a rolling restart of the blue environment\u2019s EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart a rolling restart of the Auto Scaling group for the green environment to deploy the new software on the green environment\u2019s EC2 instances. When the rolling restart is complete, update the Route 53 DNS to point to the green environment\u2019s endpoint on the ALB."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-20T00:38:00.000Z",
        "voteCount": 15,
        "content": "A is correct, cannot be D, as there is only one ALB. The ALB can be configured to send traffic to the blue environment\u2019s target group or the green environment\u2019s target group. Traffic route to blue or green, must be configure at Load balancer, in this case, update the target group."
      },
      {
        "date": "2024-07-28T23:25:00.000Z",
        "voteCount": 1,
        "content": "One Application Load Balancer (ALB)\nOne Auto Scaling Group (ASG) for Blue and one Auto Scaling Group (ASG) for Green"
      },
      {
        "date": "2024-04-14T02:10:00.000Z",
        "voteCount": 2,
        "content": "its A."
      },
      {
        "date": "2024-04-10T05:20:00.000Z",
        "voteCount": 1,
        "content": "B correct .......\noption  A  reverses the order of operations, which goes against the recommended practice of updating the load balancer first to send traffic to the new environment before deploying the new software."
      },
      {
        "date": "2024-02-05T01:06:00.000Z",
        "voteCount": 4,
        "content": "A is correct: &lt;The deployment must move traffic all at once between the software on the blue environment\u2019s EC2 instances to the newly deployed software on the green environment\u2019s EC2 instances.&gt; and &lt;The ALB can be configured to send traffic to the blue environment\u2019s target group or the green environment\u2019s target group.&gt;means we should do the traffic migration manually by config the ALB\nB and C: no mention of migration step\nD: should not use route 53 DNS, we need to configure the ALB"
      },
      {
        "date": "2023-11-01T04:41:00.000Z",
        "voteCount": 2,
        "content": "Answer is A\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html#cfn-attributes-updatepolicy-rollingupdate"
      },
      {
        "date": "2023-08-18T09:16:00.000Z",
        "voteCount": 1,
        "content": "The client-side caches the results of DNS queries, so DNS switching lacks immediacy, and it's challenging to transition traffic all at once. Therefore, option D doesn't meet the requirement of moving traffic all at once and is not suitable."
      },
      {
        "date": "2023-08-16T02:25:00.000Z",
        "voteCount": 1,
        "content": "What???? No one read the question carefully. There are two ALB.\nThe DevOps engineer has created a launch template and an Auto Scaling group for the blue environment. The DevOps engineer also has created a launch template and an Auto Scaling group for the green environment."
      },
      {
        "date": "2023-08-16T02:26:00.000Z",
        "voteCount": 2,
        "content": "D is right."
      },
      {
        "date": "2023-11-29T17:25:00.000Z",
        "voteCount": 2,
        "content": "No, you mixed it up. There is only one ALB and two ASGs. Option A is the answer as there is no need to touch Route53 in this scenario(it's pointing to the single ALB already)."
      },
      {
        "date": "2023-07-20T01:28:00.000Z",
        "voteCount": 2,
        "content": "Cannot be D as the Route 53 record will be unchanged - points to the same ALB - the target group on the ALB need to be updated"
      },
      {
        "date": "2023-07-05T21:01:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-06-20T16:09:00.000Z",
        "voteCount": 2,
        "content": "I don't think it's D, because it's a single ALB and Route 53 is already pointing at it as wwww.example.com. What needs to occur is an ASG switch within the ALB. So, A is the best bet."
      },
      {
        "date": "2023-06-08T04:45:00.000Z",
        "voteCount": 1,
        "content": "A is correct. \nFor D, DNS alias record needs to be updated, and green environment's endpoint wasn't mentioned in the question."
      },
      {
        "date": "2023-05-15T18:24:00.000Z",
        "voteCount": 1,
        "content": "Yes D sounds the best approach"
      },
      {
        "date": "2023-05-13T18:40:00.000Z",
        "voteCount": 1,
        "content": "It must be D. DNS record should be diverted to the green ALB"
      },
      {
        "date": "2023-07-18T19:30:00.000Z",
        "voteCount": 1,
        "content": "The question states there is only one ALB with two target groups (Blue and Green). The DNS record points to that one ALB. So answer D is not correct."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/amazon/view/108791-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is building a new pipeline by using AWS CodePipeline and AWS CodeBuild in a build account. The pipeline consists of two stages. The first stage is a CodeBuild job to build and package an AWS Lambda function. The second stage consists of deployment actions that operate on two different AWS accounts: a development environment account and a production environment account. The deployment stages use the AWS CloudFormation action that CodePipeline invokes to deploy the infrastructure that the Lambda function requires.<br><br>A DevOps engineer creates the CodePipeline pipeline and configures the pipeline to encrypt build artifacts by using the AWS Key Management Service (AWS KMS) AWS managed key for Amazon S3 (the aws/s3 key). The artifacts are stored in an S3 bucket. When the pipeline runs, the CloudFormation actions fail with an access denied error.<br><br>Which combination of actions must the DevOps engineer perform to resolve this error? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket in each AWS account for the artifacts. Allow the pipeline to write to the S3 buckets. Create a CodePipeline S3 action to copy the artifacts to the S3 bucket in each AWS account. Update the CloudFormation actions to reference the artifacts S3 bucket in the production account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a customer managed KMS key. Configure the KMS key policy to allow the IAM roles used by the CloudFormation action to perform decrypt operations. Modify the pipeline to use the customer managed KMS key to encrypt artifacts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS managed KMS key. Configure the KMS key policy to allow the development account and the production account to perform decrypt operations. Modify the pipeline to use the KMS key to encrypt artifacts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, configure the CodePipeline CloudFormation action to use the roles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the development account and in the production account, create an IAM role for CodePipeline. Configure the roles with permissions to perform CloudFormation operations and with permissions to retrieve and decrypt objects from the artifacts S3 bucket. In the CodePipeline account, modify the artifacts S3 bucket policy to allow the roles access. Configure the CodePipeline CloudFormation action to use the roles.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-27T12:29:00.000Z",
        "voteCount": 13,
        "content": "C = AWS KMS fundamentals. Cannot modify AWS managed KMS key policies. No Cross account access = will not work. Not sure why there is even a discussion on this. Associate level basics."
      },
      {
        "date": "2023-12-07T07:28:00.000Z",
        "voteCount": 1,
        "content": "You van modify the key policies, it is a managed key. What is wrong is change it to use for different account.\nhttps://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying.html"
      },
      {
        "date": "2023-12-10T03:27:00.000Z",
        "voteCount": 2,
        "content": "From your link: https://docs.aws.amazon.com/kms/latest/developerguide/key-policy-modifying.html\n\nWhen changing a key policy, keep in mind the following rules:\n- You can view the key policy for an AWS managed key or a customer managed key, but you can only change the key policy for a customer managed key. \n- The policies of AWS managed keys are created and managed by the AWS service that created the KMS key in your account. \n- You cannot view or change the key policy for an AWS owned key."
      },
      {
        "date": "2024-09-23T00:21:00.000Z",
        "voteCount": 1,
        "content": "From your link:\n\"You can add or remove IAM users, IAM roles, and AWS accounts in the key policy, and change the actions that are allowed or denied for those principals.\"\nThe answer is BE because you don't want to grant permissions to the KMS key for an ENTIRE account, you'd want to allow access for a particular role."
      },
      {
        "date": "2024-07-28T23:59:00.000Z",
        "voteCount": 2,
        "content": "B - Cannot modify AWS managed KMS key policies.\nE - Cross account access and we need bucket policies also to be updated, if its same account then we do not need bucket policies permissions"
      },
      {
        "date": "2024-06-19T10:50:00.000Z",
        "voteCount": 2,
        "content": "BD,\ntry it yourself, create account with a bucket, create role with access to s3 operations, and trust policy for another account. \nrole assumed by another account has full access to s3 resources thereby it's not needed to set up resource policy on s3 bucket"
      },
      {
        "date": "2024-06-08T23:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is BD , \nI have recently implemented similar solution, and my S3 bucket do not have any policy configured , my IAM role has required KMS key permission and it worked. \n\nmodifying the S3 bucket policy, but this is not necessary if the IAM roles are correctly configured and used by the CodePipeline CloudFormation action"
      },
      {
        "date": "2024-06-18T19:23:00.000Z",
        "voteCount": 3,
        "content": "I switch to BE , \n\nbecause its cross account access and we need bucket policies also to be updated, if its same account then we do not need bucket policies permissions"
      },
      {
        "date": "2024-04-21T17:48:00.000Z",
        "voteCount": 1,
        "content": "Nobody is saying why we are modifying the artifacts in S3 in Option E in the Codecommit account. Doesn't seem to make sense to me."
      },
      {
        "date": "2024-04-14T02:18:00.000Z",
        "voteCount": 2,
        "content": "BE.  are correct"
      },
      {
        "date": "2024-02-05T02:48:00.000Z",
        "voteCount": 4,
        "content": "B and E are correct: &lt;fail with an access denied error.&gt; this means there are issues with policies and permissions. \nA: no mention of policies\nC: This is what the dev team has tried but failed. Can not modify managed key policy, can only view it\nD: no mention of configuring S3 bucket policy"
      },
      {
        "date": "2023-12-10T03:36:00.000Z",
        "voteCount": 3,
        "content": "exact steps are in this doc\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/pipelines-create-cross-account.html"
      },
      {
        "date": "2023-11-02T14:26:00.000Z",
        "voteCount": 4,
        "content": "It's BE,\nAccording to this, aws managed kms key can't be used cross account:\nhttps://repost.aws/knowledge-center/cross-account-access-denied-error-s3\n\n\"Warning: AWS managed AWS KMS key policies can't be modified because they're read-only. However, you can always view both the AWS managed KMS key policies and customer managed KMS key policies. Because AWS managed KMS key policies can't be updated, cross-account permissions also can't be granted for those key policies. Additionally, objects that are encrypted using an AWS managed KMS key can't be accessed by other AWS accounts. For customer managed KMS key policies, you can change the key policy only from the AWS account that created the policy.\""
      },
      {
        "date": "2023-07-13T01:36:00.000Z",
        "voteCount": 3,
        "content": "BE, bucket policy needs to be amended also as it will assume roles in the prod and dev account"
      },
      {
        "date": "2023-07-09T08:00:00.000Z",
        "voteCount": 2,
        "content": "BE. CMEK = you determine access (key policy) and rotation period (you define instead of 365 days for AWS managed keys). Perfect for cross account resources."
      },
      {
        "date": "2023-05-23T02:58:00.000Z",
        "voteCount": 4,
        "content": "You can view the key policy for an AWS managed key or a customer managed key, but you can only change the key policy for a customer managed key."
      },
      {
        "date": "2023-05-15T18:39:00.000Z",
        "voteCount": 1,
        "content": "CE for me"
      },
      {
        "date": "2023-05-13T16:47:00.000Z",
        "voteCount": 2,
        "content": "B &amp; E , i guess too"
      },
      {
        "date": "2023-05-23T16:12:00.000Z",
        "voteCount": 1,
        "content": "I thought again, it should be A &amp; E correct. \nB is worng becasue The access denied error typically occurs when the IAM roles used by the CloudFormation action lack the necessary permissions to access the required resources. Therefore, option B does not directly address the access denied error in the given scenario."
      },
      {
        "date": "2023-05-12T22:26:00.000Z",
        "voteCount": 2,
        "content": "I think it is B and E"
      },
      {
        "date": "2023-05-09T01:52:00.000Z",
        "voteCount": 3,
        "content": "Questions says: \"A DevOps engineer creates the CodePipeline pipeline and configures the pipeline to encrypt build artifacts by using the AWS Key Management Service (AWS KMS) AWS managed key for Amazon S3 (the aws/s3 key).\" not CMK ..."
      },
      {
        "date": "2023-08-02T20:25:00.000Z",
        "voteCount": 4,
        "content": "Answer C is incorrect because you cannot \"create\" an AWS-managed key or modify its key policy. In order to modify a key policy, you need an customer-managed key (Answer B). The question states they used an AWS-managed key, but got an error. So you have to re-evaluate how to make this work, which requires a customer-managed key.\n\nhttps://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#key-mgmt"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/amazon/view/108790-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is using an organization in AWS Organizations to manage multiple AWS accounts. The company\u2019s development team wants to use AWS Lambda functions to meet resiliency requirements and is rewriting all applications to work with Lambda functions that are deployed in a VPC. The development team is using Amazon Elastic File System (Amazon EFS) as shared storage in Account A in the organization.<br><br>The company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B.<br><br>A DevOps engineer needs to reconfigure an existing EFS file system to allow Lambda functions to access the data through an existing EFS access point.<br><br>Which combination of steps should the DevOps engineer take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the EFS file system policy to provide Account B with access to mount and write to the EFS file system in Account A.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate SCPs to set permission guardrails with fine-grained control for Amazon EFS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new EFS file system in Account B. Use AWS Database Migration Service (AWS DMS) to keep data from Account A and Account B synchronized.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Lambda execution roles with permission to access the VPC and the EFS file system.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC peering connection to connect Account A to Account B.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Lambda functions in Account B to assume an existing IAM role in Account A."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "AEF",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "ADF",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-17T06:52:00.000Z",
        "voteCount": 12,
        "content": "I got ADE"
      },
      {
        "date": "2023-11-16T17:54:00.000Z",
        "voteCount": 7,
        "content": "Initially, I thought of A,E,F. But after reading the docs I came to conclusion A,D,E is correct answer.\nE: https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html#configuration-filesystem-cross-account\nA,D: https://docs.aws.amazon.com/lambda/latest/dg/configuration-filesystem.html#configuration-filesystem-permissions"
      },
      {
        "date": "2024-07-29T00:05:00.000Z",
        "voteCount": 2,
        "content": "Should be ADE\nVPC peering required."
      },
      {
        "date": "2024-04-14T02:26:00.000Z",
        "voteCount": 3,
        "content": "A,D,E is correct"
      },
      {
        "date": "2024-03-16T09:23:00.000Z",
        "voteCount": 3,
        "content": "A,D,E is correct"
      },
      {
        "date": "2024-02-20T18:21:00.000Z",
        "voteCount": 1,
        "content": "1.need to update the file system plocy on efs to allow mounting the file system into account b\n2.need vpc peering between account account a and account b as the pre-requisite\n3.need to assume cross-account iam role to descibe the mounts so that a specific mount can be chosen"
      },
      {
        "date": "2024-02-05T02:59:00.000Z",
        "voteCount": 5,
        "content": "ADE are correct: &lt;The company wants to continue to use Amazon EFS with Lambda. Company policy requires all serverless projects to be deployed in Account B.&gt; means we need assign relevant IAM policies to lambda in account b\nB: no mention of policy \nC: no mention of policy\nF: &lt;assume an existing IAM role in Account A&gt;: What role?"
      },
      {
        "date": "2024-01-15T11:52:00.000Z",
        "voteCount": 4,
        "content": "NOT F: account B will mount EFS and would read/write as a local folder. There is no way/no need to assume role. Option D would assign permission that allow account B to read/write the EFS."
      },
      {
        "date": "2023-11-15T13:34:00.000Z",
        "voteCount": 3,
        "content": "It's ADE."
      },
      {
        "date": "2023-11-10T05:22:00.000Z",
        "voteCount": 2,
        "content": "D only works if both lamda function and EFS are in the same account."
      },
      {
        "date": "2023-11-15T13:35:00.000Z",
        "voteCount": 1,
        "content": "When peering enabled between two VPCs, this is possible even if the function and EFS are in different account."
      },
      {
        "date": "2023-10-28T03:32:00.000Z",
        "voteCount": 3,
        "content": "1) Lambda in account a can get access directly to EFS using cross account policy on the efs.\n2) Access to the efs is via network, thats why vpc peering is needed.\n\nhttps://aws.amazon.com/blogs/storage/mount-amazon-efs-file-systems-cross-account-from-amazon-eks/"
      },
      {
        "date": "2023-09-04T16:21:00.000Z",
        "voteCount": 4,
        "content": "A &amp; E are obvious answers.\nD is wrong Lamda execuation role is in account B.  You cannot directly assign permission  to that role . Instead you add AWS STS AssumeRole API call to your Lambda function's code in account B"
      },
      {
        "date": "2023-07-18T20:39:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/efs/latest/ug/create-file-system-policy.html (Answer A)\nhttps://aws.amazon.com/blogs/compute/using-amazon-efs-for-aws-lambda-in-your-serverless-applications/ (Answer D)\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-efs.html (Answer E)"
      },
      {
        "date": "2023-07-17T15:31:00.000Z",
        "voteCount": 3,
        "content": "AEF Makes more sense"
      },
      {
        "date": "2023-07-17T04:08:00.000Z",
        "voteCount": 4,
        "content": "The answer is AEF because:\nA few hours ago, I just finished the DOP-C02 exam.\nMy score is 1000 points.\nThis question has come up, I choose AEF."
      },
      {
        "date": "2023-07-25T17:03:00.000Z",
        "voteCount": 3,
        "content": "I am sure you didn't get 1000 if you got this answer wrong"
      },
      {
        "date": "2023-07-18T20:38:00.000Z",
        "voteCount": 3,
        "content": "Please provide supporting links, since the documentation points to ADE.\nhttps://docs.aws.amazon.com/efs/latest/ug/create-file-system-policy.html (Answer A)\nhttps://aws.amazon.com/blogs/compute/using-amazon-efs-for-aws-lambda-in-your-serverless-applications/ (Answer D)\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-efs.html (Answer E)"
      },
      {
        "date": "2023-07-18T21:00:00.000Z",
        "voteCount": 2,
        "content": "Another support for D and not F.\nhttps://repost.aws/knowledge-center/access-efs-across-accounts\n\nThis talks about assigning IAM permissions on the account B side, with EFS located in account A. For Lambda, those IAM permissions are part of the execution role. There is nothing indicating the need for using roles from account A. Only an EFS file system policy in account A. And of course peering is needed between the two accounts.\n\nIf you did get 1000 points, and you selected AEF, this could have been one of those questions that did not count against your raw score. AWS will have some questions that are not included in your score, but are questions that may be new and are being evaluated."
      },
      {
        "date": "2023-11-15T13:37:00.000Z",
        "voteCount": 1,
        "content": "In exam there are a few questions that does not have any impact on your score. No matter you mark them right or wrong."
      },
      {
        "date": "2023-07-16T10:04:00.000Z",
        "voteCount": 1,
        "content": "ADF for me"
      },
      {
        "date": "2023-07-16T10:07:00.000Z",
        "voteCount": 1,
        "content": "E is wrong .  All accounts in same VPC so, you cant do VPC peering."
      },
      {
        "date": "2023-05-15T18:47:00.000Z",
        "voteCount": 2,
        "content": "AEF are correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/amazon/view/109364-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A media company has several thousand Amazon EC2 instances in an AWS account. The company is using Slack and a shared email inbox for team communications and important updates. A DevOps engineer needs to send all AWS-scheduled EC2 maintenance notifications to the Slack channel and the shared inbox. The solution must include the instances\u2019 Name and Owner tags.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntegrate AWS Trusted Advisor with AWS Config. Configure a custom AWS Config rule to invoke an AWS Lambda function to publish notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe a Slack channel endpoint and the shared inbox to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EventBridge to monitor for AWS Health events. Configure the maintenance events to target an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe an AWS Lambda function to the SNS topic to send notifications to the Slack channel and the shared inbox.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that sends EC2 maintenance notifications to the Slack channel and the shared inbox. Monitor EC2 health events by using Amazon CloudWatch metrics. Configure a CloudWatch alarm that invokes the Lambda function when a maintenance notification is received.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Support integration with AWS CloudTrail. Create a CloudTrail lookup event to invoke an AWS Lambda function to pass EC2 maintenance notifications to Amazon Simple Notification Service (Amazon SNS). Configure Amazon SNS to target the Slack channel and the shared inbox."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-14T02:29:00.000Z",
        "voteCount": 2,
        "content": "B is the answer"
      },
      {
        "date": "2024-02-05T03:10:00.000Z",
        "voteCount": 2,
        "content": "B is correct: &lt;eeds to send all AWS-scheduled EC2 maintenance notifications to the Slack channel and the shared inbox&gt; means SNS\nC: no mention of SNS\nA: AWS trusted advisor has nothing to do here\nD: AWS Support is support plan. It has nothing to do here. so as AWS cloud trail"
      },
      {
        "date": "2024-01-10T12:19:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2024-01-03T18:30:00.000Z",
        "voteCount": 1,
        "content": "Yes it's B"
      },
      {
        "date": "2023-07-21T04:48:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/health/latest/ug/cloudwatch-events-health.html"
      },
      {
        "date": "2023-07-20T05:00:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-05-17T06:53:00.000Z",
        "voteCount": 2,
        "content": "B is the answer I got"
      },
      {
        "date": "2023-05-15T18:51:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/amazon/view/109365-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "An AWS CodePipeline pipeline has implemented a code release process. The pipeline is integrated with AWS CodeDeploy to deploy versions of an application to multiple Amazon EC2 instances for each CodePipeline stage.<br><br>During a recent deployment, the pipeline failed due to a CodeDeploy issue. The DevOps team wants to improve monitoring and notifications during deployment to decrease resolution times.<br><br>What should the DevOps engineer do to create notifications when issues are discovered?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Amazon CloudWatch Logs for CodePipeline and CodeDeploy, create an AWS Config rule to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Amazon EventBridge for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement AWS CloudTrail to record CodePipeline and CodeDeploy API call information, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Amazon EventBridge for CodePipeline and CodeDeploy, create an Amazon Inspector assessment target to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T03:17:00.000Z",
        "voteCount": 5,
        "content": "B is correct: &lt;monitoring and notifications during deployment&gt; means eventbridge and SNS\nA: cloudwatchlog has nothing to do here. This is use for continuous monitoring of AWS services\nC: cloudtrail is for account activities monitoring\nD: Inspector is for threat detection"
      },
      {
        "date": "2024-04-14T02:32:00.000Z",
        "voteCount": 2,
        "content": "B is correc"
      },
      {
        "date": "2023-10-28T03:46:00.000Z",
        "voteCount": 2,
        "content": "Its B, They want to monitor issued DURING deployment, means near real time, so cloudwatch event will do the work.\n\nC is wrong for to reasons, first, cloudtrail alone can't trigger lambda without an event. Second, cloud trail logs are update in 5 minutes intervals, which means monitoring for the code deploy will not be during deployment."
      },
      {
        "date": "2023-06-29T18:22:00.000Z",
        "voteCount": 3,
        "content": "B. Implement Amazon EventBridge for CodePipeline and CodeDeploy, create an AWS Lambda function to evaluate code deployment issues, and create an Amazon Simple Notification Service (Amazon SNS) topic to notify stakeholders of deployment issues.\n\nExplanation:\nAmazon EventBridge provides a serverless event bus that integrates with various AWS services. By implementing EventBridge for CodePipeline and CodeDeploy, the engineer can capture deployment events and trigger actions based on those events. Creating an AWS Lambda function allows for evaluating code deployment issues and performing custom actions. Additionally, creating an Amazon SNS topic provides a means to notify stakeholders of any deployment issues detected."
      },
      {
        "date": "2023-05-17T06:55:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2023-05-15T18:58:00.000Z",
        "voteCount": 1,
        "content": "Yes it's B"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/amazon/view/109182-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A global company manages multiple AWS accounts by using AWS Control Tower. The company hosts internal applications and public applications.<br><br>Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations. One of the AWS Control Tower member accounts serves as a centralized DevOps account with CI/CD pipelines that application teams use to deploy applications to their respective target AWS accounts. An IAM role for deployment exists in the centralized DevOps account.<br><br>An application team is attempting to deploy its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in an application AWS account. An IAM role for deployment exists in the application AWS account. The deployment is through an AWS CodeBuild project that is set up in the centralized DevOps account. The CodeBuild project uses an IAM service role for CodeBuild. The deployment is failing with an Unauthorized error during attempts to connect to the cross-account EKS cluster from CodeBuild.<br><br>Which solution will resolve this error?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application account\u2019s deployment IAM role to have a trust relationship with the centralized DevOps account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account\u2019s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the centralized DevOps account\u2019s deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the centralized DevOps account\u2019s deployment IAM role to allow the required access to CodeBuild.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the centralized DevOps account\u2019s deployment IAM role to have a trust relationship with the application account. Configure the trust relationship to allow the sts:AssumeRoleWithSAML action. Configure the centralized DevOps account\u2019s deployment IAM role to allow the required access to CodeBuild.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the application account\u2019s deployment IAM role to have a trust relationship with the AWS Control Tower management account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account\u2019s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-13T01:54:00.000Z",
        "voteCount": 10,
        "content": "A. Configure the application account\u2019s deployment IAM role to have a trust relationship with the centralized DevOps account. Configure the trust relationship to allow the sts:AssumeRole action. Configure the application account\u2019s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.\n\nOptions B, C, and D are not correct because the centralized DevOps account\u2019s deployment IAM role doesn't need to trust the application account, it's the other way around. The sts:AssumeRoleWithSAML action in option C is used for federation from a SAML 2.0 compliant identity provider and is not necessary in this scenario. Lastly, there's no need to have a trust relationship with the AWS Control Tower management account as in option D, as the interaction is directly between the DevOps account and the application account."
      },
      {
        "date": "2024-02-05T05:35:00.000Z",
        "voteCount": 6,
        "content": "A is correct: &lt;Unauthorized error during attempts to connect&gt; means we need to setup relevant permissions and policies\n- A is correct because &lt; AWS CodeBuild project that is set up in the centralized DevOps account&gt;, so we should setup trust relationship on the account that has resources, which is the application account and allow codebuild from centralized account assume it\nB and C are wrong: we need to setup trust from the app account, not the centralized account.\nD: this option mentions control Tower, which is irrelevant"
      },
      {
        "date": "2024-08-02T01:01:00.000Z",
        "voteCount": 2,
        "content": "A. Configure the application account\u2019s deployment IAM role to have a trust relationship with the centralized DevOps account. \n- setup trust relationship on the account that has resources, which is the application account\n\nConfigure the trust relationship to allow the sts:AssumeRole action. \n- allow CodeBuild from centralized account assume it\n- CodeBuild is configured in Centralized DevOps account but not in application account.\n\nConfigure the application account\u2019s deployment IAM role to have the required access to the EKS cluster. Configure the EKS cluster aws-auth ConfigMap to map the role to the appropriate system permissions.\n- the application account has access to the resources"
      },
      {
        "date": "2023-06-20T13:45:00.000Z",
        "voteCount": 3,
        "content": "(A) This solution addresses the Unauthorized error by allowing the DevOps account to assume the IAM role in the application account that has the necessary permissions to access the EKS cluster. The other options don't provide the necessary cross-account permissions or correctly configure the roles for accessing EKS."
      },
      {
        "date": "2023-06-08T06:37:00.000Z",
        "voteCount": 2,
        "content": "B is correct.\nUnauthorized error happened from CodeBuild in Dev account to EKS cluster in application account, instead of reverse direction."
      },
      {
        "date": "2023-11-15T13:14:00.000Z",
        "voteCount": 2,
        "content": "CodeBuild is configured in Centralized DevOps account not in application account."
      },
      {
        "date": "2023-05-23T15:16:00.000Z",
        "voteCount": 2,
        "content": "I'd like to add more, don't get the source and destination mixed up. Because the Application team must deploy resources in the Dev account. So, the source should be the Application team and the destination should be the Dev team."
      },
      {
        "date": "2023-05-23T07:05:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-05-14T02:57:00.000Z",
        "voteCount": 1,
        "content": "A is correct Answer"
      },
      {
        "date": "2023-05-13T15:33:00.000Z",
        "voteCount": 3,
        "content": "Answer is A.\nIn the source AWS account, the IAM role used by the CI/CD pipeline should have permissions to access the source code repository, build artifacts, and any other resources required for the build process.\nIn the destination AWS accounts, the IAM role used for deployment should have permissions to access the AWS resources required for deploying the application, such as EC2 instances, RDS databases, S3 buckets, etc. The exact permissions required will depend on the specific resources being used by the application.\nthe IAM role used for deployment in the destination accounts should also have permissions to assume the IAM role for deployment in the centralized DevOps account. This is typically done using an IAM role trust policy that allows the destination account to assume the DevOps account role."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/amazon/view/109183-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A highly regulated company has a policy that DevOps engineers should not log in to their Amazon EC2 instances except in emergencies. If a DevOps engineer does log in, the security team must be notified within 15 minutes of the occurrence.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon Inspector agent on each EC2 instance. Subscribe to Amazon EventBridge notifications. Invoke an AWS Lambda function to check if a message is about user logins. If it is, send a notification to the security team using Amazon SNS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon CloudWatch agent on each EC2 instance. Configure the agent to push all logs to Amazon CloudWatch Logs and set up a CloudWatch metric filter that searches for user logins. If a login is found, send a notification to the security team using Amazon SNS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS CloudTrail with Amazon CloudWatch Logs. Subscribe CloudWatch Logs to Amazon Kinesis. Attach AWS Lambda to Kinesis to parse and determine if a log contains a user login. If it does, send a notification to the security team using Amazon SNS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a script on each Amazon EC2 instance to push all logs to Amazon S3. Set up an S3 event to invoke an AWS Lambda function, which invokes an Amazon Athena query to run. The Athena query checks for logins and sends the output to the security team using Amazon SNS."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T05:39:00.000Z",
        "voteCount": 5,
        "content": "B is correct: &lt;the security team must be notified &gt; means SNS\nA: irrelevant, inspector is for vulnerability scanning\nC: cloud trail is for monitoring account activities\nD: This options uses manual script, which is irrelevant"
      },
      {
        "date": "2024-06-27T11:14:00.000Z",
        "voteCount": 1,
        "content": "B is the cheapest and correct solution\nCloudTrail does not capture:\nSSH logins to Linux instances.\nRDP logins to Windows instances.\nCommands executed on the instances.\nLocal file access or modifications within the instances."
      },
      {
        "date": "2023-07-22T03:43:00.000Z",
        "voteCount": 1,
        "content": "While Option B can provide valuable insights into user logins and send notifications to the security team, it might not guarantee that the security team is notified within 15 minutes of a login occurrence. The time it takes for the CloudWatch metric filter to process and detect the login event, along with the potential delays in the SNS notification, could result in notifications being sent beyond the required 15-minute timeframe.\n\nOn the other hand, Option C, which uses AWS CloudTrail with Amazon CloudWatch Logs and Amazon Kinesis, allows real-time processing and immediate notifications when a user login event is detected. This makes Option C more suitable for meeting the specific requirement of notifying the security team within 15 minutes of a login occurrence."
      },
      {
        "date": "2023-09-14T08:17:00.000Z",
        "voteCount": 9,
        "content": "Cloud Trail will track calls to AWS API, but not the OS login in an EC2. That can be checked only using Cloud watch logs"
      },
      {
        "date": "2023-07-21T07:03:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/security/how-to-monitor-and-visualize-failed-ssh-access-attempts-to-amazon-ec2-linux-instances/"
      },
      {
        "date": "2023-07-10T19:47:00.000Z",
        "voteCount": 4,
        "content": "B, \nEventhough it's not stated in some questions, the cheapest solution to a problem is always AWS favorite."
      },
      {
        "date": "2023-05-30T15:17:00.000Z",
        "voteCount": 2,
        "content": "Isn't it possible to get login events with CloudTrail?"
      },
      {
        "date": "2023-05-23T03:12:00.000Z",
        "voteCount": 3,
        "content": "Subtle difference Cloudtrail is \"near\" realtime - You can use subscriptions to get access to a real-time feed of log events from CloudWatch Logs and have it delivered to other services such as an Amazon Kinesis stream, an Amazon Kinesis Data Firehose stream, or AWS Lambda for custom processing, analysis, or loading to other systems."
      },
      {
        "date": "2023-05-15T19:07:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2023-05-13T15:58:00.000Z",
        "voteCount": 1,
        "content": "i think its C, Both B&amp;C solutions are valid and can meet the requirement of notifying the security team within 15 minutes of a DevOps engineer logging into an EC2 instance.\n\nHowever, there are some differences in how quickly each solution can detect and notify the security team of a login event.\n\nThe CloudTrail-based solution can detect a login event more quickly than the CloudWatch-based solution because CloudTrail captures API events in near-real-time, while CloudWatch logs may have a delay of a few minutes before they appear in the log group. Therefore, the CloudTrail-based solution is more likely to meet the 15-minute notification requirement."
      },
      {
        "date": "2023-08-14T05:46:00.000Z",
        "voteCount": 3,
        "content": "AWS CloudTrail captures API calls made on your account and sends log files to CloudWatch Logs. The provided solution monitors for login-related API calls. While this may detect some login activity (like a RunInstances API call), it will not catch SSH logins to an EC2 instance. Therefore, this solution isn't comprehensive enough.\n=&gt; Correct answer is B."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/amazon/view/109184-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company updated the AWS CloudFormation template for a critical business application. The stack update process failed due to an error in the updated template, and AWS CloudFormation automatically began the stack rollback process. Later, a DevOps engineer discovered that the application was still unavailable and that the stack was in the UPDATE_ROLLBACK_FAILED state.<br><br>Which combination of actions should the DevOps engineer perform so that the stack rollback can complete successfully? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach the AWSCloudFormationFullAccess IAM policy to the AWS CloudFormation role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomatically recover the stack resources by using AWS CloudFormation drift detection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIssue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually adjust the resources to match the expectations of the stack.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the existing AWS CloudFormation stack by using the original template."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-13T16:08:00.000Z",
        "voteCount": 8,
        "content": "yes C &amp; D\nC. Issue a ContinueUpdateRollback command from the AWS CloudFormation console or the AWS CLI: This command allows CloudFormation to continue the rollback process from the point where it was paused. By using this command, CloudFormation will attempt to restore the resources to their previous state and delete any resources that were created during the update.\n\nD. Manually adjust the resources to match the expectations of the stack: This involves identifying and correcting the root cause of the update failure, which could involve changing the resource configuration or resolving any dependencies or inconsistencies in the stack."
      },
      {
        "date": "2024-09-23T01:50:00.000Z",
        "voteCount": 1,
        "content": "D.\n\"In most cases, you must fix the error that causes the update rollback to fail before you can continue to roll back your stack. In other cases, you can continue to roll back the update without any changes, for example when a stack operation times out.\"\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-continueupdaterollback.html"
      },
      {
        "date": "2024-02-05T05:44:00.000Z",
        "voteCount": 3,
        "content": "C and D are correct: &lt; UPDATE_ROLLBACK_FAILED state&gt; means we are left with ContinueUpdateRollback command\nA: irrelevant, AWSCloudFormationFullAccess IAM policy is used to access ACF, not to fix this\nB:  AWS CloudFormation drift detection is to check if the stack has been updated unexpectedly, not to fix irrelevant\nE: The original template didnt work, so update the stack using it wont work"
      },
      {
        "date": "2024-01-10T12:31:00.000Z",
        "voteCount": 2,
        "content": "Agree with C and D"
      },
      {
        "date": "2024-01-03T18:43:00.000Z",
        "voteCount": 1,
        "content": "C and D are right"
      },
      {
        "date": "2023-07-20T05:37:00.000Z",
        "voteCount": 3,
        "content": "Yes it's C &amp; D"
      },
      {
        "date": "2023-06-20T13:42:00.000Z",
        "voteCount": 4,
        "content": "(C) This command will try to roll back the stack to the previously working state after you have resolved the issues that caused the rollback failure.\n\n(D) Sometimes a stack update can fail because the current state of a resource differs from the state expected by AWS CloudFormation (e.g., a resource that AWS CloudFormation is trying to modify or delete is locked by another process). Manually resolving these issues, by stopping the conflicting process or by modifying the resource to match the expected state, will allow the stack update or rollback to proceed."
      },
      {
        "date": "2023-05-15T19:12:00.000Z",
        "voteCount": 2,
        "content": "Yes it's C &amp; D"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/amazon/view/109366-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A development team manually builds an artifact locally and then places it in an Amazon S3 bucket. The application has a local cache that must be cleared when a deployment occurs. The team runs a command to do this, downloads the artifact from Amazon S3, and unzips the artifact to complete the deployment.<br><br>A DevOps team wants to migrate to a CI/CD process and build in checks to stop and roll back the deployment when a failure occurs. This requires the team to track the progression of the deployment.<br><br>Which combination of actions will accomplish this? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow developers to check the code into a code repository. Using Amazon EventBridge, on every pull into the main branch, invoke an AWS Lambda function to build the artifact and store it in Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom script to clear the cache. Specify the script in the BeforeInstall lifecycle hook in the AppSpec file.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate user data for each Amazon EC2 instance that contains the clear cache script. Once deployed, test the application. If it is not successful, deploy it again.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS CodePipeline to deploy the application. Allow developers to check the code into a code repository as a source for the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeBuild to build the artifact and place it in Amazon S3. Use AWS CodeDeploy to deploy the artifact to Amazon EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to fetch the artifact from Amazon S3 and deploy it to all the instances."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-20T13:41:00.000Z",
        "voteCount": 8,
        "content": "(B) This would help ensure that the local cache is cleared before the new version of the application is installed. AppSpec (Application Specification) file is a unique file to AWS CodeDeploy. It defines the deployment actions you want AWS CodeDeploy to execute.\n\n(D) This would allow you to automate the build and deployment processes. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates.\n\n(E) This would allow you to automate the build process and ensure that the application is built in a consistent environment. AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. AWS CodeDeploy automates software deployments to a variety of compute services including Amazon EC2, AWS Fargate, AWS Lambda, and your on-premises servers."
      },
      {
        "date": "2023-07-13T02:09:00.000Z",
        "voteCount": 5,
        "content": "BDE combination will do all requirments"
      },
      {
        "date": "2024-02-06T02:20:00.000Z",
        "voteCount": 3,
        "content": "BDE are correct: &lt; migrate to a CI/CD process &gt; means codepipeline, code build and code deploy\nA,C and F: no mention of the above three"
      },
      {
        "date": "2024-01-03T18:47:00.000Z",
        "voteCount": 1,
        "content": "BDE is rigtht"
      },
      {
        "date": "2023-05-15T19:16:00.000Z",
        "voteCount": 2,
        "content": "Yes it's BD&amp;E"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/amazon/view/109181-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is working on a project that is hosted on Amazon Linux and has failed a security review. The DevOps manager has been asked to review the company buildspec.yaml file for an AWS CodeBuild project and provide recommendations. The buildspec.yaml file is configured as follows:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image8.png\"><br><br>What changes should be recommended to comply with AWS security best practices? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a post-build command to remove the temporary files from the container before termination to ensure they cannot be seen by other CodeBuild users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeBuild project role with the necessary permissions and then remove the AWS credentials from the environment variable.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the DB_PASSWORD as a SecureString value in AWS Systems Manager Parameter Store and then remove the DB_PASSWORD from the environment variables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the environment variables to the \u2018db-deploy-bucket\u2019 Amazon S3 bucket, add a prebuild stage to download, then export the variables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager run command versus scp and ssh commands directly to the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScramble the environment variables using XOR followed by Base64, add a section to install, and then run XOR and Base64 to the build phase."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "ABC",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-31T08:02:00.000Z",
        "voteCount": 9,
        "content": "BCE is correct\nA is WRONG. CodeBuild does not keep files for next builds in that way, once the build is done, the files will be deleted."
      },
      {
        "date": "2024-09-23T02:03:00.000Z",
        "voteCount": 1,
        "content": "Code Build is a managed service. There's no way for other users to see what's in the container."
      },
      {
        "date": "2024-08-02T01:18:00.000Z",
        "voteCount": 1,
        "content": "Prefer BCE\n\nOption A incorrect as \n- CodeBuild does not keep files for next builds in that way, once the build is done, the files will be deleted.\n- and don't think have such \"CodeBuild users\""
      },
      {
        "date": "2024-07-22T02:50:00.000Z",
        "voteCount": 1,
        "content": "ABC seems right."
      },
      {
        "date": "2024-07-06T14:13:00.000Z",
        "voteCount": 1,
        "content": "A - Cleans up temp files that stores the my.cnf and the instance key files\nB - Removes hardcoded AWS credentials\nC - Securely stores DB password"
      },
      {
        "date": "2024-02-22T13:34:00.000Z",
        "voteCount": 2,
        "content": "ABC seems appropriate, since the emphasis is on security."
      },
      {
        "date": "2024-02-06T08:09:00.000Z",
        "voteCount": 2,
        "content": "ABC are correct: security best practices are related to removing credentials and sensitive data\n- A remove temporary files is important because they might contain sensitive data\n- B: &lt;remove the AWS credentials&gt; is removing the access key\n- C: &lt;remove the DB_PASSWORD&gt; means removing hardcoded DB_PASSWORD\nAll other options dont relate to senstive data or password"
      },
      {
        "date": "2024-01-03T18:52:00.000Z",
        "voteCount": 1,
        "content": "its BCE\nhttps://stackoverflow.com/questions/76854227/i-want-to-copy-files-to-aws-ec2-using-buildspec-yml-file-the-22-port-is-open-fo"
      },
      {
        "date": "2023-11-15T13:02:00.000Z",
        "voteCount": 3,
        "content": "It's BCE.\nA is wrong. I don't think there is any concept of `CodeBuild users`."
      },
      {
        "date": "2023-08-14T06:04:00.000Z",
        "voteCount": 1,
        "content": "BCE\nhttps://www.examtopics.com/discussions/amazon/view/46729-exam-aws-devops-engineer-professional-topic-1-question-17/"
      },
      {
        "date": "2023-07-23T13:42:00.000Z",
        "voteCount": 4,
        "content": "BCE are the correct answers."
      },
      {
        "date": "2023-07-22T22:52:00.000Z",
        "voteCount": 1,
        "content": "A: remove sensitive data that could left behind in container\nB: remove crendentials and use role \nC: Use SecureString AWS Systems Manager Parameter Store"
      },
      {
        "date": "2023-07-20T05:49:00.000Z",
        "voteCount": 2,
        "content": "BCE are the correct ones."
      },
      {
        "date": "2023-06-26T02:56:00.000Z",
        "voteCount": 2,
        "content": "BCE are the correct ones."
      },
      {
        "date": "2023-05-23T15:13:00.000Z",
        "voteCount": 1,
        "content": "BCE is correct answer"
      },
      {
        "date": "2023-05-15T19:31:00.000Z",
        "voteCount": 3,
        "content": "Sorry, I've read again and it's AB &amp; C."
      },
      {
        "date": "2023-05-15T19:27:00.000Z",
        "voteCount": 1,
        "content": "Yes BCE are correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/amazon/view/112704-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has a legacy application. A DevOps engineer needs to automate the process of building the deployable artifact for the legacy application. The solution must store the deployable artifact in an existing Amazon S3 bucket for future deployments to reference.<br><br>Which solution will meet these requirements in the MOST operationally efficient way?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Configure a new AWS CodeBuild project to use the custom Docker image to build the deployable artifact and to save the artifact to the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a new Amazon EC2 instance. Install all the dependencies for the legacy application on the EC2 instance. Use the EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom EC2 Image Builder image. Install all the dependencies for the legacy application on the image. Launch a new Amazon EC2 instance from the image. Use the new EC2 instance to build the deployable artifact and to save the artifact to the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with an AWS Fargate profile that runs in multiple Availability Zones. Create a custom Docker image that contains all the dependencies for the legacy application. Store the custom Docker image in a new Amazon Elastic Container Registry (Amazon ECR) repository. Use the custom Docker image inside the EKS cluster to build the deployable artifact and to save the artifact to the S3 bucket."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-27T10:44:00.000Z",
        "voteCount": 7,
        "content": "The most operationally efficient solution for automating the process of building the deployable artifact for the legacy application and storing it in an existing Amazon S3 bucket is:\n\nA. This solution leverages containerization with Docker, which allows for consistent and isolated builds, making it easier to manage application dependencies. The use of AWS CodeBuild allows for scalable and automated builds using the custom Docker image, making the process efficient and reliable. The deployable artifact can then be saved to the existing S3 bucket for future reference and deployments."
      },
      {
        "date": "2024-02-06T08:13:00.000Z",
        "voteCount": 6,
        "content": "A is correct: &lt;needs to automate the process of building the deployable artifact for the legacy application&gt; means codebuild\nBCD dont mention codebuild, only A mentions"
      },
      {
        "date": "2024-08-02T01:21:00.000Z",
        "voteCount": 1,
        "content": "keywords: CodeBuild for Reusable artifacts"
      },
      {
        "date": "2023-07-09T08:52:00.000Z",
        "voteCount": 3,
        "content": "Reusable artifacts = A."
      },
      {
        "date": "2023-06-26T05:01:00.000Z",
        "voteCount": 2,
        "content": "Option A makes more sense to me."
      },
      {
        "date": "2023-06-20T13:12:00.000Z",
        "voteCount": 3,
        "content": "(A) This approach is the most operationally efficient because it leverages the benefits of containerization, such as isolation and reproducibility, as well as AWS managed services. AWS CodeBuild is a fully managed build service that can compile your source code, run tests, and produce deployable software packages. By using a custom Docker image that includes all dependencies, you can ensure that the environment in which your code is built is consistent. Using Amazon ECR to store Docker images lets you easily deploy the images to any environment. Also, you can directly upload the build artifacts to Amazon S3 from AWS CodeBuild, which is beneficial for version control and archival purposes."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/amazon/view/112703-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company builds a container image in an AWS CodeBuild project by running Docker commands. After the container image is built, the CodeBuild project uploads the container image to an Amazon S3 bucket. The CodeBuild project has an IAM service role that has permissions to access the S3 bucket.<br><br>A DevOps engineer needs to replace the S3 bucket with an Amazon Elastic Container Registry (Amazon ECR) repository to store the container images. The DevOps engineer creates an ECR private image repository in the same AWS Region of the CodeBuild project. The DevOps engineer adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository. The DevOps engineer also places new repository information into the docker build command and the docker push command that are used in the buildspec.yml file.<br><br>When the CodeBuild project runs a build job, the job fails when the job tries to access the ECR repository.<br><br>Which solution will resolve the issue of failed access to the ECR repository?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the buildspec.yml file to log in to the ECR repository by using the aws ecr get-login-password AWS CLI command to obtain an authentication token. Update the docker login command to use the authentication token to access the ECR repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an environment variable of type SECRETS_MANAGER to the CodeBuild project. In the environment variable, include the ARN of the CodeBuild project's IAM service role. Update the buildspec.yml file to use the new environment variable to log in with the docker login command to access the ECR repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the ECR repository to be a public image repository. Add an ECR repository policy that allows the IAM service role to have access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the buildspec.yml file to use the AWS CLI to assume the IAM service role for ECR operations. Add an ECR repository policy that allows the IAM service role to have access."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-20T13:05:00.000Z",
        "voteCount": 7,
        "content": "(A) When Docker communicates with an Amazon Elastic Container Registry (ECR) repository, it requires authentication. You can authenticate your Docker client to the Amazon ECR registry with the help of the AWS CLI (Command Line Interface). Specifically, you can use the \"aws ecr get-login-password\" command to get an authorization token and then use Docker's \"docker login\" command with that token to authenticate to the registry. You would need to perform these steps in your buildspec.yml file before attempting to push or pull images from/to the ECR repository."
      },
      {
        "date": "2023-07-27T10:46:00.000Z",
        "voteCount": 5,
        "content": "A:\nWhen using Amazon ECR, you need to authenticate Docker to the ECR registry before pushing or pulling container images. The authentication token can be obtained using the aws ecr get-login-password AWS CLI command. The obtained token needs to be used with the docker login command to authenticate Docker to the ECR repository.\n\nBy following this approach, the CodeBuild project will have the necessary credentials to access the ECR repository, and the build job will be able to push the container image to the ECR repository successfully."
      },
      {
        "date": "2024-02-06T08:24:00.000Z",
        "voteCount": 4,
        "content": "A is correct: &lt;the job fails when the job tries to access the ECR repository.&gt; This means there is problem when accessing the repo. &lt;adjusts the IAM service role with the permissions that are necessary to work with the new ECR repository&gt; means have got sufficient permission. Need token to access with aws ecr get-login-password command\nBCD no mention of ecr get-login-password"
      },
      {
        "date": "2023-08-16T19:29:00.000Z",
        "voteCount": 3,
        "content": "A is right."
      },
      {
        "date": "2023-07-26T15:16:00.000Z",
        "voteCount": 3,
        "content": "A..version: 0.2\n\nphases:\n  pre_build:\n    commands:\n      - $(aws ecr get-login --no-include-email --region region-name)\n  build:\n    commands:\n      - docker build -t repository-name .\n      - docker tag repository-name:latest repository-uri:latest\n  post_build:\n    commands:\n      - docker push repository-uri:latest"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/amazon/view/112702-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company manually provisions IAM access for its employees. The company wants to replace the manual process with an automated process. The company has an existing Active Directory system configured with an external SAML 2.0 identity provider (IdP).<br><br>The company wants employees to use their existing corporate credentials to access AWS. The groups from the existing Active Directory system must be available for permission management in AWS Identity and Access Management (IAM). A DevOps engineer has completed the initial configuration of AWS IAM Identity Center (AWS Single Sign-On) in the company\u2019s AWS account.<br><br>What should the DevOps engineer do next to meet the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an external IdP as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Directory Service as an identity source. Configure automatic provisioning of users and groups by using the SAML protocol.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AD Connector as an identity source. Configure automatic provisioning of users and groups by using the SCIM protocol.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an external IdP as an identity source Configure automatic provisioning of users and groups by using the SAML protocol."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-20T13:00:00.000Z",
        "voteCount": 6,
        "content": "(A) AWS SSO (Single Sign-On) integrates with external identity providers using SAML 2.0, and it can automatically synchronize users and groups from a connected directory using the SCIM (System for Cross-domain Identity Management) protocol. Thus, the DevOps engineer should configure the external IdP as an identity source and then configure automatic provisioning of users and groups by using the SCIM protocol. This will ensure the groups from the existing Active Directory system are available for permission management in AWS Identity and Access Management (IAM) and that employees can use their existing corporate credentials to access AWS."
      },
      {
        "date": "2024-08-02T01:43:00.000Z",
        "voteCount": 1,
        "content": "For Note: SAML (Security Assertion Markup Language) is primarily used for authentication and authorization \nwhile SCIM (System for Cross-domain Identity Management) is a protocol used for automating user provisioning and deprovisioning across different systems and domains"
      },
      {
        "date": "2024-02-06T08:33:00.000Z",
        "voteCount": 4,
        "content": "A is correct: &lt;The company wants employees to use their existing corporate credentials to access AWS&gt; means we need to assign the existing IdP as an identity source\nB: &lt;Configure AWS Directory Service as an identity source&gt; is irrelevant\nC: &lt; Configure an AD Connector as an identity source&gt;: AD connector is use for connecting AWS active directory with that of on-prem. This question requires AWS identity Center\nD: &lt;provisioning of users and groups by using the SAML protocol.&gt;: SAML is an authenticate protocol. SCIM is the protocol for Idp connection"
      },
      {
        "date": "2023-11-30T06:52:00.000Z",
        "voteCount": 2,
        "content": "A: Explanation: What is the difference between SCIM and SSO? SSO (single-sign on) is a way to authenticate (sign in), and SCIM is a way to provision (create an account)."
      },
      {
        "date": "2023-08-15T14:15:00.000Z",
        "voteCount": 1,
        "content": "This is quoted from aws documentationThe SAML protocol however does not provide a way to query the IdP to learn about users and groups. Therefore, you must make IAM Identity Center aware of those users and groups by provisioning them into IAM Identity Center.\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/scim-profile-saml.html"
      },
      {
        "date": "2023-07-26T15:19:00.000Z",
        "voteCount": 1,
        "content": "Answer is A : AWS Single Sign-On (AWS SSO) can be integrated with an external SAML 2.0 identity provider (IdP). AWS SSO also supports automatic provisioning (auto-provisioning) of user and group information using the System for Cross-domain Identity Management (SCIM) protocol."
      },
      {
        "date": "2023-07-24T10:00:00.000Z",
        "voteCount": 2,
        "content": "Answer A is correct. It is SCIM that can provision users and groups in AWS. Of course the IdP needs to support SCIM (AWS has a list of IdPs that use SCIM). Answer D is not correct as SAML is an authentication protocol (cannot be used to provision users in AWS).\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/scim-profile-saml.html\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/supported-idps.html"
      },
      {
        "date": "2023-07-22T03:59:00.000Z",
        "voteCount": 1,
        "content": "The AWS IAM Identity Center (AWS Single Sign-On) has been configured initially. Now, to automate the provisioning of users and groups from the external IdP into AWS IAM, the engineer should choose the SCIM protocol. SCIM is specifically designed for automatic user provisioning, making it the appropriate choice for this scenario.\n\nOption D (Configure an external IdP as an identity source and use the SAML protocol) could work, but it does not address the requirement for automatic provisioning of users and groups. The use of SCIM (Option A) is preferred for automated user and group provisioning, as it is designed for this purpose."
      },
      {
        "date": "2023-07-16T20:17:00.000Z",
        "voteCount": 1,
        "content": "The company already has an external SAML 2.0 IdP, so the DevOps engineer should configure this IdP as an identity source in AWS Single Sign-On. Vs in option A would require to configure new identity source"
      },
      {
        "date": "2023-07-09T09:04:00.000Z",
        "voteCount": 1,
        "content": "A. SCIM is the automated way to provision users. You do it in AAD/AD and it propagates automatically into AWS SSO."
      },
      {
        "date": "2023-07-05T21:17:00.000Z",
        "voteCount": 2,
        "content": "SCIM protocol is to sync the user and groups from the external identity source"
      },
      {
        "date": "2023-07-05T03:00:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/amazon/view/112701-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is using AWS to run digital workloads. Each application team in the company has its own AWS account for application hosting. The accounts are consolidated in an organization in AWS Organizations.<br><br>The company wants to enforce security standards across the entire organization. To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation. A production support team can modify resources in the production environment by using the AWS Management Console to troubleshoot and resolve application-related issues.<br><br>A DevOps engineer must implement a solution to identify in near real time any AWS service misconfiguration that results in noncompliance. The solution must automatically remediate the issue within 15 minutes of identification. The solution also must track noncompliant resources and events in a centralized dashboard with accurate timestamps.<br><br>Which solution will meet these requirements with the LEAST development overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse CloudFormation drift detection to identify noncompliant resources. Use drift detection events from CloudFormation to invoke an AWS Lambda function for remediation. Configure the Lambda function to publish logs to an Amazon CloudWatch Logs log group. Configure an Amazon CloudWatch dashboard to use the log group for tracking.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon Athena to identify noncompliant resources. Use AWS Step Functions to track query results on Athena for drift detection and to invoke an AWS Lambda function for remediation. For tracking, set up an Amazon QuickSight dashboard that uses Athena as the data source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources. Enable AWS Security Hub with the --no-enable-default-standards option in all the AWS accounts. Set up AWS Config managed rules and custom rules. Set up automatic remediation by using AWS Config conformance packs. For tracking, set up a dashboard on Security Hub in a designated Security Hub administrator account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS CloudTrail in the AWS accounts. Analyze CloudTrail logs by using Amazon CloudWatch Logs to identify noncompliant resources. Use CloudWatch Logs filters for drift detection. Use Amazon EventBridge to invoke the Lambda function for remediation. Stream filtered CloudWatch logs to Amazon OpenSearch Service. Set up a dashboard on OpenSearch Service for tracking."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-23T12:06:00.000Z",
        "voteCount": 1,
        "content": "I don't think it is A because the question is asking the LEAST development overhead. Configuring Lambdas to remediate and send logs is development. It is much easier to use the built in features of AWS Config and SecurityHub"
      },
      {
        "date": "2024-09-23T12:08:00.000Z",
        "voteCount": 1,
        "content": "Lambda functions also have a execution limit of 15 minutes. If a remediation task were to take longer than that, it would fail."
      },
      {
        "date": "2024-07-02T09:22:00.000Z",
        "voteCount": 3,
        "content": "C is the better solution. AWS CloudFormation drift detection helps identify whether the actual configuration of your AWS resources matches their expected configuration as defined in the CloudFormation stack template. While it is a powerful tool for maintaining compliance and consistency, it alone cannot fully prevent noncompliance due to security misconfigurations. Thats where you need AWS config to continuously monitor service configurations and even use aggregator to collect all aws config data from all member accounts in aws organization to Security Hub to provide a centralized dashboard."
      },
      {
        "date": "2024-06-21T17:04:00.000Z",
        "voteCount": 2,
        "content": "Leaning towards \"A\" unless someone can convince me otherwise.  Why?:\nI have a problem with this step in \"C\": \"Turn on the configuration recorder in AWS Config in all the AWS accounts to identify noncompliant resources.\"  \nThe fact is your not going to detect any \"drift\" by turning on the recorder AFTER the accounts are noncompliant.\nAWS Config rules (canned or custom) and Conformance Packs can do a lot, but it's definitely duplicating settings any security settings allready defined CloudFormation stacks. \nI lean towards \"A\" because \"To avoid noncompliance because of security misconfiguration, the company has enforced the use of AWS CloudFormation\".\nTherefore CloudFormation stacks are is where the security settings are defined, and thereby CloudFormation is implied to be part of the detection and remediation process.\nCloudFormation drift detection can be automated, and one can just \"automatically remediate the issue within 15 minutes of identification\" by just doing a stack refresh.  Easy peasy."
      },
      {
        "date": "2024-07-06T14:44:00.000Z",
        "voteCount": 1,
        "content": "Correct, A is the answer."
      },
      {
        "date": "2024-04-14T03:21:00.000Z",
        "voteCount": 1,
        "content": "answer is C with minimal overhead"
      },
      {
        "date": "2024-03-18T10:37:00.000Z",
        "voteCount": 1,
        "content": "Both Option A and C work. However, considering Option C involves a lot 'all the AWS accounts,' it undoubtedly increases development overhead"
      },
      {
        "date": "2024-02-06T08:49:00.000Z",
        "voteCount": 1,
        "content": "A is correct: drift detection is the best for this scenario, which utilizes AWS cloudformation\nB and D: using cloudtraid is for monitoring account activities\nC: AWS Config conformance packs cannot make remediation actions. It needs to trigger AWS SSM automation document"
      },
      {
        "date": "2024-05-20T20:00:00.000Z",
        "voteCount": 1,
        "content": "A not correct because not mention how to remediate"
      },
      {
        "date": "2023-09-17T15:48:00.000Z",
        "voteCount": 2,
        "content": "C is right \nhttps://aws.amazon.com/blogs/security/optimize-aws-config-for-aws-security-hub-to-effectively-manage-your-cloud-security-posture/"
      },
      {
        "date": "2023-07-16T20:21:00.000Z",
        "voteCount": 3,
        "content": "Compliance usually indicates towards config"
      },
      {
        "date": "2023-06-20T20:04:00.000Z",
        "voteCount": 2,
        "content": "compliance means aws config,automatic remedy aws config,central dashboard security hub"
      },
      {
        "date": "2023-06-20T12:53:00.000Z",
        "voteCount": 4,
        "content": "(C) This solution meets all of the requirements. AWS Config can monitor resource configurations for compliance with defined rules. The use of AWS Security Hub allows for centralized management of security alerts and compliance checks across all accounts. AWS Config conformance packs allow for automated remediation of non-compliant resources. AWS Security Hub provides a comprehensive view of high-priority security alerts and compliance status across AWS accounts. This solution is also the one with the least development overhead as it uses built-in AWS services specifically designed for configuration management and compliance tracking."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/amazon/view/112699-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Organizations to manage its AWS accounts. The organization root has an OU that is named Environments. The Environments OU has two child OUs that are named Development and Production, respectively.<br><br>The Environments OU and the child OUs have the default FullAWSAccess policy in place. A DevOps engineer plans to remove the FullAWSAccess policy from the Development OU and replace the policy with a policy that allows all actions on Amazon EC2 resources.<br><br>What will be the outcome of this policy replacement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll users in the Development OU will be allowed all API actions on all resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll users in the Development OU will be denied all API actions on all resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll users in the Development OU will be denied all API actions on EC2 resources. All other API actions will be allowed."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-30T22:05:00.000Z",
        "voteCount": 10,
        "content": "The key point is that \"SCP inheritance works differently for Allow and Deny policies\". Allowed policies are only inherited if the children don't have any Allow policy. Once they have an allow policy, only actions defined in that policy will be allowed and no \"Allow\" policy will be inherited from the parent(s) OUs. What inherits is the implicit Deny policy which is a hidden policy sitting above all.\n\nCheck the tables in this link:\nhttps://aws.amazon.com/blogs/security/get-more-out-of-service-control-policies-in-a-multi-account-environment/"
      },
      {
        "date": "2024-05-07T15:16:00.000Z",
        "voteCount": 1,
        "content": "Very good link about SCPs."
      },
      {
        "date": "2024-04-11T03:56:00.000Z",
        "voteCount": 6,
        "content": "I've just tested in my AWS account with the same scenario. I removed the SCP from the dev env and kept the EC2 policy, which by that I was denied access to all other operations except EC2."
      },
      {
        "date": "2024-10-14T06:27:00.000Z",
        "voteCount": 1,
        "content": "Best explanation I found in this forum\n\nFrom: learnwithaniket\n\"For a permission to be allowed for a specific account, there must be an explicit Allow statement at every level from the root through each OU in the direct path to the account (including the target account itself). This is why when you enable SCPs, AWS Organizations attaches an AWS managed SCP policy named FullAWSAccess which allows all services and actions. If this policy is removed and not replaced at any level of the organization, all OUs and accounts under that level would be blocked from taking any actions.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html\""
      },
      {
        "date": "2024-04-08T05:07:00.000Z",
        "voteCount": 2,
        "content": "Note: Adding an SCP with full AWS access doesn\u2019t give all the principals in an account access to everything. SCPs don\u2019t grant permissions; they are used to filter permissions. Principals still need a policy within the account that grants them access."
      },
      {
        "date": "2024-03-16T09:10:00.000Z",
        "voteCount": 1,
        "content": "A - Inherited SCPs cannot be removed so FullAWSAccess will still apply"
      },
      {
        "date": "2024-04-11T03:55:00.000Z",
        "voteCount": 2,
        "content": "no, I've just tested it in my account now, and B is the true answer. Although there were inherited SCPs coming from root and env which still showed in the SCP page for that OU, after detaching the allow all SCP,  I was denied access on any other API except EC2."
      },
      {
        "date": "2024-02-06T08:59:00.000Z",
        "voteCount": 2,
        "content": "B is correct: SCP have allow statement and this matchs"
      },
      {
        "date": "2024-01-23T20:00:00.000Z",
        "voteCount": 1,
        "content": "a is the answer"
      },
      {
        "date": "2023-12-01T20:49:00.000Z",
        "voteCount": 1,
        "content": "should be B, see example in here: https://aws.amazon.com/blogs/security/get-more-out-of-service-control-policies-in-a-multi-account-environment/"
      },
      {
        "date": "2023-11-30T07:20:00.000Z",
        "voteCount": 2,
        "content": "Answer is A: You can't remove heritage policy from child OU"
      },
      {
        "date": "2023-11-16T21:01:00.000Z",
        "voteCount": 3,
        "content": "B is the right answer.\nFor a permission to be allowed for a specific account, there must be an explicit Allow statement at every level from the root through each OU in the direct path to the account (including the target account itself). This is why when you enable SCPs, AWS Organizations attaches an AWS managed SCP policy named FullAWSAccess which allows all services and actions. If this policy is removed and not replaced at any level of the organization, all OUs and accounts under that level would be blocked from taking any actions.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html"
      },
      {
        "date": "2023-10-19T18:51:00.000Z",
        "voteCount": 4,
        "content": "\"SCP evaluation follows a deny-by-default model, meaning that any permissions not explicitly allowed in the SCPs are denied. If an allow statement is not present in the SCPs at any of the levels such as Root, Production OU or Account B, the access is denied.\"\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_evaluation.html#:~:text=SCP%20evaluation%20follows%20a%20deny%2Dby%2Ddefault%20model%2C%20meaning%20that%20any%20permissions%20not%20explicitly%20allowed%20in%20the%20SCPs%20are%20denied.%20If%20an%20allow%20statement%20is%20not%20present%20in%20the%20SCPs%20at%20any%20of%20the%20levels%20such%20as%20Root%2C%20Production%20OU%20or%20Account%20B%2C%20the%20access%20is%20denied."
      },
      {
        "date": "2023-10-12T03:29:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritance_mgmt.html"
      },
      {
        "date": "2023-11-15T12:48:00.000Z",
        "voteCount": 1,
        "content": "This URL does not explain the SCP."
      },
      {
        "date": "2023-08-22T07:37:00.000Z",
        "voteCount": 3,
        "content": "Even the default policy is removed, Child OU will inherit the SCP from the Environment OU, which is AWSFullAccess. So the Child OU will still have full access."
      },
      {
        "date": "2023-08-19T17:57:00.000Z",
        "voteCount": 2,
        "content": "Its A, the new policy is an allow policy not deny, thus all permissions are gratned to Dev OU."
      },
      {
        "date": "2023-08-16T19:45:00.000Z",
        "voteCount": 3,
        "content": "SCP can define An allow list \u2013 actions are prohibited by default, and you specify what services and actions are allowed."
      },
      {
        "date": "2023-08-03T07:33:00.000Z",
        "voteCount": 4,
        "content": "A is correct. \nDevelopment OU will inherit FullAccess from the Environments OU\nno explicit DENY in the new AllowAllEc2 Policy"
      },
      {
        "date": "2023-08-11T09:37:00.000Z",
        "voteCount": 3,
        "content": "The answer is B.\n\nWhen a policy is removed from an OU, the default policy for the parent OU is inherited. In this case, the default policy for the Environments OU is FullAWSAccess, which allows all API actions on all resources.\n\nWhen the DevOps engineer replaces the FullAWSAccess policy with a policy that allows all actions on Amazon EC2 resources, the new policy will take precedence over the default policy. This means that all users in the Development OU will be allowed all API actions on EC2 resources. All other API actions will be denied."
      },
      {
        "date": "2023-08-19T17:57:00.000Z",
        "voteCount": 1,
        "content": "The last part is wrong. SCP doesnt deny anything unless you explicit define it."
      },
      {
        "date": "2023-11-25T07:46:00.000Z",
        "voteCount": 1,
        "content": "because SCPs define the maximum permissions for an organization or organizational unit (OU) in AWS Organizations. \nIf an SCP doesn\u2019t explicitly grant permissions for an action, then that action is implicitly denied."
      },
      {
        "date": "2023-11-25T07:48:00.000Z",
        "voteCount": 1,
        "content": "link;\nhttps://repost.aws/questions/QUSHz1PpiJTOqWRuguGn_Trw/resource-and-iam-policy-with-scp"
      },
      {
        "date": "2023-06-26T03:05:00.000Z",
        "voteCount": 4,
        "content": "B is the correct option."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/amazon/view/119654-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is examining its disaster recovery capability and wants the ability to switch over its daily operations to a secondary AWS Region. The company uses AWS CodeCommit as a source control tool in the primary Region.<br><br>A DevOps engineer must provide the capability for the company to develop code in the secondary Region. If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodeCommit repository in the secondary Region. Create an AWS CodeBuild project to perform a Git mirror operation of the primary Region's CodeCommit repository to the secondary Region's CodeCommit repository. Create an AWS Lambda function that invokes the CodeBuild project. Create an Amazon EventBridge rule that reacts to merge events in the primary Region's CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket in the secondary Region. Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket. Create an AWS Lambda function that initiates the Fargate task. Create an Amazon EventBridge rule that reacts to merge events in the CodeCommit repository. Configure the EventBridge rule to invoke the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeArtifact repository in the secondary Region. Create an AWS CodePipeline pipeline that uses the primary Region\u2019s CodeCommit repository for the source action. Create a cross-Region stage in the pipeline that packages the CodeCommit repository contents and stores the contents in the CodeArtifact repository when a pull request is merged into the CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Cloud9 environment and a CodeCommit repository in the secondary Region. Configure the primary Region's CodeCommit repository as a remote repository in the AWS Cloud9 environment. Connect the secondary Region's CodeCommit repository to the AWS Cloud9 environment."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-02T12:27:00.000Z",
        "voteCount": 1,
        "content": "Why is not D a solution? If the developers in the secondary region can configure primary region's codecommit repository as a remote repository in the AWS Cloud9 environment they can do development and do all git functions remote."
      },
      {
        "date": "2024-06-21T22:52:00.000Z",
        "voteCount": 1,
        "content": "A: (NO)\t\"Create an AWS CodeBuild project to perform a Git mirror operation of the primary Region's CodeCommit repository to the secondary Region's CodeCommit repository.\"\nCodeBuild doesn't have the ability to do a \"git mirror\" operation itself.  All online examples have CodeCommit actions calling Lambda (directly or through EventWatch) which calls fargate (or EC2) which does the actual git mirror\n\nA: (NO)\t\"Create an AWS Lambda function that invokes the CodeBuild project.\nThe is exactly the reverse from online examples\n\nB: (NO) \"Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket.\"\nDoes it really make sense to use a \"git\" mirror operations copy from a CodeCommit repo to an S3 bucket?  All online examples using \"git\" \"mirror\" have CodeCommit repo as remote target."
      },
      {
        "date": "2024-06-21T22:54:00.000Z",
        "voteCount": 1,
        "content": "The specific requirement here isn't \"disaster recovery capability\" and \"ability to switch over its daily operations to a secondary AWS Region.\"  That is just being investigated.\nThe specific requirement is to \"provide the capability for the company to develop code in the secondary Region.\"\n\"If the company needs to use the secondary Region, developers can add an additional remote URL to their local Git configuration.\"\n\nTo me this sounds to like the specific requirement here is only to provide developers with a complete remote development environment (not to provide a DR solution)\nIf that is true, then using Cloud9 web development environment (includes git, etc.) with same local CodeCommit repo is acceptiable\nI'm not a developer, but the specific criteria wording and logic make me lean towards \"C\""
      },
      {
        "date": "2024-06-21T23:01:00.000Z",
        "voteCount": 2,
        "content": "Actually, I meant to say I lean towards \"D\" (using Cloud9 as remote development environment)"
      },
      {
        "date": "2024-06-21T23:24:00.000Z",
        "voteCount": 1,
        "content": "Also want to add that \"D\" would would work fine if you presume that the \"git\" \"mirror\" is also being done (though additional undefined step in the solution).  Nothing says \"D\" is the complete solution.  The ONLY requirement here is to provide developers a remote environment to develop in."
      },
      {
        "date": "2024-06-21T22:55:00.000Z",
        "voteCount": 2,
        "content": "Flow:\n\nAWS Example:  CodeCommit(action) ----------------&gt; Lambda -&gt; Fargate task (\"git clone --mirror\" local repo, \"git remote set-url --push origin\" destination repo) -&gt; CodeCommit(remote repo)\n\nSolution \"B\": CodeCommit(action) -&gt; EventBridge -&gt; Lambda -&gt; Fargate task (\"git clone --mirror\" local repo, \"git remote set-url --push origin\" destination repo) -&gt; S3(remote bucket)\n\nReferences:\nhttps://aws.amazon.com/blogs/devops/replicate-aws-codecommit-repository-between-regions-using-aws-fargate/\nhttps://aws.amazon.com/cloud9/"
      },
      {
        "date": "2024-02-06T20:11:00.000Z",
        "voteCount": 3,
        "content": "A is correct: &lt;A DevOps engineer must provide the capability for the company to develop code in the secondary Region&gt; means code commit"
      },
      {
        "date": "2024-02-09T06:24:00.000Z",
        "voteCount": 2,
        "content": "A is correct: &lt; develop code in the secondary Region&gt;: code commit cannot automatically clone cross-region.Must use a tool to do this duplication task\nB: Using S3 as a secondary repo is incorrect\nC and D: no mention of using codecommit as the secondary repo"
      },
      {
        "date": "2024-01-10T12:49:00.000Z",
        "voteCount": 2,
        "content": "Agree answer is A"
      },
      {
        "date": "2023-12-08T07:19:00.000Z",
        "voteCount": 2,
        "content": "B- It does the replication out of the box and meets the requirements\nhttps://aws.amazon.com/blogs/devops/replicate-aws-codecommit-repository-between-regions-using-aws-fargate/"
      },
      {
        "date": "2023-12-17T11:01:00.000Z",
        "voteCount": 1,
        "content": "This part of B is incorrect. -  It should use Code commit instead of S3. \"Create an AWS Fargate task to perform a Git mirror operation of the primary Region's CodeCommit repository and copy the result to the S3 bucket. \""
      },
      {
        "date": "2023-11-18T21:41:00.000Z",
        "voteCount": 2,
        "content": "A is at least operation and cost"
      },
      {
        "date": "2023-09-23T01:11:00.000Z",
        "voteCount": 4,
        "content": "This solution meets all of the company's requirements:\n\nIt allows developers to add an additional remote URL to their local Git configuration to develop code in the secondary Region.\nIt is automated: the EventBridge rule will automatically invoke the Lambda function whenever a merge event occurs in the primary Region's CodeCommit repository.\nIt is reliable: the CodeBuild project will use Git to ensure that a perfect copy of the primary Region's CodeCommit repository is created in the secondary Region."
      },
      {
        "date": "2023-09-20T06:24:00.000Z",
        "voteCount": 4,
        "content": "https://dev.to/apatil88/replicate-aws-codecommit-repositories-between-regions-using-codebuild-and-codepipeline-5fh1"
      },
      {
        "date": "2023-09-01T07:07:00.000Z",
        "voteCount": 2,
        "content": "B is right. https://aws.amazon.com/cn/blogs/devops/replicate-aws-codecommit-repository-between-regions-using-aws-fargate/"
      },
      {
        "date": "2023-09-23T02:06:00.000Z",
        "voteCount": 3,
        "content": "B is wrong because it uses S3. Developers need a valid git remote URL.\nCorrect answer is A"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/amazon/view/119637-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps team is merging code revisions for an application that uses an Amazon RDS Multi-AZ DB cluster for its production database. The DevOps team uses continuous integration to periodically verify that the application works. The DevOps team needs to test the changes before the changes are deployed to the production database.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a buildspec file in AWS CodeBuild to restore the DB cluster from a snapshot of the production database, run integration tests, and drop the restored database after verification.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application to production. Configure an audit log of data control language (DCL) operations to capture database activities to perform if verification fails.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of the DB cluster before deploying the application. Use the Update requires:Replacement property on the DB instance in AWS CloudFormation to deploy the application and apply the changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates. Fail over to the standby instance if verification fails."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-10T08:14:00.000Z",
        "voteCount": 5,
        "content": "A is the solution which will allow testing without any such consequence"
      },
      {
        "date": "2024-07-29T01:06:00.000Z",
        "voteCount": 2,
        "content": "A correct as allow testing before real deployment. \nhttps://aws.amazon.com/blogs/devops/enhancing-automated-database-continuous-integration-with-aws-codebuild-and-amazon-rds-database-snapshot/"
      },
      {
        "date": "2024-02-09T06:43:00.000Z",
        "voteCount": 4,
        "content": "A is correct: This option allow testing before real deployment\nB: &lt; Deploy the application to production&gt; : this would not allow testing before changes are made\nC: &lt;Create a snapshot of the DB cluster before deploying the application&gt;: This means the same as B - would not allow testing before changes are made\nD: &lt;Ensure that the DB cluster is a Multi-AZ deployment. Deploy the application with the updates&gt; - This deploy the app before testing, so it is incoorect"
      },
      {
        "date": "2024-02-06T20:13:00.000Z",
        "voteCount": 1,
        "content": "A is correct: &lt;The DevOps team uses continuous integration to periodically verify that the application works&gt; and &lt;The DevOps team needs to test the changes before the changes are deployed to the production database&gt; means codebuild"
      },
      {
        "date": "2023-09-23T01:30:00.000Z",
        "voteCount": 1,
        "content": "This solution meets all of the company's requirements:\n\nIt allows the DevOps team to test the changes before they are deployed to the production database.\nIt is automated: the CodeBuild buildspec file will automatically restore the DB cluster from a snapshot, run the integration tests, and drop the restored database after verification.\nIt is reliable: the CodeBuild buildspec file will ensure that the integration tests are run against a copy of the production database."
      },
      {
        "date": "2023-09-01T07:08:00.000Z",
        "voteCount": 4,
        "content": "A is right. All others will change the prod db."
      },
      {
        "date": "2023-09-01T03:00:00.000Z",
        "voteCount": 2,
        "content": "I think it is A\n\nhttps://aws.amazon.com/blogs/devops/enhancing-automated-database-continuous-integration-with-aws-codebuild-and-amazon-rds-database-snapshot/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/amazon/view/119638-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company manages a multi-tenant environment in its VPC and has configured Amazon GuardDuty for the corresponding AWS account. The company sends all GuardDuty findings to AWS Security Hub.<br><br>Traffic from suspicious sources is generating a large number of findings. A DevOps engineer needs to implement a solution to automatically deny traffic across the entire VPC when GuardDuty discovers a new suspicious source.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GuardDuty threat list. Configure GuardDuty to reference the list. Create an AWS Lambda function that will update the threat list. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS WAF web ACL that includes a custom rule group. Create an AWS Lambda function that will create a block rule in the custom rule group. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a firewall in AWS Network Firewall. Create an AWS Lambda function that will create a Drop action rule in the firewall policy. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that will create a GuardDuty suppression rule. Configure the Lambda function to run in response to new Security Hub findings that come from GuardDuty."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-01T03:02:00.000Z",
        "voteCount": 14,
        "content": "I think C:\nhttps://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/"
      },
      {
        "date": "2023-09-02T07:41:00.000Z",
        "voteCount": 1,
        "content": "Sorry i means B"
      },
      {
        "date": "2023-11-05T06:37:00.000Z",
        "voteCount": 1,
        "content": "You mean C?"
      },
      {
        "date": "2023-09-22T15:48:00.000Z",
        "voteCount": 10,
        "content": "C is correct . Only Network Firewall can block traffic at VPC level.  \nA only updates the list , no blocking action\nB-  WAF and Web ACL can block only HTTPS traffic  for a API/VPC endpoint/ Cloudfron distribution not for enire VPC"
      },
      {
        "date": "2024-07-29T01:10:00.000Z",
        "voteCount": 1,
        "content": "C, AWS Network Firewall can block traffic at VPC level.\nhttps://aws.amazon.com/blogs/security/automatically-block-suspicious-traffic-with-aws-network-firewall-and-amazon-guardduty/"
      },
      {
        "date": "2024-07-02T14:37:00.000Z",
        "voteCount": 1,
        "content": "B blocks traffic at the http/https web traffic layer not for VPC layer"
      },
      {
        "date": "2024-02-06T20:16:00.000Z",
        "voteCount": 3,
        "content": "C is correct: &lt;a solution to automatically deny traffic&gt; means network FW. \nA: irrelevant\nB: We need network fw, not WAF\nD: irrelevant"
      },
      {
        "date": "2023-11-25T13:16:00.000Z",
        "voteCount": 1,
        "content": "hmmm\nis this the last question as of now(25th Nov 23)"
      },
      {
        "date": "2023-09-23T01:38:00.000Z",
        "voteCount": 6,
        "content": "Here's the rationale for choosing this option:\n\nAWS Network Firewall:\nAWS Network Firewall is designed to provide centralized network traffic inspection and filtering. It's a suitable choice for implementing network-level controls.\n\nLambda Function for Automation:\nCreating a Lambda function to trigger the creation of a Drop action rule in the firewall policy allows for automated response based on Security Hub findings. This enables you to take immediate action when suspicious sources are detected.\n\nSpecific Action (Drop):\nThe Drop action rule is effective for denying traffic from suspicious sources, effectively controlling access and preventing unwanted traffic.\n\nThis approach aligns well with the requirement to automatically deny traffic when GuardDuty identifies a new suspicious source, enhancing security in the multi-tenant VPC environment."
      },
      {
        "date": "2023-09-20T06:35:00.000Z",
        "voteCount": 1,
        "content": "A only will upadte threat list. the requirement is to block the taffic.\nB is corerect. Also it is event driven immditae action"
      },
      {
        "date": "2023-09-18T03:53:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/amazon/view/126917-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Secrets Manager to store a set of sensitive API keys that an AWS Lambda function uses. When the Lambda function is invoked the Lambda function retrieves the API keys and makes an API call to an external service. The Secrets Manager secret is encrypted with the default AWS Key Management Service (AWS KMS) key.<br><br>A DevOps engineer needs to update the infrastructure to ensure that only the Lambda function\u2019s execution role can access the values in Secrets Manager. The solution must apply the principle of least privilege.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the default KMS key for Secrets Manager to allow only the Lambda function\u2019s execution role to decrypt",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a KMS customer managed key that trusts Secrets Manager and allows the Lambda function's execution role to decrypt. Update Secrets Manager to use the new customer managed key\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a KMS customer managed key that trusts Secrets Manager and allows the account's root principal to decrypt. Update Secrets Manager to use the new customer managed key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the Lambda function\u2019s execution role has the KMS permissions scoped on the resource level. Configure the permissions so that the KMS key can encrypt the Secrets Manager secret\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove all KMS permissions from the Lambda function\u2019s execution role"
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-06T20:21:00.000Z",
        "voteCount": 5,
        "content": "B and D are correct: &lt;update the infrastructure to ensure that only the Lambda function\u2019s execution role&gt; means we need to ensure that lambda's IAM role has sufficient permissions and KMS policy allows Lambda's IAM role\nA: cannot update default key\nC: &lt;allows the account's root principal to decrypt&gt; this against the principal of least privilege\nE: irrelevant"
      },
      {
        "date": "2024-09-23T12:43:00.000Z",
        "voteCount": 1,
        "content": "If default keys are the same as the AWS managed keys, then the answer is B. You cannot modify the \"default\" key's policy to allow access only from the Lambda execution role."
      },
      {
        "date": "2024-07-29T01:17:00.000Z",
        "voteCount": 2,
        "content": "I go for BD"
      },
      {
        "date": "2024-03-09T10:37:00.000Z",
        "voteCount": 2,
        "content": "The requirement is to update the infrastructure to ensure that only the Lambda function\u2019s execution\nrole can access the values in Secrets Manager. The solution must apply the principle of least\nprivilege, which means granting the minimum permissions necessary to perform a task."
      },
      {
        "date": "2024-02-04T02:49:00.000Z",
        "voteCount": 1,
        "content": "{\n\"Version\": \"2012-10-17\",\n\"Id\": \"key-consolepolicy-2\",\n\"Statement\": [\n{\n\"Sid\": \"Allow use of the key\",\n\"Effect\": \"Allow\",\n\"Principal\": {\"AWS\": [\n\"arn:aws:iam::111122223333:role/KeyCreatorRole\"\n]},\n\"Action\": [\n\"kms:Encrypt\",\n\"kms:Decrypt\",\n\"kms:ReEncrypt*\",\n\"kms:GenerateDataKey*\",\n\"kms:DescribeKey\"\n],\n\"Resource\": here arn of secret manager\n}\n]\n}\n\nI think A is correct answer , why to create CMK as customer is using default KMS"
      },
      {
        "date": "2023-11-30T07:30:00.000Z",
        "voteCount": 4,
        "content": "I think B:D"
      },
      {
        "date": "2023-11-23T02:25:00.000Z",
        "voteCount": 4,
        "content": "B, D\nA is incorrect because updating the default KMS key for Secrets Manager to allow only the Lambda function's execution role to decrypt would grant access to all other resources using the default key, which violates the principle of least privilege.\n\nC is incorrect because allowing the account's root principal to decrypt the secret would grant unnecessary access to the secret, which violates the principle of least privilege.\n\nE is incorrect because removing all KMS permissions from the Lambda function's execution role would prevent the Lambda function from decrypting the secret, which is required for it to function properly."
      },
      {
        "date": "2024-02-04T02:42:00.000Z",
        "voteCount": 1,
        "content": "{\n  \"Version\": \"2012-10-17\",\n  \"Id\": \"key-consolepolicy-2\",\n  \"Statement\": [\n    {\n      \"Sid\": \"Allow use of the key\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\"AWS\": [\n        \"arn:aws:iam::111122223333:role/KeyCreatorRole\"\n      ]},\n      \"Action\": [\n        \"kms:Encrypt\",\n        \"kms:Decrypt\",\n        \"kms:ReEncrypt*\",\n        \"kms:GenerateDataKey*\",\n        \"kms:DescribeKey\"\n      ],\n      \"Resource\": here arn of secret manager\n    }\n  ]\n}\n\nI think A is correct answer , why to create CMK as customer is using default KMS"
      },
      {
        "date": "2024-02-04T02:46:00.000Z",
        "voteCount": 1,
        "content": "Or we can ad below condition also \n\n\"Condition\": {\n        \"StringEquals\": {\n          \"kms:CallerAccount\": \"111122223333\",\n          \"kms:ViaService\": \"secretsmanager.us-west-2.amazonaws.com\"\n        }\n      }"
      },
      {
        "date": "2023-11-22T06:15:00.000Z",
        "voteCount": 2,
        "content": "I vote B,D"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/amazon/view/127016-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's DevOps engineer is creating an AWS Lambda function to process notifications from an Amazon Simple Notification Service (Amazon SNS) topic. The Lambda function will process the notification messages and will write the contents of the notification messages to an Amazon RDS Multi-AZ DB instance.<br><br>During testing, a database administrator accidentally shut down the DB instance. While the database was down the company lost several of the SNS notification messages that were delivered during that time.<br><br>The DevOps engineer needs to prevent the loss of notification messages in the future.<br><br>Which solutions will meet this requirement? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the RDS Multi-AZ DB instance with an Amazon DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Simple Queue Service (Amazon SQS) queue as a destination of the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the SNS topic with an Amazon EventBridge event bus. Configure an EventBridge rule on the new event bus to invoke the Lambda function for each event."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-23T05:29:00.000Z",
        "voteCount": 11,
        "content": "The two solutions that will meet the requirement of preventing the loss of notification messages in the future are:\n\nD. Subscribe an Amazon Simple Queue Service (Amazon SQS) queue to the SNS topic. Configure the Lambda function to process messages from the SQS queue.\n\nThis solution will ensure that notification messages are delivered to the SQS queue even if the Lambda function is unavailable or the RDS DB instance is down. The Lambda function can then process the messages from the SQS queue at its own pace.\n\nC. Configure an Amazon Simple Queue Service (Amazon SQS) dead-letter queue for the SNS topic.\n\nThis solution will ensure that notification messages that cannot be delivered to the RDS DB instance are not lost. Instead, they will be moved to a dead-letter queue. The DevOps engineer can then manually process the messages from the dead-letter queue."
      },
      {
        "date": "2023-11-30T07:34:00.000Z",
        "voteCount": 5,
        "content": "C:D , D is a best practice for this scenario, C because you can send failed SNS o SQS Dead letter queue, https://docs.aws.amazon.com/sns/latest/dg/sns-dead-letter-queues.html"
      },
      {
        "date": "2024-10-10T00:34:00.000Z",
        "voteCount": 1,
        "content": "BD\nC.  Dead-letter queues can only be added to SNS subscriptions, not to topics."
      },
      {
        "date": "2024-07-22T07:03:00.000Z",
        "voteCount": 2,
        "content": "AD.\n\nC is wrong, \"Configuring an Amazon SNS dead-letter queue for a subscription\" not for SNS topic\nA is correct, with Dynamodb, admin can no longer \"accidentally shut down the DB instance.\"\n\nA fixes the root cause. With D an SQS is there, no need for DLQ for SNS. If lambda process data from SQS, what is SNS DLQ help here?"
      },
      {
        "date": "2024-02-06T20:27:00.000Z",
        "voteCount": 4,
        "content": "C and D are correct: &lt;. While the database was down the company lost several of the SNS notification messages that were delivered during that time&gt; means dead-letter queue in SQS and output SNS to SQS to store dead-letter queue"
      },
      {
        "date": "2023-11-28T07:52:00.000Z",
        "voteCount": 1,
        "content": "B &amp; D are correct"
      },
      {
        "date": "2024-06-24T13:55:00.000Z",
        "voteCount": 2,
        "content": "Here's what I get when you break it down graphically between CD and BC:\n\nCD:  SNS &gt; SQS|DLQ) &gt; Lambda &gt; RDS\nBD:  SNS &gt; SQS &gt; Lambda &gt; SQS &gt; RDS\nThe DLQ is just there to handle any SNS messages that have errors and can't be processed.  There is no way you want/need two SQS queues in series (on either side of the Lambda).    The ONLY thing you need to add for the requirements is queue to hold stuff while DB is down.  The DLQ just makes sure even an messed up message data is captured for later review.  Only C&amp;D make any sense here."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/amazon/view/127270-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an application that runs on Amazon EC2 instances. The company uses an AWS CodePipeline pipeline to deploy the application into multiple AWS Regions. The pipeline is configured with a stage for each Region. Each stage contains an AWS CloudFormation action for each Region.<br><br>When the pipeline deploys the application to a Region, the company wants to confirm that the application is in a healthy state before the pipeline moves on to the next Region. Amazon Route 53 record sets are configured for the application in each Region. A DevOps engineer creates a Route 53 health check that is based on an Amazon CloudWatch alarm for each Region where the application is deployed.<br><br>What should the DevOps engineer do next to meet the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Step Functions workflow to check the state of the CloudWatch alarm. Configure the Step Functions workflow to exit with an error if the alarm is in the ALARM state. Create a new stage in the pipeline between each Region deployment stage. In each new stage, include an action to invoke the Step Functions workflow.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an AWS CodeDeploy application to deploy a CloudFormation template with automatic rollback. Configure the CloudWatch alarm as the instance health check for the CodeDeploy application. Remove the CloudFormation actions from the pipeline. Create a CodeDeploy action in the pipeline stage for each Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action for the new stage to check the state of the CloudWatch alarm and to exit with an error if the alarm is in the ALARM state",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the CloudWatch agent on the EC2 instances to report the application status to the Route 53 health check. Create a new pipeline stage for each Region where the application is deployed. Configure a CloudWatch alarm action to exit with an error if the CloudWatch alarm is in the ALARM state."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-23T14:15:00.000Z",
        "voteCount": 1,
        "content": "There are no such things as cloudwatch alarm actions. The only things alarms can do is send notifications to an SNS topic. You can perform actions by using EventBridge (CloudWatch Log events) or Step Functions."
      },
      {
        "date": "2024-07-09T14:43:00.000Z",
        "voteCount": 1,
        "content": "D seems to be simple solution for me\nThe CloudWatch agent on EC2 instances can be configured to report the application status, and this information can then be used by Route 53 health checks.\nCreate Route 53 health checks that are based on the CloudWatch alarms. When you create a health check in Route 53, you can specify that the health check should be based on the state of a CloudWatch alarm. Route 53 health checks can be configured to treat the CloudWatch alarm state as Healthy or Unhealthy."
      },
      {
        "date": "2024-02-09T05:27:00.000Z",
        "voteCount": 1,
        "content": "D is correct - https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/monitoring-cloudwatch.html"
      },
      {
        "date": "2024-02-06T20:49:00.000Z",
        "voteCount": 3,
        "content": "A is correct: &lt;confirm that the application is in a healthy state before the pipeline moves on to the next Region.&gt; means we need a new stage\nB and C: no mention of creating a new stage\nD: irrelevant"
      },
      {
        "date": "2024-01-15T12:29:00.000Z",
        "voteCount": 4,
        "content": "Exact scenario for Step usage: different routing options based on choices"
      },
      {
        "date": "2023-11-30T07:38:00.000Z",
        "voteCount": 4,
        "content": "A: https://dev.to/aws-builders/dynamic-build-orchestration-using-codepipeline-codebuild-and-step-functions-2kpa"
      },
      {
        "date": "2023-11-28T08:01:00.000Z",
        "voteCount": 3,
        "content": "A is correct answer"
      },
      {
        "date": "2023-11-26T11:14:00.000Z",
        "voteCount": 3,
        "content": "A - 'If the state machine execution reaches a terminal status of FAILED, TIMED_OUT, or ABORTED, the action execution fails.' https://docs.aws.amazon.com/codepipeline/latest/userguide/action-reference-StepFunctions.html\n\nCan't be D because you can't update a Route53 healhcheck via the Cloudwatch agent"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/amazon/view/126987-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company plans to use Amazon CloudWatch to monitor its Amazon EC2 instances. The company needs to stop EC2 instances when the average of the NetworkPacketsIn metric is less than 5 for at least 3 hours in a 12-hour time window. The company must evaluate the metric every hour. The EC2 instances must continue to run if there is missing data for the NetworkPacketsIn metric during the evaluation period.<br><br>A DevOps engineer creates a CloudWatch alarm for the NetworkPacketsIn metric. The DevOps engineer configures a threshold value of 5 and an evaluation period of 1 hour.<br><br>Which set of additional actions should the DevOps engineer take to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Datapoints to Alarm value to be 3 out of 12. Configure the alarm to treat missing data as not breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as breaching the threshold. Add an EC2 action to stop the instance when the alarm enters the ALARM state.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Datapoints to Alarm value to be 9 out of 12. Configure the alarm to treat missing data as not breaching the threshold. Add an AWS Systems Manager action to stop the instance when the alarm enters the ALARM state."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T07:43:00.000Z",
        "voteCount": 7,
        "content": "B: This is the reason https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/UsingAlarmActions.html#AddingStopActions"
      },
      {
        "date": "2024-08-21T10:08:00.000Z",
        "voteCount": 1,
        "content": "In CloudWatch alarms, datapoints are the individual metric values collected during each period. Here the period is 1 hour and 1 datapoint every hour collected. So 3 datapoints out of 12 is the alrm state because the network threshold has to be less than 5 for atleast 3 hours to Alarm state. The DevOps Engineer sets evaluation for every hour to look for the threshold value of 5. So if 1 hour has no data it is not breaching threshold. If there The alarm evaluates these datapoints against the conditions you set to determine whether it should trigger an action which is stopping an EC2 instance. My explanation for B"
      },
      {
        "date": "2023-12-29T20:48:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      },
      {
        "date": "2023-11-24T11:36:00.000Z",
        "voteCount": 3,
        "content": "I think B"
      },
      {
        "date": "2023-11-23T07:06:00.000Z",
        "voteCount": 4,
        "content": "B should be corrected"
      },
      {
        "date": "2023-11-23T01:25:00.000Z",
        "voteCount": 3,
        "content": "B should be corrected"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/amazon/view/126988-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company manages 500 AWS accounts that are in an organization in AWS Organizations. The company discovers many unattached Amazon Elastic Block Store (Amazon EBS) volumes in all the accounts. The company wants to automatically tag the unattached EBS volumes for investigation.<br><br>A DevOps engineer needs to deploy an AWS Lambda function to all the AWS accounts. The Lambda function must run every 30 minutes to tag all the EBS volumes that have been unattached for a period of 7 days or more.<br><br>Which solution will meet these requirements in the MOST operationally efficient manner?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization. Create an Amazon EventBridge event bus in the delegated administrator account to invoke the Lambda function in each member account every 30 minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cross-account IAM role in the organization's member accounts. Attach the AWSLambda_FullAccess policy and the AWSCloudFormationFullAccess policy to the role. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Create a custom script in the organization\u2019s management account that assumes the role and deploys the CloudFormation template to the member accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a delegated administrator account for the organization. Create an AWS CloudFormation template that contains the Lambda function and an Amazon EventBridge scheduled rule to invoke the Lambda function every 30 minutes. Use CloudFormation StackSets to deploy the CloudFormation template from the delegated administrator account to all the member accounts in the organization\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cross-account IAM role in the organization's member accounts. Attach the AmazonS3FullAccess policy and the AWSCodeDeployDeployerAccess policy to the role. Use AWS CodeDeploy to assume the role to deploy the Lambda function from the organization's management account. Configure an Amazon EventBridge scheduled rule in the member accounts to invoke the Lambda function every 30 minutes."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-07T06:28:00.000Z",
        "voteCount": 5,
        "content": "C is correct: &lt;The Lambda function must run every 30 minutes to tag all the EBS volumes&gt;: we should use a combination of eventbridge and Lambda\nA: &lt;. Create an Amazon EventBridge event bus in the delegated administrator account to invoke the Lambda function&gt;: event bridge should be in each member account to monitor event, not in the delegated admin's account\nB and D: These options create an IAM role in every member account, which is incorrect"
      },
      {
        "date": "2024-03-16T08:59:00.000Z",
        "voteCount": 3,
        "content": "C make the most sense"
      },
      {
        "date": "2024-01-12T10:46:00.000Z",
        "voteCount": 3,
        "content": "NOT A: you don't want to run it for every  user accounts"
      },
      {
        "date": "2024-01-11T03:32:00.000Z",
        "voteCount": 3,
        "content": "Agree with C"
      },
      {
        "date": "2024-01-09T13:05:00.000Z",
        "voteCount": 2,
        "content": "Why no A?"
      },
      {
        "date": "2023-11-28T08:09:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-11-24T11:35:00.000Z",
        "voteCount": 2,
        "content": "C makes sense"
      },
      {
        "date": "2023-11-23T01:27:00.000Z",
        "voteCount": 2,
        "content": "I vote C"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/amazon/view/126989-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's production environment uses an AWS CodeDeploy blue/green deployment to deploy an application. The deployment incudes Amazon EC2 Auto Scaling groups that launch instances that run Amazon Linux 2.<br><br>A working appspec.yml file exists in the code repository and contains the following text:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image9.png\"><br><br>A DevOps engineer needs to ensure that a script downloads and installs a license file onto the instances before the replacement instances start to handle request traffic. The DevOps engineer adds a hooks section to the appspec.yml file.<br><br>Which hook should the DevOps engineer use to run the script that downloads and installs the license file?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAfterBlockTraffic",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBeforeBlockTraffic",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBeforeInstall\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownloadBundle"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-07T06:47:00.000Z",
        "voteCount": 6,
        "content": "C is correct: For blue/green deployment, Before install is one of several hooks that come before &lt;the replacement instances&gt; start to handle request traffic.\nA and B: these hooks come after the replacement instances start to handle request traffic. They are hooks from the original instance, which are two of 3 last steps.\nD: There is no such hook in blue/green deployment"
      },
      {
        "date": "2023-11-24T11:47:00.000Z",
        "voteCount": 5,
        "content": "A &amp; B are not available for replacement instances - https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html#reference-appspec-file-structure-hooks-availability\nD - \"Reserved for CodeDeploy operations. Cannot be used to run scripts.\""
      },
      {
        "date": "2024-03-16T08:57:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2024-01-19T01:51:00.000Z",
        "voteCount": 2,
        "content": "is C: A DevOps engineer needs to ensure that a script downloads and \"installs\" a license file onto the instances \"before\" the replacement instances start to handle request traffic"
      },
      {
        "date": "2023-11-23T01:28:00.000Z",
        "voteCount": 3,
        "content": "C should be correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/amazon/view/126990-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an application that includes AWS Lambda functions. The Lambda functions run Python code that is stored in an AWS CodeCommit repository. The company has recently experienced failures in the production environment because of an error in the Python code. An engineer has written unit tests for the Lambda functions to help avoid releasing any future defects into the production environment.<br><br>The company's DevOps team needs to implement a solution to integrate the unit tests into an existing AWS CodePipeline pipeline. The solution must produce reports about the unit tests for the company to view.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the CodeCommit repository with Amazon CodeGuru Reviewer. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a buildspec.yml file in the CodeCommit repository. In the buildspec yml file, define the actions to run a CodeGuru review.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a CodeBuild report group. Create a buildspec.yml file in the CodeCommit repository. In the buildspec.yml file, define the actions to run the unit tests with an output of JUNITXML in the build phase section. Configure the test reports to be uploaded to the new CodeBuild report group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CodeArtifact repository. Create a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create an appspec.yml file in the original CodeCommit repository. In the appspec.yml file, define the actions to run the unit tests with an output of CUCUMBERJSON in the build phase section. Configure the tests reports to be sent to the new CodeArtifact repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS CodeBuild project. In the CodePipeline pipeline, configure a test stage that uses the new CodeBuild project. Create a new Amazon S3 bucket. Create a buildspec.yml file in the CodeCommit repository. In the buildspec yml file, define the actions to run the unit tests with an output of HTML in the phases section. In the reports section, upload the test reports to the S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-07T06:56:00.000Z",
        "voteCount": 6,
        "content": "B is correct: for unit test, we need codebuild \nA: codeguru is for code analysis, not unit test\nC: This option mentions pushing reports to CodeArtifact repository, which is incorrect\nD: This option push reports to S3, which is incorrect. We should upload report to codebuild report group"
      },
      {
        "date": "2023-11-28T08:15:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2023-11-27T06:40:00.000Z",
        "voteCount": 3,
        "content": "I think B as per link:\nhttps://docs.aws.amazon.com/codebuild/latest/userguide/test-reporting.html"
      },
      {
        "date": "2023-11-24T11:53:00.000Z",
        "voteCount": 3,
        "content": "I think it should be B"
      },
      {
        "date": "2023-11-23T01:30:00.000Z",
        "voteCount": 4,
        "content": "B is corrected"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/amazon/view/127130-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company manages multiple AWS accounts in AWS Organizations. The company\u2019s security policy states that AWS account root user credentials for member accounts must not be used. The company monitors access to the root user credentials.<br><br>A recent alert shows that the root user in a member account launched an Amazon EC2 instance. A DevOps engineer must create an SCP at the organization's root level that will prevent the root user in member accounts from making any AWS service API calls.<br><br>Which SCP will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image10.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image11.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image12.png\"><br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image13.png\">"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-24T12:00:00.000Z",
        "voteCount": 8,
        "content": "I believe it should be C\nhttps://docs.aws.amazon.com/organizations/latest/userguide/best-practices_member-acct.html#bp_member-acct_use-scp"
      },
      {
        "date": "2024-02-07T07:09:00.000Z",
        "voteCount": 6,
        "content": "C is correct: &lt; will prevent the root user in member accounts&gt; this means deny action\nA and D: irrelevant (mention allow statement)\nB: scp does not have principal element. only condition"
      },
      {
        "date": "2024-06-25T16:23:00.000Z",
        "voteCount": 1,
        "content": "A slightly more consise version of \"C\" is a \"strongly recommended\" control to deny root access in member accounts.  See the example:\nhttps://docs.aws.amazon.com/controltower/latest/controlreference/strongly-recommended-controls.html#disallow-root-auser-actions"
      },
      {
        "date": "2024-04-23T10:10:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/best-practices_member-acct.html#bp_member-acct_use-scp"
      },
      {
        "date": "2024-03-16T08:53:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-02-29T17:23:00.000Z",
        "voteCount": 2,
        "content": "C no debate"
      },
      {
        "date": "2023-12-09T00:47:00.000Z",
        "voteCount": 4,
        "content": "It's C, based on the documentation :\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_examples_general.html#example-scp-root-user"
      },
      {
        "date": "2023-11-28T08:16:00.000Z",
        "voteCount": 2,
        "content": "C looks correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/amazon/view/127013-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS and has a VPC that contains critical compute infrastructure with predictable traffic patterns. The company has configured VPC flow logs that are published to a log group in Amazon CloudWatch Logs.<br><br>The company's DevOps team needs to configure a monitoring solution for the VPC flow logs to identify anomalies in network traffic to the VPC over time. If the monitoring solution detects an anomaly, the company needs the ability to initiate a response to the anomaly.<br><br>How should the DevOps team configure the monitoring solution to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis data stream. Subscribe the log group to the data stream. Configure Amazon Kinesis Data Analytics to detect log anomalies in the data stream. Create an AWS Lambda function to use as the output of the data stream. Configure the Lambda function to write to the default Amazon EventBridge event bus in the event of an anomaly finding.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Firehose delivery stream that delivers events to an Amazon S3 bucket. Subscribe the log group to the delivery stream. Configure Amazon Lookout for Metrics to monitor the data in the S3 bucket for anomalies. Create an AWS Lambda function to run in response to Lookout for Metrics anomaly findings. Configure the Lambda function to publish to the default Amazon EventBridge event bus.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to detect anomalies. Configure the Lambda function to publish an event to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Subscribe the Lambda function to the log group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis data stream. Subscribe the log group to the data stream. Create an AWS Lambda function to detect log anomalies. Configure the Lambda function to write to the default Amazon EventBridge event bus if the Lambda function detects an anomaly. Set the Lambda function as the processor for the data stream."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-26T12:14:00.000Z",
        "voteCount": 6,
        "content": "I think it's B, Amazon Lookout for metrics can detect anomalies from S3 bucket and trigger Lambda\nhttps://aws.amazon.com/lookout-for-metrics/"
      },
      {
        "date": "2024-08-20T04:29:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      },
      {
        "date": "2024-08-03T02:17:00.000Z",
        "voteCount": 2,
        "content": "- Data Streaming: Use Amazon Kinesis Data Firehose to deliver VPC flow logs from CloudWatch Logs to an Amazon S3 bucket.\n- Anomaly Detection: Amazon Lookout for Metrics will monitor the data in the S3 bucket and automatically detect anomalies in the network traffic.\n- Event Response: When Lookout for Metrics detects an anomaly, it triggers an AWS Lambda function. The Lambda function will then publish an event to the Amazon EventBridge event bus, which can further initiate automated responses, notifications, or alerts."
      },
      {
        "date": "2024-07-09T17:27:00.000Z",
        "voteCount": 2,
        "content": "Although option A uses Kinesis Data Analytics for anomaly detection, setting up and maintaining custom analytics and anomaly detection logic is more complex and less efficient compared to using a managed service like Lookout for Metrics."
      },
      {
        "date": "2024-07-03T13:43:00.000Z",
        "voteCount": 2,
        "content": "A is wrong because kinesis data analytics output must be either kinesis data stream or firehose, can't be lambda directly so there is a missing component"
      },
      {
        "date": "2024-06-25T18:39:00.000Z",
        "voteCount": 2,
        "content": "I've reviewed most of the comments, and it seems like everyone is just repeating themselves.  I've \"googled\" and looked at the references.  I found examples of both kinesis data streams, kinesis data analytics and firehose.  The one step in \"A\" I have a problem with is \"Create an AWS Lambda function to use as the output of the data stream.\"  How can Lambda be an output of a data stream \"over time\"?  I don't think you can identify an anomaly \"over time\" unless you've got persistent storage for the data (which can be reparsed as necessary to compare past with present).  I'm leaning towards \"B\" unless someone can convince me otherwise (and not by just repeating what others have already said)."
      },
      {
        "date": "2024-06-18T23:24:00.000Z",
        "voteCount": 2,
        "content": "Option B involves using Amazon Lookout for Metrics, which is not designed for real-time anomaly detection."
      },
      {
        "date": "2024-06-25T18:50:00.000Z",
        "voteCount": 1,
        "content": "I see the \"over time\" requirement as implying some ability to parse the past with the present in order for ML to assess an anomaly.  I don't see the words \"real time\" in the requirements.  The \"over time\" requirement is not specific enough, but until there are more specifics, it would be reasonable to presume it means your trying to discover current anomalies by comparing traffic from against  days, weeks or months ago."
      },
      {
        "date": "2024-05-02T05:52:00.000Z",
        "voteCount": 2,
        "content": "i think B"
      },
      {
        "date": "2024-04-23T10:26:00.000Z",
        "voteCount": 3,
        "content": "Lookout for Metrics automatically detects and diagnoses anomalies (outliers from the norm) in business and operational data. It\u2019s a fully managed ML service, which uses specialized ML models to detect anomalies based on the characteristics of your data. You don\u2019t need ML experience to use Lookout for Metrics.\n\nKinesis Data Analytics Studio provides an interactive notebook experience powered by Apache Zeppelin and Apache Flink to analyze streaming data. It also helps productionize your analytics application by building and deploying code as a Kinesis data analytics application straight from the notebook. https://aws.amazon.com/blogs/machine-learning/smart-city-traffic-anomaly-detection-using-amazon-lookout-for-metrics-and-amazon-kinesis-data-analytics-studio/"
      },
      {
        "date": "2024-03-28T00:36:00.000Z",
        "voteCount": 1,
        "content": "A. If you google \"detecting anomalies in vpc flow logs\" every article suggests Kinesis Data Analytics"
      },
      {
        "date": "2024-03-18T04:35:00.000Z",
        "voteCount": 2,
        "content": "I'll go with A. Mainly because Kinesis data analytics has anomoly detection using a random cut forest function: https://docs.aws.amazon.com/kinesisanalytics/latest/dev/app-anomaly-detection.html"
      },
      {
        "date": "2024-03-16T08:51:00.000Z",
        "voteCount": 3,
        "content": "B - Amazon Lookout for Metrics Automatically detect anomalies within metrics and identify their root causes. So would fit the requirements"
      },
      {
        "date": "2024-03-16T02:59:00.000Z",
        "voteCount": 1,
        "content": "Option A is preferable for scenarios requiring real-time processing and anomaly detection in streaming data, such as VPC flow logs, with the capability to quickly initiate responses to detected anomalies. It offers a more streamlined and immediate approach to monitoring and responding to network traffic anomalies, making it highly suitable for the company's needs regarding their critical compute infrastructure with predictable traffic patterns.\n\nOption B might still be considered if the company's workflow is more adapted to batch processing and the delays inherent in data delivery and processing are acceptable. However, for immediate anomaly detection and response, Option A stands out as the more appropriate solution."
      },
      {
        "date": "2024-03-02T16:23:00.000Z",
        "voteCount": 3,
        "content": "Kinesis Data Firehose determines how often to write to S3 by buffer settings, which is not realtime enough to handle VPC flow log, which can be fatal depending on the content of the `CRITICAL compute infrastructure`. Kinesis Data Analytics has machine learning solutions such as RANDOM_CUT_FOREST in addition to fixed detection by normal SQL."
      },
      {
        "date": "2024-02-29T17:24:00.000Z",
        "voteCount": 3,
        "content": "B without a doubt"
      },
      {
        "date": "2024-02-26T10:10:00.000Z",
        "voteCount": 3,
        "content": "Option B is the most suitable for the scenario.\nKinesis Data Firehose: It allows the streaming of data to an S3 bucket, providing a durable storage solution.\nLookout for Metrics: It is designed to detect anomalies in your data and can be configured to monitor the data stored in the S3 bucket for anomalies."
      },
      {
        "date": "2024-02-25T15:48:00.000Z",
        "voteCount": 4,
        "content": "Question keyword :\n- predictable traffic patterns\n- anomalies\n\nThus, B."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/amazon/view/126991-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "AnyCompany is using AWS Organizations to create and manage multiple AWS accounts. AnyCompany recently acquired a smaller company, Example Corp. During the acquisition process, Example Corp's single AWS account joined AnyCompany's management account through an Organizations invitation. AnyCompany moved the new member account under an OU that is dedicated to Example Corp.<br><br>AnyCompany's DevOps engineer has an IAM user that assumes a role that is named OrganizationAccountAccessRole to access member accounts. This role is configured with a full access policy. When the DevOps engineer tries to use the AWS Management Console to assume the role in Example Corp's new member account, the DevOps engineer receives the following error message: \"Invalid information in one or more fields. Check your information or contact your administrator.\"<br><br>Which solution will give the DevOps engineer access to the new member account?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the management account, grant the DevOps engineer's IAM user permission to assume the OrganizationAccountAccessRole IAM role in the new member account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the management account, create a new SCP. In the SCP, grant the DevOps engineer's IAM user full access to all resources in the new member account. Attach the SCP to the OU that contains the new member account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the new member account, create a new IAM role that is named OrganizationAccountAccessRole. Attach the AdministratorAccess AWS managed policy to the role. In the role's trust policy, grant the management account permission to assume the role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the new member account, edit the trust policy for the OrganizationAccountAccessRole IAM role. Grant the management account permission to assume the role."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-23T22:30:00.000Z",
        "voteCount": 1,
        "content": "The question states that the role already exists with full access policy. This role exists in the new member account. We need the IAM user from the management account the ability to assume it."
      },
      {
        "date": "2024-09-09T05:14:00.000Z",
        "voteCount": 1,
        "content": "This role is created by default in member accounts. See:\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html"
      },
      {
        "date": "2024-09-23T22:35:00.000Z",
        "voteCount": 1,
        "content": "\"By default, if you create a member account as part of your organization, AWS automatically creates a role in the account that grants administrator permissions to IAM users in the management account who can assume the role. By default, that role is named OrganizationAccountAccessRole\"\n\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create-cross-account-role.html"
      },
      {
        "date": "2024-04-23T10:38:00.000Z",
        "voteCount": 3,
        "content": "To create an AWS Organizations administrator role in a member account\nSign in to the IAM console at https://console.aws.amazon.com/iam/. You must sign in as an IAM user, assume an IAM role, or sign in as the root user (not recommended) in the member account. The user or role must have permission to create IAM roles and policies.\n\nIn the IAM console, navigate to Roles and then choose Create role.\n\nChoose AWS account, and then select Another AWS account.\n\nEnter the 12-digit account ID number of the management account that you want to grant administrator access to. Under Options, please note the following:\n\nOn the Add permissions page, choose the AWS managed policy named AdministratorAccess and then choose. https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html#orgs_manage_accounts_create-cross-account-role"
      },
      {
        "date": "2024-04-09T00:51:00.000Z",
        "voteCount": 1,
        "content": "not c, the role is already created"
      },
      {
        "date": "2024-06-18T02:23:00.000Z",
        "voteCount": 1,
        "content": "From reading the question, I'm not sure."
      },
      {
        "date": "2024-04-08T04:48:00.000Z",
        "voteCount": 1,
        "content": "D.. the role is already created, what is needed is just update the trust policy"
      },
      {
        "date": "2024-03-16T08:49:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-02-07T07:23:00.000Z",
        "voteCount": 4,
        "content": "C is correct: &lt;assume the role in Example Corp's new member account&gt; means this role has not been properly configured (or even not created)\nA: only mention assuming the role, not create it.\nB: scp has nothing to do here\nD: only mention create trust relationship"
      },
      {
        "date": "2024-01-17T09:57:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-11-28T08:18:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-11-25T03:59:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access.html#orgs_manage_accounts_create-cross-account-role"
      },
      {
        "date": "2023-11-24T12:13:00.000Z",
        "voteCount": 2,
        "content": "For invited accounts the OrganizationAccountAccessRole needs to be created:\nMember accounts that you invite to join your organization do not automatically get an administrator role created. You have to do this manually, as shown in the following procedure. This essentially duplicates the role automatically set up for created accounts. We recommend that you use the same name, OrganizationAccountAccessRole, for your manually created roles for consistency and ease of remembering."
      },
      {
        "date": "2023-11-24T12:14:00.000Z",
        "voteCount": 2,
        "content": "So I believe it's C."
      },
      {
        "date": "2023-11-23T02:00:00.000Z",
        "voteCount": 1,
        "content": "A should be correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/amazon/view/126992-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is designing an application that integrates with a legacy REST API. The application has an AWS Lambda function that reads records from an Amazon Kinesis data stream. The Lambda function sends the records to the legacy REST API.<br><br>Approximately 10% of the records that the Lambda function sends from the Kinesis data stream have data errors and must be processed manually. The Lambda function event source configuration has an Amazon Simple Queue Service (Amazon SQS) dead-letter queue as an on-failure destination. The DevOps engineer has configured the Lambda function to process records in batches and has implemented retries in case of failure.<br><br>During testing, the DevOps engineer notices that the dead-letter queue contains many records that have no data errors and that already have been processed by the legacy REST API. The DevOps engineer needs to configure the Lambda function's event source options to reduce the number of errorless records that are sent to the dead-letter queue.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the retry attempts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the setting to split the batch when an error occurs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the concurrent batches per shard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the maximum age of record."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-23T11:16:00.000Z",
        "voteCount": 7,
        "content": "When consuming records from a Kinesis data stream using AWS Lambda, the function can process records in batches. By default, if any record in the batch fails to process, the entire batch is sent to the dead-letter queue.\nTo avoid sending errorless records to the dead-letter queue, the Lambda function's event source options should be configured to split the batch when an error occurs. This setting is called batchWindow and can be configured in the event source mapping for the Lambda function.\nWhen batchWindow is set to TRIM_HORIZON, the Lambda function will split the batch at the first record that causes an error and send only the failed records to the dead-letter queue. The remaining errorless records in the batch will continue to be processed by the function."
      },
      {
        "date": "2024-06-25T22:14:00.000Z",
        "voteCount": 1,
        "content": "Seemingly very good explanation, though I had trouble finding any references other than this:\n\"BisectBatchOnFunctionError\" \"If the function returns an error, split the batch in two and retry. The default value is false.\"\naws lambda update-event-source-mapping --bisect-batch-on-function-error [...]"
      },
      {
        "date": "2023-11-30T08:53:00.000Z",
        "voteCount": 6,
        "content": "B: https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html#services-kinesis-eventsourcemapping"
      },
      {
        "date": "2024-08-10T15:51:00.000Z",
        "voteCount": 1,
        "content": "B: \nhttps://aws.amazon.com/blogs/big-data/best-practices-for-consuming-amazon-kinesis-data-streams-using-aws-lambda/"
      },
      {
        "date": "2024-02-07T07:46:00.000Z",
        "voteCount": 3,
        "content": "B is correct: &lt;(Amazon SQS) dead-letter queue as an on-failure destination&gt;: split the batch into 2 parts: success ones and error ones. error ones come to dead queue"
      },
      {
        "date": "2023-11-23T11:41:00.000Z",
        "voteCount": 3,
        "content": "B is  correct"
      },
      {
        "date": "2023-11-23T02:02:00.000Z",
        "voteCount": 4,
        "content": "B is corrected"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/amazon/view/129671-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has microservices running in AWS Lambda that read data from Amazon DynamoDB. The Lambda code is manually deployed by developers after successful testing. The company now needs the tests and deployments be automated and run in the cloud. Additionally, traffic to the new versions of each microservice should be incrementally shifted over time after deployment.<br><br>What solution meets all the requirements, ensuring the MOST developer velocity?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodePipeline configuration and set up a post-commit hook to trigger the pipeline after tests have passed. Use AWS CodeDeploy and create a Canary deployment configuration that specifies the percentage of traffic and interval.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeBuild configuration that triggers when the test code is pushed. Use AWS CloudFormation to trigger an AWS CodePipeline configuration that deploys the new Lambda versions and specifies the traffic shift percentage and interval.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodePipeline configuration and set up the source code step to trigger when code is pushed. Set up the build step to use AWS CodeBuild to run the tests. Set up an AWS CodeDeploy configuration to deploy, then select the CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AWS CLI to set up a post-commit hook that uploads the code to an Amazon S3 bucket after tests have passed Set up an S3 event trigger that runs a Lambda function that deploys the new version. Use an interval in the Lambda function to deploy the code over time at the required percentage."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-23T11:22:00.000Z",
        "voteCount": 2,
        "content": "This solution provides the following benefits:\n\nAutomation: The entire process, from code push to testing and deployment, is automated, reducing manual effort and increasing developer velocity.\nIntegration: By using AWS CodePipeline, CodeBuild, and CodeDeploy, you leverage fully managed services that are designed to work together seamlessly.\nIncremental Deployment: The CodeDeployDefault.LambdaLinear10PercentEvery3Minutes option ensures a smooth and controlled migration of traffic to the new versions of your microservices, minimizing the risk of downtime or disruption."
      },
      {
        "date": "2024-03-16T08:45:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2024-03-06T07:54:00.000Z",
        "voteCount": 3,
        "content": "C \nThere is no 'pre-commit' hook option in the Lambda deployment hook (Canary); only 'before allowing traffic' and 'after allowing traffic' options are available. Therefore, the 'LambdaLinear10PercentEvery3Minutes' option, which is a canary deployment method, enables a linear deployment strategy, gradually shifting traffic to the new versions at a rate of 10% every 3 minutes.\nhttps://medium.com/@Da_vidgf/canary-deployments-in-serverless-applications-b0f47fa9b409"
      },
      {
        "date": "2024-02-12T09:56:00.000Z",
        "voteCount": 1,
        "content": "if it is canary, why not a?"
      },
      {
        "date": "2024-02-07T07:48:00.000Z",
        "voteCount": 1,
        "content": "A is correct: canary deployment"
      },
      {
        "date": "2024-01-17T10:05:00.000Z",
        "voteCount": 3,
        "content": "c is correct"
      },
      {
        "date": "2024-01-12T11:07:00.000Z",
        "voteCount": 3,
        "content": "B is wrong, why would you trigger a pipeline when TEST code is pushed"
      },
      {
        "date": "2023-12-29T21:15:00.000Z",
        "voteCount": 4,
        "content": "Agree C is correct"
      },
      {
        "date": "2023-12-29T06:50:00.000Z",
        "voteCount": 3,
        "content": "Answer is C, canary deployment"
      },
      {
        "date": "2023-12-29T04:53:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/amazon/view/129690-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is building a web and mobile application that uses a serverless architecture powered by AWS Lambda and Amazon API Gateway. The company wants to fully automate the backend Lambda deployment based on code that is pushed to the appropriate environment branch in an AWS CodeCommit repository.<br><br>The deployment must have the following:<br><br>\u2022\tSeparate environment pipelines for testing and production<br>\u2022\tAutomatic deployment that occurs for test environments only<br><br>Which steps should be taken to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a new AWS CodePipeline service. Create a CodeCommit repository for each environment. Set up CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create a CodeCommit repository for each environment. Set up each CodePipeline to retrieve the source code from the appropriate repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two AWS CodePipeline configurations for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Set up each CodePipeline to retrieve the source code from the appropriate branch in the repository. Set up the deployment step to deploy the Lambda functions with AWS CloudFormation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeBuild configuration for test and production environments. Configure the production pipeline to have a manual approval step. Create one CodeCommit repository with a branch for each environment. Push the Lambda function code to an Amazon S3 bucket. Set up the deployment step to deploy the Lambda functions from the S3 bucket."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-07T07:53:00.000Z",
        "voteCount": 5,
        "content": "C is correct: &lt;Separate environment pipelines for testing and production&gt; means codepipeline &lt;code that is pushed to the appropriate environment branch in an AWS CodeCommit repository.&gt; means code CodeCommit\nA: no mention of creating Separate env for test and dev\nB: &lt;Create a CodeCommit repository for each environment&gt; should not do this. We should create a branch for each env\nD: no mention of code pipelines"
      },
      {
        "date": "2024-04-23T11:29:00.000Z",
        "voteCount": 3,
        "content": "By creating two CodePipeline configurations, using a single CodeCommit repository with branches for each environment, and deploying Lambda functions with CloudFormation, this solution meets the requirements while following best practices for source code management, continuous delivery, and infrastructure as code."
      },
      {
        "date": "2023-12-29T21:19:00.000Z",
        "voteCount": 3,
        "content": "C is correct\nFirst, A&amp;B both are in-correct: As a basic policy - do not create a repo for the same code for multiple environments. Always create a branch from the same repo. The strategy is wrong for A&amp;B.\nNow C&amp;D: D uses Lambda function with s3, whereas C uses code pipeline to store and build. Using code pipeline is a smart choice rather than using S3 as a code pipeline that offers better branching strategy and controls. I will go with \u2018C\u201d."
      },
      {
        "date": "2023-12-29T06:57:00.000Z",
        "voteCount": 2,
        "content": "It\u2019s C - unique env and also distinct resources in aws codepipeline would result to pull from both repos on every update of either repo."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/amazon/view/129698-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer wants to find a solution to migrate an application from on premises to AWS. The application is running on Linux and needs to run on specific versions of Apache Tomcat, HAProxy, and Varnish Cache to function properly. The application's operating system-level parameters require tuning. The solution must include a way to automate the deployment of new application versions. The infrastructure should be scalable and faulty servers should be replaced automatically.<br><br>Which solution should the DevOps engineer use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the application as a Docker image that contains all the necessary software to Amazon ECR. Create an Amazon ECS cluster using an AWS Fargate launch type and an Auto Scaling group. Create an AWS CodePipeline pipeline that uses Amazon ECR as a source and Amazon ECS as a deployment provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the application code to an AWS CodeCommit repository with a saved configuration file to configure and install the software. Create an AWS Elastic Beanstalk web server tier and a load balanced-type environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the application code to an AWS CodeCommit repository with a set of .ebextensions files to configure and install the software. Create an AWS Elastic Beanstalk worker tier environment that uses the Tomcat solution stack. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and Elastic Beanstalk as a deployment provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the application code to an AWS CodeCommit repository with an appspec.yml file to configure and install the necessary software. Create an AWS CodeDeploy deployment group associated with an Amazon EC2 Auto Scaling group. Create an AWS CodePipeline pipeline that uses CodeCommit as a source and CodeDeploy as a deployment provider.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-29T07:36:00.000Z",
        "voteCount": 7,
        "content": "I go with D, simply because with Fargate you have very limited access to the OS."
      },
      {
        "date": "2024-08-05T21:25:00.000Z",
        "voteCount": 1,
        "content": "keywords: \n- specific versions of Apache Tomcat, HAProxy, and Varnish Cache \n- operating system-level parameters \n\nNot A as Fargate is serverless"
      },
      {
        "date": "2024-04-17T04:23:00.000Z",
        "voteCount": 4,
        "content": "Key point: The application's operating system-level parameters require tuning"
      },
      {
        "date": "2024-04-13T03:30:00.000Z",
        "voteCount": 2,
        "content": "ill go with D"
      },
      {
        "date": "2024-04-13T02:19:00.000Z",
        "voteCount": 3,
        "content": "I doubt this question will come up since both A and D are correct: https://aws.amazon.com/blogs/containers/announcing-additional-linux-controls-for-amazon-ecs-tasks-on-aws-fargate/\n\nthey need to add some words to the question tat force you to choose either A or D"
      },
      {
        "date": "2024-07-29T02:54:00.000Z",
        "voteCount": 1,
        "content": "Me too, I still go for D but seen like new update for AWS Fargate have some change"
      },
      {
        "date": "2024-03-26T06:49:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2024-03-21T05:40:00.000Z",
        "voteCount": 4,
        "content": "While option A is a strong candidate due to its serverless nature and ease of deployment, option D is the most suitable solution given the need for specific software versions, OS-level tuning, and the requirement for a scalable and fault-tolerant infrastructure. Option D provides the necessary control over the software environment and infrastructure configuration, along with the benefits of automation and scalability."
      },
      {
        "date": "2024-03-16T07:51:00.000Z",
        "voteCount": 2,
        "content": "D - The application's operating system-level parameters require tuning - means option D is the only answer. A,B and C dont allow this os level tuning"
      },
      {
        "date": "2024-04-23T11:38:00.000Z",
        "voteCount": 1,
        "content": "Today we\u2019re excited to announce that customers can now tune Linux kernel parameters in ECS tasks on AWS Fargate. Tuning Linux kernel parameters can help customers optimize their network throughput when running containerized network proxies or achieve higher levels of workload resilience by terminating stale connections. This launch provides parity for ECS tasks launched on AWS Fargate and Amazon EC2 container instances. https://aws.amazon.com/blogs/containers/announcing-additional-linux-controls-for-amazon-ecs-tasks-on-aws-fargate/"
      },
      {
        "date": "2024-03-03T19:15:00.000Z",
        "voteCount": 1,
        "content": "AWS Fargate can tune linux kernel parameters.\nhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html"
      },
      {
        "date": "2024-02-09T07:35:00.000Z",
        "voteCount": 4,
        "content": "D is correct: &lt;The application's operating system-level parameters require tuning&gt; means the user need controls over os-level\nA: AWS Fargate and Docker dont provide os-level controls\nB and C: Beanstalk does not support Varnish Cache and no os-level controls provided"
      },
      {
        "date": "2024-01-25T06:31:00.000Z",
        "voteCount": 2,
        "content": "The question only needs Linux OS, its not asking for any extra customizations. AWS fargate allows you to choose the linux platform - and you can package the specific packages in a docker container."
      },
      {
        "date": "2024-01-12T11:14:00.000Z",
        "voteCount": 3,
        "content": "It need specific versions of Apache, so, B and C are out,  it requires OS level customization, so no Fargate (A)"
      },
      {
        "date": "2023-12-30T12:32:00.000Z",
        "voteCount": 4,
        "content": "D: you can't customize fargate OS, Also the docker images only ocntains the application software"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/amazon/view/129672-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is using AWS CodeDeploy across a fleet of Amazon EC2 instances in an EC2 Auto Scaling group. The associated CodeDeploy deployment group, which is integrated with EC2 Auto Scaling, is configured to perform in-place deployments with CodeDeployDefault.OneAtATime. During an ongoing new deployment, the engineer discovers that, although the overall deployment finished successfully, two out of five instances have the previous application revision deployed. The other three instances have the newest application revision.<br><br>What is likely causing this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe two affected instances failed to fetch the new deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CodeDeploy agent was not installed in two affected instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be deployed on the affected instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T07:51:00.000Z",
        "voteCount": 5,
        "content": "D is correct: In-place deployment search for all available agents at the time of the deployment and update the app version. If there are new instances launched after the search, they would be omitted and they fetch the lastest app version available, which is the previous revision\nA and B: If this happened, the other three would be affected as well\nC: If code deploy agents were not installed, no version would be installed on the two instances"
      },
      {
        "date": "2024-02-17T11:26:00.000Z",
        "voteCount": 1,
        "content": ", B. A failed AfterInstall lifecycle event hook caused the CodeDeploy agent to roll back to the previous version on the affected instances seems to be the most plausible explanation. It accounts for the scenario where the deployment was successful overall, but specific instances reverted to the previous application revision due to issues encountered during post-installation steps. It's important for the DevOps engineer to review the deployment logs, especially focusing on lifecycle event hooks and their outcomes, to confirm this hypothesis and take corrective actions."
      },
      {
        "date": "2024-01-26T09:40:00.000Z",
        "voteCount": 4,
        "content": "D. EC2 Auto Scaling launched two new instances while the new deployment had not yet finished, causing the previous version to be deployed on the affected instances.\n\nExplanation:\n\nIn an EC2 Auto Scaling group, when a deployment is in progress and new instances are launched, they may receive the previous version of the application if the deployment has not yet completed. This is because the new instances join the Auto Scaling group and need to fetch the latest revision during the deployment process. If the deployment has not finished when the new instances are launched, they will fetch the current revision available in the Auto Scaling group, which might be the previous version."
      },
      {
        "date": "2024-02-12T04:28:00.000Z",
        "voteCount": 1,
        "content": "A. The explanation provided for D summarizes A-  The two affected instances failed to fetch the new deployment."
      },
      {
        "date": "2024-01-12T11:15:00.000Z",
        "voteCount": 3,
        "content": "only D makes sense"
      },
      {
        "date": "2023-12-29T07:39:00.000Z",
        "voteCount": 4,
        "content": "Got to be D. The rest choices would impact the entire ASG and not only two out of the five intances."
      },
      {
        "date": "2023-12-29T05:03:00.000Z",
        "voteCount": 2,
        "content": "D is correct \nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/troubleshooting-auto-scaling.html#troubleshooting-auto-scaling-provision-termination-loo"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/amazon/view/129673-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A security team is concerned that a developer can unintentionally attach an Elastic IP address to an Amazon EC2 instance in production. No developer should be allowed to attach an Elastic IP address to an instance. The security team must be notified if any production server has an Elastic IP address at any time.<br><br>How can this task be automated?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Athena to query AWS CloudTrail logs to check for any associate-address attempts. Create an AWS Lambda function to disassociate the Elastic IP address from the instance, and alert the security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an IAM policy to the developers' IAM group to deny associate-address permissions. Create a custom AWS Config rule to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that all IAM groups associated with developers do not have associate-address permissions. Create a scheduled AWS Lambda function to check whether an Elastic IP address is associated with any instance tagged as production, and alert the security team if an instance has an Elastic IP address associated with it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Config rule to check that all production instances have EC2 IAM roles that include deny associate-address permissions. Verify whether there is an Elastic IP address associated with any instance, and alert the security team if an instance has an Elastic IP address associated with it."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T08:07:00.000Z",
        "voteCount": 5,
        "content": "B is correct: &lt; Attach an IAM policy to the developers' IAM group to deny associate-address permissions&gt;: means we can deny all address-assosiate attempts\nA: AWS CloudTrail logs is used for monitoring users' actions. Though it would reveal associate-address attempts, it would not trigger AWS lambda to disassosiate the IPs\nC: &lt;Ensure that all IAM groups associated with developers do not have associate-address permissions&gt;: This is unnecessary and can be done more easily with option B. \nD: &lt;check that all production instances have EC2 IAM roles&gt;: We dont need to check the role of the EC2, we need to handle the role of developers.\n\nSummary: D is irrelevant while A and C, though can achive the requirements, consume more efforts and resources."
      },
      {
        "date": "2024-06-26T15:13:00.000Z",
        "voteCount": 1,
        "content": "For what it's worth:\n{\n  \"Statement\": [\n    {\n      \"Action\": [\n        \"ec2:AssociateAddress\",\n        \"ec2:DisassociateAddress\"\n      ],\n      \"Effect\": \"Deny\",\n      \"Resource\": \"*\"\n    }\n  ]\n}"
      },
      {
        "date": "2024-04-13T03:43:00.000Z",
        "voteCount": 2,
        "content": "answer B"
      },
      {
        "date": "2024-01-12T11:19:00.000Z",
        "voteCount": 2,
        "content": "so easy, almost copy/paste from the two requirements listed inside the question"
      },
      {
        "date": "2023-12-29T07:42:00.000Z",
        "voteCount": 3,
        "content": "It's B, the only who meets the question criteria."
      },
      {
        "date": "2023-12-29T05:05:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/amazon/view/129674-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is using AWS Organizations to create separate AWS accounts for each of its departments. The company needs to automate the following tasks:<br><br>\u2022\tUpdate the Linux AMIs with new patches periodically and generate a golden image<br>\u2022\tInstall a new version of Chef agents in the golden image, if available<br>\u2022\tProvide the newly generated AMIs to the department's accounts<br><br>Which solution meets these requirements with the LEAST management overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a script to launch an Amazon EC2 instance from the previous golden image. Apply the patch updates. Install the new version of the Chef agent, generate a new golden image, and then modify the AMI permissions to share only the new image with the department's accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Use AWS Resource Access Manager to share EC2 Image Builder images with the department's accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Systems Manager Automation runbook to update the Linux AMI by using the previous image. Provide the URL for the script that will update the Chef agent. Use AWS Organizations to replace the previous golden image in the department's accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon EC2 Image Builder to create an image pipeline that consists of the base Linux AMI and components to install the Chef agent. Create a parameter in AWS Systems Manager Parameter Store to store the new AMI ID that can be referenced by the department's accounts."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-23T11:54:00.000Z",
        "voteCount": 2,
        "content": "By leveraging EC2 Image Builder and RAM, solution B provides a fully automated and centralized approach to creating, updating, and sharing golden images with the department's accounts, minimizing manual effort and management overhead."
      },
      {
        "date": "2024-04-13T03:57:00.000Z",
        "voteCount": 2,
        "content": "ill go with B"
      },
      {
        "date": "2024-03-16T07:44:00.000Z",
        "voteCount": 2,
        "content": "B - The function of Amazon EC2 Image Builder is to build you golden AMIs"
      },
      {
        "date": "2024-02-09T08:16:00.000Z",
        "voteCount": 4,
        "content": "B is correct: &lt;generate a golden image&gt; means we need EC2 image builder for an automated pipeline to build a golden image\nA and C: no mention of EC2 image builder\nD: This option utilizes SSM to share the image, which is not correct. We need AWS resource sharing to share resources cross-account"
      },
      {
        "date": "2024-01-17T10:42:00.000Z",
        "voteCount": 2,
        "content": "This should be B"
      },
      {
        "date": "2024-01-12T11:24:00.000Z",
        "voteCount": 4,
        "content": "Only B provided solution for sharing AMI image"
      },
      {
        "date": "2024-01-10T04:27:00.000Z",
        "voteCount": 3,
        "content": "Going for B"
      },
      {
        "date": "2023-12-31T01:19:00.000Z",
        "voteCount": 4,
        "content": "Builder Image to streamline the AMI baking process and use RAM to easily share the AMI among the whole organization or select accounts.\n\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/manage-shared-resources.html#manage-shared-resources-using"
      },
      {
        "date": "2023-12-29T07:47:00.000Z",
        "voteCount": 2,
        "content": "Least management overhead is provided by C"
      },
      {
        "date": "2023-12-29T15:24:00.000Z",
        "voteCount": 2,
        "content": "Reading it again, B is a better option"
      },
      {
        "date": "2023-12-29T05:09:00.000Z",
        "voteCount": 1,
        "content": "It looks like C is the right one.\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/sharingamis-explicit.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/amazon/view/129675-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has a mission-critical application on AWS that uses automatic scaling. The company wants the deployment lifecycle to meet the following parameters:<br><br>\u2022\tThe application must be deployed one instance at a time to ensure the remaining fleet continues to serve traffic.<br>\u2022\tThe application is CPU intensive and must be closely monitored.<br>\u2022\tThe deployment must automatically roll back if the CPU utilization of the deployment instance exceeds 85%.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation to create an AWS Step Functions state machine and Auto Scaling lifecycle hooks to move to one instance at a time into a wait state. Use AWS Systems Manager automation to deploy the update to each instance and move it back into the Auto Scaling group using the heartbeat timeout.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CodeDeploy with Amazon EC2 Auto Scaling Configure an alarm tied to the CPU utilization metric. Use the CodeDeployDefault OneAtAtime configuration as a deployment strategy. Configure automatic rollbacks within the deployment group to roll back the deployment if the alarm thresholds are breached.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Elastic Beanstalk for load balancing and AWS Auto Scaling. Configure an alarm tied to the CPU utilization metric. Configure rolling deployments with a fixed batch size of one instance. Enable enhanced health to monitor the status of the deployment and roll back based on the alarm previously created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager to perform a blue/green deployment with Amazon EC2 Auto Scaling. Configure an alarm tied to the CPU utilization metric. Deploy updates one at a time. Configure automatic rollbacks within the Auto Scaling group to roll back the deployment if the alarm thresholds are breached."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-29T03:08:00.000Z",
        "voteCount": 1,
        "content": "B correct - CodeDeployDefault OneAtAtime configuration (One at a time, but not AllAtATime)"
      },
      {
        "date": "2024-06-26T16:27:00.000Z",
        "voteCount": 2,
        "content": "\"B\"\n\"You can now monitor and automatically react to changes in your AWS CodeDeploy deployments using Amazon CloudWatch alarms.\"\n\"Using CloudWatch with CodeDeploy, you can monitor metrics for Amazon EC2 instances or Auto Scaling groups being managed by CodeDeploy and then invoke an action if the metric you are tracking crosses a certain threshold for a defined period of time.\"\n\"You can monitor metrics such as instance CPU utilization.\"\n\"If the alarm is activated, CloudWatch initiates actions such as sending a notification to Amazon Simple Notification Service, stopping a CodeDeploy deployment, or changing the state of an instance (e.g. reboot, terminate, recover).\"\n\"CodeDeploy now also lets you automatically roll back a deployment when a deployment fails or when a CloudWatch alarm is activated.\"\nhttps://aws.amazon.com/about-aws/whats-new/2016/09/aws-codedeploy-introduces-deployment-monitoring-with-amazon-cloudwatch-alarms-and-automatic-deployment-rollback/"
      },
      {
        "date": "2024-04-23T11:56:00.000Z",
        "voteCount": 2,
        "content": "By using AWS CodeDeploy with Amazon EC2 Auto Scaling, configuring the CodeDeployDefault.OneAtAtime deployment strategy, and setting up automatic rollbacks based on a CloudWatch alarm for CPU utilization, this solution meets all the specified requirements. It ensures a controlled deployment process, monitors the CPU-intensive application, and automatically rolls back the deployment if the CPU utilization threshold is breached, providing a reliable and automated deployment lifecycle for the mission-critical application."
      },
      {
        "date": "2024-04-13T04:03:00.000Z",
        "voteCount": 2,
        "content": "answer B"
      },
      {
        "date": "2024-02-09T09:22:00.000Z",
        "voteCount": 3,
        "content": "B is correct: &lt; must be deployed one instance at a time&gt; means codedeploy, which provides this option\nA: AWS Step Functions state machine does not provide deployment Functions\nC: Beanstalk does not work with EC2\nD: AWS SSM does not provides deployment Functions"
      },
      {
        "date": "2024-01-10T04:37:00.000Z",
        "voteCount": 3,
        "content": "Why not BeansTalk?"
      },
      {
        "date": "2023-12-29T07:53:00.000Z",
        "voteCount": 3,
        "content": "It's B - the only one that fulfils all the requirements."
      },
      {
        "date": "2023-12-29T05:12:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/amazon/view/129676-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has a single developer writing code for an automated deployment pipeline. The developer is storing source code in an Amazon S3 bucket for each project. The company wants to add more developers to the team but is concerned about code conflicts and lost work. The company also wants to build a test environment to deploy newer versions of code for testing and allow developers to automatically deploy to both environments when code is changed in the repository.<br><br>What is the MOST efficient way to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeCommit repository for each project, use the main branch for production code, and create a testing branch for code deployed to testing. Use feature branches to develop new features and pull requests to merge code to testing and main branches.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate another S3 bucket for each project for testing code, and use an AWS Lambda function to promote code changes between testing and production buckets. Enable versioning on all buckets to prevent code conflicts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeCommit repository for each project, and use the main branch for production and test code with different deployment pipelines for each environment. Use feature branches to develop new features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable versioning and branching on each S3 bucket, use the main branch for production code, and create a testing branch for code deployed to testing. Have developers use each branch for developing in each environment."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-23T11:59:00.000Z",
        "voteCount": 2,
        "content": "By leveraging AWS CodeCommit and following Git branching best practices, the company can efficiently manage code changes, facilitate collaboration among developers, and automate deployments to both production and testing environments. This solution provides a scalable and organized approach to software development and deployment, while minimizing the risk of code conflicts and lost work."
      },
      {
        "date": "2024-04-13T05:29:00.000Z",
        "voteCount": 2,
        "content": "answer is A"
      },
      {
        "date": "2024-03-16T07:41:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-02-09T09:26:00.000Z",
        "voteCount": 3,
        "content": "A is correct: &lt; The developer is storing source code in an Amazon S3 bucket for each project&gt; and &lt; The company wants to add more developers to the team but is concerned about code conflicts and lost work&gt; means we need codecommit\nB and D: no mention of code commnit\nC: Should not use the main branch for both production and test"
      },
      {
        "date": "2024-01-12T11:32:00.000Z",
        "voteCount": 2,
        "content": "typical branching strategy"
      },
      {
        "date": "2023-12-31T01:31:00.000Z",
        "voteCount": 3,
        "content": "Definitely A. C lacks proper strategy for pull requests for code merging. In general, it's a proper patter in software development."
      },
      {
        "date": "2023-12-29T07:58:00.000Z",
        "voteCount": 3,
        "content": "It's A, since C suggests to merge to main branch non-production code. Although it's a working pattern, A is definitely more industry standard and safer."
      },
      {
        "date": "2023-12-29T05:15:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A.\nS3 not good choice , so eliminating B &amp; D"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/amazon/view/129679-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer notices that all Amazon EC2 instances running behind an Application Load Balancer in an Auto Scaling group are failing to respond to user requests. The EC2 instances are also failing target group HTTP health checks.<br><br>Upon inspection, the engineer notices the application process was not running in any EC2 instances. There are a significant number of out of memory messages in the system logs. The engineer needs to improve the resilience of the application to cope with a potential application memory leak. Monitoring and notifications should be enabled to alert when there is an issue.<br><br>Which combination of actions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Auto Scaling configuration to replace the instances when they fail the load balancer's health checks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the target group health check HealthCheckIntervalSeconds parameter to reduce the interval between health checks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the target group health checks from HTTP to TCP to check if the port where the application is listening is reachable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the available memory consumption metric within the Amazon CloudWatch dashboard for the entire Auto Scaling group. Create an alarm when the memory utilization is high. Associate an Amazon SNS topic to the alarm to receive notifications when the alarm goes off.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon CloudWatch agent to collect the memory utilization of the EC2 instances in the Auto Scaling group. Create an alarm when the memory utilization is high and associate an Amazon SNS topic to receive a notification.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-04T11:36:00.000Z",
        "voteCount": 2,
        "content": "from doc:\n\nWhen Amazon EC2 Auto Scaling determines that an InService instance is unhealthy, it replaces it with a new instance to maintain the desired capacity of the group.\n\nso you can't actually have a ASG that doesn't replace unhealthy instance so A doesn't make sense, but B makes sense since faster replacement will improve the resilience."
      },
      {
        "date": "2024-04-13T05:34:00.000Z",
        "voteCount": 2,
        "content": "Answer is A&amp;E"
      },
      {
        "date": "2024-03-16T07:40:00.000Z",
        "voteCount": 2,
        "content": "A - Autoscaling to replace teh EC2s\nE - CloudWatch agent to monitor Memory"
      },
      {
        "date": "2024-03-14T01:11:00.000Z",
        "voteCount": 2,
        "content": "I choose AE, but I am not comfortable with E, because CW doesn't by default give memory metrics, it requires customization."
      },
      {
        "date": "2024-02-09T10:40:00.000Z",
        "voteCount": 4,
        "content": "A and E are correct: If there is failed instance, we should replace it with a new one to restart. Use cloudwatch agent to monitor metrics\nB: this does not do anything\nC: should not change from HTTP to TCP\nD: Cloudwatch work with agents, about which this option does not mention"
      },
      {
        "date": "2024-01-12T11:35:00.000Z",
        "voteCount": 2,
        "content": "It's A &amp; E"
      },
      {
        "date": "2024-01-10T05:18:00.000Z",
        "voteCount": 2,
        "content": "We don't have memory metrics for autoscaling, here is the list of metrics \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/appinsights-metrics-ec2.html#appinsights-metrics-ec2-linux\n\nHere is the list of metrics from cloudwatch agent \n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/appinsights-metrics-ec2.html#appinsights-metrics-ec2-linux"
      },
      {
        "date": "2023-12-31T01:34:00.000Z",
        "voteCount": 1,
        "content": "Memory monitoring requires an agent and auto scaling needs the self-healing feature turned on."
      },
      {
        "date": "2023-12-29T08:03:00.000Z",
        "voteCount": 2,
        "content": "It's A &amp; E - no memory can be added in Cloudwatch dashboard, needs to be exported using Cloudwatch agent first."
      },
      {
        "date": "2023-12-29T05:25:00.000Z",
        "voteCount": 1,
        "content": "I'll go with A, E\nB is wrong because it does not talk about problem solving\nC is wrong because it does not talk about problem solving http/tcp \nD is wrong because of notifications when the alarm goes off"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/amazon/view/129680-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "An ecommerce company uses a large number of Amazon Elastic Block Store (Amazon EBS) backed Amazon EC2 instances. To decrease manual work across all the instances, a DevOps engineer is tasked with automating restart actions when EC2 instance retirement events are scheduled.<br><br>How can this be accomplished?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a scheduled Amazon EventBridge rule to run an AWS Systems Manager Automation runbook that checks if any EC2 instances are scheduled for retirement once a week. If the instance is scheduled for retirement, the runbook will hibernate the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable EC2 Auto Recovery on all of the instances. Create an AWS Config rule to limit the recovery to occur during a maintenance window only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReboot all EC2 instances during an approved maintenance window that is outside of standard business hours. Set up Amazon CloudWatch alarms to send a notification in case any instance is failing EC2 instance status checks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an AWS Health Amazon EventBridge rule to run AWS Systems Manager Automation runbooks that stop and start the EC2 instance when a retirement scheduled event occurs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T10:53:00.000Z",
        "voteCount": 5,
        "content": "D is correct: &lt;automating restart actions when EC2 instance retirement events are scheduled&gt;: these even occurs when the underlying infra that runs the instances needs to be repaired. To deal with this, just need to stop and start the instance to re-locate the instance to a different physical machine\nA: Hibernate would not work for this scenario\nB: This would also bring back the instance. However, AWS config rule cannot limit the recovery. It only reports, not actions\nC: This option is a manual work, not automated one"
      },
      {
        "date": "2024-04-23T20:31:00.000Z",
        "voteCount": 3,
        "content": "By leveraging AWS Health events, Amazon EventBridge, and AWS Systems Manager Automation runbooks, you can create an automated and event-driven solution that responds to EC2 instance retirement events in a timely and consistent manner, minimizing manual effort and reducing the risk of service disruptions."
      },
      {
        "date": "2024-04-13T05:53:00.000Z",
        "voteCount": 2,
        "content": "answer D"
      },
      {
        "date": "2024-03-16T07:38:00.000Z",
        "voteCount": 3,
        "content": "D  - Retirement relates to AWS Health"
      },
      {
        "date": "2024-02-02T04:38:00.000Z",
        "voteCount": 2,
        "content": "D is right answer!\n\nGet up-to-date https://www.pinterest.com/pin/937522847419120438"
      },
      {
        "date": "2024-02-01T19:11:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2024-01-12T11:36:00.000Z",
        "voteCount": 2,
        "content": "retirement is tied with Health"
      },
      {
        "date": "2023-12-31T14:22:00.000Z",
        "voteCount": 3,
        "content": "Answer is D. Once a week is a joke :D"
      },
      {
        "date": "2023-12-29T08:06:00.000Z",
        "voteCount": 2,
        "content": "It 's D"
      },
      {
        "date": "2023-12-29T05:37:00.000Z",
        "voteCount": 2,
        "content": "D is correct\nhttps://aws.amazon.com/blogs/mt/automate-remediation-actions-for-amazon-ec2-notifications-and-beyond-using-ec2-systems-manager-automation-and-aws-health/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/amazon/view/129706-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company manages AWS accounts for application teams in AWS Control Tower. Individual application teams are responsible for securing their respective AWS accounts.<br><br>A DevOps engineer needs to enable Amazon GuardDuty for all AWS accounts in which the application teams have not already enabled GuardDuty. The DevOps engineer is using AWS CloudFormation StackSets from the AWS Control Tower management account.<br><br>How should the DevOps engineer configure the CloudFormation template to prevent failure during the StackSets deployment?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation custom resource that invokes an AWS Lambda function. Configure the Lambda function to conditionally enable GuardDuty if GuardDuty is not already enabled in the accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Conditions section of the CloudFormation template to enable GuardDuty in accounts where GuardDuty is not already enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the CloudFormation Fn::GetAtt intrinsic function to check whether GuardDuty is already enabled. If GuardDuty is not already enabled, use the Resources section of the CloudFormation template to enable GuardDuty.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually discover the list of AWS account IDs where GuardDuty is not enabled. Use the CloudFormation Fn::ImportValue intrinsic function to import the list of account IDs into the CloudFormation template to skip deployment for the listed AWS accounts."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T11:25:00.000Z",
        "voteCount": 5,
        "content": "A is correct:  &lt;configure the CloudFormation template&gt; is the requirement of the question. By default, cloudformation doesnot support turning on Guarduty. To turn it on, need to use ACF template in combination with lambda. \nA: perfectly correct\nB: no mention of lambda\nC: Fn::GetAtt intrinsic: This is used to check only. No mention of using lambda to enable Guarduty\nD: This might work. However, a manual approach is not recommeded"
      },
      {
        "date": "2024-04-13T05:59:00.000Z",
        "voteCount": 2,
        "content": "answer A"
      },
      {
        "date": "2024-03-16T07:36:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-01-12T11:38:00.000Z",
        "voteCount": 2,
        "content": "standard pattern: use lambda to conditional DO something"
      },
      {
        "date": "2023-12-31T01:40:00.000Z",
        "voteCount": 4,
        "content": "A is correct. Conditions are designed to decide whether or not create resources. GetAtt is to retrieve the value of an attribute from a resource in  the same template. and manual processes are usually not good."
      },
      {
        "date": "2023-12-30T06:18:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-12-29T08:10:00.000Z",
        "voteCount": 4,
        "content": "It's a standard pattern, so A\n\nHere is a reference: \nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/enable-amazon-guardduty-conditionally-by-using-aws-cloudformation-templates.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/amazon/view/129829-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an AWS Control Tower landing zone. The company's DevOps team creates a workload OU. A development OU and a production OU are nested under the workload OU. The company grants users full access to the company's AWS accounts to deploy applications.<br><br>The DevOps team needs to allow only a specific management IAM role to manage the IAM roles and policies of any AWS accounts in only the production OU.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that denies full access with a condition to exclude the management IAM role for the organization root.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the FullAWSAccess SCP is applied at the organization root.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that allows IAM related actions. Attach the SCP to the development OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the workload OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that denies IAM related actions with a condition to exclude the management IAM role. Attach the SCP to the production OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-31T01:54:00.000Z",
        "voteCount": 9,
        "content": "You need to understand how SCP inheritance works in AWS. The way it works for Deny policies is different that allow policies. \n\nAllow polices are passing down to children ONLY if they don't have an allow policy.\n\nDeny policies always pass down to children.\n\nThat's why there is always an SCP set to the Root to allow everything by default. If you limit this policy, the whole organization will be limited, not matter what other policies are saying for the other OUs. So it's not A. It's not D because it restricts the wrong OU."
      },
      {
        "date": "2024-05-08T07:25:00.000Z",
        "voteCount": 1,
        "content": "CE\nFullAWSAccess is applied be default, no need to check it since the question did not say it has been removed.\nFor an Action to be permitted it has to be allowed from the Root OUs all the way to the accounts."
      },
      {
        "date": "2024-04-13T06:03:00.000Z",
        "voteCount": 1,
        "content": "ANS: B&amp;E"
      },
      {
        "date": "2024-03-16T07:35:00.000Z",
        "voteCount": 1,
        "content": "B &amp; E are correct"
      },
      {
        "date": "2024-03-03T23:28:00.000Z",
        "voteCount": 2,
        "content": "B-E, no debate"
      },
      {
        "date": "2024-02-13T07:03:00.000Z",
        "voteCount": 2,
        "content": "B and E are correct because he requirement for dev ou user should still be able to do what they need to"
      },
      {
        "date": "2024-02-09T11:33:00.000Z",
        "voteCount": 2,
        "content": "B and E are correct: \nA: this does not make sense. It would mess with permissions for all OUs\nC: The question requires &lt;only the production OU&gt;: we need to target the production OU, not development OU\nD: &lt;Attach the SCP to the workload OU&gt;: we need to target only the production OU. This option affects both dev and prod OUS"
      },
      {
        "date": "2024-01-20T04:19:00.000Z",
        "voteCount": 1,
        "content": "B &amp; E it is"
      },
      {
        "date": "2024-01-12T11:43:00.000Z",
        "voteCount": 1,
        "content": "A is wrong, we only want to limit production OU, development OU users should be able to do anything"
      },
      {
        "date": "2023-12-31T14:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is B &amp; E.\n\nA is not correct because it would prevent the developers team to access the Developer OU. That wouldn't make sense."
      },
      {
        "date": "2023-12-29T22:02:00.000Z",
        "voteCount": 1,
        "content": "A and E"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/amazon/view/129708-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company hired a penetration tester to simulate an internal security breach. The tester performed port scans on the company's Amazon EC2 instances. The company's security measures did not detect the port scans.<br><br>The company needs a solution that automatically provides notification when port scans are performed on EC2 instances. The company creates and subscribes to an Amazon Simple Notification Service (Amazon SNS) topic.<br><br>What should the company do next to meet the requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that Amazon GuardDuty is enabled. Create an Amazon CloudWatch alarm for detected EC2 and port scan findings. Connect the alarm to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected network reachability findings that indicate port scans. Connect the event to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that Amazon Inspector is enabled. Create an Amazon EventBridge event for detected CVEs that cause open port vulnerabilities. Connect the event to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that AWS CloudTrail is enabled. Create an AWS Lambda function to analyze the CloudTrail logs for unusual amounts of traffic from an IP address range. Connect the Lambda function to the SNS topic."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-07T13:50:00.000Z",
        "voteCount": 1,
        "content": "- GuardDuty is focused on real-time threat detection and alerting, while Inspector is focused on vulnerability scanning and remediation. \n- GuardDuty operates continuously in the background, whereas Inspector is typically run on-demand or scheduled for specific workloads."
      },
      {
        "date": "2024-08-21T07:34:00.000Z",
        "voteCount": 1,
        "content": "The Answer is A. C is wrong because while you can use Inspector to detect open ports and software vulnerabilities, you can't use it to detect port scanning."
      },
      {
        "date": "2024-06-27T18:32:00.000Z",
        "voteCount": 1,
        "content": "Per ChatGPT \"AWS offers several services and features that can help detect port scans:\"\n\"GuardDuty\" (using VPC Flow Logs), \"WAF\", and \"Network Firewall\"\nWas able to also provide references\nhttps://aws.amazon.com/blogs/aws/amazon-guardduty-continuous-security-monitoring-threat-detection/"
      },
      {
        "date": "2024-05-07T07:02:00.000Z",
        "voteCount": 1,
        "content": "Bad question.\n\nAlthough you can do it via GuardDuty, the answer doesn't mention the required VPC flow logs.\nThere is no mention online of how to create a CloudWatch ALARM for GuardDuty only CloudWatch events."
      },
      {
        "date": "2024-04-13T06:13:00.000Z",
        "voteCount": 1,
        "content": "answer a is correct"
      },
      {
        "date": "2024-02-11T01:48:00.000Z",
        "voteCount": 3,
        "content": "A is correct: To detect port scans in real time, we need Guarduty, not inspector\nB, C and D: no mention of Guarduty"
      },
      {
        "date": "2024-01-12T11:45:00.000Z",
        "voteCount": 3,
        "content": "only GuardDuty would detect port scanning activities"
      },
      {
        "date": "2024-01-10T06:01:00.000Z",
        "voteCount": 1,
        "content": "https://medium.com/aws-architech/use-case-aws-inspector-vs-guardduty-3662bf80767a"
      },
      {
        "date": "2023-12-31T15:37:00.000Z",
        "voteCount": 1,
        "content": "GuardDuty should be the answer as it best detects whether a port scan has happened on an EC2 instances; we don't care about whether the port is open or not, we care if it was scanned."
      },
      {
        "date": "2023-12-31T02:48:00.000Z",
        "voteCount": 3,
        "content": "Inspector is designed to find vulnerabilities across EC2 servers and detect open ports. It doesn't detect port scans against EC2 servers. The reachability analyzer mentioned below is the port scanner itself. I doesn't detect other port scanners.\n\nhttps://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/\n\nGuardDuty on the other hand draws upon traffic logs to find specious activities such as port scans in a form of a finding."
      },
      {
        "date": "2023-12-29T08:32:00.000Z",
        "voteCount": 1,
        "content": "It's B - here is a reference for the network reachability package: \n\nhttps://aws.amazon.com/blogs/security/amazon-inspector-assess-network-exposure-ec2-instances-aws-network-reachability-assessments/"
      },
      {
        "date": "2023-12-31T15:44:00.000Z",
        "voteCount": 2,
        "content": "AWS inspector doesn't detect whether a PenTester performed a port scan against an EC2. It only detects open port vulnerabilities. You need a system that detects a threat which is by definition GuardDuty"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/amazon/view/129682-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs applications in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster uses an Application Load Balancer to route traffic to the applications that run in the cluster.<br><br>A new application that was migrated to the EKS cluster is performing poorly. All the other applications in the EKS cluster maintain appropriate operation. The new application scales out horizontally to the preconfigured maximum number of pods immediately upon deployment, before any user traffic routes to the web application.<br><br>Which solution will resolve the scaling behavior of the web application in the EKS cluster?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the Horizontal Pod Autoscaler in the EKS cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the Vertical Pod Autoscaler in the EKS cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the Cluster Autoscaler.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the AWS Load Balancer Controller in the EKS cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-21T00:45:00.000Z",
        "voteCount": 9,
        "content": "In my opinion A is incorrect because \"The new application scales out horizontally to the preconfigured maximum number of pods immediately upon deployment\" means that the Horizontal Pod Autoscaler is already implemented but doesn't resolve the issue with poor performance. This may indicate inappropriate resource allocation. \nBut Vertical Pod Autoscaler will help us \"right size\" our application. \nSo, for me it's B."
      },
      {
        "date": "2024-01-15T13:13:00.000Z",
        "voteCount": 6,
        "content": "scaled out to maximum when there is no user traffic: this means that the configured pod instance is wrong-sized, for example, need more memory or CPU."
      },
      {
        "date": "2024-08-20T04:58:00.000Z",
        "voteCount": 1,
        "content": "i think b"
      },
      {
        "date": "2024-08-12T13:10:00.000Z",
        "voteCount": 1,
        "content": "The app is currently deployed as Deployment with a set of replicas which explains that it scales to the maximum set without any traffic.\n\nIt needs HPA to scale up in response to traffic not Vertical Pod Autoscaler which is in response to adding more cpu/mem resources to already running pods"
      },
      {
        "date": "2024-07-29T20:23:00.000Z",
        "voteCount": 3,
        "content": "Should be B\nkeywords: \"scaled out to maximum before user traffic route to web application\" \n- this means that the configured pod instance is wrong-sized before user traffic, which need more cpu or memory. \n\nincrease CPU/memory for resources - Vertical Pod Autoscaler\nincrease pod for traffic - Horizontal Pod Autoscaler"
      },
      {
        "date": "2024-05-07T07:07:00.000Z",
        "voteCount": 2,
        "content": "It's A, in Kubernetes you can specific the number of pod replicas without the use of HPA.\n\n\"The new application scales out horizontally to the preconfigured maximum number of pods\n\nThis would imply that they are doing it statically currently."
      },
      {
        "date": "2024-05-02T06:03:00.000Z",
        "voteCount": 2,
        "content": "B for me"
      },
      {
        "date": "2024-04-13T06:26:00.000Z",
        "voteCount": 1,
        "content": "Ans: A\nHorizontal Pod Autoscaler (Option A) is the most appropriate solution for adjusting the number of pods based on CPU and memory utilization.\n\nVertical Pod Autoscaler (Option B) adjusts the CPU and memory reservations for pods, which might not directly address the scaling behavior issue in this scenario."
      },
      {
        "date": "2024-03-30T15:13:00.000Z",
        "voteCount": 2,
        "content": "A.\nAssuming that the first part of the question is related to replicas, which is the max number controlled by the deployment, the Replicaset will set it to the maximum\nKind: deployment\nReplicas:4\nwhich means this is NOT necessarily HPA, just replica set. So, there is a need to configure the HPA properly, BASED on CPU other them STATiC"
      },
      {
        "date": "2024-03-30T15:18:00.000Z",
        "voteCount": 1,
        "content": "but then, when it comes to resources, seems to be B, so changing my to B"
      },
      {
        "date": "2024-03-16T04:46:00.000Z",
        "voteCount": 3,
        "content": "Given the nature of the problem \u2014 scaling out to the maximum number of pods prematurely, the issue appears to be related to resource allocation rather than the need to handle increased traffic/load. Option B (Implement the Vertical Pod Autoscaler in the EKS cluster) is more likely to address the underlying issue by optimizing the resource allocation for the pods, which could prevent the unnecessary immediate scale-out of the application. VPA adjusts pod resources to match their needs more accurately, potentially mitigating the need for immediate horizontal scaling."
      },
      {
        "date": "2024-02-11T04:45:00.000Z",
        "voteCount": 2,
        "content": "A is correct: There are problem with horizontal scale so we need to implement it properly"
      },
      {
        "date": "2024-01-05T08:06:00.000Z",
        "voteCount": 2,
        "content": "The key here is \"preconfigured maximum number of pods\". The HPA should be configured to scale the number of pods up based on actual CPU utilization, not based on a preconfigured maximum number of pods."
      },
      {
        "date": "2024-01-02T07:13:00.000Z",
        "voteCount": 2,
        "content": "B: Can't be A because without traffic the horizontal scaling is full, you need to increase the CPU capacity, https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html"
      },
      {
        "date": "2024-01-01T00:20:00.000Z",
        "voteCount": 1,
        "content": "We need to note this like - new application that was migrated to the EKS cluster is performing poorly\n\nHere, due to the new application, cluster is performing poorly. So, we need to only focus on this pod rather overall capacity. so, C will not be the right option as it will take care of entire pods. A is the right way.\n\nThe Horizontal Pod AutoScaler (HPA) is typically more suitable for adjusting the number of replicas (pods) based on metrics like CPU utilization, ensuring that the application scales in or out based on demand"
      },
      {
        "date": "2023-12-31T13:11:00.000Z",
        "voteCount": 1,
        "content": "It needs more node, so that the answer is C"
      },
      {
        "date": "2023-12-31T03:00:00.000Z",
        "voteCount": 5,
        "content": "If the new application in the Amazon EKS cluster is performing poorly despite scaling out to the maximum number of pods, it's possible that the issue might be related to the resources allocated to each individual pod, rather than the number of pods."
      },
      {
        "date": "2023-12-29T08:38:00.000Z",
        "voteCount": 1,
        "content": "It's A - horizontal pod autoscaler can scale in or horizontally based on CPU utilisation. \n\nhttps://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/amazon/view/129985-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an AWS Control Tower landing zone that manages its organization in AWS Organizations. The company created an OU structure that is based on the company's requirements. The company's DevOps team has established the core accounts for the solution and an account for all centralized AWS CloudFormation and AWS Service Catalog solutions.<br><br>The company wants to offer a series of customizations that an account can request through AWS Control Tower.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable trusted access for CloudFormation with Organizations by using service-managed permissions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that is named AWSControlTowerBlueprintAccess. Configure the role with a trust policy that allows the AWSControlTowerAdmin role in the management account to assume the role. Attach the AWSServiceCatalogAdminFullAccess IAM policy to the AWSControlTowerBlueprintAccess role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Service Catalog product for each CloudFormation template.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation stack set for each CloudFormation template. Enable automatic deployment for each stack set. Create a CloudFormation stack instance that targets specific OUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Customizations for AWS Control Tower (CfCT) CloudFormation stack.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation template that contains the resources for each customization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-25T04:24:00.000Z",
        "voteCount": 1,
        "content": "I think the correct answer is ACE.\n1- Enable trusted access for CloudFormation with Organizations to allow cross-account deployments.\n2- Create Service Catalog products for each CloudFormation template to provide a self-service portal.\n3- Deploy the Customizations for AWS Control Tower (CfCT) stack to automate and manage customizations across your organization."
      },
      {
        "date": "2024-06-27T22:09:00.000Z",
        "voteCount": 2,
        "content": "\"Steps to set up Account Factory for the customization process\":\n1. \"Create the required role....\"\n   - \"The role must be named AWSControlTowerBlueprintAccess.\"\n   - \"The AWSControlTowerBlueprintAccess role must be set up to grant trust to\" [...]\n     - \"The role named AWSControlTowerAdmin in the AWS Control Tower management account.\"\n   - AWS Control Tower requires that the managed policy named AWSServiceCatalogAdminFullAccess must be attached to the AWSControlTowerBlueprintAccess role. (Required permissions policy)\n2. \"Create the AWS Service Catalog product...\"\n[...]\nhttps://docs.aws.amazon.com/controltower/latest/userguide/af-customization-page.html\nhttps://docs.aws.amazon.com/controltower/latest/userguide/afc-setup-steps.html"
      },
      {
        "date": "2024-04-13T15:06:00.000Z",
        "voteCount": 1,
        "content": "ans: BCF"
      },
      {
        "date": "2024-03-30T15:19:00.000Z",
        "voteCount": 1,
        "content": "B,C,F is the only answer"
      },
      {
        "date": "2024-02-11T04:55:00.000Z",
        "voteCount": 3,
        "content": "BCF are correct: &lt;an account for all centralized AWS CloudFormation &gt; means we need to set up a role for this accoun\nA: Trusted access is for assuming role only\nD: no mention of customization\nE: CFCT is needed only when we need to apply to best practice"
      },
      {
        "date": "2024-02-11T04:44:00.000Z",
        "voteCount": 1,
        "content": "BCF are correct: &lt;an account for all centralized AWS CloudFormation &gt; means we need to set up a role for this accoun\nE: CFCT is for when we need to apply to best practice"
      },
      {
        "date": "2024-01-12T01:49:00.000Z",
        "voteCount": 2,
        "content": "answer BCF"
      },
      {
        "date": "2024-01-01T14:28:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/afc-setup-steps.html"
      },
      {
        "date": "2023-12-31T03:25:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/afc-setup-steps.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/amazon/view/129710-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs a workload on Amazon EC2 instances. The company needs a control that requires the use of Instance Metadata Service Version 2 (IMDSv2) on all EC2 instances in the AWS account. If an EC2 instance does not prevent the use of Instance Metadata Service Version 1 (IMDSv1), the EC2 instance must be terminated.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS Config in the account. Use a managed rule to check EC2 instances. Configure the rule to remediate the findings by using AWS Systems Manager Automation to terminate the instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a permissions boundary that prevents the ec2:RunInstance action if the ec2:MetadataHttpTokens condition key is not set to a value of required. Attach the permissions boundary to the IAM role that was used to launch the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Amazon Inspector in the account. Configure Amazon Inspector to activate deep inspection for EC2 instances. Create an Amazon EventBridge rule for an Inspector2 finding. Set an AWS Lambda function as the target to terminate the instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule for the EC2 instance launch successful event. Send the event to an AWS Lambda function to inspect the EC2 metadata and to terminate the instance."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-29T08:58:00.000Z",
        "voteCount": 7,
        "content": "AWS Config can do this using the managed ec2-imdsv2-check rule.\n\nHere is a reference:\nhttps://docs.aws.amazon.com/config/latest/developerguide/ec2-imdsv2-check.html"
      },
      {
        "date": "2024-08-21T07:49:00.000Z",
        "voteCount": 1,
        "content": "Using the ec2-imdsv2-check AWS Config managed rule, you can Checks if your Amazon EC2 instance metadata version is configured with Instance Metadata Service Version 2 (IMDSv2)."
      },
      {
        "date": "2024-04-24T08:50:00.000Z",
        "voteCount": 2,
        "content": "AWS Config is a service that enables you to assess, audit, and evaluate the configurations of your AWS resources. It provides managed rules that you can use to evaluate the configuration settings of your resources against desired configurations.\nIn this case, you can use the AWS Config managed rule \"ec2-imdsv2-check\" to evaluate whether your EC2 instances are using the Instance Metadata Service Version 2 (IMDSv2) or not. This rule checks if the EC2 instances have the HTTP token request enabled for the Instance Metadata Service (IMDS), which is a requirement for using IMDSv2.\nIf an EC2 instance is found to be non-compliant with the rule (i.e., not using IMDSv2), AWS Config can be configured to automatically remediate the non-compliant resource. You can set up AWS Systems Manager Automation to terminate the non-compliant EC2 instance as the remediation action."
      },
      {
        "date": "2024-03-16T06:53:00.000Z",
        "voteCount": 1,
        "content": "A - AWS Config"
      },
      {
        "date": "2024-03-16T06:52:00.000Z",
        "voteCount": 1,
        "content": "A - AWS Config"
      },
      {
        "date": "2024-02-11T04:53:00.000Z",
        "voteCount": 3,
        "content": "A is correct: use Config to monitor and SSM Automation to terminate instances\nB: permission boundary cannot spot the need-to-terminate instances\nC: Inspector is for vul scanning\nD: EC2 instance launch successful event wont provide sufficient information"
      },
      {
        "date": "2023-12-31T16:03:00.000Z",
        "voteCount": 1,
        "content": "Answer A"
      },
      {
        "date": "2023-12-31T03:31:00.000Z",
        "voteCount": 1,
        "content": "Only viable option"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/amazon/view/129715-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company builds an application that uses an Application Load Balancer in front of Amazon EC2 instances that are in an Auto Scaling group. The application is stateless. The Auto Scaling group uses a custom AMI that is fully prebuilt. The EC2 instances do not have a custom bootstrapping process.<br><br>The AMI that the Auto Scaling group uses was recently deleted. The Auto Scaling group's scaling activities show failures because the AMI ID does not exist.<br><br>Which combination of steps should a DevOps engineer take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new launch template that uses the new AMI.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Auto Scaling group to use the new launch template.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the Auto Scaling group's desired capacity to 0.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the Auto Scaling group's desired capacity by 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AMI from a running EC2 instance in the Auto Scaling group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AMI by copying the most recent public AMI of the operating system that the EC2 instances use."
    ],
    "answer": "ABE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABE",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-29T20:38:00.000Z",
        "voteCount": 1,
        "content": "ABE\nother options not related to solve problem"
      },
      {
        "date": "2024-04-13T15:15:00.000Z",
        "voteCount": 2,
        "content": "ans ABE"
      },
      {
        "date": "2024-03-30T15:23:00.000Z",
        "voteCount": 1,
        "content": "A,B,E\nThe others do not actually help solve the problem"
      },
      {
        "date": "2024-03-16T06:25:00.000Z",
        "voteCount": 2,
        "content": "ABE\nE - Create new AMI from existing EC2 (we can do this as they are stateless)\nA - New lanuch template with the new AMI\nB - Tell autoscaling to use this new launch template"
      },
      {
        "date": "2024-02-11T05:02:00.000Z",
        "voteCount": 4,
        "content": "ABE are correct: &lt;The AMI that the Auto Scaling group uses was recently deleted&gt; means we need a new AMI image from a runnung EC2 instance\nC and D: irrelevant, auto Scaling group's desired capacity has nothing to do here\nF: This option doesnt make sense"
      },
      {
        "date": "2024-01-27T05:57:00.000Z",
        "voteCount": 2,
        "content": "To address the issue of the deleted AMI in the Auto Scaling group, you can follow these steps:\n\nCreate a new AMI from a running EC2 instance in the Auto Scaling group. (Option E)\n\nThis will ensure that you have a new AMI based on the current state of one of the running instances in the group.\nCreate a new launch template that uses the new AMI. (Option A)\n\nAfter creating the new AMI, update the launch template to use this new AMI. Launch templates provide a versioned and more structured way to define the launch configuration for your Auto Scaling group.\nUpdate the Auto Scaling group to use the new launch template. (Option B)\n\nUpdate the Auto Scaling group to use the new launch template that includes the new AMI. This will ensure that new instances launched by the Auto Scaling group will use the updated configuration."
      },
      {
        "date": "2024-01-25T07:24:00.000Z",
        "voteCount": 1,
        "content": "Create a new AMI from running instance, Update launch template to use new AMI, update ASG to use new launch template"
      },
      {
        "date": "2024-01-12T12:02:00.000Z",
        "voteCount": 1,
        "content": "create a new image and use it"
      },
      {
        "date": "2023-12-31T15:11:00.000Z",
        "voteCount": 1,
        "content": "Only viable steps"
      },
      {
        "date": "2023-12-29T09:07:00.000Z",
        "voteCount": 2,
        "content": "Create a new AMI from a running instance in ASG. Use it to create a new launch template, finally update ASG to use the new template."
      },
      {
        "date": "2023-12-29T09:09:00.000Z",
        "voteCount": 1,
        "content": "btw, creating a new launch template in this case can also mean a new version of it."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/amazon/view/129720-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company deploys a web application on Amazon EC2 instances that are behind an Application Load Balancer (ALB). The company stores the application code in an AWS CodeCommit repository. When code is merged to the main branch, an AWS Lambda function invokes an AWS CodeBuild project. The CodeBuild project packages the code, stores the packaged code in AWS CodeArtifact, and invokes AWS Systems Manager Run Command to deploy the packaged code to the EC2 instances.<br><br>Previous deployments have resulted in defects, EC2 instances that are not running the latest version of the packaged code, and inconsistencies between instances.<br><br>Which combination of actions should a DevOps engineer take to implement a more reliable deployment solution? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Configure pipeline stages that run the CodeBuild project in parallel to build and test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS CodeDeploy action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in AWS CodePipeline that uses the CodeCommit repository as a source provider. Create separate pipeline stages that run a CodeBuild project to build and then test the application. In the pipeline, pass the CodeBuild project output artifact to an AWS CodeDeploy action.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeDeploy application and a deployment group to deploy the packaged code to the EC2 instances. Configure the ALB for the deployment group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate individual Lambda functions that use AWS CodeDeploy instead of Systems Manager to run build, test, and deploy actions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket. Modify the CodeBuild project to store the packages in the S3 bucket instead of in CodeArtifact. Use deploy actions in CodeDeploy to deploy the artifact to the EC2 instances."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-29T20:41:00.000Z",
        "voteCount": 1,
        "content": "BC\nCodePipeline, CodeCommit, CodeDeploy"
      },
      {
        "date": "2024-04-13T15:34:00.000Z",
        "voteCount": 1,
        "content": "B (Sequential Stages) if:\nBuild and test stages have dependencies (tests rely on build output).\nYou need manual approvals or gates between stages for control."
      },
      {
        "date": "2024-03-30T15:28:00.000Z",
        "voteCount": 1,
        "content": "B,C\nA is wrong because doesn't make sense to do it in parallel since it'll cause more problems.\nD and E are dumb"
      },
      {
        "date": "2024-03-16T06:23:00.000Z",
        "voteCount": 1,
        "content": "B and C\nCodePipeline and CodeDeploy"
      },
      {
        "date": "2024-02-11T05:08:00.000Z",
        "voteCount": 4,
        "content": "B and C are correct: We need to use codedeploy to build instead of using codebuild\nA: &lt;run the CodeBuild project in parallel&gt; - this is in correct. Should run the pipiline respectedly\nD: Should not use lambda\nE: Codeartifact is good, no need to change to S3"
      },
      {
        "date": "2024-01-17T11:08:00.000Z",
        "voteCount": 1,
        "content": "B &amp; C BeCause"
      },
      {
        "date": "2024-01-12T04:34:00.000Z",
        "voteCount": 1,
        "content": "Answer B and C"
      },
      {
        "date": "2023-12-31T16:36:00.000Z",
        "voteCount": 1,
        "content": "B &amp; C for sure."
      },
      {
        "date": "2023-12-31T15:17:00.000Z",
        "voteCount": 1,
        "content": "B. because in this case sequential approach is more reliable and ensures consistency.\nC. Because in the only possible next step in the process."
      },
      {
        "date": "2023-12-29T22:14:00.000Z",
        "voteCount": 1,
        "content": "B and C"
      },
      {
        "date": "2023-12-29T09:21:00.000Z",
        "voteCount": 1,
        "content": "Use Codepipeline to orchestrate the build and codedeploy to deploy it on EC2."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/amazon/view/129727-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an organization in AWS Organizations to manage its AWS accounts. The company's automation account contains a CI/CD pipeline that creates and configures new AWS accounts.<br><br>The company has a group of internal service teams that provide services to accounts in the organization. The service teams operate out of a set of services accounts. The service teams want to receive an AWS CloudTrail event in their services accounts when the CreateAccount API call creates a new account.<br><br>How should the company share this CloudTrail event with the service accounts?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule in the automation account to send account creation events to the default event bus in the services accounts. Update the default event bus in the services accounts to allow events from the automation account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom Amazon EventBridge event bus in the services accounts. Update the custom event bus to allow events from the automation account. Create an EventBridge rule in the services account that directly listens to CloudTrail events from the automation account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom Amazon EventBridge event bus in the automation account and the services accounts. Create an EventBridge rule and policy that connects the custom event buses that are in the automation account and the services accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom Amazon EventBridge event bus in the automation account. Create an EventBridge rule and policy that connects the custom event bus to the default event buses in the services accounts."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-01T17:36:00.000Z",
        "voteCount": 5,
        "content": "A is right. \"Create an Amazon EventBridge rule in the automation account to send account creation events to the default event bus in the services accounts\": propagation of provision events to the service accounts. \"Update the default event bus in the services accounts to allow events from the automation account.\": correct\n\n\nB. \"Create a custom Amazon EventBridge event bus in the services accounts. Update the custom event bus to allow events from the automation account.\": correct however  \"Create an EventBridge rule in the services account that directly listens to CloudTrail events from the automation account.\": Why do you create a rule in the services account listening the events from automation account, in opposite, the rule should be created in the automation account to push the events to the bus in the services account."
      },
      {
        "date": "2024-02-11T06:55:00.000Z",
        "voteCount": 5,
        "content": "A is correct: We need account creation events and this option provides us with exactly that\nB: &lt; Create an EventBridge rule in the services account that directly listens to CloudTrail events&gt;: This does not make sense. We should apply rule to eventbus to send event\nC and D: Both options send all events, not just account creation events"
      },
      {
        "date": "2024-05-05T11:15:00.000Z",
        "voteCount": 1,
        "content": "I'm voting B\n\nA - there could be more than 5 accounts: https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-rules-best-practices.html - \"While you can specify up to five targets for a given rule\"\n\nIt's perfectly viable to create rule in one account to receive events from second account."
      },
      {
        "date": "2024-05-21T02:01:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/vi/blogs/aws/new-cross-account-delivery-of-cloudwatch-events/\nOption B Wrong. Answer is A"
      },
      {
        "date": "2024-04-24T09:30:00.000Z",
        "voteCount": 1,
        "content": "The steps to configure EventBridge to send events to or receive events from an event bus in a different account include the following:\nOn the receiver account, edit the permissions on an event bus to allow specified AWS accounts, an organization, or all AWS accounts to send events to the receiver account.\nOn the sender account, set up one or more rules that have the receiver account's event bus as the target.\nIf the sender account inherits permissions to send events from an AWS Organization, the sender account also must have an IAM role with policies that enable it to send events to the receiver account. If you use the AWS Management Console to create the rule that targets the event bus in the receiver account, the role is created automatically. \nOn the receiver account, set up one or more rules that match events that come from the sender account."
      },
      {
        "date": "2024-04-13T15:49:00.000Z",
        "voteCount": 3,
        "content": "answer A"
      },
      {
        "date": "2024-03-28T01:40:00.000Z",
        "voteCount": 2,
        "content": "of course its A!\n(CloudTrail events) ---EventBridge rule---&gt; [automation account default EventBridge event bus] ---allow---&gt; [service accounts custom EventBridge event bus]"
      },
      {
        "date": "2024-03-28T01:41:00.000Z",
        "voteCount": 1,
        "content": "I mean B"
      },
      {
        "date": "2024-01-16T06:51:00.000Z",
        "voteCount": 2,
        "content": "its A, rest don't include account creation."
      },
      {
        "date": "2024-01-12T12:21:00.000Z",
        "voteCount": 2,
        "content": "B is wrong,  the event is in automation account.  It lacks the step to send the event from automation to service account."
      },
      {
        "date": "2024-01-10T08:20:00.000Z",
        "voteCount": 2,
        "content": "Go for A \n\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-cross-account.html"
      },
      {
        "date": "2024-02-15T06:43:00.000Z",
        "voteCount": 1,
        "content": "The link you pasted clearly says it is D."
      },
      {
        "date": "2024-01-01T14:47:00.000Z",
        "voteCount": 1,
        "content": "I will go with B.\n\nGiven that \"listening directly to CloudTrail\" is mentioned in the below AWS documentation in bullet point number 8:\n\nhttps://aws.amazon.com/blogs/machine-learning/onboard-users-to-amazon-sagemaker-studio-with-active-directory-group-specific-iam-roles/"
      },
      {
        "date": "2023-12-29T09:46:00.000Z",
        "voteCount": 1,
        "content": "It's B - create an Eventbridge rule in the source account, and point the rule to a custom event bus in the service accounts."
      },
      {
        "date": "2023-12-29T10:01:00.000Z",
        "voteCount": 2,
        "content": "reading it again, I'm more inclined to A given that B says about eventbridge rule listening *directly* from Cloudtrail"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/amazon/view/129832-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is building a solution that uses Amazon Simple Queue Service (Amazon SQS) standard queues. The solution also includes an AWS Lambda function and an Amazon DynamoDB table. The Lambda function pulls content from an SQS queue event source and writes the content to the DynamoDB table.<br><br>The solution must maximize the scalability of Lambda and must prevent successfully processed SQS messages from being processed multiple times.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the batch window to 1 second when configuring the Lambda function's event source mapping.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the batch size to 1 when configuring the Lambda function's event source mapping.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude the ReportBatchItemFailures value in the FunctionResponseTypes list in the Lambda function's event source mapping.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the queue visibility timeout on the Lambda function's event source mapping to account for invocation throttling of the Lambda function."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-29T23:43:00.000Z",
        "voteCount": 5,
        "content": "It's C, here is a reference: \n\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting"
      },
      {
        "date": "2024-07-16T19:33:00.000Z",
        "voteCount": 1,
        "content": "link updated:\n\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html"
      },
      {
        "date": "2024-08-21T08:24:00.000Z",
        "voteCount": 2,
        "content": "To prevent Lambda from processing a message multiple times, you can either configure your event source mapping to include batch item failures in your function response, or you can use the DeleteMessage API to remove messages from the queue as your Lambda function successfully processes them.\n\nTo avoid reprocessing successfully processed messages in a failed batch, you can configure your event source mapping to make only the failed messages visible again. This is called a partial batch response. To turn on partial batch responses, specify ReportBatchItemFailures for the FunctionResponseTypes action when configuring your event source mapping. This lets your function return a partial success, which can help reduce the number of unnecessary retries on records.\n\nSource: https://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html#services-sqs-batchfailurereporting"
      },
      {
        "date": "2024-06-28T15:35:00.000Z",
        "voteCount": 1,
        "content": "\"Implementing partial batch responses\nWhen \"Lambda function encounters an error while processing a batch, all messages\"... \"become visible in the queue\"... \"including messages that Lambda processed successfully.\"\n\"...your function can end up processing the same message several times.\n\"To avoid reprocessing successfully processed messages in a failed batch\" \"configure your event source mapping to make only the failed messages visible again.\"\n\"To turn on partial batch responses, specify ReportBatchItemFailures for the FunctionResponseTypes action when configuring your event source mapping.\"\n\"This lets your function return a partial success, which can help reduce the number of unnecessary retries on records.\"\nhttps://docs.aws.amazon.com/lambda/latest/dg/services-sqs-errorhandling.html"
      },
      {
        "date": "2024-06-28T15:36:00.000Z",
        "voteCount": 1,
        "content": "The URL with \"services-sqs-batchfailurereporting\" pointer seems to be invalid now.  I think the preceeding URL replaced it."
      },
      {
        "date": "2024-05-14T23:00:00.000Z",
        "voteCount": 1,
        "content": "C doesn't address the \"maximize the scalability of Lambda\" while B addresses both,\nwhile batch size is 1, you either fail or success"
      },
      {
        "date": "2024-04-24T09:43:00.000Z",
        "voteCount": 4,
        "content": "To avoid reprocessing successfully processed messages in a failed batch, you can configure your event source mapping to make only the failed messages visible again. This is called a partial batch response. To turn on partial batch responses, specify ReportBatchItemFailures for the FunctionResponseTypes action when configuring your event source mapping. This lets your function return a partial success, which can help reduce the number of unnecessary retries on records. https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting"
      },
      {
        "date": "2024-04-13T15:58:00.000Z",
        "voteCount": 2,
        "content": "answer C"
      },
      {
        "date": "2024-03-16T06:19:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting"
      },
      {
        "date": "2024-02-11T07:05:00.000Z",
        "voteCount": 4,
        "content": "C is correct. We need ReportBatchItemFailures to return only failed items \nA: batch window is the interval process time\nB: batch size is the size of the job\nD: queue visibility timeout is about re-process"
      },
      {
        "date": "2024-01-01T17:45:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting"
      },
      {
        "date": "2023-12-31T15:36:00.000Z",
        "voteCount": 4,
        "content": "Lambda process messages in batches. If one message in the batch fails the whole batch considered failed and all messages in the batch return to the queue. For example if batch has 10 messages and message 5 and 7 failed to get processed, all 10 messages will return to the queue. So, successfully processed messages can get processed again.\n\nNow to prevent this to happen you have two ways (used to be one)\n1. Write your code in a way to identify processed messages and delete them manually from SQS.\n2. Partial batch response that returns only the messages that were failed to be processed (supported since Dec 2021).\n\nReferences:\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-sqs.html#services-sqs-batchfailurereporting"
      },
      {
        "date": "2023-12-31T14:15:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-12-29T22:20:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/amazon/view/129683-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has a new AWS account that teams will use to deploy various applications. The teams will create many Amazon S3 buckets for application-specific purposes and to store AWS CloudTrail logs. The company has enabled Amazon Macie for the account.<br><br>A DevOps engineer needs to optimize the Macie costs for the account without compromising the account's functionality.<br><br>Which solutions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExclude S3 buckets that contain CloudTrail logs from automated discovery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExclude S3 buckets that have public read access from automated discovery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure scheduled daily discovery jobs for all S3 buckets in the account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure discovery jobs to include S3 objects based on the last modified criterion.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure discovery jobs to include S3 objects that are tagged as production only."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-24T09:54:00.000Z",
        "voteCount": 3,
        "content": "Make your sensitive data discovery jobs as targeted and specific as possible in their scope by using the Object criteria"
      },
      {
        "date": "2024-04-13T16:12:00.000Z",
        "voteCount": 2,
        "content": "A&amp;D \nOptions to make discovery jobs more targeted include:\nInclude objects by using the \u201clast modified\u201d criterion \nDon\u2019t scan CloudTrail logs \nConsider using random object sampling \nInclude objects with specific extensions, tags, or storage size with specific tag key/value pairs such as Environment: Production.\nConsider scheduling jobs based on how long objects live in your S3 buckets"
      },
      {
        "date": "2024-04-13T03:20:00.000Z",
        "voteCount": 2,
        "content": "https://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/\n\nOptions to make discovery jobs more targeted include:\n\n    Include objects by using the \u201clast modified\u201d criterion \u2014 \n    Consider using random object sampling \u2014 \n    Include objects with specific extensions, tags, or storage size \u2014"
      },
      {
        "date": "2024-03-30T15:34:00.000Z",
        "voteCount": 2,
        "content": "A - No need to scan these\nD - Reduce costs but not functionallity"
      },
      {
        "date": "2024-03-16T06:11:00.000Z",
        "voteCount": 1,
        "content": "A - No need to scan these\nD - Reduce costs but not functionallity"
      },
      {
        "date": "2024-02-24T12:53:00.000Z",
        "voteCount": 1,
        "content": "AD - Correct\n\nhttps://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/"
      },
      {
        "date": "2024-02-11T07:18:00.000Z",
        "voteCount": 2,
        "content": "A and D are correct: \nA: We dont need to scan Cloudtrail logs, so this is good\nB: Excluding S3 that have public read is just wrong\nC: We have excluded cloudtrail logs S3, so scanning all S3 is not correct\nD: This is good\nE: &lt;Amazon S3 buckets for application-specific purposes and to store AWS CloudTrail logs&gt; means that these S3 buckets are used to store logs and for productions only. Therefore, there will be no production tag, because all of them are production S3 bukets"
      },
      {
        "date": "2024-01-12T12:28:00.000Z",
        "voteCount": 1,
        "content": "E sounds right, but the question is about how to optimize,  so E would make sense it mentioned skipping non-prod log, or scan prod data only"
      },
      {
        "date": "2024-01-12T07:27:00.000Z",
        "voteCount": 1,
        "content": "Answer AD"
      },
      {
        "date": "2024-01-01T17:54:00.000Z",
        "voteCount": 3,
        "content": "Don\u2019t scan CloudTrail logs, Include objects by using the \u201clast modified\u201d criterion :https://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/"
      },
      {
        "date": "2023-12-31T16:54:00.000Z",
        "voteCount": 1,
        "content": "Answer is A &amp; E."
      },
      {
        "date": "2023-12-31T15:59:00.000Z",
        "voteCount": 1,
        "content": "Between D and E: Since the question didn't give any details I picked the broader option. Plus the question mentioned that the account is new, so the team would probably know when the account was created and they can use the last modified criteria. But nowhere mentions the organization's tagging policy. Maybe there is no production tag."
      },
      {
        "date": "2023-12-30T04:49:00.000Z",
        "voteCount": 2,
        "content": "It's A &amp; E. See her for reference: https://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-reduce-the-cost-of-discovering-sensitive-data/"
      },
      {
        "date": "2023-12-30T02:55:00.000Z",
        "voteCount": 1,
        "content": "A and D is correct"
      },
      {
        "date": "2023-12-29T06:17:00.000Z",
        "voteCount": 1,
        "content": "B and E is correct"
      },
      {
        "date": "2023-12-31T16:54:00.000Z",
        "voteCount": 1,
        "content": "You need to include ONLY public ones for cost effective not exclude and hence A."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/amazon/view/129732-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an organization in AWS Organizations to manage its AWS accounts. The company recently acquired another company that has standalone AWS accounts. The acquiring company's DevOps team needs to consolidate the administration of the AWS accounts for both companies and retain full administrative control of the accounts. The DevOps team also needs to collect and group findings across all the accounts to implement and maintain a security posture.<br><br>Which combination of steps should the DevOps team take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvite the acquired company's AWS accounts to join the organization. Create an SCP that has full administrative privileges. Attach the SCP to the management account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvite the acquired company's AWS accounts to join the organization. Create the OrganizationAccountAccessRole IAM role in the invited accounts. Grant permission to the management account to assume the role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Security Hub to collect and group findings across all accounts. Use Security Hub to automatically detect new accounts as the accounts are added to the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Firewall Manager to collect and group findings across all accounts. Enable all features for the organization. Designate an account in the organization as the delegated administrator account for Firewall Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Inspector to collect and group findings across all accounts. Designate an account in the organization as the delegated administrator account for Amazon Inspector."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-11T07:23:00.000Z",
        "voteCount": 5,
        "content": "B and C are correct: &lt;needs to consolidate the administration of the AWS accounts for both companies and retain full administrative control of the accounts&gt; means we need to invite the new accounts to the existing AWS organization. &lt;collect and group findings across all the accounts&gt; means security hub\nA: &lt;Attach the SCP to the management account.&gt;: this is incorrect\nD: Firewall manager is use to centrally manage all FWs, not to collect and group findings\nE: Inspector is used for vulnerability scanning only"
      },
      {
        "date": "2024-04-24T10:07:00.000Z",
        "voteCount": 2,
        "content": "B) https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html\nC) https://docs.aws.amazon.com/securityhub/latest/userguide/securityhub-concepts.html"
      },
      {
        "date": "2024-04-13T16:18:00.000Z",
        "voteCount": 2,
        "content": "answer is BC"
      },
      {
        "date": "2024-03-16T06:09:00.000Z",
        "voteCount": 2,
        "content": "B - Add accounts to the org\nC - collect and group findings"
      },
      {
        "date": "2024-01-12T07:43:00.000Z",
        "voteCount": 4,
        "content": "Answer B and C https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html"
      },
      {
        "date": "2023-12-31T16:03:00.000Z",
        "voteCount": 1,
        "content": "They seem correct"
      },
      {
        "date": "2023-12-29T10:35:00.000Z",
        "voteCount": 3,
        "content": "B is required to access from the management account the new account.\nC will provide an aggregate view of all the security findings."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/amazon/view/129734-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an application and a CI/CD pipeline. The CI/CD pipeline consists of an AWS CodePipeline pipeline and an AWS CodeBuild project. The CodeBuild project runs tests against the application as part of the build process and outputs a test report. The company must keep the test reports for 90 days.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a new stage in the CodePipeline pipeline after the stage that contains the CodeBuild project. Create an Amazon S3 bucket to store the reports. Configure an S3 deploy action type in the new CodePipeline stage with the appropriate path and format for the reports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure an Amazon EventBridge rule that invokes an AWS Lambda function to copy the reports to the S3 bucket when a build is completed. Create an S3 Lifecycle rule to expire the objects after 90 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a new stage in the CodePipeline pipeline. Configure a test action type with the appropriate path and format for the reports. Configure the report expiration time to be 90 days in the CodeBuild project buildspec file.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure the report group as an artifact in the CodeBuild project buildspec file. Configure the S3 bucket as the artifact destination. Set the object expiration to 90 days."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 48,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-17T08:16:00.000Z",
        "voteCount": 8,
        "content": "Best Option: Option D appears to be the most straightforward and effective solution that meets the requirements. It simplifies the process by utilizing CodeBuild's feature to directly send reports to an S3 bucket configured as the artifact destination. By setting the object expiration to 90 days in the S3 bucket settings, it fulfills the requirement to keep the test reports for 90 days. This option does not require additional services for moving the reports to S3, assuming the CodeBuild report group configuration allows for direct report storage in S3 with specified retention policies."
      },
      {
        "date": "2024-03-12T19:35:00.000Z",
        "voteCount": 7,
        "content": "question key word : \nhow to expire objects in S3.\n\nonly S3 Lifecycle rule can expire objects."
      },
      {
        "date": "2024-05-12T07:58:00.000Z",
        "voteCount": 2,
        "content": "I think this is the most valid point."
      },
      {
        "date": "2024-08-20T05:10:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      },
      {
        "date": "2024-05-05T13:15:00.000Z",
        "voteCount": 2,
        "content": "I think D even though this sentence sounds ridiculous: \"Configure the report group as an artifact in the CodeBuild project buildspec file\" they probably meant that report should be sent as an artifact and that's correct."
      },
      {
        "date": "2024-08-21T08:53:00.000Z",
        "voteCount": 2,
        "content": "D is incorrect because it talks about storing report groups as artifacts but since report groups are not CodeBuild artifacts, this cannot be done."
      },
      {
        "date": "2024-05-02T06:10:00.000Z",
        "voteCount": 2,
        "content": "I think B"
      },
      {
        "date": "2024-04-24T10:19:00.000Z",
        "voteCount": 5,
        "content": "Not sure why we are configuring the report group as an artifact in the CodeBuild project buildspec file. Doesn't make sense and we use S3 lifecycle to expire objects automatically."
      },
      {
        "date": "2024-04-13T16:38:00.000Z",
        "voteCount": 5,
        "content": "Both B &amp; D seem to work, but object expiration settings still need to set the lifecycle rule manually for option D"
      },
      {
        "date": "2024-03-26T11:17:00.000Z",
        "voteCount": 1,
        "content": "answer D"
      },
      {
        "date": "2024-03-23T02:05:00.000Z",
        "voteCount": 1,
        "content": "Options B and D both provide viable solutions to meet the requirements. However, D offers a more direct and simplified approach by leveraging the capabilities of AWS CodeBuild and Amazon S3, including the use of S3 Lifecycle policies for managing the expiration of the test reports. Option B is also a valid solution but involves additional components like EventBridge and Lambda, which might not be necessary for this specific requirement. Therefore, D is the recommended solution for its simplicity and direct alignment with the requirements."
      },
      {
        "date": "2024-03-11T19:07:00.000Z",
        "voteCount": 2,
        "content": "rtifacts:\n  files:\n    - '**/*'\n  name: '&lt;artifact-name&gt;'\n  artifactPrefix: '&lt;path-prefix&gt;'\n  discardPaths: yes\n  baseDirectory: 'test-reports'\n  reports:\n    reportGroupName:\n      files:\n        - '**/*'\n      baseDirectory: 'test-reports'\n\nReplace &lt;path-prefix&gt; with the desired path prefix within the S3 bucket."
      },
      {
        "date": "2024-03-04T23:24:00.000Z",
        "voteCount": 2,
        "content": "AWS Lambda is not necessary. Test report files specified in the `base-directory` and `files` in the buildspec.yml reports section are uploaded to S3 by specifying them in the artifacts section as well. This is mean of `Configure the report group as an artifact in the CodeBuild project buildspec file.`"
      },
      {
        "date": "2024-02-12T02:46:00.000Z",
        "voteCount": 4,
        "content": "B is correct: &lt;build process and outputs a test report.&gt; means we need report group in codebuild and store report in S3\nA: No mention of report group in codebuild \nC: No mention of s3 and report group\nD: report group is not the same as an artifact"
      },
      {
        "date": "2024-01-18T10:35:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      },
      {
        "date": "2024-01-12T12:35:00.000Z",
        "voteCount": 3,
        "content": "Pattern: run lambda inside PostBuild to zip up unit test result folder and copy to S3"
      },
      {
        "date": "2024-01-12T12:38:00.000Z",
        "voteCount": 1,
        "content": "B is not exactly use this pattern,  but using eventBridge also works"
      },
      {
        "date": "2024-01-11T10:25:00.000Z",
        "voteCount": 4,
        "content": "https://malsouli.medium.com/10-smart-ways-to-use-aws-codebuild-22a8ee0d9302 \n\nThe test reports have to be handled separately , either via postbuild, or via eventbridge as suggested in B"
      },
      {
        "date": "2024-01-09T07:21:00.000Z",
        "voteCount": 4,
        "content": "definitely B"
      },
      {
        "date": "2024-01-02T01:29:00.000Z",
        "voteCount": 5,
        "content": "\"B. Add a report group in the CodeBuild project buildspec file with the appropriate path and format for the reports. Create an Amazon S3 bucket to store the reports. Configure an Amazon EventBridge rule that invokes an AWS Lambda function to copy the reports to the S3 bucket when a build is completed. Create an S3 Lifecycle rule to expire the objects after 90 days.\": A test report expires 30 days after it is created. You cannot view an expired test report, but you can export the test results to raw test result files in an S3 bucket. Exported raw test files do not expire. https://docs.aws.amazon.com/codebuild/latest/userguide/test-report.html when you export a test group to s3, it is zipped https://docs.aws.amazon.com/codebuild/latest/userguide/test-report-group-create-cli.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/amazon/view/129685-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an Amazon API Gateway regional REST API to host its application API. The REST API has a custom domain. The REST API's default endpoint is deactivated.<br><br>The company's internal teams consume the API. The company wants to use mutual TLS between the API and the internal teams as an additional layer of authentication.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Certificate Manager (ACM) to create a private certificate authority (CA). Provision a client certificate that is signed by the private CA.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision a client certificate that is signed by a public certificate authority (CA). Import the certificate into AWS Certificate Manager (ACM).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the provisioned client certificate to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the client certificate that is stored in the S3 bucket as the trust store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the provisioned client certificate private key to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the private key that is stored in the S3 bucket as the trust store.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpload the root private certificate authority (CA) certificate to an Amazon S3 bucket. Configure the API Gateway mutual TLS to use the private CA certificate that is stored in the S3 bucket as the trust store.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-31T16:38:00.000Z",
        "voteCount": 6,
        "content": "A. Because it's only for internal teams.\nE. Because the truststore dictates which CAs to trust. If you have intermediate CAs those also need to be present in the S3 bucket."
      },
      {
        "date": "2023-12-31T16:38:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-mutual-tls.html"
      },
      {
        "date": "2024-05-09T02:48:00.000Z",
        "voteCount": 2,
        "content": "A. use ACM to generate cert\nE. See https://aws.amazon.com/blogs/compute/introducing-mutual-tls-authentication-for-amazon-api-gateway/"
      },
      {
        "date": "2024-04-17T06:36:00.000Z",
        "voteCount": 4,
        "content": "C is incorrect because the trust store should contain the root CA certificate, not the client certificate.\nRoot CA certificate is used to validate the client certificates (can be many) presented by the clients. If the client certificate itself is in the trust store, it would mean that only that specific client is trusted, which is not practical in a scenario where there are multiple clients (read it as company's internal teams)."
      },
      {
        "date": "2024-03-30T16:21:00.000Z",
        "voteCount": 1,
        "content": "A and C. Details are everything in an Investigation...\nWhat API Gateway needs is the Client Certificate generated by option A and not the CA"
      },
      {
        "date": "2024-03-16T06:02:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-mutual-tls.html\nAfter reading the above documentation I would determine A &amp; E"
      },
      {
        "date": "2024-03-14T09:24:00.000Z",
        "voteCount": 1,
        "content": "Check this article, https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-mutual-tls.html. You need to upload the truststore to an Amazon S3 bucket in a single file"
      },
      {
        "date": "2024-02-13T07:23:00.000Z",
        "voteCount": 3,
        "content": "After reading this I would suggest A &amp; E"
      },
      {
        "date": "2024-02-12T02:54:00.000Z",
        "voteCount": 1,
        "content": "A and C is correct: \nA: we prefer AWS service more than a public one, which is B\nB: The reason is explained in option a\nC: Upload the provisioned to S3 bucket. \nD: should not upload private key to anywhere. \nE: This option has no connection to option A."
      },
      {
        "date": "2024-02-06T12:01:00.000Z",
        "voteCount": 2,
        "content": "Details can be found here: https://aws.amazon.com/blogs/compute/introducing-mutual-tls-authentication-for-amazon-api-gateway/"
      },
      {
        "date": "2024-01-11T10:34:00.000Z",
        "voteCount": 3,
        "content": "you need to use ROOT CA , or whatever the certificated being used to sign other certificate in truststore,"
      },
      {
        "date": "2023-12-31T17:13:00.000Z",
        "voteCount": 2,
        "content": "You shall NEVER upload private cert or key to an S3 bucket. This is a bad practise and hence C.\n\nI also choose A because you need private cert between the internal teams and the API."
      },
      {
        "date": "2024-01-06T01:00:00.000Z",
        "voteCount": 2,
        "content": "Option C and Option D involve uploading the client certificate or its private key to an S3 bucket and configuring the API Gateway to use them as the trust store. This is not a recommended practice as it exposes sensitive information to potential security risks. The trust store for mutual TLS should typically involve the CA certificate or a certificate chain that verifies the client certificates, not the client certificates or private keys themselves."
      },
      {
        "date": "2023-12-29T06:29:00.000Z",
        "voteCount": 3,
        "content": "A and E"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/amazon/view/129833-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Directory Service for Microsoft Active Directory as its identity provider (IdP). The company requires all infrastructure to be defined and deployed by AWS CloudFormation.<br><br>A DevOps engineer needs to create a fleet of Windows-based Amazon EC2 instances to host an application. The DevOps engineer has created a CloudFormation template that contains an EC2 launch template, IAM role, EC2 security group, and EC2 Auto Scaling group. The DevOps engineer must implement a solution that joins all EC2 instances to the domain of the AWS Managed Microsoft AD directory.<br><br>Which solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template, create an AWS::SSM::Document resource that joins the EC2 instance to the AWS Managed Microsoft AD domain by using the parameters for the existing directory. Update the launch template to include the SSMAssociation property to use the new SSM document. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances use.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the CloudFormation template, update the launch template to include specific tags that propagate on launch. Create an AWS::SSM::Association resource to associate the AWS-JoinDirectoryServiceDomain Automation runbook with the EC2 instances that have the specified tags. Define the required parameters to join the AWS Managed Microsoft AD directory. Attach the AmazonSSMManagedInstanceCore and AmazonSSMDirectoryServiceAccess AWS managed policies to the IAM role that the EC2 instances use.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the existing AWS Managed Microsoft AD domain connection details in AWS Secrets Manager. In the CloudFormation template, create an AWS::SSM::Association resource to associate the AWS-CreateManagedWindowsInstanceWithApproval Automation runbook with the EC2 Auto Scaling group. Pass the ARNs for the parameters from Secrets Manager to join the domain. Attach the AmazonSSMDirectoryServiceAccess and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the existing AWS Managed Microsoft AD domain administrator credentials in AWS Secrets Manager. In the CloudFormation template, update the EC2 launch template to include user data. Configure the user data to pull the administrator credentials from Secrets Manager and to join the AWS Managed Microsoft AD domain. Attach the AmazonSSMManagedInstanceCore and SecretsManagerReadWrite AWS managed policies to the IAM role that the EC2 instances use."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-10T00:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct - https://docs.aws.amazon.com/directoryservice/latest/admin-guide/step4_test_ec2_access.html"
      },
      {
        "date": "2024-04-13T17:19:00.000Z",
        "voteCount": 2,
        "content": "ans is B"
      },
      {
        "date": "2024-02-12T03:06:00.000Z",
        "voteCount": 3,
        "content": "B is correct: we need to use AWS:SSM::Document with the AWS-JoinDirectoryServiceDomain automation runbook for this task\nA: no mention of the name of runbook to join domain  \nC: AWS-CreateManagedWindowsInstanceWithApproval Automation runbook is used for creating a windows instance, not to join domain\nD: no mention of AWS::SSM::Document"
      },
      {
        "date": "2024-01-18T10:44:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-01-12T12:48:00.000Z",
        "voteCount": 2,
        "content": "keyword: JoinDirectoryServiceDomain"
      },
      {
        "date": "2023-12-31T16:52:00.000Z",
        "voteCount": 2,
        "content": "Must be B:\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/walkthrough-powershell.html#walkthrough-powershell-domain-join"
      },
      {
        "date": "2023-12-30T01:05:00.000Z",
        "voteCount": 2,
        "content": "It\u2019s B"
      },
      {
        "date": "2023-12-29T22:32:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/amazon/view/129736-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Organizations to manage its AWS accounts. The company has a root OU that has a child OU. The root OU has an SCP that allows all actions on all resources. The child OU has an SCP that allows all actions for Amazon DynamoDB and AWS Lambda, and denies all other actions.<br><br>The company has an AWS account that is named vendor-data in the child OU. A DevOps engineer has an IAM user that is attached to the Administrator Access IAM policy in the vendor-data account. The DevOps engineer attempts to launch an Amazon EC2 instance in the vendor-data account but receives an access denied error.<br><br>Which change should the DevOps engineer make to launch the EC2 instance in the vendor-data account?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach the AmazonEC2FullAccess IAM policy to the IAM user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new SCP that allows all actions for Amazon EC2. Attach the SCP to the vendor-data account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the SCP in the child OU to allow all actions for Amazon EC2.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new SCP that allows all actions for Amazon EC2. Attach the SCP to the root OU."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-25T05:07:00.000Z",
        "voteCount": 7,
        "content": "I vote B. can't understand why B is not correct answer. SCP can be attached to account.\n\nFor the C, it is possible. but the potential risk is it's not only allow all EC2 action on \"vendor-data\" account, but also allow all EC2 actions in other account under the child OU. which is not a best practice."
      },
      {
        "date": "2023-12-29T11:07:00.000Z",
        "voteCount": 5,
        "content": "It's C - Allow must be explicit from root all the way down to the account level. Since it's not specified in the OU the only way to make it available to vendor-account is to change the OU policy."
      },
      {
        "date": "2024-08-15T23:41:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. You can attach SCP to vendor-data account. however there is deny rule at OU level and that will apply and without updating that your SCP at vendor data account is not useful. As the account will inherit SCP applied at OU."
      },
      {
        "date": "2024-08-12T12:36:00.000Z",
        "voteCount": 1,
        "content": "B - Incorrect IMO - The question doesn't ask about taking away anything currently allowed in the existing SCP"
      },
      {
        "date": "2024-04-24T10:37:00.000Z",
        "voteCount": 3,
        "content": "By updating the SCP in the child OU to allow all actions for Amazon EC2, the DevOps engineer can grant the necessary permissions to launch EC2 instances in the vendor-data account while maintaining the desired restrictions for other services and accounts within the child OU."
      },
      {
        "date": "2024-04-13T17:22:00.000Z",
        "voteCount": 3,
        "content": "answer is C"
      },
      {
        "date": "2024-03-30T16:28:00.000Z",
        "voteCount": 2,
        "content": "C, details are everything during an investigation"
      },
      {
        "date": "2024-03-26T11:44:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer!!!!"
      },
      {
        "date": "2024-03-28T02:07:00.000Z",
        "voteCount": 2,
        "content": "Edit: C is correct"
      },
      {
        "date": "2024-03-16T05:54:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2024-02-12T03:10:00.000Z",
        "voteCount": 3,
        "content": "C is correct: \nA: We need to modify SCP not IAM policy\nB: SCP is attached to OUs, not account\nD: This option changes nothing, as the roout OU has already allowed all actions"
      },
      {
        "date": "2024-03-26T11:44:00.000Z",
        "voteCount": 3,
        "content": "SCP can be attached to account: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html"
      },
      {
        "date": "2024-01-12T12:50:00.000Z",
        "voteCount": 1,
        "content": "update policy to include EC2"
      },
      {
        "date": "2023-12-31T16:56:00.000Z",
        "voteCount": 1,
        "content": "The only correct option"
      },
      {
        "date": "2023-12-29T22:33:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/amazon/view/129737-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's security policies require the use of security hardened AMIs in production environments. A DevOps engineer has used EC2 Image Builder to create a pipeline that builds the AMIs on a recurring schedule.<br><br>The DevOps engineer needs to update the launch templates of the company's Auto Scaling groups. The Auto Scaling groups must use the newest AMIs during the launch of Amazon EC2 instances.<br><br>Which solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Systems Manager Run Command document that updates the launch templates of the Auto Scaling groups with the newest AMI ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge rule to receive new AMI events from Image Builder. Target an AWS Lambda function that updates the launch templates of the Auto Scaling groups with the newest AMI ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the launch template to use a value from AWS Systems Manager Parameter Store for the AMI ID. Configure the Image Builder pipeline to update the Parameter Store value with the newest AMI ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Image Builder distribution settings to update the launch templates with the newest AMI IConfigure the Auto Scaling groups to use the newest version of the launch template.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T03:17:00.000Z",
        "voteCount": 7,
        "content": "D is correct: Image builder has a built-in that allow updating EC2 launch template\nA: AWS Systems Manager Run Command document is used for running scripts on EC2, not to update\nB: Lambda is used for other tasks, not this one\nC: This seems to be a feasible option, but we can update the launch template directly without using parameter store"
      },
      {
        "date": "2023-12-31T17:00:00.000Z",
        "voteCount": 5,
        "content": "Definitely D according to this:\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/dist-using-launch-template.html"
      },
      {
        "date": "2024-08-26T05:16:00.000Z",
        "voteCount": 1,
        "content": "I would go with C even when D seems to be the primary choice.\nIn D, you would need to maintain all of the launchTemplateConfigurations in a list, which means there is a lot of overhead. \nWith C this is not the case"
      },
      {
        "date": "2024-07-01T15:39:00.000Z",
        "voteCount": 1,
        "content": "Answers B, C, and D can work.  I'm leaning towards \"D\", but I'm witholding a formal vote for now.  It appears the \"correct\" answer may depend on how you interpret requirements.\nNOT B: EventBridge/Lamba can work, but not as simple as D or C.  It DOES \"update the launch templates of the company's Auto Scaling groups.\"\nNOT C: Answer C can work and is fairly simple, but it DOES NOT \"update the launch templates of the company's Auto Scaling groups\", because it does not need to, which could be argued is \"operationally efficient\".\nYES D: Seems like simple solution.  ASG does need to be updated, but I don't know if that means defining someting like an $LATEST AMI alias (pointer) in ASG, or if ASG actually needs to be updated for each new version of Launch template.  This solution could be more complex than C:."
      },
      {
        "date": "2024-05-13T11:31:00.000Z",
        "voteCount": 1,
        "content": "C: This involves configuring the launch template to reference the AMI ID stored in the AWS Systems Manager Parameter Store. The EC2 Image Builder pipeline is then set up to update this Parameter Store value each time a new AMI is built. By doing so, the launch template always points to the latest AMI without requiring manual updates each time a new AMI is built. This approach automates the update process and ensures that Auto Scaling groups always use the most recent and secure AMIs, with minimal manual intervention and operational overhead."
      },
      {
        "date": "2024-04-13T17:25:00.000Z",
        "voteCount": 2,
        "content": "ans is D"
      },
      {
        "date": "2024-03-30T16:32:00.000Z",
        "voteCount": 4,
        "content": "D is the correct and best practice suggested by aws\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/dist-using-launch-template.html"
      },
      {
        "date": "2024-03-26T11:49:00.000Z",
        "voteCount": 2,
        "content": "answer B"
      },
      {
        "date": "2024-03-24T03:12:00.000Z",
        "voteCount": 1,
        "content": "C is not efficiency .\nhttps://aws.amazon.com/blogs/compute/tracking-the-latest-server-images-in-amazon-ec2-image-builder-pipelines/"
      },
      {
        "date": "2024-03-16T08:57:00.000Z",
        "voteCount": 2,
        "content": "its D, 100%\nConfigure the Image Builder distribution settings to update the launch templates with the newest AMI IConfigure the Auto Scaling groups to use the newest version of the launch template."
      },
      {
        "date": "2024-02-19T02:35:00.000Z",
        "voteCount": 2,
        "content": "add Explanation 'c' cause   = chat gpt4.0  = c and i think\nThe most operationally efficient solution is to use AWS systems manager parameter store1 to store the ami di and reference it in the launch template2. this way, the launch template does not nned to be updated event titme a new ami is created by image buider, instead the image builder prpeline, can update the parameter store value with the newest ami id3,j and the auto scaling gorup can launch instances using the lastest value from parameter store"
      },
      {
        "date": "2024-03-30T16:36:00.000Z",
        "voteCount": 2,
        "content": "don't trust chat gpt to help you pass exam, studying is the right way. \nQuestion says \"Which solution will meet these requirements with the MOST operational efficiency?\" you are adding more steps than it needs in D.\n\nOption C involves using Systems Manager Parameter Store to manage the AMI ID, but it requires manual updates to the Parameter Store value, which may not be as efficient or automated as directly configuring Image Builder to update the launch templates \n\nremember that Parameter store is not supported in distribution settings of image builder"
      },
      {
        "date": "2024-02-18T22:47:00.000Z",
        "voteCount": 2,
        "content": "Given these options, C represents the most operationally efficient solution that meets the requirements. It automates the process of using the newest AMIs for EC2 instance launches within Auto Scaling groups by leveraging the AWS Systems Manager Parameter Store and EC2 Image Builder. This method ensures that the Auto Scaling groups always use the latest security-hardened AMIs without needing to manually update launch templates for each new AMI release, thereby streamlining operations and maintaining compliance with the company's security policies."
      },
      {
        "date": "2024-01-12T13:03:00.000Z",
        "voteCount": 3,
        "content": "D is correct. Actually C is also a valid option to pass AMI ID into launch template,  but it has lots of limitations and not used in enterprise environment"
      },
      {
        "date": "2023-12-31T17:31:00.000Z",
        "voteCount": 2,
        "content": "Answer is D."
      },
      {
        "date": "2023-12-29T22:35:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-12-29T11:17:00.000Z",
        "voteCount": 2,
        "content": "B seems like an option"
      },
      {
        "date": "2023-12-29T11:22:00.000Z",
        "voteCount": 1,
        "content": "now that I think twice about it, D seems to be the most operationally efficient. I change my answer to D."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/amazon/view/129686-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has configured an Amazon S3 event source on an AWS Lambda function. The company needs the Lambda function to run when a new object is created or an existing object is modified in a particular S3 bucket. The Lambda function will use the S3 bucket name and the S3 object key of the incoming event to read the contents of the created or modified S3 object. The Lambda function will parse the contents and save the parsed contents to an Amazon DynamoDB table.<br><br>The Lambda function's execution role has permissions to read from the S3 bucket and to write to the DynamoDB table. During testing, a DevOps engineer discovers that the Lambda function does not run when objects are added to the S3 bucket or when existing objects are modified.<br><br>Which solution will resolve this problem?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the memory of the Lambda function to give the function the ability to process large files from the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a resource policy on the Lambda function to grant Amazon S3 the permission to invoke the Lambda function for the S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon Simple Queue Service (Amazon SQS) queue as an OnFailure destination for the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision space in the /tmp folder of the Lambda function to give the function the ability to process large files from the S3 bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T03:24:00.000Z",
        "voteCount": 6,
        "content": "B: is correct: Need to add a permission in lambda resource-based policy, which allow S3 to invode lambda\nA: If there is insufficient memory, lambda still runs. This case is about lambda not running at all \nC: We dont need SQS for dead-letter queue here\nD: Lambda does not run in a test, which proves that the problem does not lie in disk space because in tests, testers usually wont use large objects"
      },
      {
        "date": "2024-07-30T00:02:00.000Z",
        "voteCount": 1,
        "content": "B due to permission issue"
      },
      {
        "date": "2024-04-13T17:48:00.000Z",
        "voteCount": 2,
        "content": "b is correct"
      },
      {
        "date": "2024-03-16T04:22:00.000Z",
        "voteCount": 2,
        "content": "B - Is correct\nA C and D have no relevance to the problem"
      },
      {
        "date": "2024-01-18T10:51:00.000Z",
        "voteCount": 3,
        "content": "b is correct"
      },
      {
        "date": "2023-12-31T17:03:00.000Z",
        "voteCount": 3,
        "content": "Lambda must allow S3 to invoke it"
      },
      {
        "date": "2023-12-29T06:40:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/amazon/view/129834-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has deployed a critical application in two AWS Regions. The application uses an Application Load Balancer (ALB) in both Regions. The company has Amazon Route 53 alias DNS records for both ALBs.<br><br>The company uses Amazon Route 53 Application Recovery Controller to ensure that the application can fail over between the two Regions. The Route 53 ARC configuration includes a routing control for both Regions. The company uses Route 53 ARC to perform quarterly disaster recovery (DR) tests.<br><br>During the most recent DR test, a DevOps engineer accidentally turned off both routing controls. The company needs to ensure that at least one routing control is turned on at all times.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Route 53 ARC, create a new assertion safety rule. Apply the assertion safety rule to the two routing controls. Configure the rule with the ATLEAST type with a threshold of 1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Route 53 ARC, create a new gating safety rule. Apply the assertion safety rule to the two routing controls. Configure the rule with the OR type with a threshold of 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Route 53 ARC, create a new resource set. Configure the resource set with an AWS::Route53::HealthCheck resource type. Specify the ARNs of the two routing controls as the target resource. Create a new readiness check for the resource set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Route 53 ARC, create a new resource set. Configure the resource set with an AWS::Route53RecoveryReadiness::DNSTargetResource resource type. Add the domain names of the two Route 53 alias DNS records as the target resource. Create a new readiness check for the resource set."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-25T00:03:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html"
      },
      {
        "date": "2024-04-13T18:03:00.000Z",
        "voteCount": 1,
        "content": "answer is A"
      },
      {
        "date": "2024-03-30T16:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct\nOption D is incorrect due to a few reasons:\n\n1. No Automation for Report Copying: Unlike Option B, there's no mention of any automated process to copy the reports to the S3 bucket. It relies solely on configuring the S3 bucket as an artifact destination. Without automation, someone would have to manually manage the copying process, which is less efficient and prone to errors.\n\n2. Expiration: While Option D mentions setting object expiration to 90 days, it doesn't specify how this would be achieved. S3 Lifecycle rules are typically used to manage object expiration, but there's no mention of setting up such a rule in this option."
      },
      {
        "date": "2024-02-12T03:38:00.000Z",
        "voteCount": 3,
        "content": "A is correct: assertion rule to make sure that atleast on gate is always open. This rules are basically things that users cannot do or only allow to do\nB: This gating rule is basically an on-off swith for a set of ARCs. If there is a controller that we dont want to turn off, specify this rule. This rule might help us achive the goal of the question. However, this requires we specify the exact name of the controller that should not be turned off. Meanwhile, the question requires that any controller can be turned off but at least one must be up and running. Therefore, this is not the right option"
      },
      {
        "date": "2024-02-12T03:38:00.000Z",
        "voteCount": 1,
        "content": "C and D are irrelevant. They are about creating new resource"
      },
      {
        "date": "2024-01-18T11:00:00.000Z",
        "voteCount": 2,
        "content": "A is correct : https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html"
      },
      {
        "date": "2024-01-12T08:16:00.000Z",
        "voteCount": 2,
        "content": "answer A https://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html"
      },
      {
        "date": "2023-12-30T00:18:00.000Z",
        "voteCount": 3,
        "content": "It's an assertion rule with ATLEAST threshold set to 1. So, A.\n\nHere is a reference: \nhttps://docs.aws.amazon.com/r53recovery/latest/dg/routing-control.safety-rules.html\nhttps://docs.aws.amazon.com/r53recovery/latest/dg/getting-started-cli-routing.html#getting-started-cli-routing.safety"
      },
      {
        "date": "2023-12-29T22:36:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/amazon/view/129687-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A healthcare services company is concerned about the growing costs of software licensing for an application for monitoring patient wellness. The company wants to create an audit process to ensure that the application is running exclusively on Amazon EC2 Dedicated Hosts. A DevOps engineer must create a workflow to audit the application to ensure compliance.<br><br>What steps should the engineer take to meet this requirement with the LEAST administrative overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Systems Manager Configuration Compliance. Use calls to the put-compliance-items API action to scan and build a database of noncompliant EC2 instances based on their host placement configuration. Use an Amazon DynamoDB table to store these instance IDs for fast access. Generate a report through Systems Manager by calling the list-compliance-summaries API action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse custom Java code running on an EC2 instance. Set up EC2 Auto Scaling for the instance depending on the number of instances to be checked. Send the list of noncompliant EC2 instance IDs to an Amazon SQS queue. Set up another worker instance to process instance IDs from the SQS queue and write them to Amazon DynamoDUse an AWS Lambda function to terminate noncompliant instance IDs obtained from the queue, and send them to an Amazon SNS email topic for distribution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Config. Identify all EC2 instances to be audited by enabling Config Recording on all Amazon EC2 resources for the region. Create a custom AWS Config rule that triggers an AWS Lambda function by using the \"config-rule-change -triggered\" blueprint. Modify the Lambda evaluateCompliance() function to verify host placement to return a NON_COMPLIANT result if the instance is not running on an EC2 Dedicated Host. Use the AWS Config report to address noncompliant instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudTrail. Identify all EC2 instances to be audited by analyzing all calls to the EC2 RunCommand API action. Invoke an AWS Lambda function that analyzes the host placement of the instance. Store the EC2 instance ID of noncompliant resources in an Amazon RDS for MySQL DB instance. Generate a report by querying the RDS instance and exporting the query results to a CSV text file."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-02T06:13:00.000Z",
        "voteCount": 1,
        "content": "C for me"
      },
      {
        "date": "2024-02-12T03:45:00.000Z",
        "voteCount": 4,
        "content": "C is correct: Using Config is the right way\nA: &lt;Use an Amazon DynamoDB table to store these instance IDs for fast access&gt; DynamoDB is used primarily for storing web section data, not to store these IDs\nB: Should not use custom Java code on Ec2. Additionally, This option mention terminating non-compliance ones, which is incorrect. We only need an audit workflow\nD: Cloud trail is for auditing user activities, not to check non-compliance EC2 instacnes"
      },
      {
        "date": "2023-12-30T00:20:00.000Z",
        "voteCount": 2,
        "content": "It's C"
      },
      {
        "date": "2023-12-29T06:44:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/amazon/view/129688-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is planning to deploy a Ruby-based application to production. The application needs to interact with an Amazon RDS for MySQL database and should have automatic scaling and high availability. The stored data in the database is critical and should persist regardless of the state of the application stack.<br><br>The DevOps engineer needs to set up an automated deployment strategy for the application with automatic rollbacks. The solution also must alert the application team when a deployment fails.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on AWS Elastic Beanstalk. Deploy an Amazon RDS for MySQL DB instance as part of the Elastic Beanstalk configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the application on AWS Elastic Beanstalk. Deploy a separate Amazon RDS for MySQL DB instance outside of Elastic Beanstalk.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a notification email address that alerts the application team in the AWS Elastic Beanstalk configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge rule to monitor AWS Health events. Use an Amazon Simple Notification Service (Amazon SNS) topic as a target to alert the application team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the immutable deployment method to deploy new application versions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the rolling deployment method to deploy new application versions."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "BDE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "ADE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-29T11:50:00.000Z",
        "voteCount": 7,
        "content": "Move RDS out of Beanstalk as it is critical. AWS Health can't check deployment health but services only."
      },
      {
        "date": "2024-04-25T00:30:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html"
      },
      {
        "date": "2024-04-13T18:33:00.000Z",
        "voteCount": 4,
        "content": "BCE\nImmutable deployments perform an immutable update to launch a full set of new instances running the new version of the application in a separate Auto Scaling group, alongside the instances running the old version. Immutable deployments can prevent issues caused by partially completed rolling deployments. If the new instances don't pass health checks, Elastic Beanstalk terminates them, leaving the original instances untouched."
      },
      {
        "date": "2024-03-30T16:51:00.000Z",
        "voteCount": 2,
        "content": "B, C, E is the correct answer since\nA. its is not a good practice deploy database layer with EB\nD. is not directly related to the deployment strategy for the application and does not address the requirement for automatic rollbacks.\nF. In a rolling deployment, the application version is gradually deployed across the existing instances in the environment. Each instance is updated one at a time, and the application remains available throughout the deployment process. This approach updates the application in a phased manner, minimizing downtime but potentially complicating rollback if issues arise."
      },
      {
        "date": "2024-03-26T12:03:00.000Z",
        "voteCount": 1,
        "content": "BDE for sure"
      },
      {
        "date": "2024-03-28T02:20:00.000Z",
        "voteCount": 1,
        "content": "BCE is correct"
      },
      {
        "date": "2024-03-16T09:27:00.000Z",
        "voteCount": 2,
        "content": "BCE is correct"
      },
      {
        "date": "2024-03-11T19:35:00.000Z",
        "voteCount": 1,
        "content": "WS Elastic Beanstalk itself does not directly provide email notification capabilities. However, you can integrate AWS Elastic Beanstalk with other AWS services to achieve email notifications."
      },
      {
        "date": "2024-02-24T14:26:00.000Z",
        "voteCount": 4,
        "content": "BCE - \nB - Makes sense as you want the RDS to persist.\nC- You can set an email notification during the Elastic Beanstalk configuration\nE - Immutable for roll back as previous versions persists"
      },
      {
        "date": "2024-02-22T07:34:00.000Z",
        "voteCount": 1,
        "content": "BDE seems correct"
      },
      {
        "date": "2024-02-22T07:38:00.000Z",
        "voteCount": 3,
        "content": "Apologies it will be BCE.\nBeanstalk can directly send notifications via SNS: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.sns.html"
      },
      {
        "date": "2024-02-12T03:57:00.000Z",
        "voteCount": 1,
        "content": "BD and E are correct:\nA: Beanstalk does not support Amazon RDS for MySQL DB instance\nB: We need to deploy a separate Amazon RDS for MySQL DB instance \nC: Beanstalk does not send notification to email address. We need SNS\nD: This option monitor service health and send to SNS\nE: Immutable allow roll back because it does not delete the original one\nF: This replaces the original one, rolling out the new version gradually. Therefore, we cannot roll back"
      },
      {
        "date": "2023-12-31T17:57:00.000Z",
        "voteCount": 3,
        "content": "Same reasons as csG13"
      },
      {
        "date": "2023-12-29T06:46:00.000Z",
        "voteCount": 1,
        "content": "ADE is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/amazon/view/129689-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is using AWS CodePipeline to deploy an application. According to a new guideline, a member of the company's security team must sign off on any application changes before the changes are deployed into production. The approval must be recorded and retained.<br><br>Which combination of actions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure CodePipeline to write actions to Amazon CloudWatch Logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure CodePipeline to write actions to an Amazon S3 bucket at the end of each pipeline stage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudTrail trail to deliver logs to Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodePipeline custom action to invoke an AWS Lambda function for approval. Create a policy that gives the security team access to manage CodePipeline custom actions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CodePipeline manual approval action before the deployment step. Create a policy that grants the security team access to approve manual approval stages.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T04:01:00.000Z",
        "voteCount": 6,
        "content": "C and E are correct: \nA: Cloudwatch Logs is to store logs from AWS resources like EC2, not codepipeline\nB: We dont need to store codepipeline actions in S3. \nC: We need to monitor users'actions, so using cloudtrail to store logs to S3 is the recommended one\nD: We should not invoke AWS lambda for approval\nE: This is the recommended one"
      },
      {
        "date": "2024-04-25T00:37:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html"
      },
      {
        "date": "2024-04-13T18:40:00.000Z",
        "voteCount": 1,
        "content": "ans ce"
      },
      {
        "date": "2024-03-30T16:58:00.000Z",
        "voteCount": 3,
        "content": "C- Logging, since Cloudwatch Logs and writelogs to S3 can not capture the Approval that only CloudTrail can\nE - Manual Approval Step is natively supported by codepipeline, no need to make it more complex with anything"
      },
      {
        "date": "2024-03-16T04:12:00.000Z",
        "voteCount": 2,
        "content": "C- Logging\nE - Manual Approval Step"
      },
      {
        "date": "2024-02-28T02:12:00.000Z",
        "voteCount": 2,
        "content": "C-E for sure"
      },
      {
        "date": "2024-01-10T11:37:00.000Z",
        "voteCount": 2,
        "content": "https://stelligent.com/2019/06/11/aws-codepipeline-approval-gate-tracking/"
      },
      {
        "date": "2024-01-02T11:35:00.000Z",
        "voteCount": 3,
        "content": "C and E: The approval process is an AWS API Event and this is managed by CloudTrail\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/incident-response.html"
      },
      {
        "date": "2024-01-02T03:37:00.000Z",
        "voteCount": 2,
        "content": "CE:\nAWS CodePipeline is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in CodePipeline;. CloudTrail captures all API calls for CodePipeline as events. The calls captured include calls from the CodePipeline console and code calls to the CodePipeline API operations. If you create a trail, you can enable continuous delivery of CloudTrail events to an Amazon S3 bucket, including events for CodePipeline. If you don't configure a trail, you can still view the most recent events in the CloudTrail console in Event history. Using the information collected by CloudTrail, you can determine the request that was made to CodePipeline, the IP address from which the request was made, who made the request, when it was made, and additional details.\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html"
      },
      {
        "date": "2023-12-31T18:09:00.000Z",
        "voteCount": 3,
        "content": "C. because actions performed by the security team are api calls. And api calls go into CloudTrail, if you want to retain them we have to send them into an S3 bucket.\n\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/monitoring-cloudtrail-logs.html"
      },
      {
        "date": "2023-12-30T05:31:00.000Z",
        "voteCount": 2,
        "content": "E is for Manual Approval\nA is for recorded and retained"
      },
      {
        "date": "2023-12-29T06:48:00.000Z",
        "voteCount": 1,
        "content": "D and E is correct"
      },
      {
        "date": "2024-03-30T16:56:00.000Z",
        "voteCount": 1,
        "content": "Option D does not address the need \"The approval must be recorded and retained.\""
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/amazon/view/129738-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company requires its internal business teams to launch resources through pre-approved AWS CloudFormation templates only. The security team requires automated monitoring when resources drift from their expected state.<br><br>Which strategy should be used to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow users to deploy CloudFormation stacks using a CloudFormation service role only. Use CloudFormation drift detection to detect when resources have drifted from their expected state.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow users to deploy CloudFormation stacks using a CloudFormation service role only. Use AWS Config rules to detect when resources have drifted from their expected state.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a launch constraint. Use AWS Config rules to detect when resources have drifted from their expected state.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow users to deploy CloudFormation stacks using AWS Service Catalog only. Enforce the use of a template constraint. Use Amazon EventBridge notifications to detect when resources have drifted from their expected state."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-08T12:55:00.000Z",
        "voteCount": 1,
        "content": "In the Option A, Drift detection must be run manually or scheduled, which doesn't fully meet the requirement for \"automated monitoring.\""
      },
      {
        "date": "2024-07-30T00:31:00.000Z",
        "voteCount": 1,
        "content": "Keypoint: AWS Config for drift detection"
      },
      {
        "date": "2024-04-25T03:35:00.000Z",
        "voteCount": 3,
        "content": "Checks if the actual configuration of a AWS CloudFormation (AWS CloudFormation) stack differs, or has drifted, from the expected configuration. A stack is considered to have drifted if one or more of its resources differ from their expected configuration. https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html"
      },
      {
        "date": "2024-04-13T18:49:00.000Z",
        "voteCount": 2,
        "content": "anwer c"
      },
      {
        "date": "2024-03-16T04:08:00.000Z",
        "voteCount": 3,
        "content": "C - Service Catalog + AWS Config"
      },
      {
        "date": "2024-02-12T04:09:00.000Z",
        "voteCount": 3,
        "content": "C is correct: &lt;pre-approved AWS CloudFormation templates only&gt; means we need service catalog\nA and B: &lt; Allow users to deploy CloudFormation stacks using a CloudFormation service role only&gt;: With service role, users can modify anything in the template\nD: Eventbridge cannot detect drift"
      },
      {
        "date": "2024-01-18T13:26:00.000Z",
        "voteCount": 2,
        "content": "Use config for drift detection"
      },
      {
        "date": "2024-01-12T13:14:00.000Z",
        "voteCount": 3,
        "content": "Config for drift detection"
      },
      {
        "date": "2023-12-31T18:17:00.000Z",
        "voteCount": 4,
        "content": "You can use AWS Managed Config cloudformation-stack-drift-detection-check rule to evaluate drift in CloudFormation stacks.\n\nhttps://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html"
      },
      {
        "date": "2023-12-29T22:44:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-12-29T11:57:00.000Z",
        "voteCount": 1,
        "content": "it's C"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 182,
    "url": "https://www.examtopics.com/discussions/amazon/view/129692-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has multiple development groups working in a single shared AWS account. The senior manager of the groups wants to be alerted via a third-party API call when the creation of resources approaches the service limits for the account.<br><br>Which solution will accomplish this with the LEAST amount of development effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that runs periodically and targets an AWS Lambda function. Within the Lambda function, evaluate the current state of the AWS environment and compare deployed resource values to resource limits on the account. Notify the senior manager if the account is approaching a service limit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Lambda function that refreshes AWS Trusted Advisor checks, and configure an Amazon EventBridge rule to run the Lambda function periodically. Create another EventBridge rule with an event pattern matching Trusted Advisor events and a target Lambda function. In the target Lambda function, notify the senior manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an AWS Lambda function that refreshes AWS Health Dashboard checks, and configure an Amazon EventBridge rule to run the Lambda function periodically. Create another EventBridge rule with an event pattern matching Health Dashboard events and a target Lambda function. In the target Lambda function, notify the senior manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an AWS Config custom rule that runs periodically, checks the AWS service limit status, and streams notifications to an Amazon Simple Notification Service (Amazon SNS) topic. Deploy an AWS Lambda function that notifies the senior manager, and subscribe the Lambda function to the SNS topic."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-25T04:02:00.000Z",
        "voteCount": 4,
        "content": "Understanding your service limits (and how close you are to them) is an important part of managing your AWS deployments \u2013 continuous monitoring allows you to request limit increases or shut down resources before the limit is reached.\n\nOne of the easiest ways to do this is via AWS Trusted Advisor\u2019s Service Limit Dashboard, which currently covers 39 limits across 10 services. https://aws.amazon.com/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/"
      },
      {
        "date": "2024-04-13T18:58:00.000Z",
        "voteCount": 2,
        "content": "answer b"
      },
      {
        "date": "2024-03-16T04:07:00.000Z",
        "voteCount": 2,
        "content": "B - Trusted Advisor"
      },
      {
        "date": "2024-01-18T13:32:00.000Z",
        "voteCount": 3,
        "content": "Trusted advisor"
      },
      {
        "date": "2024-01-12T13:16:00.000Z",
        "voteCount": 3,
        "content": "service quote limit === trusted advisor"
      },
      {
        "date": "2024-01-12T08:33:00.000Z",
        "voteCount": 2,
        "content": "Answer B https://aws.amazon.com/blogs/mt/monitoring-service-limits-with-trusted-advisor-and-amazon-cloudwatch/"
      },
      {
        "date": "2023-12-31T18:20:00.000Z",
        "voteCount": 1,
        "content": "Trusted advisor checks the service limits"
      },
      {
        "date": "2023-12-29T11:59:00.000Z",
        "voteCount": 1,
        "content": "It's B"
      },
      {
        "date": "2023-12-29T07:22:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 183,
    "url": "https://www.examtopics.com/discussions/amazon/view/129693-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is setting up a container-based architecture. The engineer has decided to use AWS CloudFormation to automatically provision an Amazon ECS cluster and an Amazon EC2 Auto Scaling group to launch the EC2 container instances. After successfully creating the CloudFormation stack, the engineer noticed that, even though the ECS cluster and the EC2 instances were created successfully and the stack finished the creation, the EC2 instances were associating with a different cluster.<br><br>How should the DevOps engineer update the CloudFormation template to resolve this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the EC2 instances in the AWS::ECS::Cluster resource and reference the ECS cluster in the AWS::ECS::Service resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the ECS cluster in the AWS::AutoScaling::LaunchConfiguration resource of the UserData property.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the ECS cluster in the AWS::EC2::Instance resource of the UserData property.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the ECS cluster in the AWS::CloudFormation::CustomResource resource to trigger an AWS Lambda function that registers the EC2 instances with the appropriate ECS cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T00:37:00.000Z",
        "voteCount": 1,
        "content": "Link the Auto Scaling Group with ECS Cluster"
      },
      {
        "date": "2024-05-02T06:15:00.000Z",
        "voteCount": 1,
        "content": "B for me"
      },
      {
        "date": "2024-04-13T19:08:00.000Z",
        "voteCount": 2,
        "content": "answer B"
      },
      {
        "date": "2024-02-12T05:19:00.000Z",
        "voteCount": 3,
        "content": "B is correct: &lt;even though the ECS cluster and the EC2 instances were created successfully and the stack finished the creation, the EC2 instances were associating with a different cluster&gt; means we need to link the auto scaling group to the ECS cluster\nA: incorrect. AWS::ECS::Cluster creates an ECS cluster. AWS::ECS::Service creates its services. However, it does not link the EC2 auto scaling group to the ECS cluster\nC: incorrect. AWS::EC2::Instance creates an EC2 instance\nD: incorrect. AWS::CloudFormation::CustomResource creates CustomResource"
      },
      {
        "date": "2024-01-12T13:28:00.000Z",
        "voteCount": 3,
        "content": "B,  here is a sample code  https://github.com/thinegan/cloudformation-project2/blob/master/infrastructure/ecs-autoscaling-appserver.yaml"
      },
      {
        "date": "2024-01-02T04:09:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-autoscaling-launchconfiguration.html#aws-resource-autoscaling-launchconfiguration--examples"
      },
      {
        "date": "2023-12-29T07:26:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 184,
    "url": "https://www.examtopics.com/discussions/amazon/view/129694-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is implementing governance controls for a company that requires its infrastructure to be housed within the United States. The engineer must restrict which AWS Regions can be used, and ensure an alert is sent as soon as possible if any activity outside the governance policy takes place. The controls should be automatically enabled on any new Region outside the United States (US).<br><br>Which combination of actions will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Organizations SCP that denies access to all non-global services in non-US Regions. Attach the policy to the root of the organization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS CloudTrail to send logs to Amazon CloudWatch Logs and enable it for all Regions. Use a CloudWatch Logs metric filter to send an alert on any service activity in non-US Regions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function that checks for AWS service activity and deploy it to all Regions. Write an Amazon EventBridge rule that runs the Lambda function every hour, sending an alert if activity is found in a non-US Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an AWS Lambda function to query Amazon Inspector to look for service activity in non-US Regions and send alerts if any activity is found.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an SCP using the aws:RequestedRegion condition key limiting access to US Regions. Apply the policy to all users, groups, and roles."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-22T12:35:00.000Z",
        "voteCount": 2,
        "content": "Option A suggests creating an AWS Organizations SCP that denies access to all non-global services in non-US Regions. This is a valid approach.\nOption B recommends configuring AWS CloudTrail to send logs to Amazon CloudWatch Logs and enabling it for all Regions."
      },
      {
        "date": "2024-05-02T06:15:00.000Z",
        "voteCount": 1,
        "content": "AB for me"
      },
      {
        "date": "2024-04-13T19:10:00.000Z",
        "voteCount": 2,
        "content": "A&amp; B are correct answer"
      },
      {
        "date": "2024-03-16T04:04:00.000Z",
        "voteCount": 4,
        "content": "A - SCP to restrict \nB - CloudTrail to monitor"
      },
      {
        "date": "2024-02-12T05:24:00.000Z",
        "voteCount": 4,
        "content": "A and B are correct: SCP to restrict and AWS cloutrail to monitor\nC: Lambda cannot check AWS service activity\nD: AWS inspector has nothing to do here\nE: &lt;Apply the policy to all users, groups, and roles&gt;: cannot assign a SCP to all users, groups and roles."
      },
      {
        "date": "2024-05-22T12:31:00.000Z",
        "voteCount": 1,
        "content": "Your logic for SCP not applying to users, groups and roles is incorrect. SCP can be applied to users and roles. Groups will therefore be indirectly affected."
      },
      {
        "date": "2024-01-18T13:36:00.000Z",
        "voteCount": 2,
        "content": "It's A and B"
      },
      {
        "date": "2023-12-31T18:30:00.000Z",
        "voteCount": 2,
        "content": "A &amp; B are correct."
      },
      {
        "date": "2023-12-29T19:32:00.000Z",
        "voteCount": 2,
        "content": "A &amp; B Correct\nhttps://www.examtopics.com/discussions/amazon/view/47872-exam-aws-devops-engineer-professional-topic-1-question-260/"
      },
      {
        "date": "2023-12-29T12:01:00.000Z",
        "voteCount": 2,
        "content": "It's A &amp; B"
      },
      {
        "date": "2023-12-29T07:29:00.000Z",
        "voteCount": 2,
        "content": "B &amp; C correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 185,
    "url": "https://www.examtopics.com/discussions/amazon/view/129696-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company sells products through an ecommerce web application. The company wants a dashboard that shows a pie chart of product transaction details. The company wants to integrate the dashboard with the company's existing Amazon CloudWatch dashboards.<br><br>Which solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Use CloudWatch Logs Insights to query the log group and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch dashboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the ecommerce application to emit a JSON object to an Amazon S3 bucket for each processed transaction. Use Amazon Athena to query the S3 bucket and to visualize the results in a pie chart format. Export the results from Athena. Attach the results to the desired CloudWatch dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the ecommerce application to use AWS X-Ray for instrumentation. Create a new X-Ray subsegment. Add an annotation for each processed transaction. Use X-Ray traces to query the data and to visualize the results in a pie chart format. Attach the results to the desired CloudWatch dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the ecommerce application to emit a JSON object to a CloudWatch log group for each processed transaction. Create an AWS Lambda function to aggregate and write the results to Amazon DynamoDB. Create a Lambda subscription filter for the log file. Attach the results to the desired CloudWatch dashboard."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-05T23:23:00.000Z",
        "voteCount": 1,
        "content": "Pie Chart support by Cloudwatch \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph_a_metric.html"
      },
      {
        "date": "2024-04-13T19:15:00.000Z",
        "voteCount": 2,
        "content": "A: the pie chart can be achieved using cloud watch log insights"
      },
      {
        "date": "2024-03-30T17:07:00.000Z",
        "voteCount": 2,
        "content": "A is obvious"
      },
      {
        "date": "2024-03-16T04:03:00.000Z",
        "voteCount": 3,
        "content": "A is the correct answer"
      },
      {
        "date": "2024-03-14T21:58:00.000Z",
        "voteCount": 1,
        "content": "update in January 2022, Amazon CloudWatch does not provide native support for generating pie charts directly within the CloudWatch console. However, you can achieve this by exporting CloudWatch Logs to Amazon S3 and then analyzing the logs using other AWS services or third-party tools that support pie chart visualization."
      },
      {
        "date": "2024-02-28T02:16:00.000Z",
        "voteCount": 2,
        "content": "A, no debate"
      },
      {
        "date": "2024-02-12T05:32:00.000Z",
        "voteCount": 4,
        "content": "A is correct: &lt;a pie chart of product transaction details&gt; means we need to collect application logs and analyze the log. The recommended way is to use cloudwatch logs and cloudwatch logs insights\nB: Should not use S3. We should use cloudwatch logs so that we can integrate easily to the existing cloudwatch dashboards\nC: X-ray is for troubleshooting, which is short-term, not for long-term app monitoring\nD: Should not use lambda and should not write the results to dynamodb, which is primarily used to store session data\nhttps://www.examtopics.com/exams/amazon/aws-certified-devops-engineer-professional-dop-c02/view/1/#"
      },
      {
        "date": "2024-01-12T13:34:00.000Z",
        "voteCount": 2,
        "content": "cloudwatch insight can generate graph"
      },
      {
        "date": "2024-01-10T12:19:00.000Z",
        "voteCount": 2,
        "content": "Go For A \nhttps://www.delenamalan.co.za/til/2021-06-07-cloudwatch-insights-graph.html"
      },
      {
        "date": "2023-12-31T18:33:00.000Z",
        "voteCount": 2,
        "content": "Must be A"
      },
      {
        "date": "2023-12-29T12:05:00.000Z",
        "voteCount": 2,
        "content": "it's A"
      },
      {
        "date": "2023-12-29T07:42:00.000Z",
        "voteCount": 2,
        "content": "Sry, it should be A"
      },
      {
        "date": "2023-12-29T07:36:00.000Z",
        "voteCount": 1,
        "content": "Prefer D"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 186,
    "url": "https://www.examtopics.com/discussions/amazon/view/129699-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is launching an application. The application must use only approved AWS services. The account that runs the application was created less than 1 year ago and is assigned to an AWS Organizations OU.<br><br>The company needs to create a new Organizations account structure. The account structure must have an appropriate SCP that supports the use of only services that are currently active in the AWS account. The company will use AWS Identity and Access Management (IAM) Access Analyzer in the solution.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that allows the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the new OU. Detach the default FullAWSAccess SCP from the new OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that denies the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the new OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that allows the services that IAM Access Analyzer identifies. Attach the new SCP to the organization's root.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that allows the services that IAM Access Analyzer identifies. Create an OU for the account. Move the account into the new OU. Attach the new SCP to the management account. Detach the default FullAWSAccess SCP from the new OU."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T00:56:00.000Z",
        "voteCount": 1,
        "content": "This question and answers are confusing but I think \"Management Account\" in option D and \"the account\" are different accounts. \nHence, A is correct."
      },
      {
        "date": "2024-04-13T19:20:00.000Z",
        "voteCount": 2,
        "content": "Answer A"
      },
      {
        "date": "2024-03-16T04:02:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2024-02-12T05:44:00.000Z",
        "voteCount": 3,
        "content": "A is correct: &lt;AWS Identity and Access Management (IAM) Access Analyzer&gt; is a solution for least privilege, which is allow some, deny all. So we need to defy allowed permissions and then remove the &lt;default FullAWSAccess&gt;\nB: least privilege is allow some, deny all, not allow all, deny some. \nC: The step mentioned would have no effect. The root already had default FullAWSAccess SCP. Allowing some more services does not change anything\nD: &lt;Attach the new SCP to the management account&gt;: Cannot attach a SCP to an account"
      },
      {
        "date": "2024-02-12T06:25:00.000Z",
        "voteCount": 3,
        "content": "Correct: D - We can attach SCP to an account. But it only affects an account. We need to impose the scp rule on the entire accounts in the new OU"
      },
      {
        "date": "2024-01-18T01:03:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-01-01T15:39:00.000Z",
        "voteCount": 1,
        "content": "I agree with @d262e67."
      },
      {
        "date": "2023-12-31T18:45:00.000Z",
        "voteCount": 3,
        "content": "It's A. To those who selected D, why would you assign the SCP to the management account??? The application account goes into an OU, and the SCP must be associated with that OU, period!"
      },
      {
        "date": "2023-12-29T21:54:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2023-12-31T06:06:00.000Z",
        "voteCount": 1,
        "content": "Could you please explain more?"
      },
      {
        "date": "2023-12-29T14:06:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\nOption D: Attaching the SCP to the management account and detaching FullAWSAccess from the new OU may lead to unintended access restrictions for other accounts and services under the management account."
      },
      {
        "date": "2023-12-29T07:44:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 187,
    "url": "https://www.examtopics.com/discussions/amazon/view/129700-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has multiple development teams in different business units that work in a shared single AWS account. All Amazon EC2 resources that are created in the account must include tags that specify who created the resources. The tagging must occur within the first hour of resource creation.<br><br>A DevOps engineer needs to add tags to the created resources that include the user ID that created the resource and the cost center ID. The DevOps engineer configures an AWS Lambda function with the cost center mappings to tag the resources. The DevOps engineer also sets up AWS CloudTrail in the AWS account. An Amazon S3 bucket stores the CloudTrail event logs.<br><br>Which solution will meet the tagging requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 event notification on the S3 bucket to invoke the Lambda function for s3:ObjectTagging:Put events. Enable bucket versioning on the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable server access logging on the S3 bucket. Create an S3 event notification on the S3 bucket for s3:ObjectTagging:* events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a recurring hourly Amazon EventBridge scheduled rule that invokes the Lambda function. Modify the Lambda function to read the logs from the S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that uses Amazon EC2 as the event source. Configure the rule to match events delivered by CloudTrail. Configure the rule to target the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T19:27:00.000Z",
        "voteCount": 2,
        "content": "D looks more relevant"
      },
      {
        "date": "2024-03-16T03:59:00.000Z",
        "voteCount": 3,
        "content": "Answer is D."
      },
      {
        "date": "2024-02-13T07:50:00.000Z",
        "voteCount": 3,
        "content": "D is corect. \nA and B: irrelevant\nC: using lambda to read log is a bad idea because it takes a lot of time."
      },
      {
        "date": "2024-01-12T13:42:00.000Z",
        "voteCount": 4,
        "content": "the trigger event is the EC2 creation, so D"
      },
      {
        "date": "2024-01-01T15:50:00.000Z",
        "voteCount": 2,
        "content": "Answer is D. \n\nThe answer must have CloudTrail for EC2 tagging."
      },
      {
        "date": "2023-12-29T12:57:00.000Z",
        "voteCount": 3,
        "content": "It's D. It says within an hour so it can't be C, looping over the S3 logs may take a lot (apparently there is also consideration about the 15mins limit of lambda)"
      },
      {
        "date": "2023-12-29T07:47:00.000Z",
        "voteCount": 1,
        "content": "C looks correct"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 188,
    "url": "https://www.examtopics.com/discussions/amazon/view/129701-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs an application for multiple environments in a single AWS account. An AWS CodePipeline pipeline uses a development Amazon Elastic Container Service (Amazon ECS) cluster to test an image for the application from an Amazon Elastic Container Registry (Amazon ECR) repository. The pipeline promotes the image to a production ECS cluster.<br><br>The company needs to move the production cluster into a separate AWS account in the same AWS Region. The production cluster must be able to download the images over a private connection.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. In the separate AWS account, create an ECR repository. Set the repository policy to allow the production ECS tasks to pull images from the main AWS account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a repository policy on the production ECR repository in the main AWS account. Configure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the main account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure ECR private image replication in the main AWS account. Activate cross-account replication. Define the destination account ID of the separate AWS account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ECR VPC endpoints and an Amazon S3 gateway endpoint. Set a repository policy on the production ECR repository in the main AWS account. Configure the repository policy to allow the production ECS tasks in the separate AWS account to pull images from the main account. Configure the production ECS task execution role to have permission to download the image from the ECR repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-15T09:49:00.000Z",
        "voteCount": 1,
        "content": "\"You can configure your Amazon ECR private registry to support the replication of your repositories. Amazon ECR supports both cross-Region and cross-account replication\"\n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/replication.html\n\nIf cross-account replication is enabled, then for Cross-account replication, choose the cross-account replication setting for the registry. For Destination account, enter the account ID for the destination account and one or more Destination regions to replicate to. Choose Destination account + to configure additional accounts as replication destinations.\n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/registry-settings-configure.html"
      },
      {
        "date": "2024-07-26T06:41:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A: The company needs to move the production cluster into a separate AWS account in the same AWS Region. The repository is in a separate account, and permissions are set there, giving better isolation between environments."
      },
      {
        "date": "2024-09-02T11:54:00.000Z",
        "voteCount": 1,
        "content": "I will go with option C because cross-account replication is straightforward and secure."
      },
      {
        "date": "2024-07-03T16:29:00.000Z",
        "voteCount": 1,
        "content": "Based the references provided, it would appear that both \"C\" and \"D\" could work to distribute an image, EXCEPT for the \"\"private connection\" requirement.  It's also seems like a cleaner solution to just rely on one ECR repository, rather than replicate repo's to other accounts in same region."
      },
      {
        "date": "2024-04-25T05:00:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html"
      },
      {
        "date": "2024-04-13T19:34:00.000Z",
        "voteCount": 2,
        "content": "Ans D:\nAmazon ECS tasks to pull private images from Amazon ECR, you must create a gateway endpoint for Amazon S3. The gateway endpoint is required because Amazon ECR uses Amazon S3 to store your image layers."
      },
      {
        "date": "2024-03-16T03:58:00.000Z",
        "voteCount": 2,
        "content": "ECR VPC endpoints is needed to meet  \"download the images over a private connection.\""
      },
      {
        "date": "2024-03-04T02:52:00.000Z",
        "voteCount": 3,
        "content": "Use ECR VPC endpoints is necessary to meet the below requirements.\n`download the images over a private connection.`"
      },
      {
        "date": "2024-02-22T17:11:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/replication.html"
      },
      {
        "date": "2024-02-12T05:59:00.000Z",
        "voteCount": 2,
        "content": "C is correct: ECR private image replication can allow replicate image to the new account\nA and D: both mentions S3 gw, which is unnecessary\nB: no mention of how to replicate images cross account"
      },
      {
        "date": "2024-01-11T11:43:00.000Z",
        "voteCount": 1,
        "content": "Don't see the difference between A &amp; D"
      },
      {
        "date": "2024-01-18T01:50:00.000Z",
        "voteCount": 3,
        "content": "It's D,  no need to create a seperate ECR repo in the other account, just update the policy of the ECR repo in the main account to allow cross-account access."
      },
      {
        "date": "2024-01-02T04:43:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonECR/latest/userguide/vpc-endpoints.html"
      },
      {
        "date": "2024-01-01T15:52:00.000Z",
        "voteCount": 2,
        "content": "Answer is D."
      },
      {
        "date": "2023-12-29T13:43:00.000Z",
        "voteCount": 2,
        "content": "It's D"
      },
      {
        "date": "2023-12-29T07:50:00.000Z",
        "voteCount": 2,
        "content": "D - Using Amazon ECR VPC endpoints ensures that the ECS tasks in both the development and production clusters can pull Docker images securely over a private connection."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 189,
    "url": "https://www.examtopics.com/discussions/amazon/view/129753-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company needs to ensure that flow logs remain configured for all existing and new VPCs in its AWS account. The company uses an AWS CloudFormation stack to manage its VPCs. The company needs a solution that will work for any VPCs that any IAM user creates.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the AWS::EC2::FlowLog resource to the CloudFormation stack that creates the VPCs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an organization in AWS Organizations. Add the company's AWS account to the organization. Create an SCP to prevent users from modifying VPC flow logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS Config. Create an AWS Config rule to check whether VPC flow logs are turned on. Configure automatic remediation to turn on VPC flow logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy to deny the use of API calls for VPC flow logs. Attach the IAM policy to all IAM users."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T06:02:00.000Z",
        "voteCount": 5,
        "content": "C is correct: This will monitor and remediate all existing and new VPCs\nA: AWS::EC2::FlowLog: This is used to configured flow log, not monitor it\nB: SCP wont address existing VPCs\nD: IAM policy has nothing to do here"
      },
      {
        "date": "2024-04-13T19:41:00.000Z",
        "voteCount": 2,
        "content": "option c is correct"
      },
      {
        "date": "2024-01-02T13:41:00.000Z",
        "voteCount": 4,
        "content": "SCPs only prevent people from changing the VPC flow log configuration. It doesn't ensure it's on."
      },
      {
        "date": "2024-01-02T05:16:00.000Z",
        "voteCount": 1,
        "content": "both AWS config and SCP work, however, SCP is more preventive compared to proactive AWS Config. therefore, I opted B."
      },
      {
        "date": "2024-01-01T15:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is C."
      },
      {
        "date": "2023-12-29T13:48:00.000Z",
        "voteCount": 3,
        "content": "It's C, here is a reference how to do it:\n\nhttps://aws.amazon.com/blogs/mt/how-to-enable-vpc-flow-logs-automatically-using-aws-config-rules/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 190,
    "url": "https://www.examtopics.com/discussions/amazon/view/129758-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's application teams use AWS CodeCommit repositories for their applications. The application teams have repositories in multiple AWS accounts. All accounts are in an organization in AWS Organizations.<br><br>Each application team uses AWS IAM Identity Center (AWS Single Sign-On) configured with an external IdP to assume a developer IAM role. The developer role allows the application teams to use Git to work with the code in the repositories.<br><br>A security audit reveals that the application teams can modify the main branch in any repository. A DevOps engineer must implement a solution that allows the application teams to modify the main branch of only the repositories that they manage.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the SAML assertion to pass the user's team name. Update the IAM role's trust policy to add an access-team session tag that has the team name.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an approval rule template for each team in the Organizations management account. Associate the template with all the repositories. Add the developer role ARN as an approver.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an approval rule template for each account. Associate the template with all repositories. Add the \"aws:ResourceTag/access-team\": \"$ ;{aws:PrincipalTag/access-team}\" condition to the approval rule template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each CodeCommit repository, add an access-team tag that has the value set to the name of the associated team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach an SCP to the accounts. Include the following statement:<br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image14.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM permissions boundary in each account. Include the following statement:<br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image15.png\">"
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "ADF",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "ACE",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-25T12:44:00.000Z",
        "voteCount": 6,
        "content": "A- SAML Assertion\nD - Tag the resource\nE - Will work with D above ad condition is based on resource tag"
      },
      {
        "date": "2024-08-20T05:26:00.000Z",
        "voteCount": 1,
        "content": "ade for me"
      },
      {
        "date": "2024-07-30T01:18:00.000Z",
        "voteCount": 2,
        "content": "I go for ADE\n\nOption E, this SCP ensures that only users from the team that manages a repository can modify the main branch by using the access-team tag. It denies actions if the team tags do not match."
      },
      {
        "date": "2024-05-22T13:43:00.000Z",
        "voteCount": 1,
        "content": "Why are people choosing option E? \nAs per the requirement:\n\"A company's application teams use AWS CodeCommit repositories for their applications. The application teams have repositories in multiple AWS\naccounts. All accounts are in an organization in AWS Organizations.\nEach application team uses AWS IAM Identity Center (AWS Single Sign-On) configured with an external IdP to assume a developer IAM role. The\ndeveloper role allows the application teams to use Git to work with the code in the repositories\"\nShouldn't we then ALLOW GitPush, PutFile and Merge*?\nI think it should be ADF."
      },
      {
        "date": "2024-06-18T06:10:00.000Z",
        "voteCount": 2,
        "content": "F is Wrong. E is correct. This SCP ensures that only users from the team that manages a repository can modify the main branch by using the access-team tag. It denies actions if the team tags do not match."
      },
      {
        "date": "2024-05-22T13:44:00.000Z",
        "voteCount": 1,
        "content": "Sorry, i wanted to highlight this: \n\"A DevOps engineer must implement a solution that allows the application teams to modify the main branch of only the repositories that they manage.\"\n\nThis is why it should be ADF."
      },
      {
        "date": "2024-05-28T12:40:00.000Z",
        "voteCount": 3,
        "content": "they can already access all the repositories, the requirement is to scope down their access hense DENY not ALLOW"
      },
      {
        "date": "2024-05-02T06:18:00.000Z",
        "voteCount": 2,
        "content": "ADE for me"
      },
      {
        "date": "2024-04-13T19:46:00.000Z",
        "voteCount": 3,
        "content": "ADE seems more appropriate"
      },
      {
        "date": "2024-03-17T11:25:00.000Z",
        "voteCount": 2,
        "content": "ADF, 100%"
      },
      {
        "date": "2024-03-25T10:05:00.000Z",
        "voteCount": 4,
        "content": "Correction, its ADE:\nPermissions boundaries (Option F) are more granular and would be set on each IAM role individually. While they could achieve a similar effect, they are not as broad in scope as SCPs and would require setting up on every IAM role, which could be less efficient than a blanket policy across the organization with an SCP"
      },
      {
        "date": "2024-03-16T03:55:00.000Z",
        "voteCount": 4,
        "content": "A D E\nThere no mention of an approval step being needed so rules out B &amp; C. and F is an allow policy not deny"
      },
      {
        "date": "2024-03-12T04:07:00.000Z",
        "voteCount": 1,
        "content": "ACE \nreference for option c\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/how-to-create-template.html"
      },
      {
        "date": "2024-02-19T00:53:00.000Z",
        "voteCount": 2,
        "content": "adf -&gt; correct"
      },
      {
        "date": "2024-02-17T04:07:00.000Z",
        "voteCount": 1,
        "content": "isn't ADF? \" Attach an SCP to the accounts. Include the following statement:\" scp are for the organizations no?"
      },
      {
        "date": "2024-02-19T23:10:00.000Z",
        "voteCount": 2,
        "content": "i think . adf~"
      },
      {
        "date": "2024-02-13T09:37:00.000Z",
        "voteCount": 3,
        "content": "I will go with ADE"
      },
      {
        "date": "2024-02-12T06:22:00.000Z",
        "voteCount": 2,
        "content": "ACE are correct:\nA: &lt; IAM Identity Center (AWS Single Sign-On) configured with an external IdP&gt; means we need SAML\nC and E are just similar with \"aws:ResourceTag/access-team\": \"$ ;{aws:PrincipalTag/access-team}\" condition"
      },
      {
        "date": "2024-01-10T12:42:00.000Z",
        "voteCount": 2,
        "content": "Go For ADE, We don't need approval Rule here, And we use organizations, that's why SCP"
      },
      {
        "date": "2024-01-02T14:10:00.000Z",
        "voteCount": 4,
        "content": "As far as I know, the approval rule templates are designed to manage pull requests, not direct pushes to branches."
      },
      {
        "date": "2024-01-02T05:29:00.000Z",
        "voteCount": 2,
        "content": "question sounds like ABAC assessment: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_abac-saml.html\n\napproval rule templates are good to audit pull requests and if the developer is the repo owner, he/she is free to do anything."
      },
      {
        "date": "2024-01-04T03:22:00.000Z",
        "voteCount": 2,
        "content": "edit: ADE"
      },
      {
        "date": "2024-01-01T16:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is A, C, &amp; E."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 191,
    "url": "https://www.examtopics.com/discussions/amazon/view/129762-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS WAF to protect its cloud infrastructure. A DevOps engineer needs to give an operations team the ability to analyze log messages from AWS WAF. The operations team needs to be able to create alarms for specific patterns in the log output.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Logs log group. Configure the appropriate AWS WAF web ACL to send log messages to the log group. Instruct the operations team to create CloudWatch metric filters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon OpenSearch Service cluster and appropriate indexes. Configure an Amazon Kinesis Data Firehose delivery stream to stream log data to the indexes. Use OpenSearch Dashboards to create filters and widgets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket for the log output. Configure AWS WAF to send log outputs to the S3 bucket. Instruct the operations team to create AWS Lambda functions that detect each desired log message pattern. Configure the Lambda functions to publish to an Amazon Simple Notification Service (Amazon SNS) topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket for the log output. Configure AWS WAF to send log outputs to the S3 bucket. Use Amazon Athena to create an external table definition that fits the log message pattern. Instruct the operations team to write SQL queries and to create Amazon CloudWatch metric filters for the Athena queries."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-15T10:23:00.000Z",
        "voteCount": 1,
        "content": "To send logs to Amazon CloudWatch Logs, you create a CloudWatch Logs log group. When you enable logging in AWS WAF, you provide the log group ARN. After you enable logging for your web ACL, AWS WAF delivers logs to the CloudWatch Logs log group in log streams.\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/logging-cw-logs.html"
      },
      {
        "date": "2024-04-25T07:57:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/waf/latest/developerguide/logging-management.html"
      },
      {
        "date": "2024-04-13T19:51:00.000Z",
        "voteCount": 2,
        "content": "A &amp; D can work, least operation overheard is A"
      },
      {
        "date": "2024-03-16T03:51:00.000Z",
        "voteCount": 2,
        "content": "LEAST operational overhead = A"
      },
      {
        "date": "2024-02-12T06:32:00.000Z",
        "voteCount": 2,
        "content": "D is correct: We have two tasks: collect log and analyze data. S3 bucket can store log and athena is for log analysis.\nA: This options does not mention of log analysis. Additionally, AWS WAF web ACL cannot send log to AWS logs group\nB: OpenSearch Service  and  Amazon Kinesis Data Firehose are used for other purposes. They are high-end features and cost a lots.\nC: Should not use lambda to analys log"
      },
      {
        "date": "2024-05-22T13:49:00.000Z",
        "voteCount": 1,
        "content": "You fail to notice that the question is asking about LEAST operational overhead. Therefore, it should be A."
      },
      {
        "date": "2024-01-15T14:53:00.000Z",
        "voteCount": 1,
        "content": "cloudwatch"
      },
      {
        "date": "2024-01-12T13:53:00.000Z",
        "voteCount": 1,
        "content": "cloudwatch for WAF logging"
      },
      {
        "date": "2024-01-02T05:32:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/waf/latest/developerguide/logging.html"
      },
      {
        "date": "2024-01-01T16:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is A based on the following AWS documentation:\n\nhttps://docs.aws.amazon.com/waf/latest/developerguide/web-acl-creating.html"
      },
      {
        "date": "2023-12-29T14:29:00.000Z",
        "voteCount": 3,
        "content": "A seems to involve the least operational overhead"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 192,
    "url": "https://www.examtopics.com/discussions/amazon/view/129703-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A software team is using AWS CodePipeline to automate its Java application release pipeline. The pipeline consists of a source stage, then a build stage, and then a deploy stage. Each stage contains a single action that has a runOrder value of 1.<br><br>The team wants to integrate unit tests into the existing release pipeline. The team needs a solution that deploys only the code changes that pass all unit tests.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the build stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the build stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the deploy stage. Add a test action that has a runOrder value of 1. Use AWS CodeDeploy as the action provider to run unit tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the deploy stage. Add a test action that has a runOrder value of 2. Use AWS CodeBuild as the action provider to run unit tests."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T06:36:00.000Z",
        "voteCount": 5,
        "content": "B is correct: Runorder value of 2 ensure that we do unit tests after we build artifacts. \nA: The unit tests would run in parellel with the build step, which is incorrect. We can only test after we have done building\nC and D: The unit tests would not run before the deploy step."
      },
      {
        "date": "2024-07-04T17:00:00.000Z",
        "voteCount": 1,
        "content": "C: (YES) aws:SecureTransport = data in transit (TLS/HTTPS)\nD: (NO)  ...server-side-encryption-aws... = data at rest (in S3)"
      },
      {
        "date": "2024-04-25T08:00:00.000Z",
        "voteCount": 4,
        "content": "By modifying the build stage, adding a test action with a runOrder value of 2, and using AWS CodeBuild as the action provider to run unit tests, the solution ensures that unit tests are executed as part of the build process and that only the code changes that pass all unit tests are deployed, meeting the requirements of the software team."
      },
      {
        "date": "2024-04-13T19:53:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-03-16T03:51:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-03-01T02:45:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2024-01-12T13:54:00.000Z",
        "voteCount": 2,
        "content": "order of 2 would create sequence order"
      },
      {
        "date": "2023-12-29T14:09:00.000Z",
        "voteCount": 2,
        "content": "it's definitely B"
      },
      {
        "date": "2023-12-29T08:01:00.000Z",
        "voteCount": 3,
        "content": "Option B - The runOrder value of 2 ensures that the test action runs after the build action, allowing the unit tests to be executed only if the build is successful."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 193,
    "url": "https://www.examtopics.com/discussions/amazon/view/129704-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an organization in AWS Organizations to manage several AWS accounts that the company's developers use. The company requires all data to be encrypted in transit.<br><br>Multiple Amazon S3 buckets that were created in developer accounts allow unencrypted connections. A DevOps engineer must enforce encryption of data in transit for all existing S3 buckets that are created in accounts in the organization.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation StackSets to deploy an AWS Network Firewall firewall to each account. Route all outbound requests from the AWS environment through the firewall. Deploy a policy to block access to all outbound requests on port 80.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation StackSets to deploy an AWS Network Firewall firewall to each account. Route all inbound requests to the AWS environment through the firewall. Deploy a policy to block access to all inbound requests on port 80.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS Config for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the value of the aws:SecureTransport condition key is false.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on AWS Config for the organization. Deploy a conformance pack that uses the s3-bucket-ssl-requests-only managed rule and an AWS Systems Manager Automation runbook. Use a runbook that adds a bucket policy statement to deny access to an S3 bucket when the value of the s3:x-amz-server-side-encryption-aws-kms-key-id condition key is null."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-01T16:18:00.000Z",
        "voteCount": 7,
        "content": "Answer C 100%.\n\naws:SecureTransport condition this will be allowing only encrypted connections over HTTPS (TLS) --&gt; THIS IS WHAT WE NEED\n\ns3:x-amz-server-side-encryption-aws-kms-key-id --&gt; To require that a particular AWS KMS key be used to encrypt the objects in a bucket.  WE DON'T NEED THIS HERE!"
      },
      {
        "date": "2024-02-12T06:39:00.000Z",
        "voteCount": 6,
        "content": "C is correct:\nA and B: we should not use AWS Network Firewall firewall here. It is just incorrect\nD: s3:x-amz-server-side-encryption-aws-kms-key-id: This is for data encryption at rest, not in transit"
      },
      {
        "date": "2024-04-13T19:56:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2024-03-01T02:37:00.000Z",
        "voteCount": 3,
        "content": "C is correct:"
      },
      {
        "date": "2024-01-02T05:40:00.000Z",
        "voteCount": 2,
        "content": "https://repost.aws/knowledge-center/s3-bucket-policy-for-config-rule"
      },
      {
        "date": "2023-12-30T09:18:00.000Z",
        "voteCount": 2,
        "content": "C is correct. D is encryption for rest not transit."
      },
      {
        "date": "2023-12-29T14:12:00.000Z",
        "voteCount": 2,
        "content": "It's C - they want to enforce SSL (i.e., encryption of data in transit)."
      },
      {
        "date": "2023-12-29T08:03:00.000Z",
        "voteCount": 1,
        "content": "option D is correct"
      },
      {
        "date": "2024-08-23T07:34:00.000Z",
        "voteCount": 1,
        "content": "Option D is wrong as it is for server side encryption and what the question requires is the encryption of data in transit not when the data is at rest within an S3 bucket."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 194,
    "url": "https://www.examtopics.com/discussions/amazon/view/129705-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is reviewing its IAM policies. One policy written by the DevOps engineer has been flagged as too permissive. The policy is used by an AWS Lambda function that issues a stop command to Amazon EC2 instances tagged with Environment: NonProduction over the weekend. The current policy is:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image16.png\"><br><br>What changes should the engineer make to achieve a policy of least permission? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the following conditional expression:<br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image17.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange \"Resource\": \"*\"to \"Resource\": \"arn:aws:ec2:*:*:instance/*\"\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the following conditional expression:<br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image18.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the following conditional expression:<br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image19.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange \"Action\": \"ec2:*\"to \"Action\": \"ec2:StopInstances\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the following conditional expression:<br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image20.png\">"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T06:50:00.000Z",
        "voteCount": 6,
        "content": "B, D and E are correct:\nA: This allow all any lambda func to do the task, wont make any change\nB: This allow action only on EC2, so it is correct\nC: We need to allow action on Ec2 instances tagged with NonProdction only. Using this would grant permissions to other tags as well\nD: perfectly correct\nE: Only permit stop action, so it is correct\nF: irrelevant"
      },
      {
        "date": "2024-02-03T13:20:00.000Z",
        "voteCount": 5,
        "content": "there is no point to restrict \"Resource\": into instances, when you restricting action to \"ec2:StopInstances\". Only EC2 instance have such action. So whats the point to restrict Resource."
      },
      {
        "date": "2024-09-06T19:07:00.000Z",
        "voteCount": 1,
        "content": "A, B, D\nThe engineer should make the following changes to achieve a policy of least permission:\n\nA:Add a condition to ensure that the principal making the request is an AWS Lambda function. This ensures that only Lambda functions can execute this policy.\n\nB:Narrow down the resources by specifying the ARN of EC2 instances instead of allowing all resources. This ensures that the policy only affects EC2 instances.\n\nD:Add a condition to ensure that this policy only applies to EC2 instances tagged with ''Environment: NonProduction''. This ensures that production environments are not affected by this policy."
      },
      {
        "date": "2024-08-15T10:47:00.000Z",
        "voteCount": 1,
        "content": "I'll go with BDE\n\nB - restrict resource from wildcard to only \"arn:aws:ec2:*:*:instance/*\"\nD - this condition limits to non Prod only\nE - limit actions to \"ec2:StopInstances\" and not all ec2 actions\n\nas for F, although YOU CAN allow access based on date/time. The typical format is:\n\n            \"Condition\": {\n                \"DateGreaterThan\": {\"aws:CurrentTime\": \"2020-04-01T00:00:00Z\"},\n                \"DateLessThan\": {\"aws:CurrentTime\": \"2020-06-30T23:59:59Z\"}\n\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html"
      },
      {
        "date": "2024-07-30T12:39:00.000Z",
        "voteCount": 1,
        "content": "DEF.\nwhen using E, there is no need for B.\nF: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws-dates.html"
      },
      {
        "date": "2024-07-30T01:33:00.000Z",
        "voteCount": 1,
        "content": "BDE are correct.\n\nOption F is irrelevant and can use Amazon EventBridge Rule to execute the Lambda"
      },
      {
        "date": "2024-07-30T01:35:00.000Z",
        "voteCount": 2,
        "content": "And in question, there is keywords: least permission\nSo B is needed"
      },
      {
        "date": "2024-07-26T03:29:00.000Z",
        "voteCount": 2,
        "content": "DEF\nE is more prefect answer rather than B. \nD.  restricted the ec2 using env tag as \"nonproduction\"\nF.  support scope of running time which mention in the question."
      },
      {
        "date": "2024-05-09T07:13:00.000Z",
        "voteCount": 2,
        "content": "DEF\nPrincipal condition is usually for resource policies."
      },
      {
        "date": "2024-05-02T06:20:00.000Z",
        "voteCount": 4,
        "content": "BDE for me"
      },
      {
        "date": "2024-04-20T10:09:00.000Z",
        "voteCount": 3,
        "content": "D we need non prod,E we need specific action,F we need dates restriction"
      },
      {
        "date": "2024-04-13T20:02:00.000Z",
        "voteCount": 2,
        "content": "BDE seems correct"
      },
      {
        "date": "2024-02-13T04:29:00.000Z",
        "voteCount": 1,
        "content": "Why is B the correct answer?"
      },
      {
        "date": "2024-02-10T12:12:00.000Z",
        "voteCount": 4,
        "content": "A,D,E\nprincipalType could be a condition key \nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html"
      },
      {
        "date": "2024-01-27T09:38:00.000Z",
        "voteCount": 2,
        "content": "B, D, and E."
      },
      {
        "date": "2024-01-16T11:58:00.000Z",
        "voteCount": 3,
        "content": "Vote for BDE"
      },
      {
        "date": "2024-01-13T01:35:00.000Z",
        "voteCount": 1,
        "content": "BDE is the correct answer."
      },
      {
        "date": "2024-01-02T15:08:00.000Z",
        "voteCount": 2,
        "content": "B, D, and E. Principal is not for an IAM policy. And it's not possible to include weekdays in the policy."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 195,
    "url": "https://www.examtopics.com/discussions/amazon/view/133291-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is developing an application that will generate log events. The log events consist of five distinct metrics every one tenth of a second and produce a large amount of data.<br><br>The company needs to configure the application to write the logs to Amazon Timestream. The company will configure a daily query against the Timestream table.<br><br>Which combination of steps will meet these requirements with the FASTEST query performance? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse batch writes to write multiple log events in a single write operation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite each log event as a single write operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTreat each log as a single-measure record.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTreat each log as a multi-measure record.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the memory store retention period to be longer than the magnetic store retention period.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the memory store retention period to be shorter than the magnetic store retention period.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "ADE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "ACD",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-10T12:17:00.000Z",
        "voteCount": 5,
        "content": "While E suggests configuring the memory store retention period to be longer than the magnetic store retention period, this is typically not aimed at optimizing query performance but rather at keeping data in the faster-access memory store for longer periods, which could be beneficial for workloads requiring frequent access to recent data. However, for the scenario described, focusing on efficient data ingestion methods (A and D) and understanding the role of retention periods (F) provides a balanced approach to achieving the fastest query performance for daily queries."
      },
      {
        "date": "2024-07-26T06:18:00.000Z",
        "voteCount": 2,
        "content": "E fast performance\nA writer more throughput\n D multi measure means less records to store each data point. Faster query"
      },
      {
        "date": "2024-07-04T22:30:00.000Z",
        "voteCount": 2,
        "content": "My only hesitation is in regards to how batch writes might improve query performance, other than if the stored data is in a contiguous chunk, that could hep a query later.  As far as for multi-measure and more memory, I defer to references:\nA: (YES) \"When writing data to InfluxDB, write data in batches to minimize the network overhead related to every write request.\"\t\nD: (YES) \"Multi-measure records results in lower query latency for most query types when compared to single-measure records.\"\t\t\nE: (YES) \"The memory store is optimized for high throughput data writes and fast point-in-time queries.\"\t\t\t\t\nF: (NO)  \"The magnetic store is optimized for lower throughput late-arriving data writes, long term data storage, and fast analytical queries.\""
      },
      {
        "date": "2024-07-04T22:46:00.000Z",
        "voteCount": 1,
        "content": "Memory based storage is always going to provide \"FASTEST query performance\" compared to magnetic storage.  You want faster query, provide higher ratio of memory storage compared to magnetic."
      },
      {
        "date": "2024-04-18T01:30:00.000Z",
        "voteCount": 2,
        "content": "ADF\nA - improve write performance and efficiency\nD - query for a specific measure in a multi-measure record, Timestream only scans the relevant measure, not the entire record. This means that even though the record contains multiple measures, the query performance for a specific measure is not negatively impacted.   Multi-measure record reduces the number of records that need to be written and subsequently queried, which improve query performance.\nF - memory store, which is optimised for write and query performance, is not filled with older data that is not frequently accessed"
      },
      {
        "date": "2024-04-13T20:09:00.000Z",
        "voteCount": 2,
        "content": "ADF seems more relevant"
      },
      {
        "date": "2024-02-25T13:36:00.000Z",
        "voteCount": 3,
        "content": "ADF \u2013 batch writes, Treat log as multi-measure record, Memory story should be shorter,.\nhttps://aws.amazon.com/blogs/database/improve-query-performance-and-reduce-cost-using-scheduled-queries-in-amazon-timestream/#:~:text=Improve%20query%20performance%20and%20reduce%20cost%20using%20scheduled,6%20Query%20performance%20metrics%20...%207%20Conclusion%20"
      },
      {
        "date": "2024-02-13T09:57:00.000Z",
        "voteCount": 3,
        "content": "A. Batch writes: This significantly reduces overhead associated with individual write operations and improves overall write throughput.\nC. Single-measure record: For daily queries summarizing multiple metrics, treating each log as a single record helps Timestream leverage its optimized storage and query processing for single measures.\nD. Multi-measure record: While it seems counterintuitive, Timestream performs better with multiple measures within a single record compared to separate records for each metric. This allows for efficient data retrieval and aggregation during queries."
      },
      {
        "date": "2024-02-13T09:57:00.000Z",
        "voteCount": 1,
        "content": "Options B, E, and F are not recommended for optimal performance:\n&nbsp;\nB. Single write operations: This increases overhead and reduces write throughput, negating Timestream's scalability benefits.\nE. Longer memory store: While faster for recent data, it increases cost and doesn't impact daily queries focused on older, magnetic store data.\nF. Shorter memory store: Reduces cost but sacrifices potential performance gains for frequently accessed recent data, which might not be relevant for daily queries.\n&nbsp;\nBy combining batch writes, single-measure records, and multi-measure records, the company can achieve the fastest query performance for their daily Timestream use case."
      },
      {
        "date": "2024-02-12T07:14:00.000Z",
        "voteCount": 2,
        "content": "A,D and F are correct:\nA: do job in batch optimize costs and performance\nB: should not do single write\nC: The app emits multiple records the same time. So it should be multi-measure record, not single one\nD: correct\nE: Should not do this\nF: correct"
      },
      {
        "date": "2024-02-07T08:58:00.000Z",
        "voteCount": 1,
        "content": "ACE. Batch the write, write as whole and stay in memory longer"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 196,
    "url": "https://www.examtopics.com/discussions/amazon/view/132880-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer has created an AWS CloudFormation template that deploys an application on Amazon EC2 instances. The EC2 instances run Amazon Linux. The application is deployed to the EC2 instances by using shell scripts that contain user data. The EC2 instances have an IAM instance profile that has an IAM role with the AmazonSSMManagedinstanceCore managed policy attached.<br><br>The DevOps engineer has modified the user data in the CloudFormation template to install a new version of the application. The engineer has also applied the stack update. However, the application was not updated on the running EC2 instances. The engineer needs to ensure that the changes to the application are installed on the running EC2 instances.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the user data content to use the Multipurpose Internet Mail Extensions (MIME) multipart format. Set the scripts-user parameter to always in the text/cloud-config section.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the user data commands to use the cfn-init helper script. Update the user data to install and configure the cfn-hup and cfn-init helper scripts to monitor and apply the metadata changes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an EC2 launch template for the EC2 instances. Create a new EC2 Auto Scaling group. Associate the Auto Scaling group with the EC2 launch template. Use the AutoScalingScheduledAction update policy for the Auto Scaling group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the user data commands to use an AWS Systems Manager document (SSM document). Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the user data command to use an AWS Systems Manager document (SSM document). Use Systems Manager State Manager to create an association between the SSM document and the EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-10T12:23:00.000Z",
        "voteCount": 7,
        "content": "B and E are the most effective in ensuring that updates to the application are installed on the running EC2 instances by leveraging CloudFormation's and AWS Systems Manager's capabilities for managing and applying updates."
      },
      {
        "date": "2024-04-13T20:20:00.000Z",
        "voteCount": 2,
        "content": "B&amp;E\nD. Systems Manager Run Command (with user data): Using Run Command within user data to apply an SSM document introduces an unnecessary step. Option E with State Manager automates the process."
      },
      {
        "date": "2024-03-30T17:17:00.000Z",
        "voteCount": 1,
        "content": "B, E.\n\"Association\" is the key. Details are everything during an Investigation..."
      },
      {
        "date": "2024-03-24T23:22:00.000Z",
        "voteCount": 1,
        "content": "User data is executed when the system starts, not executed in runing EC2."
      },
      {
        "date": "2024-03-24T03:45:00.000Z",
        "voteCount": 2,
        "content": "EC2 instance profile with AmazonSSMManagedinstanceCore policy doesn't have permissions to SSM Run Command, so D is incorrect. \nSo for me it's BE."
      },
      {
        "date": "2024-03-17T11:55:00.000Z",
        "voteCount": 1,
        "content": "\"Add an AWS CLI command in the user data to use Systems Manager Run Command to apply the SSM document to the EC2 instances.\" \n**Add an AWS CLI command in the ****user data*****\nit will not help to update running instances, tricky question.\nOnly Systems Manager State Manager will update the running instances in this question"
      },
      {
        "date": "2024-02-29T12:41:00.000Z",
        "voteCount": 1,
        "content": "B and D\nA. This option is not applicable for updating applications on EC2 instances.\n\nB. Refactoring the user data commands to use the cfn-init helper script helps in handling metadata changes and applying them to the EC2 instances. This is especially useful in CloudFormation stack updates.\n\nC. Creating a new EC2 Auto Scaling group with an update policy doesn't necessarily address the application update requirement in this scenario.\n\nD. Refactoring the user data commands to use an AWS Systems Manager document and using Run Command to apply the SSM document is a valid approach for updating applications on EC2 instances.\n\nE. While using Systems Manager documents and State Manager is a valid approach, it might be more complex than needed for a straightforward update of an application on EC2 instances.\n\nTherefore, options B and D together provide a good solution for updating the application on the running EC2 instances."
      },
      {
        "date": "2024-02-13T09:59:00.000Z",
        "voteCount": 1,
        "content": "ption B: cfn-init is a powerful tool for managing configuration on EC2 instances. By using cfn-init, the DevOps engineer can ensure that the new application version is installed regardless of the current state of the instances Option D: SSM documents provide a centralized and reusable way to manage configurations. By using Run Command, the engineer can trigger the application update on all instances directly from the template."
      },
      {
        "date": "2024-02-13T10:00:00.000Z",
        "voteCount": 1,
        "content": "Options A, C, and E are not suitable for this scenario:\n&nbsp;\nOption A: MIME multipart format isn't necessary for this scenario.\nOption C: While Auto Scaling offers flexibility, creating a new launch template and Auto Scaling group is unnecessary for a simple application update.\nOption E: State Manager is generally used for ongoing configuration management, not one-time deployments like this.\n&nbsp;\nBy using both cfn-init and SSM documents, the DevOps engineer can achieve a reliable and manageable way to update the application on the running EC2 instances."
      },
      {
        "date": "2024-02-12T07:22:00.000Z",
        "voteCount": 2,
        "content": "B and D:\nA: irrelevant\nB: cfn-init is perfectly correct for this purpose\nC: irrelevant. The question does not mention autoscaling group\nD: &lt;Systems Manager Run Command &gt; can help install packages, so it is correct\nE: &lt; Systems Manager State Manager&gt; is used to maintain, not to update"
      },
      {
        "date": "2024-02-12T04:48:00.000Z",
        "voteCount": 2,
        "content": "Here's why these options are correct:\nOption B: cfn-init is a powerful tool for managing configuration on EC2 instances. By using cfn-init, the DevOps engineer can ensure that the new application version is installed regardless of the current state of the instances. cfn-hup helps keep cfn-init updated with the latest configuration changes.\n&nbsp;\nOption D: SSM documents provide a centralized and reusable way to manage configurations. By using Run Command, the engineer can trigger the application update on all instances directly from the template. This approach allows for easier management and updates in the future."
      },
      {
        "date": "2024-02-05T09:45:00.000Z",
        "voteCount": 2,
        "content": "cfn-hup to chek for updates in cloudformation and ssm run command to run commands if required for application"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 197,
    "url": "https://www.examtopics.com/discussions/amazon/view/133261-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is refactoring applications to use AWS. The company identifies an internal web application that needs to make Amazon S3 API calls in a specific AWS account.<br><br>The company wants to use its existing identity provider (IdP) auth.company.com for authentication. The IdP supports only OpenID Connect (OIDC). A DevOps engineer needs to secure the web application's access to the AWS account.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS IAM Identity Center (AWS Single Sign-On). Configure an IdP. Upload the IdP metadata from the existing IdP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM IdP by using the provider URL, audience, and signature from the existing IP.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that has a policy that allows the necessary S3 actions. Configure the role's trust policy to allow the OIDC IP to assume the role if the sts.amazon.com:aud context key is appid_from_idp.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that has a policy that allows the necessary S3 actions. Configure the role's trust policy to allow the OIDC IP to assume the role if the auth.company.com:aud context key is appid_from_idp.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the web application to use the AssumeRoleWithWebIdentity API operation to retrieve temporary credentials. Use the temporary credentials to make the S3 API calls.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the web application to use the GetFederationToken API operation to retrieve temporary credentials. Use the temporary credentials to make the S3 API calls."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "ADE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CDE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-10T12:33:00.000Z",
        "voteCount": 5,
        "content": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-idp_oidc.html"
      },
      {
        "date": "2024-07-05T13:12:00.000Z",
        "voteCount": 2,
        "content": "\"Use OpenID Connect (OIDC) federated identity providers instead of creating\" IAM  users.\"  \"With an\" IdP \"you can manage\" \"user identities outside of AWS and give these external user identities permissions to access AWS resources in your account.\"\nB: (YES) \"IAM OIDC identity Providers\" \"This is useful when creating a mobile app or web application that requires access to AWS resources, but you don't want to create custom sign-in code or manage your own user identities.\"\nD: (YES) \"For OIDC providers, use the fully qualified URL of the OIDC IdP with the aud context key\" e.g.: \"Condition\": {\"StringEquals\": {\"server.example.com:aud\": \"appid_from_oidc_idp\"}}\"\nE: (YES) \"AssumeRoleWithWebIdentity\" \"Federation through a web-based\" IDP \"returns a set of temporary security credentials for federated users\" \"authenticated\" \"with a public identity provider.\" \"This operation is useful for\" \"client-based web applications that require access to AWS.\""
      },
      {
        "date": "2024-05-02T06:21:00.000Z",
        "voteCount": 1,
        "content": "BDE for me"
      },
      {
        "date": "2024-04-13T20:40:00.000Z",
        "voteCount": 1,
        "content": "DE is correct not sure between A &amp; B\nA. Configure AWS IAM Identity Center (AWS Single Sign-On). Configure an IdP. Upload the IdP metadata from the existing IdP.\nPros: Integrates with AWS SSO and allows for IdP metadata upload.\nCons: AWS SSO is generally used for managing multiple AWS accounts and SSO for multiple AWS services, might be overkill for a single account and application.\nB. Create an IAM IdP by using the provider URL, audience, and signature from the existing IP.\nPros: Creates a custom IAM IdP using the existing IdP's details.\nCons: Manual configuration of IAM IdP might be error-prone and not the best practice for OIDC integration."
      },
      {
        "date": "2024-02-12T07:48:00.000Z",
        "voteCount": 3,
        "content": "BDE: \nA: we need to create an IDP. We dont need a AWS Single Sign-On\nB: correct\nC: we need to authen. sts.amazon.com:aud does not for authen\nD: auth.company.com:aud is for authen\nE: This used for authen  AssumeRoleWithWebIdentity\nF: This is not used for authen"
      },
      {
        "date": "2024-02-12T05:11:00.000Z",
        "voteCount": 1,
        "content": "C &amp; D: Creating an IAM role with specific S3 permissions and configuring the trust policy based on the appropriate audience (sts.amazon.com:aud or auth.company.com:aud) allows secure role assumption by the OIDC IdP on behalf of authenticated users.\nE: Using AssumeRoleWithWebIdentity fetches temporary credentials with restricted privileges, enhancing security compared to long-lived credentials."
      },
      {
        "date": "2024-02-12T05:11:00.000Z",
        "voteCount": 1,
        "content": "Options A, B, and F are not suitable for this scenario:\nA: AWS SSO is currently not available for public AWS accounts and wouldn't address the specific OIDC integration requirement.\nB: While creating an IAM IdP is possible, it's generally less secure than leveraging the existing, trusted IdP with OIDC support.\nF: GetFederationToken is often used with SAML-based federation and wouldn't work directly with OIDC."
      },
      {
        "date": "2024-02-07T09:03:00.000Z",
        "voteCount": 3,
        "content": "BDE is my answer"
      },
      {
        "date": "2024-02-07T04:52:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-idp_oidc.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 198,
    "url": "https://www.examtopics.com/discussions/amazon/view/132881-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses Amazon RDS for all databases in its AWS accounts. The company uses AWS Control Tower to build a landing zone that has an audit and logging account. All databases must be encrypted at rest for compliance reasons. The company's security engineer needs to receive notification about any noncompliant databases that are in the company\u2019s accounts.<br><br>Which solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Control Tower to activate the optional detective control (guardrail) to determine whether the RDS storage is encrypted. Create an Amazon Simple Notification Service (Amazon SNS) topic in the company's audit account. Create an Amazon EventBridge rule to filter noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer's email address to the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation StackSets to deploy AWS Lambda functions to every account. Write the Lambda function code to determine whether the RDS storage is encrypted in the account the function is deployed to. Send the findings as an Amazon CloudWatch metric to the management account. Create an Amazon Simple Notification Service (Amazon SNS) topic. Create a CloudWatch alarm that notifies the SNS topic when metric thresholds are met. Subscribe the security engineer's email address to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom AWS Config rule in every account to determine whether the RDS storage is encrypted. Create an Amazon Simple Notification Service (Amazon SNS) topic in the audit account. Create an Amazon EventBidge rule to filter noncompliant events from the AWS Control Tower control (guardrail) to notify the SNS topic. Subscribe the security engineer's email address to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an Amazon C2 instance. Run an hourly cron job by using the AWS CLI to determine whether the RDS storage is encrypted in each AWS account. Store the results in an RDS database. Notify the security engineer by sending email messages from the EC2 instance when noncompliance is detected"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T02:22:00.000Z",
        "voteCount": 1,
        "content": "Keywords: Control Tower\nA company uses Amazon RDS for all databases in its AWS accounts. The company uses AWS Control Tower"
      },
      {
        "date": "2024-04-20T10:22:00.000Z",
        "voteCount": 2,
        "content": "A\nhttps://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted"
      },
      {
        "date": "2024-04-13T20:47:00.000Z",
        "voteCount": 2,
        "content": "most efficient way is A"
      },
      {
        "date": "2024-03-16T02:46:00.000Z",
        "voteCount": 2,
        "content": "A - least operational overhead"
      },
      {
        "date": "2024-03-09T04:46:00.000Z",
        "voteCount": 3,
        "content": "Guardrail uses AWS Config for compliance detection"
      },
      {
        "date": "2024-02-25T14:14:00.000Z",
        "voteCount": 2,
        "content": "Answer: C - https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted"
      },
      {
        "date": "2024-02-12T07:52:00.000Z",
        "voteCount": 3,
        "content": "A is correct: we need guardraild to detect non-compliances\nB and D: no mention of guardrail.\nC: Though this option mentions guardrail, it uses AWS Config to detect non-compliances"
      },
      {
        "date": "2024-02-13T08:00:00.000Z",
        "voteCount": 1,
        "content": "correction: C"
      },
      {
        "date": "2024-02-12T05:10:00.000Z",
        "voteCount": 2,
        "content": "Leverages existing infrastructure: It utilizes native AWS Control Tower functionality for compliance checks and integrates seamlessly with SNS for notifications.\nCentralized management: Configuration and monitoring are done in the audit account, eliminating the need for individual resources in each account.\nScalability: Handles future account growth without manual intervention."
      },
      {
        "date": "2024-02-10T12:41:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted"
      },
      {
        "date": "2024-02-07T04:51:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommended-controls.html#disallow-rds-storage-unencrypted"
      },
      {
        "date": "2024-02-05T09:48:00.000Z",
        "voteCount": 2,
        "content": "Compliance == Aws config"
      },
      {
        "date": "2024-02-07T10:57:00.000Z",
        "voteCount": 2,
        "content": "thanks for the summary"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 199,
    "url": "https://www.examtopics.com/discussions/amazon/view/133267-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is migrating from its on-premises data center to AWS. The company currently uses a custom on-premises Cl/CD pipeline solution to build and package software.<br><br>The company wants its software packages and dependent public repositories to be available in AWS CodeArtifact to facilitate the creation of application-specific pipelines.<br><br>Which combination of steps should the company take to update the CI/CD pipeline solution and to configure CodeArtifact with the LEAST operational overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the C1ICD pipeline to create a VM image that contains newly packaged software. Use AWS Import/Export to make the VM image available as an Amazon EC2 AMI. Launch the AMI with an attached IAM instance profile that allows CodeArtifact actions. Use AWS CLI commands to publish the packages to a CodeArtifact repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Identity and Access Management Roles Anywhere trust anchor. Create an IAM role that allows CodeArtifact actions and that has a trust relationship on the trust anchor. Update the on-premises CI/CD pipeline to assume the new IAM role and to publish the packages to CodeArtifact.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Amazon S3 bucket. Generate a presigned URL that allows the PutObject request. Update the on-premises CI/CD pipeline to use the presigned URL to publish the packages from the on-premises location to the S3 bucket. Create an AWS Lambda function that runs when packages are created in the bucket through a put command. Configure the Lambda function to publish the packages to CodeArtifact.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each public repository, create a CodeArutact repository that is configured with an external connection. Configure the dependent repositories as upstream public repositories.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Codeartitact repository that is configured with a set of external connections to the public repositories. Configure the external connections to be downstream of the repository."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T08:00:00.000Z",
        "voteCount": 5,
        "content": "B and D are correct: &lt;wants its software packages and dependent public repositories to be available in AWS CodeArtifact &gt;: we need to push onprem artifact to CodeArtifact with IAM Anywhere Role and create an upstream for public repositories\nA: irrelevant\nB: correct\nC: irrelevant\nD: correct\nE: there is no downstream in CodeArtifact"
      },
      {
        "date": "2024-04-13T20:54:00.000Z",
        "voteCount": 2,
        "content": "ANS B&amp;D"
      },
      {
        "date": "2024-03-16T02:45:00.000Z",
        "voteCount": 4,
        "content": "B &amp; D\nB - https://docs.aws.amazon.com/rolesanywhere/latest/userguide/getting-started.html\nD - Best practice for external connections is to have one repository per domain with an external connection to a given public repository."
      },
      {
        "date": "2024-02-12T05:03:00.000Z",
        "voteCount": 3,
        "content": "B &amp; D\n\nThe other options have drawbacks:\nA:\nComplex setup: Requires VM image creation, import, and AMI launching, adding unnecessary complexity.\nSecurity concerns: Using EC2 instances might introduce security risks compared to IAM roles.\nInefficient publishing: Relies on manual CLI commands for publishing, less automated than other options."
      },
      {
        "date": "2024-02-12T05:03:00.000Z",
        "voteCount": 1,
        "content": "B:\nMinimal infrastructure: Only requires an IAM role and trust anchor setup in AWS, without creating additional resources like VMs or S3 buckets.\nSecure access: Leverages IAM for secure communication between the on-premises pipeline and CodeArtifact.\nDirect publishing: Enables direct package publishing from the pipeline to CodeArtifact.\n&nbsp;\nD:\nCentralized management: Manages public repositories through a single CodeArtifact repository with external connections.\nAutomatic updates: Upstream repository changes are automatically reflected in CodeArtifact.\nReduced bandwidth: Packages stored in public repositories, minimizing data transfer to AWS."
      },
      {
        "date": "2024-02-10T12:49:00.000Z",
        "voteCount": 2,
        "content": "https://www.pulumi.com/ai/answers/bddaepm6EeuDs9du1MVtC8/aws-codeartifact-and-iam-roles-setup"
      },
      {
        "date": "2024-02-07T05:18:00.000Z",
        "voteCount": 1,
        "content": "D. In CodeArtifact, the intended way to use external connections is to have one repository per domain with an external connection to a given public repository.\n\nA. Using aws codeartifact with rolesanywhere is the LEAST operational overhead =&gt; https://www.pulumi.com/ai/answers/bddaepm6EeuDs9du1MVtC8/aws-codeartifact-and-iam-roles-setup"
      },
      {
        "date": "2024-02-07T05:20:00.000Z",
        "voteCount": 1,
        "content": "I meant B :-)  (not A)"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 200,
    "url": "https://www.examtopics.com/discussions/amazon/view/133298-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps team uses AWS CodePipeline, AWS CodeBuild, and AWS CodeDeploy to deploy an application. The application is a REST API that uses AWS Lambda functions and Amazon API Gateway. Recent deployments have introduced errors that have affected many customers.<br><br>The DevOps team needs a solution that reverts to the most recent stable version of the application when an error is detected. The solution must affect the fewest customers possible.<br><br>Which solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the deployment configuration in CodeDeploy to LambdaAllAtOnce. Configure automatic rollbacks on the deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Configure the deployment group to roll back when the number of alarms meets the alarm threshold.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the deployment configuration in CodeDeploy to LambdaCanary10Percent10Minutes. Configure automatic rollbacks on the deployment group. Create an Amazon CloudWatch alarm that detects HTTP Bad Gateway errors on API Gateway. Configure the deployment group to roll back when the number of alarms meets the alarm threshold.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the deployment configuration in CodeDeploy to LambdaAllAtOnce. Configure manual rollbacks on the deployment group. Create an Amazon Simple Notification Service (Amazon SNS) topic to send notifications every time a deployment fails. Configure the SNS topic to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the deployment configuration in CodeDeploy to LambdaCanary10Percent10Minutes. Configure manual rollbacks on the deployment group. Create a metric filter on an Amazon CloudWatch log group for API Gateway to monitor HTTP Bad Gateway errors. Configure the metric filter to invoke a new Lambda function that stops the current deployment and starts the most recent successful deployment."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T08:06:00.000Z",
        "voteCount": 8,
        "content": "B is correct:\nA and C: &lt;CodeDeploy to LambdaAllAtOnce&gt;: Replacing all at once would not allow roll back\nD: Metric filter cannot trigger lambda. Additionally, this options is a manual work"
      },
      {
        "date": "2024-04-25T23:03:00.000Z",
        "voteCount": 3,
        "content": "Option B provides the most operationally efficient solution by combining canary deployments, automatic rollbacks, and CloudWatch alarms to detect and respond to issues quickly while minimizing customer impact."
      },
      {
        "date": "2024-04-13T20:56:00.000Z",
        "voteCount": 3,
        "content": "Answer B"
      },
      {
        "date": "2024-03-19T09:44:00.000Z",
        "voteCount": 3,
        "content": "Its B, 100%"
      },
      {
        "date": "2024-02-13T10:32:00.000Z",
        "voteCount": 1,
        "content": "option C - LambdaAllAtOnce: Rolling back everything at once minimizes the window for potential customer impact compared to canary deployments.\nManual rollbacks: While automatic rollbacks may seem faster, they can be triggered by false positives, leading to unnecessary rollbacks and service disruptions. Manual rollbacks offer more control and allow the team to assess the situation before reverting.\nSNS notifications: Alerts about failing deployments are crucial for quick response.\nLambda function for rollback: Automating the rollback process with a Lambda function triggered by SNS notification streamlines the operation and reduces manual intervention.\nStarts the most recent successful deployment: This ensures reverting to a known-good state without manual selection, saving time and avoiding errors."
      },
      {
        "date": "2024-02-13T10:32:00.000Z",
        "voteCount": 1,
        "content": "Here's how the other options fall short:\n&nbsp;\nOption A: While LambdaAllAtOnce is good, automatic rollbacks based on alarms might be triggered by transient issues, leading to unnecessary disruptions.\nOption B: Canary deployments are beneficial for testing, but rolling back 10% at a time might not be efficient if the error affects all users. Also, relying on automatic rollbacks has the same drawbacks as mentioned in A.\nOption D: While manual control is good, relying solely on CloudWatch logs and a separate Lambda function for stopping and starting deployments adds complexity and requires more manual intervention compared to the streamlined SNS-triggered rollback in option C."
      },
      {
        "date": "2024-04-25T23:04:00.000Z",
        "voteCount": 1,
        "content": "don't say rubbish with confidence. B is the answer"
      },
      {
        "date": "2024-02-07T11:03:00.000Z",
        "voteCount": 1,
        "content": "D. Rolling deployment with the option to stop once it detects any errors."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 201,
    "url": "https://www.examtopics.com/discussions/amazon/view/133299-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company recently deployed its web application on AWS. The company is preparing for a large-scale sales event and must ensure that the web application can scale to meet the demand.<br><br>The application's frontend infrastructure includes an Amazon CloudFront distribution that has an Amazon S3 bucket as an origin. The backend infrastructure includes an Amazon API Gateway API, several AWS Lambda functions, and an Amazon Aurora DB cluster.<br><br>The company's DevOps engineer conducts a load test and identifies that the Lambda functions can fulfil the peak number of requests. However, the DevOps engineer notices request latency during the initial burst of requests. Most of the requests to the Lambda functions produce queries to the database. A large portion of the invocation time is used to establish database connections.<br><br>Which combination of steps will provide the application with the required scalability? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a higher reserved concurrency for the Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a higher provisioned concurrency for the Lambda functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the DB cluster to an Aurora global database. Add additional Aurora Replicas in AWS Regions based on the locations of the company's customers.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefactor the Lambda functions. Move the code blocks that initialize database connections into the function handlers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS Proxy to create a proxy for the Aurora database. Update the Lambda functions to use the proxy endpoints for database connections.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCF",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "BDF",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "ABF",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "CDF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-30T14:49:00.000Z",
        "voteCount": 13,
        "content": "A. this doesn't directly address the database connection issue and there will be moments were you will be not using it, so spending money\nB. correct, Configure a higher provisioned concurrency for the Lambda functions: This ensures that Lambda instances are ready to handle bursts of traffic, reducing cold start latency.\nC. Is correct, if they want to read only\nD. is wrong because it says \"... into the function handlers...\" while best practices say to do it OUTSIDE the function handlers. Starting NEW CONNECTIONS is bad thing.\nF. Is correct, it is a best practice"
      },
      {
        "date": "2024-05-12T12:46:00.000Z",
        "voteCount": 2,
        "content": "Glad I read this. I read D wrong the first time."
      },
      {
        "date": "2024-03-30T14:50:00.000Z",
        "voteCount": 5,
        "content": "Also, please notice that \"The company's DevOps engineer conducts a load test and identifies that the Lambda functions can fulfil the peak number of requests.\""
      },
      {
        "date": "2024-03-16T02:36:00.000Z",
        "voteCount": 5,
        "content": "A - Provisioned concurrency \u2013 This is the number of pre-initialized execution environments allocated to your function\nhttps://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html\n\nD -  Initialize SDK clients and database connections outside of the function handler\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html\n\nF - RDS Proxy improves scalability by pooling and sharing database connections\nhttps://aws.amazon.com/rds/proxy/faqs/?nc=sn&amp;loc=4"
      },
      {
        "date": "2024-08-24T05:59:00.000Z",
        "voteCount": 1,
        "content": "I think BCF"
      },
      {
        "date": "2024-08-20T06:36:00.000Z",
        "voteCount": 2,
        "content": "BCF for me"
      },
      {
        "date": "2024-08-19T07:24:00.000Z",
        "voteCount": 3,
        "content": "B C D i correct"
      },
      {
        "date": "2024-07-10T17:38:00.000Z",
        "voteCount": 3,
        "content": "The person who chose D doesn't understand Lambda at all"
      },
      {
        "date": "2024-07-06T01:20:00.000Z",
        "voteCount": 3,
        "content": "A. (NO) \"Lambda functions can fulfil the peak number of requests.\"\nB.(YES) \"The number of pre-initialized execution environments allocated to a function. These execution environments are ready to respond immediately to incoming function requests.\"\nC.(YES) Chosen in part by process of elimination because neither \"A\" or \"D\" is correct. \nD. (NO) Declaratons \"outside of the function's handler method remain initialized\" \"when the function is invoked again.\" \"if your Lambda function establishes a database connection\" \"the original connection is used in subsequent invocations.\"\nF.(YES) Chosen in part by process of elimination because neither \"A\" or \"D\" is correct."
      },
      {
        "date": "2024-04-12T18:31:00.000Z",
        "voteCount": 3,
        "content": "BCF\nB.Configure a higher provisioned concurrency for the Lambda functions: This will help in maintaining a set number of initialized Lambda instances, reducing cold starts, and providing better scalability.\n\nC. Convert the DB cluster to an Aurora global database: This will help in reducing database connection latency for global users by replicating Aurora across multiple regions.\n\nF. Use Amazon RDS Proxy to create a proxy for the Aurora database: This will manage database connections efficiently with connection pooling, reducing the time to establish new connections and improving database interaction efficiency."
      },
      {
        "date": "2024-04-02T23:51:00.000Z",
        "voteCount": 1,
        "content": "BDF\nOption A is a waste of resources\nOption D is not practicable"
      },
      {
        "date": "2024-04-26T06:18:00.000Z",
        "voteCount": 1,
        "content": "but you choose option B"
      },
      {
        "date": "2024-03-19T10:03:00.000Z",
        "voteCount": 1,
        "content": "ABF, 100%"
      },
      {
        "date": "2024-04-02T23:53:00.000Z",
        "voteCount": 3,
        "content": "Why would you want to configure both Reserved and Provisioned concurrency at the same time? would that not amount to a waste of resurces?"
      },
      {
        "date": "2024-03-13T01:41:00.000Z",
        "voteCount": 3,
        "content": "BCF \nreferance: https://repost.aws/knowledge-center/lambda-cold-start"
      },
      {
        "date": "2024-03-09T06:37:00.000Z",
        "voteCount": 2,
        "content": "D is bad pratice as mentioned here.\nhttps://docs.aws.amazon.com/lambda/latest/dg/best-practices.html#function-code:~:text=Initialize%20SDK%20clients%20and%20database%20connections%20outside%20of%20the%20function%20handler\nC - unsure if that helps, if the Lambda function is not replicated to other regions."
      },
      {
        "date": "2024-03-06T22:52:00.000Z",
        "voteCount": 2,
        "content": "D is bad practice in this situation.\nConnecting DBs in global scope and using RDS Proxy can further improve performance."
      },
      {
        "date": "2024-02-18T20:22:00.000Z",
        "voteCount": 4,
        "content": "Configuring a higher reserved concurrency for the Lambda functions (Option A) ensures that a specific number of Lambda instances are available for your function, but it doesn't address the cold start issue as effectively as provisioned concurrency, nor does it directly address the database connection overhead.\n\nTherefore, the most effective combination of steps to provide the required scalability and address the identified issue would be Options B (Provisioned Concurrency), F (Amazon RDS Proxy), and a revised understanding of D that focuses on optimizing connection management for efficiency."
      },
      {
        "date": "2024-02-18T07:33:00.000Z",
        "voteCount": 3,
        "content": "B. Configure a higher provisioned concurrency for the Lambda functions: This ensures that Lambda instances are ready to handle bursts of traffic, reducing cold start latency.\nF. Use Amazon RDS Proxy to create a proxy for the Aurora database: This directly addresses the issue of database connection overhead, significantly reducing latency by pooling and reusing connections.\nA. Configure a higher reserved concurrency for the Lambda functions (optional based on specific needs): While this doesn't directly address the database connection issue, it ensures that enough Lambda instances are available to handle the application load, complementing the benefits of provisioned concurrency and RDS Proxy."
      },
      {
        "date": "2024-02-13T10:37:00.000Z",
        "voteCount": 1,
        "content": "D: By moving connection initialization into the function handler, you avoid the cold start penalty encountered when a new Lambda instance is spun up. Each request can establish a fresh connection, reducing latency during the initial burst.\n&nbsp;\nF: RDS Proxy creates a connection pool, eliminating the need for each Lambda invocation to establish a new connection. Reusing connections significantly reduces request latency, especially for short-lived interactions.\n&nbsp;\nC: Aurora Global Database distributes data across multiple regions, improving performance for users in different locations. Adding replicas provides additional read capacity, increasing overall database scalability."
      },
      {
        "date": "2024-02-13T10:38:00.000Z",
        "voteCount": 1,
        "content": "While the other options have some merit:\nA &amp; B: Increasing reserved/provisioned concurrency might help, but it has ongoing costs and might not be optimal for unpredictable surges.\nE: CloudFront primarily improves content delivery latency, not database-related delays."
      },
      {
        "date": "2024-02-12T08:10:00.000Z",
        "voteCount": 4,
        "content": "BDF are correct:\nA: &lt;the Lambda functions can fulfil the peak number of requests&gt; means we dont need to increase this \nB: correct\nC: irrelevant\nD and F: both correct, handling the connection issue"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 202,
    "url": "https://www.examtopics.com/discussions/amazon/view/133300-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs a web application that extends across multiple Availability Zones. The company uses an Application Load Balancer (ALB) for routing, AWS Fargate for the application, and Amazon Aurora for the application data. The company uses AWS CloudFormation templates to deploy the application. The company stores all Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository in the same AWS account and AWS Region.<br><br>A DevOps engineer needs to establish a disaster recovery (DR) process in another Region. The solution must meet an RPO of 8 hours and an RTO of 2 hours. The company sometimes needs more than 2 hours to build the Docker images from the Dockerfile.<br><br>Which solution will meet the RTO and RPO requirements MOST cost-effectively?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the CloudFormation templates and the Dockerfile to an Amazon S3 bucket in the DR Region. Use AWS Backup to configure automated Aurora cross-Region hourly snapshots. In case of DR, build the most recent Docker image and upload the Docker image to an ECR repository in the DR Region. Use the CloudFormation template that has the most recent Aurora snapshot and the Docker image from the ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Configure Aurora automated backup Cross-Region Replication. Configure ECR Cross-Region Replication. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region. Update the application DNS records to point to the new ALB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Use Amazon EventBridge to schedule an AWS Lambda function to take an hourly snapshot of the Aurora database and of the most recent Docker image in the ECR repository. Copy the snapshot and the Docker image to the DR Region. In case of DR, use the CloudFormation template with the most recent Aurora snapshot and the Docker image from the local ECR repository to launch a new CloudFormation stack in the DR Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the CloudFormation templates to an Amazon S3 bucket in the DR Region. Deploy a second application CloudFormation stack in the DR Region. Reconfigure Aurora to be a global database. Update both CloudFormation stacks when a new application release in the current Region is needed. In case of DR, update the application DNS records to point to the new ALB."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T08:16:00.000Z",
        "voteCount": 5,
        "content": "B is correct:\nA: &lt; build the most recent Docker image and upload the Docker image to an ECR repository in the DR Region&gt;: This process might take longer than 2 hours, which does not meet the RTO \nB: correct\nC: &lt;AWS Lambda function to take an hourly snapshot of the Aurora database&gt;: lambda has a maximum running time of 15 minutes. Cannot do an hourly task\nD: No mention about the docker image for the ECS"
      },
      {
        "date": "2024-04-12T18:45:00.000Z",
        "voteCount": 2,
        "content": "answer is B"
      },
      {
        "date": "2024-03-16T02:31:00.000Z",
        "voteCount": 3,
        "content": "B: Least operational overhead, lower cost and meets RPO and RTO"
      },
      {
        "date": "2024-02-29T13:23:00.000Z",
        "voteCount": 3,
        "content": "B provides a cost-effective solution that meets the RTO and RPO"
      },
      {
        "date": "2024-02-09T09:51:00.000Z",
        "voteCount": 2,
        "content": "I also go with B"
      },
      {
        "date": "2024-02-07T11:15:00.000Z",
        "voteCount": 2,
        "content": "B. Most cost effective"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 203,
    "url": "https://www.examtopics.com/discussions/amazon/view/133301-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's application runs on Amazon EC2 instances. The application writes to a log file that records the username, date, time, and source IP address of the login. The log is published to a log group in Amazon CloudWatch Logs.<br><br>The company is performing a root cause analysis for an event that occurred on the previous day. The company needs to know the number of logins for a specific user from the past 7 days.<br><br>Which solution will provide this information?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch Logs metric filter on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch Logs subscription on the log group. Use a filter pattern that matches the username. Publish a CloudWatch metric that sums the number of logins over the past 7 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch dashboard. Add a number widget that has a filter pattern that counts the number of logins for the username over the past 7 days directly from the log group."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-25T22:46:00.000Z",
        "voteCount": 1,
        "content": "There is no cloudwatch metric that logs user logins\n\n"
      },
      {
        "date": "2024-08-10T07:58:00.000Z",
        "voteCount": 1,
        "content": "D\nFor A you would have to setup the cloudwatch metric filters beforehand as you won't be able to analyze past logs without the setup"
      },
      {
        "date": "2024-04-12T19:04:00.000Z",
        "voteCount": 4,
        "content": "answer C"
      },
      {
        "date": "2024-04-07T01:01:00.000Z",
        "voteCount": 3,
        "content": "The solution that will provide the number of logins for a specific user from the past 7 days is Option C: Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group."
      },
      {
        "date": "2024-03-13T01:54:00.000Z",
        "voteCount": 2,
        "content": "C \nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_AnalyzeLogData_AggregationQuery.html"
      },
      {
        "date": "2024-02-29T13:36:00.000Z",
        "voteCount": 4,
        "content": "C is the most suitable solution for obtaining the required information"
      },
      {
        "date": "2024-02-17T10:04:00.000Z",
        "voteCount": 4,
        "content": ", C. Create a CloudWatch Logs Insights query that uses an aggregation function to count the number of logins for the username over the past 7 days. Run the query against the log group. is the best solution. This method directly addresses the need to analyze log data for a specific pattern (user logins) and aggregate the counts over a specified period, which is exactly what's needed for a root cause analysis of the event."
      },
      {
        "date": "2024-02-13T10:50:00.000Z",
        "voteCount": 4,
        "content": "C - CloudWatch Logs Insights: This service allows you to run ad-hoc queries against your log data without creating additional infrastructure like metrics or subscriptions.\nAggregation function: Functions like count() can specifically calculate the number of occurrences based on specific criteria.\nFiltering by username and timeframe: The query can be tailored to include the specific username and filter for entries within the past 7 days."
      },
      {
        "date": "2024-02-13T10:50:00.000Z",
        "voteCount": 1,
        "content": "A. Metric filter: It can count occurrences, but requires additional metric creation and subscription, introducing complexity.\nB. Subscription: Similar to metric filter, requires creating an additional subscription and pushing data elsewhere.\nD. Dashboard widget: Limited in its capabilities, might not allow complex filtering and aggregation needed for this specific analysis.\n&nbsp;\nTherefore, CloudWatch Logs Insights offers the most direct and flexible solution for analyzing the desired login data within the given timeframe and user criteria."
      },
      {
        "date": "2024-02-12T08:22:00.000Z",
        "voteCount": 1,
        "content": "A is correct: CloudWatch Logs metric filter can filter out relevant logs and count\nB: irrelevant, this is for sharing logs to other sources\nC: This can accomplish the task. However, loudWatch Logs metric filter offers us the same function with less cost. Using CloudWatch Logs Insights query incur more costs. This feature is primarily used for data analysis\nD: irrelevant"
      },
      {
        "date": "2024-02-07T11:25:00.000Z",
        "voteCount": 1,
        "content": "C. It must be CloudWatch Logs Insights query"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 204,
    "url": "https://www.examtopics.com/discussions/amazon/view/133302-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an AWS CodeDeploy application. The application has a deployment group that uses a single tag group to identify instances for the deployment of Application. The single tag group configuration identifies instances that have Environment=Production and Name=ApplicationA tags for the deployment of ApplicationA.<br><br>The company launches an additional Amazon EC2 instance with Department=Marketing, Environment=Production, and Name=ApplicationB tags. On the next CodeDeploy deployment of Application, the additional instance has ApplicationA installed on it. A DevOps engineer needs to configure the existing deployment group to prevent ApplicationA from being installed on the additional instance.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Name=ApplicationA tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the current single tag group to include the Department=Marketing, Environment=production, and Name=ApplicationA tags.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd another single tag group that includes only the Department=Marketing tag. Keep the Environment=Production and Name=ApplicationA tags with the current single tag group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the current single tag group to include only the Environment=Production tag. Add another single tag group that includes only the Department=Marketing tag."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T08:32:00.000Z",
        "voteCount": 7,
        "content": "A is correct: All tags in one tag group are OR operator. Tags are in multiple tag groups are AND operator"
      },
      {
        "date": "2024-07-08T13:24:00.000Z",
        "voteCount": 3,
        "content": "I Found this question largely incomprehensible untill I understood the following concepts and mapped out everything\n- Single tag group:\tAny instance identified by at least one tag in the group is included in the deployment group.\t(OR Operator within individual tag groups)\n- Multiple tag groups:\tInstances that are identified by at least one tag in each of the tag groups are included.\t(AND Operator between multiple tag groups)\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html"
      },
      {
        "date": "2024-07-08T13:24:00.000Z",
        "voteCount": 2,
        "content": "A:(YES) Block ApplicationA deployments on EC2B because it would now have to have both Production AND ApplicationA tags (AND Operator between tag groups)\nB: (NO)\tAllow ApplicationA deployments on EC2B because it has the Production tag (OR Operator within tag group)\nC: (NO) Allow ApplicationA deployments on EC2B because it has the Production and Marketing tags (OR Operator within tag group, AND Operator between tag groups)\nD: (NO) Allow ApplicationA deployments on EC2B because it has the Production and Marketing tags (OR Operator within tag group, AND Operator between tag groups)"
      },
      {
        "date": "2024-03-16T02:26:00.000Z",
        "voteCount": 4,
        "content": "A:  A single tag group can only contain 1 tag, so multiple Single tag groups will be needed.\nhttps://docs.aws.amazon.com/codedeploy/latest/userguide/instances-tagging.html"
      },
      {
        "date": "2024-02-29T13:29:00.000Z",
        "voteCount": 1,
        "content": "Option B is the most appropriate solution for meeting the requirements:\n\nChanging the current single tag group to include the specific tags (Department=Marketing, Environment=Production, and Name=ApplicationA) ensures that only instances with these specific tags are identified for the deployment of ApplicationA.\nThe other options are not suitable for achieving the desired outcome"
      },
      {
        "date": "2024-02-13T10:56:00.000Z",
        "voteCount": 4,
        "content": "A - Original configuration: The single tag group with Environment=Production and Name=ApplicationA tags targets any instance with both tags, resulting in ApplicationA being deployed on the new Marketing instance despite its different Name tag.\nSolution A:\nChanging the current tag group to \"Environment=Production\" ensures only instances in the Production environment are considered.\nAdding a separate tag group with \"Name=ApplicationA\" specifically targets instances meant for that application.\nThe Marketing instance with Department=Marketing tag doesn't match the new criteria and avoids unintended ApplicationA installation."
      },
      {
        "date": "2024-02-13T10:56:00.000Z",
        "voteCount": 1,
        "content": "Reasons why other options won't work:\n&nbsp;\nB: Including Department=Marketing in the current tag group wouldn't change anything - all instances in Production with ApplicationA tag would be targeted.\nC: Adding a Department=Marketing tag group alone still leaves the original tag group targeting the Marketing instance due to the Name=ApplicationA tag.\nD: Removing Name=ApplicationA from the initial tag group removes a necessary criterion, potentially deploying ApplicationA to all Production instances regardless of their Name tag.\n&nbsp;\nTherefore, solution A provides the most precise and effective way to exclude the Marketing instance from ApplicationA deployment while maintaining the desired targeting for other instances."
      },
      {
        "date": "2024-02-07T11:28:00.000Z",
        "voteCount": 1,
        "content": "A. need to applicationA tag"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 205,
    "url": "https://www.examtopics.com/discussions/amazon/view/133303-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is launching an application that stores raw data in an Amazon S3 bucket. Three applications need to access the data to generate reports. The data must be redacted differently for each application before the applications can access the data.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 bucket for each application. Configure S3 Same-Region Replication (SRR) from the raw data's S3 bucket to each application's S3 bucket. Configure each application to consume data from its own S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis data stream. Create an AWS Lambda function that is invoked by object creation events in the raw data\u2019s S3 bucket. Program the Lambda function to redact data for each application. Publish the data on the Kinesis data stream. Configure each application to consume data from the Kinesis data stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each application, create an S3 access point that uses the raw data's S3 bucket as the destination. Create an AWS Lambda function that is invoked by object creation events in the raw data's S3 bucket. Program the Lambda function to redact data for each application. Store the data in each application's S3 access point. Configure each application to consume data from its own S3 access point.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 access point that uses the raw data\u2019s S3 bucket as the destination. For each application, create an S3 Object Lambda access point that uses the S3 access point. Configure the AWS Lambda function for each S3 Object Lambda access point to redact data when objects are retrieved. Configure each application to consume data from its own S3 Object Lambda access point\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-04T06:20:00.000Z",
        "voteCount": 2,
        "content": "keywords: S3 Object Lambda access point to redact data"
      },
      {
        "date": "2024-04-26T10:12:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html"
      },
      {
        "date": "2024-04-12T19:47:00.000Z",
        "voteCount": 2,
        "content": "answer D"
      },
      {
        "date": "2024-03-16T22:04:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/tutorial-s3-object-lambda-uppercase.html"
      },
      {
        "date": "2024-03-07T05:30:00.000Z",
        "voteCount": 2,
        "content": "S3 Object Lambda access point is not suitable for generating reports. Generally, creating a report requires an aggregate process, which is expensive. Since reports are expected to be viewed multiple times, it is inefficient to pay for Lambda processing time and CPU costs each time they are viewed. \nTo adopt D, CloudFront should be added to the front.\nhttps://aws.amazon.com/jp/blogs/aws/new-use-amazon-s3-object-lambda-with-amazon-cloudfront-to-tailor-content-for-end-users/"
      },
      {
        "date": "2024-02-29T13:40:00.000Z",
        "voteCount": 3,
        "content": "D, using S3 Object Lambda access points, is the most appropriate solution for the requirements:\nS3 Object Lambda allows you to add your own code to S3 GET requests to modify and process data at the time of retrieval. In this scenario, you can create an S3 access point that uses the raw data's S3 bucket as the destination. For each application, create a separate S3 Object Lambda access point that uses the S3 access point as the source. Configure the AWS Lambda function for each S3 Object Lambda access point to redact data when objects are retrieved.\nThis solution ensures that each application can access the data with its own redaction rules, and the redaction is applied dynamically at the time of retrieval."
      },
      {
        "date": "2024-02-13T11:03:00.000Z",
        "voteCount": 2,
        "content": "Single source of truth: This solution maintains a single copy of the raw data in the original S3 bucket, avoiding data duplication and associated costs.\nFine-grained redaction: Each application has its own S3 Object Lambda access point, allowing independent Lambda functions to redact data according to specific needs. This ensures targeted redaction without creating multiple S3 buckets with potentially inefficient data copies.\nEfficient access: Applications access the data through their respective S3 Object Lambda access points, incurring the redaction processing only when data is retrieved, improving cost-effectiveness compared to upfront redaction approaches."
      },
      {
        "date": "2024-02-13T11:03:00.000Z",
        "voteCount": 1,
        "content": "A: Duplicates data for each application, increasing storage costs and maintenance overhead.\nB: While Kinesis Data Streams can handle large data volumes, it adds an extra layer of complexity and latency compared to direct S3 access with redaction.\nC: Still requires upfront redaction for each application's specific needs, potentially duplicating redacted data across S3 access points."
      },
      {
        "date": "2024-02-12T08:36:00.000Z",
        "voteCount": 2,
        "content": "D is correct: S3 access point is actually S3 lambda access point, which is option D\nA and B: too expensive\nC: is not correct"
      },
      {
        "date": "2024-02-07T11:31:00.000Z",
        "voteCount": 2,
        "content": "D. S3 Bucker endpoint plus Lambda"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 206,
    "url": "https://www.examtopics.com/discussions/amazon/view/133304-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Control Tower and AWS CloudFormation to manage its AWS accounts and to create AWS resources. The company requires all Amazon S3 buckets to be encrypted with AWS Key Management Service (AWS KMS) when the S3 buckets are created in a CloudFormation stack.<br><br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Organizations. Attach an SCP that denies the s3:PutObject permission if the request does not include an x-amz-server-side-encryption header that requests server-side encryption with AWS KMS keys (SSE-KMS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Control Tower with a multi-account environment. Configure and enable proactive AWS Control Tower controls on all OUs with CloudFormation hooks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Control Tower with a multi-account environment. Configure and enable detective AWS Control Tower controls on all OUs with CloudFormation hooks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Organizations. Create an AWS Config organizational rule to check whether a KMS encryption key is enabled for all S3 buckets. Deploy the rule. Create and apply an SCP to prevent users from stopping and deleting AWS Config across all AWS accounts,"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-04T21:48:00.000Z",
        "voteCount": 2,
        "content": "keywords: proactive"
      },
      {
        "date": "2024-07-08T15:44:00.000Z",
        "voteCount": 3,
        "content": "Here's the Control Tower proactive control:\n\"[CT.S3.PR.10] Require an Amazon S3 bucket to have server-side encryption configured using an AWS KMS key\"\nhttps://docs.aws.amazon.com/controltower/latest/controlreference/s3-rules.html#ct-s3-pr-10-description"
      },
      {
        "date": "2024-06-14T22:52:00.000Z",
        "voteCount": 2,
        "content": "Clearly answer is B , here is article that explains the same.\nhttps://aws.amazon.com/blogs/mt/how-aws-control-tower-users-can-proactively-verify-compliance-in-aws-cloudformation-stacks/\n\nAnswer D with config rule also fits the bill (if no control tower), but since we have Control tower managing the accounts already its better to make use of the features that Control tower leverages"
      },
      {
        "date": "2024-04-12T20:00:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      },
      {
        "date": "2024-04-07T03:05:00.000Z",
        "voteCount": 2,
        "content": "B is better than D..."
      },
      {
        "date": "2024-03-19T11:03:00.000Z",
        "voteCount": 2,
        "content": "B, 100%"
      },
      {
        "date": "2024-03-03T02:19:00.000Z",
        "voteCount": 1,
        "content": "D provides a solution that leverages AWS Organizations and AWS Config to enforce the requirement for AWS KMS encryption on all S3 buckets created through CloudFormation:\nAWS Config Organizational Rule: Create an AWS Config organizational rule to check whether a KMS encryption key is enabled for all S3 buckets. This rule helps ensure that the encryption requirement is enforced.\nOptions A, B, and C do not directly address the requirement for AWS KMS encryption on S3 buckets created through CloudFormation:\nOption A mentions using an SCP but focuses on denying s3:PutObject without the required encryption header. However, this approach doesn't ensure that the encryption is enforced through AWS KMS.\nOptions B and C mention using AWS Control Tower with proactive or detective controls, but they don't specifically address the encryption requirement for S3 buckets."
      },
      {
        "date": "2024-04-07T03:05:00.000Z",
        "voteCount": 2,
        "content": "I am changing to B - Option B leverages AWS Control Tower, which is designed for managing multiple AWS accounts in a centralized and automated manner. By configuring and enabling proactive AWS Control Tower controls on all Organizational Units (OUs) with CloudFormation hooks, the company can ensure that all S3 buckets created within CloudFormation stacks adhere to the encryption requirement."
      },
      {
        "date": "2024-02-13T11:11:00.000Z",
        "voteCount": 4,
        "content": "Proactive controls: Proactive controls are preventative measures that block actions violating defined policies before they occur. This ensures encryption gets applied automatically during S3 bucket creation within CloudFormation stacks.\nCloudFormation hooks: Hooks enable Control Tower to intercept and enforce policies on CloudFormation stack operations, making it ideal for enforcing encryption during resource creation.\nMulti-account environment: Since the requirement applies across all accounts, Control Tower's multi-account capabilities ensure consistent policy enforcement throughout the organization."
      },
      {
        "date": "2024-02-13T11:12:00.000Z",
        "voteCount": 1,
        "content": "The other options have limitations:\n \n \nA: While SCPs enforce policies, they react to actions instead of proactively preventing them. Additionally, denying s3:PutObject might be too restrictive as it can impact other legitimate operations.\nC: Detective controls monitor and report on existing resources, not preventing non-compliant creations.\nD: Config and SCPs combined address encryption checks and user limitations, but lack the direct integration with CloudFormation stacks crucial for enforcing during creation."
      },
      {
        "date": "2024-02-12T08:41:00.000Z",
        "voteCount": 4,
        "content": "B is correct: &lt;AWS Control Tower&gt; means we need to use the proactive control \nA: SCP s3:PutObject permission only deny action related to put object to S3, not when creating it\nB: Detective controls used only for monitoring\nC: correct\nD: This option can achive the goal of the question. However, it is way more complicated than B."
      },
      {
        "date": "2024-02-07T11:32:00.000Z",
        "voteCount": 1,
        "content": "Maybe D"
      },
      {
        "date": "2024-06-14T22:52:00.000Z",
        "voteCount": 1,
        "content": "its B \n\nhttps://aws.amazon.com/blogs/mt/how-aws-control-tower-users-can-proactively-verify-compliance-in-aws-cloudformation-stacks/"
      },
      {
        "date": "2024-02-07T11:34:00.000Z",
        "voteCount": 1,
        "content": "Because of AWS Config"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 207,
    "url": "https://www.examtopics.com/discussions/amazon/view/133084-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer has developed an AWS Lambda function. The Lambda function starts an AWS CloudFormation drift detection operation on all supported resources for a specific CloudFormation stack. The Lambda function then exits its invocation.<br><br>The DevOps engineer has created an Amazon EventBridge scheduled rule that invokes the Lambda function every hour. An Amazon Simple Notification Service (Amazon SNS) topic already exists in the AWS account. The DevOps engineer has subscribed to the SNS topic to receive notifications.<br><br>The DevOps engineer needs to receive a notification as soon as possible when drift is detected in this specific stack configuration.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the existing EventBridge rule to also target the SNS topic. Configure an SNS subscription filter policy to match the CloudFormation stack. Attach the subscription filter policy to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second Lambda function to query the CloudFormation API for the drift detection results for the stack. Configure the second Lambda function to publish a message to the SNS topic if drift is detected. Adjust the existing EventBridge rule to also target the second Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon GuardDuty in the account with drift detection for all CloudFormation stacks. Create a second EventBridge rule that reacts to the GuardDuty drift detection event finding for the specific CloudFormation stack. Configure the SNS topic as a target of the second EventBridge rule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Config in the account. Use the cloudformation-stack-drift-detection-check managed rule. Create a second EventBridge rule that reacts to a compliance change event for the CloudFormation stack. Configure the SNS topic as a target of the second EventBridge rule.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-22T22:02:00.000Z",
        "voteCount": 7,
        "content": "I recommend checking out this blog which utilizes AWS Config and discusses Edenbridge. Here is the link: https://aws.amazon.com/blogs/mt/implementing-an-alarm-to-automatically-detect-drift-in-aws-cloudformation-stacks/\""
      },
      {
        "date": "2024-07-08T18:10:00.000Z",
        "voteCount": 1,
        "content": "Info gleaned from following the link(++):\n\ncloudformation-stack-drift-detection-check\nAWS Config rule that checks if the actual configuration of a AWS CloudFormation (AWS CloudFormation) stack differs, or has drifted, from the expected configuration. \n\nMaximumExecutionFrequency\nThe maximum frequency with which AWS Config runs evaluations for a rule.\n\nExample stack to detect and notify on drift:\n[...]\nMaximumExecutionFrequency:\n  Description: \"The maximum frequency with which drift in CloudFormation stacks need to be evaluated (default - One_Hour)\"\n  Type: \"String\"\n  Default: \"One_Hour\"\n  AllowedValues: [\"One_Hour\",\"Three_Hours\",\"Six_Hours\",\"Twelve_Hours\",\"TwentyFour_Hours\"]\n[...]"
      },
      {
        "date": "2024-02-19T01:10:00.000Z",
        "voteCount": 5,
        "content": "Given the options and the requirement for immediate notification upon drift detection, Option D is the most appropriate solution. It leverages AWS Config to continuously monitor and evaluate the configurations of AWS resources, including CloudFormation stacks. When AWS Config detects a drift from the desired configuration, it can trigger an EventBridge rule, which in turn can notify the interested parties via the SNS topic. This approach does not require additional custom logic to check for drift results, as AWS Config handles the evaluation and notification process based on configuration changes."
      },
      {
        "date": "2024-08-05T06:27:00.000Z",
        "voteCount": 2,
        "content": "The solution that will meet the requirements of receiving a notification as soon as possible when drift is detected in the specific CloudFormation stack configuration is: \n\nB. Create a second Lambda function to query the CloudFormation API for the drift detection results for the stack. Configure the second Lambda function to publish a message to the SNS topic if drift is detected. Adjust the existing EventBridge rule to also target the second Lambda function.\n\nOption D (Using AWS Config) would introduce additional complexity and potential delays, as AWS Config periodically evaluates resource configurations and may not provide immediate notifications upon drift detection.\n\nBy creating a separate Lambda function dedicated to monitoring drift detection results and publishing notifications to the existing SNS topic, you can ensure timely and reliable notifications while maintaining a modular and scalable architecture."
      },
      {
        "date": "2024-08-29T23:43:00.000Z",
        "voteCount": 1,
        "content": "This approach introduces additional complexity by adding another Lambda function to query and check for drift manually. Who is going to trigger this Lambda function? Even if you do it on a interval, it defeats the purpose of getting notified immediately.  \n\nAWS Config provides a more straightforward and managed way to detect and notify on drift with a managed rule, cloudformation-stack-drift-detection-check.\nhttps://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html"
      },
      {
        "date": "2024-04-12T20:52:00.000Z",
        "voteCount": 4,
        "content": "answer D\nAWS Config Integration: AWS Config is specifically designed to monitor and detect configuration changes and drifts in AWS resources, including CloudFormation stacks. Using AWS Config's built-in cloudformation-stack-drift-detection-check managed rule ensures comprehensive and reliable drift detection for CloudFormation stacks.\n\nEvent-Driven Architecture: Creating an EventBridge rule that reacts to a compliance change event for the CloudFormation stack allows you to trigger an alert as soon as drift is detected. This event-driven approach ensures timely detection and alerting for CloudFormation stack drift.\n\nSNS Notification: By configuring the SNS topic as a target of the EventBridge rule, you can easily send notifications/alerts to various endpoints, including email, SMS, or other AWS services, ensuring immediate alerting when drift is detected."
      },
      {
        "date": "2024-03-30T04:45:00.000Z",
        "voteCount": 4,
        "content": "D, Use the cloudformation-stack-drift-detection-check managed rule\nB uses scheduled rule will not notify as soon as possible as it runs hourly"
      },
      {
        "date": "2024-03-16T02:19:00.000Z",
        "voteCount": 5,
        "content": "D woudl be suitable - https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html\nB would not work as it would still only be triggered once per hour as is using the same event bridge rule"
      },
      {
        "date": "2024-03-14T02:19:00.000Z",
        "voteCount": 2,
        "content": "D\nrefer this: https://docs.aws.amazon.com/config/latest/developerguide/cloudformation-stack-drift-detection-check.html"
      },
      {
        "date": "2024-03-06T03:07:00.000Z",
        "voteCount": 1,
        "content": "The minimum interval for the `cloudformation-stack-drift-detection-check` managed rule in AWS config is 1 hour and does not meet the following requirements.\n`as soon as possible when drift is detected`"
      },
      {
        "date": "2024-03-09T07:26:00.000Z",
        "voteCount": 1,
        "content": "Any reference to 1 hour limit, ?"
      },
      {
        "date": "2024-03-03T02:10:00.000Z",
        "voteCount": 2,
        "content": "B is a suitable solution for meeting the requirements:\nThis solution provides a more direct and responsive approach.\nThe other options involve additional services like GuardDuty (Option C), which is not designed for CloudFormation drift detection, or AWS Config with managed rules (Option D), which may introduce unnecessary complexity for this specific scenario. Option A doesn't provide a straightforward way to react to drift detection events."
      },
      {
        "date": "2024-02-13T11:18:00.000Z",
        "voteCount": 1,
        "content": "Leverages existing infrastructure: This approach utilizes the existing EventBridge rule and SNS topic, avoiding the need for additional resources or complex configurations.\nImmediate notification: Since the EventBridge rule already triggers the Lambda function every hour, adding the SNS topic as a target ensures drift detection results are published directly to the topic for immediate notification.\nFiltering for specific stack: Implementing an SNS subscription filter policy ensures you only receive notifications for the specific CloudFormation stack you're interested in, avoiding irrelevant noise."
      },
      {
        "date": "2024-02-13T11:19:00.000Z",
        "voteCount": 1,
        "content": "B: Introduces an additional Lambda function and complexity, and requires polling for drift status, possibly delaying notification compared to real-time detection.\nC: While GuardDuty offers centralized drift detection, setting up a separate EventBridge rule and relying on event findings adds extra steps and might not be as timely as direct notification from the Lambda function.\nD: Although Config's cloudformation-stack-drift-detection-check rule identifies drift, triggering an EventBridge rule on compliance changes adds another layer of complexity and might not offer real-time notification like option A."
      },
      {
        "date": "2024-02-12T08:45:00.000Z",
        "voteCount": 1,
        "content": "B: is correct\nA: SNS topic would be trigger consistenly by the existing evenbridge, so this is incorrect\nC: Guarduty is for threat detection, not this\nD: irrelevant, the question requires using ACF drif detection, not AWS config for drift detection"
      },
      {
        "date": "2024-02-07T11:42:00.000Z",
        "voteCount": 2,
        "content": "D. AWS Config"
      },
      {
        "date": "2024-02-07T09:18:00.000Z",
        "voteCount": 2,
        "content": "B is the most appropriate solution for this scenario.\n\nA is incorrect because although it involves configuring the existing EventBridge rule to target the SNS topic and using an SNS subscription filter policy, it does not involve querying the CloudFormation API for drift detection results.\n\nC is incorrect because it involves using Amazon GuardDuty, which is not specifically designed for CloudFormation drift detection.\n\nD is incorrect because although it involves using AWS Config and EventBridge to react to compliance change events, it does not directly address CloudFormation drift detection.\n\nWith CloudWatch Events (now a part of EventBridge) https://aws.amazon.com/fr/blogs/mt/implement-automatic-drift-remediation-for-aws-cloudformation-using-amazon-cloudwatch-and-aws-lambda/"
      },
      {
        "date": "2024-02-06T11:27:00.000Z",
        "voteCount": 4,
        "content": "B is wrong, you can not query the CloudFormation API"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 208,
    "url": "https://www.examtopics.com/discussions/amazon/view/133085-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has deployed a complex container-based workload on AWS. The workload uses Amazon Managed Service for Prometheus for monitoring. The workload runs in an Amazon<br>Elastic Kubernetes Service (Amazon EKS) cluster in an AWS account.<br><br>The company\u2019s DevOps team wants to receive workload alerts by using the company\u2019s Amazon Simple Notification Service (Amazon SNS) topic. The SNS topic is in the same AWS account as the EKS cluster.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Amazon Managed Service for Prometheus remote write URL to send alerts to the SNS topic",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an alerting rule that checks the availability of each of the workload\u2019s containers.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an alert manager configuration for the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the access policy of the SNS topic. Grant the aps.amazonaws.com service principal the sns:Publish permission and the sns:GetTopicAttributes permission for the SNS topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the IAM role that Amazon Managed Service for Prometheus uses. Grant the role the sns:Publish permission and the sns:GetTopicAttributes permission for the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an OpenID Connect (OIDC) provider for the EKS cluster. Create a cluster service account. Grant the account the sns:Publish permission and the sns:GetTopicAttributes permission by using an IAM role."
    ],
    "answer": "BCD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCD",
        "count": 40,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "BCF",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "ADE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CDE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "BDF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-24T06:20:00.000Z",
        "voteCount": 1,
        "content": "I think BCD is true"
      },
      {
        "date": "2024-08-20T06:41:00.000Z",
        "voteCount": 2,
        "content": "BCD for me"
      },
      {
        "date": "2024-07-30T03:16:00.000Z",
        "voteCount": 3,
        "content": "BCD\nFor D as You must give Amazon Managed Service for Prometheus permission to send messages to your Amazon SNS topic. The following policy statement will give that permission. ...\n https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html"
      },
      {
        "date": "2024-07-10T20:59:00.000Z",
        "voteCount": 3,
        "content": "Agree with BCD"
      },
      {
        "date": "2024-07-08T20:51:00.000Z",
        "voteCount": 4,
        "content": "B:(YES) Steps towards \"configuring rules and the alert manager in Amazon Managed Service for Prometheus via the AWS management console.\"\n\"define an alerting rule that causes the Alert Manager to send a notification if a certain condition (defined in expr) holds true for a specified time period (for).\"\ncat &lt;&lt; EOF &gt; rules.yaml\ngroups:\n[...]\n    rules:\n    - alert: metric:alerting_rule\n      expr: rate(adot_test_counter0[5m]) &gt; 0.014\n      for: 5m\nEOF\n\nC:(YES) Add \"SNS receiver to\" \"alert manager configuration\" using ARN of \"SNS topic\"(Q208.5)\n\nD:(YES) \"Give Amazon Managed Service for Prometheus permission to send messages to\" SNS\n\"Choose Access policy and add the following policy statement to the existing policy.\"\n[...]\n    \"Principal\": {\n        \"Service\": \"aps.amazonaws.com\"\n    },\n    \"Action\": [\n        \"sns:Publish\",\n        \"sns:GetTopicAttributes\""
      },
      {
        "date": "2024-07-08T20:52:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-config.html\nhttps://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html\nhttps://aws.amazon.com/blogs/mt/amazon-managed-service-for-prometheus-is-now-generally-available/"
      },
      {
        "date": "2024-06-29T08:52:00.000Z",
        "voteCount": 3,
        "content": "BCD is answer"
      },
      {
        "date": "2024-06-22T09:09:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/prometheus/latest/userguide/Troubleshooting-alerting-no-policy.html"
      },
      {
        "date": "2024-05-13T12:12:00.000Z",
        "voteCount": 4,
        "content": "B, C, D, you need to grant the AMP Workspace access to the SQS queue via the SQS resource policy."
      },
      {
        "date": "2024-05-21T05:05:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html Agree with BCD"
      },
      {
        "date": "2024-05-02T06:56:00.000Z",
        "voteCount": 1,
        "content": "BCE for me"
      },
      {
        "date": "2024-04-26T11:05:00.000Z",
        "voteCount": 2,
        "content": "Amazon Managed Service for Prometheus uses an IAM role to assume permissions, not a service principal. https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-IAM-permissions.html"
      },
      {
        "date": "2024-05-13T12:10:00.000Z",
        "voteCount": 1,
        "content": "This is incorrect. This is for users/tools to manage alerts, not to publish to SQS from AMP."
      },
      {
        "date": "2024-04-12T21:28:00.000Z",
        "voteCount": 4,
        "content": "ill go with bcd"
      },
      {
        "date": "2024-03-30T04:42:00.000Z",
        "voteCount": 4,
        "content": "B,C,D.\nThere is no way to exclude D, as it is really necessary as per all AWS documentations.\nYou can be in doubt of all the others, but not D"
      },
      {
        "date": "2024-03-23T23:03:00.000Z",
        "voteCount": 4,
        "content": "I'll go with BC &amp; E. Im convinced that the Prometheus service role will need permissions added to push messages to SNS topic"
      },
      {
        "date": "2024-03-19T11:25:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alert-manager.html"
      },
      {
        "date": "2024-03-17T11:28:00.000Z",
        "voteCount": 2,
        "content": "B: https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-APIReference-CreateAlertManagerAlerts.html\nC: https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html\nD: https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html"
      },
      {
        "date": "2024-03-16T02:00:00.000Z",
        "voteCount": 3,
        "content": "BCD Makes the most sense from the docs \nhttps://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html"
      },
      {
        "date": "2024-03-14T02:42:00.000Z",
        "voteCount": 1,
        "content": "BCD\nAccording to this reference D is for sure : https://docs.aws.amazon.com/prometheus/latest/userguide/AMP-alertmanager-receiver-AMPpermission.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 209,
    "url": "https://www.examtopics.com/discussions/amazon/view/133293-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's organization in AWS Organizations has a single OU. The company runs Amazon EC2 instances in the OU accounts. The company needs to limit the use of each EC2 instance\u2019s credentials to the specific EC2 instance that the credential is assigned to. A DevOps engineer must configure security for the EC2 instances.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that specifies the VPC CIDR block. Configure the SCP to check whether the value of the aws:VpcSourcelp condition key is in the specified block. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivatelPv4 and aws:SourceVpc condition keys are the same. Deny access if either condition is false. Apply the SCP to the OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that checks whether the values of the aws:EC2InstanceSourceVPC and aws:SourceVpc condition keys are the same. Deny access if the values are not the same. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4 and aws:VpcSourceIp condition keys are the same. Deny access if the values are not the same. Apply the SCP to the OU.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that includes a list of acceptable VPC values and checks whether the value of the aws:SourceVpc condition key is in the list. In the same SCP check, define a list of acceptable IP address values and check whether the value of the aws:VpcSourceIp condition key is in the list. Deny access if either condition is false. Apply the SCP to each account in the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that checks whether the values of the aws:EC2InstanceSourceVPC and aws:VpcSourceIp condition keys are the same. Deny access if the values are not the same. In the same SCP check, check whether the values of the aws:EC2InstanceSourcePrivateIPv4 and aws:SourceVpc condition keys are the same. Deny access if the values are not the same. Apply the SCP to each account in the organization."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-01T08:06:00.000Z",
        "voteCount": 1,
        "content": "NOT C,D:\n\"Apply the SCP to each account in the organization\" - SCPs apply to OUs, not accounts"
      },
      {
        "date": "2024-04-13T13:33:00.000Z",
        "voteCount": 4,
        "content": "B obviously : https://aws.amazon.com/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/"
      },
      {
        "date": "2024-03-03T02:55:00.000Z",
        "voteCount": 3,
        "content": "B is the most appropriate solution:\nOption A introduces unnecessary complexity with multiple conditions and may not provide the intended restriction.\nOption C suggests creating an SCP with lists of acceptable values, but it might be challenging to maintain and is less straightforward.\nOption D has the same issues as option A, introducing complexity with multiple conditions."
      },
      {
        "date": "2024-02-26T15:57:00.000Z",
        "voteCount": 3,
        "content": "Answer: B - aws:EC2InstanceSourceVPC = aws:SourceVpc and aws:EC2InstanceSourcePrivateIPv4 = aws:VpcSourceIp\nhttps://aws.amazon.com/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/"
      },
      {
        "date": "2024-02-12T08:57:00.000Z",
        "voteCount": 3,
        "content": "B is correct:  aws:EC2InstanceSourceVPC and aws:SourceVpc must be the same. Additionally,  aws:EC2InstanceSourcePrivateIPv4 and aws:VpcSourceIp must be the same\nA: irrelevant\nC: &lt;define a list of acceptable IP address values&gt; is not correct\nD: &lt;aws:EC2InstanceSourceVPC and aws:VpcSourceIp&gt; is incorrect"
      },
      {
        "date": "2024-02-12T08:57:00.000Z",
        "voteCount": 2,
        "content": "Finally, I 've made it to the last one"
      },
      {
        "date": "2024-02-10T21:40:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/fr/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/"
      },
      {
        "date": "2024-02-07T11:49:00.000Z",
        "voteCount": 1,
        "content": "B. checks whether the values of the aws:EC2InstanceSourceVPC and aws:SourceVpc condition keys are the same and Apply the SCP to the OU."
      },
      {
        "date": "2024-02-07T09:39:00.000Z",
        "voteCount": 1,
        "content": "Source: https://aws.amazon.com/fr/blogs/security/how-to-use-policies-to-restrict-where-ec2-instance-credentials-can-be-used-from/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 210,
    "url": "https://www.examtopics.com/discussions/amazon/view/135847-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has a fleet of Amazon EC2 instances that run Linux in a single AWS account. The company is using an AWS Systems Manager Automation task across the EC2 instances.<br><br>During the most recent patch cycle, several EC2 instances went into an error state because of insufficient available disk space. A DevOps engineer needs to ensure that the EC2 instances have sufficient available disk space during the patching process in the future.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the Amazon CloudWatch agent is installed on all EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cron job that is installed on each EC2 instance to periodically delete temporary files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch log group for the EC2 instances. Configure a cron job that is installed on each EC2 instance to write the available disk space to a CloudWatch log stream for the relevant EC2 instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch alarm to monitor available disk space on all EC2 instances. Add the alarm as a safety control to the Systems Manager Automation task.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to periodically check for sufficient available disk space on all EC2 instances by evaluating each EC2 instance's respective Amazon CloudWatch log stream."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-10T21:20:00.000Z",
        "voteCount": 2,
        "content": "A. Ensure that the Amazon CloudWatch agent is installed on all EC2 instances.\n\nThe Amazon CloudWatch agent can collect system-level metrics, including disk space usage, and send them to Amazon CloudWatch. This will allow you to monitor the available disk space on each EC2 instance.\n\nD. Create an Amazon CloudWatch alarm to monitor available disk space on all EC2 instances. Add the alarm as a safety control to the Systems Manager Automation task.\n\nBy setting up a CloudWatch alarm to monitor the available disk space, you can trigger actions or notifications when the disk space falls below a certain threshold. Adding this alarm as a safety control to the Systems Manager Automation task ensures that the patching process will only proceed if there is sufficient available disk space."
      },
      {
        "date": "2024-04-12T22:03:00.000Z",
        "voteCount": 2,
        "content": "answer A &amp; D\nto configure disk usage, we can use custom metrics in the Cloudwatch agent configuration. don't need a cron job to pipe the disk usage."
      },
      {
        "date": "2024-03-30T04:29:00.000Z",
        "voteCount": 3,
        "content": "A,D, Simple and accurate"
      },
      {
        "date": "2024-03-13T08:26:00.000Z",
        "voteCount": 3,
        "content": "This article details the solution: \nhttps://aws.amazon.com/blogs/mt/avoid-patching-failures-due-to-low-disk-space-with-aws-systems-manager-automation-and-cloudwatch-alarms/"
      },
      {
        "date": "2024-03-12T22:21:00.000Z",
        "voteCount": 3,
        "content": "A: Install AWS CloudWatch agent which will push disk information to a log group.\nB: Alarm depends on disk space."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 211,
    "url": "https://www.examtopics.com/discussions/amazon/view/135848-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is building an application that uses an AWS Lambda function to query an Amazon Aurora MySQL DB cluster. The Lambda function performs only read queries. Amazon EventBridge events invoke the Lambda function.<br><br>As more events invoke the Lambda function each second, the database's latency increases and the database's throughput decreases. The DevOps engineer needs to improve the performance of the application.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS Proxy to create a proxy. Connect the proxy to the Aurora cluster reader endpoint. Set a maximum connections percentage on the proxy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement database connection pooling inside the Lambda code. Set a maximum number of connections on the database connection pool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the database connection opening outside the Lambda event handler code.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the database connection opening and closing inside the Lambda event handler code.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the proxy endpoint from the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the Aurora cluster endpoint from the Lambda function."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "ADE",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-12T22:15:00.000Z",
        "voteCount": 5,
        "content": "Opening and closing database connections outside the Lambda handler allows for efficient reuse of connections and implements connection pooling"
      },
      {
        "date": "2024-03-30T04:24:00.000Z",
        "voteCount": 5,
        "content": "A, D, E\nFor those going with A,C,E, the same link already provided here https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/\nin the nodejs/javascript code you can see that database connection opening (connection.connect) and closing (connection.end) are being handled INSIDE the handler function, which is correct because you want to open connections but it is a good practice to close connections"
      },
      {
        "date": "2024-08-16T01:18:00.000Z",
        "voteCount": 2,
        "content": "\"By opening the database connection outside the event handler, the connection can be reused across multiple invocations, which reduces the overhead of establishing new connections repeatedly.\""
      },
      {
        "date": "2024-07-30T05:39:00.000Z",
        "voteCount": 2,
        "content": "Should be ACE\nOpening database connections OUTSIDE the Lambda handler allows for efficient reuse of connections and implements connection pooling"
      },
      {
        "date": "2024-04-26T11:55:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/"
      },
      {
        "date": "2024-03-17T11:52:00.000Z",
        "voteCount": 4,
        "content": "ACE: I also agree - https://aws.amazon.com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/"
      },
      {
        "date": "2024-03-16T01:51:00.000Z",
        "voteCount": 4,
        "content": "I agree"
      },
      {
        "date": "2024-03-12T22:22:00.000Z",
        "voteCount": 4,
        "content": "perfect"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 212,
    "url": "https://www.examtopics.com/discussions/amazon/view/137360-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an AWS CloudFormation stack that is deployed in a single AWS account. The company has configured the stack to send event notifications to an Amazon Simple Notification Service (Amazon SNS) topic.<br><br>A DevOps engineer must implement an automated solution that applies a tag to the specific CloudFormation stack instance only after a successful stack update occurs. The DevOps engineer has created an AWS Lambda function that applies and updates this tag for the specific stack instance.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the AWS-UpdateCloudFormationStack AWS Systems ManagerAutomation runbook when Systems Manager detects an UPDATE_COMPLETE event for the instance status of the CloudFormation stack. Configure the runbook to invoke the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom AWS Config rule that produces a compliance change event if the CloudFormation stack has an UPDATE_COMPLETE instance status. Configure AWS Config to directly invoke the Lambda function to automatically remediate the change event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that matches the UPDATE_COMPLETE event pattern for the instance status of the CloudFormation stack. Configure the rule to invoke the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdjust the configuration of the CloudFormation stack to send notifications for only an UPDATE_COMPLETE instance status event to the SNS topic. Subscribe the Lambda function to the SNS topic."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-16T01:25:00.000Z",
        "voteCount": 2,
        "content": "EventBridge is designed to detect specific events in AWS services, and it can be configured to match events such as UPDATE_COMPLETE from CloudFormation.\nThis allows you to automate the process of tagging the CloudFormation stack instancess whenever the UPDATE_COMPLETE event occurs.\nThe EventBridge rule will trigger the Lambda function, which will then apply the necessary tag to the stack."
      },
      {
        "date": "2024-04-12T23:19:00.000Z",
        "voteCount": 2,
        "content": "options C and D are suitable for implementing the automated solution. However, using Option C with Amazon EventBridge is more direct and does not require additional SNS configuration"
      },
      {
        "date": "2024-03-30T04:10:00.000Z",
        "voteCount": 2,
        "content": "C,\nEventBridge + Lambda Function\nhttps://docs.aws.amazon.com/eventbridge/latest/userguide/eb-run-lambda-schedule.html"
      },
      {
        "date": "2024-03-27T09:36:00.000Z",
        "voteCount": 2,
        "content": "Its C, 100%"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 213,
    "url": "https://www.examtopics.com/discussions/amazon/view/136239-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company deploys an application to two AWS Regions. The application creates and stores objects in an Amazon S3 bucket that is in the same Region as the application. Both deployments of the application need to have access to all the objects and their metadata from both Regions. The company has configured two-way replication between the S3 buckets and has enabled S3 Replication metrics on each S3 bucket.<br><br>A DevOps engineer needs to implement a solution that retries the replication process if an object fails to replicate.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that listens to S3 event notifications for failed replication events. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket. Configure the EventBridge rule to invoke the Lambda function to handle the object that failed to replicate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Simple Queue Service (Amazon SQS) queue. Configure S3 event notifications to send failed replication notifications to the SQS queue. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket. Configure the Lambda function to poll the queue for notifications to process.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that listens to S3 event notifications for failed replications. Create an AWS Lambda function that downloads the failed replication object and then runs a PutObject command for the object to the destination bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that will use S3 batch operations to retry the replication on the existing object for a failed replication. Configure S3 event notifications to send failed replication notifications to the Lambda function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-20T11:02:00.000Z",
        "voteCount": 5,
        "content": "S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication.\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html"
      },
      {
        "date": "2024-08-05T07:03:00.000Z",
        "voteCount": 2,
        "content": "By using S3 Batch Replication, you can replicate the following types of objects:\n\n    Objects that existed before a replication configuration was in place\n\n    Objects that have previously been replicated\n\n    Objects that have failed replication"
      },
      {
        "date": "2024-06-22T11:35:00.000Z",
        "voteCount": 1,
        "content": "100% D \nS3 Batch Replication is one of actions provided by S3 batch operations https://docs.aws.amazon.com/AmazonS3/latest/userguide/batch-ops-operations.html\n\nbatch operations can be used for objects that failed replication\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html#:~:text=Objects%20that%20have%20failed%20replication"
      },
      {
        "date": "2024-04-26T12:12:00.000Z",
        "voteCount": 2,
        "content": "S3 Batch Replication provides you a way to replicate objects that existed before a replication configuration was in place, objects that have previously been replicated, and objects that have failed replication. This is done through the use of a Batch Operations job. https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html"
      },
      {
        "date": "2024-03-30T04:02:00.000Z",
        "voteCount": 2,
        "content": "D,\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html"
      },
      {
        "date": "2024-03-17T12:12:00.000Z",
        "voteCount": 4,
        "content": "This post suggests D: https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-batch-replication-batch.html"
      },
      {
        "date": "2024-03-16T11:54:00.000Z",
        "voteCount": 3,
        "content": "It's A for me."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 214,
    "url": "https://www.examtopics.com/discussions/amazon/view/136241-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company needs to implement failover for its application. The application includes an Amazon CloudFront distribution and a public Application Load Balancer (ALB) in an AWS Region. The company has configured the ALB as the default origin for the distribution.<br><br>After some recent application outages, the company wants a zero-second RTO. The company deploys the application to a secondary Region in a warm standby configuration. A DevOps engineer needs to automate the failover of the application to the secondary Region so that HTTP GET requests meet the desired RTO.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a second CloudFront distribution that has the secondary ALB as the default origin. Create Amazon Route 53 alias records that have a failover policy and Evaluate Target Health set to Yes for both CloudFront distributions. Update the application to use the new record set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new origin on the distribution for the secondary ALCreate a new origin group. Set the original ALB as the primary origin. Configure the origin group to fail over for HTTP 5xx status codes. Update the default behavior to use the origin group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon Route 53 alias records that have a failover policy and Evaluate Target Health set to Yes for both ALBs. Set the TTL of both records to 0. Update the distribution's origin to use the new record set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFront function that detects HTTP 5xx status codes. Configure the function to return a 307 Temporary Redirect error response to the secondary ALB if the function detects 5xx status codes. Update the distribution's default behavior to send origin responses to the function."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-13T00:11:00.000Z",
        "voteCount": 2,
        "content": "answer B"
      },
      {
        "date": "2024-03-30T04:00:00.000Z",
        "voteCount": 3,
        "content": "B,\nmazon CloudFront offers origin failover, where if a given request to the primary endpoint fails, CloudFront routes the request to the secondary endpoint. Unlike the failover operations described previously, all subsequent requests still go to the primary endpoint, and failover is done per each request.\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html#concept_origin_groups.creating"
      },
      {
        "date": "2024-03-16T12:20:00.000Z",
        "voteCount": 3,
        "content": "It's B for me.\n\nhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 215,
    "url": "https://www.examtopics.com/discussions/amazon/view/136243-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A cloud team uses AWS Organizations and AWS IAM Identity Center (AWS Single Sign-On) to manage a company's AWS accounts. The company recently established a research team. The research team requires the ability to fully manage the resources in its account. The research team must not be able to create IAM users.<br><br>The cloud team creates a Research Administrator permission set in IAM Identity Center for the research team. The permission set has the AdministratorAccess AWS managed policy attached. The cloud team must ensure that no one on the research team can create IAM users.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy that denies the iam:CreateUser action. Attach the IAM policy to the Research Administrator permission set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy that allows all actions except the iam:CreateUser action. Use the IAM policy to set the permissions boundary for the Research Administrator permission set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that denies the iam:CreateUser action. Attach the SCP to the research team's AWS account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that deletes IAM users. Create an Amazon EventBridge rule that detects the IAM CreateUser event. Configure the rule to invoke the Lambda function."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-16T12:38:00.000Z",
        "voteCount": 7,
        "content": "It's C for me, here is a link with a similar scenario:\n\nhttps://asecure.cloud/a/scp_deny_iam_user_creation_w_exception/"
      },
      {
        "date": "2024-04-13T00:47:00.000Z",
        "voteCount": 5,
        "content": "While IAM policies can deny actions, they are typically attached to individual users or roles. In this scenario, you want to restrict user creation across the entire research team's account, making an SCP the more appropriate choice."
      },
      {
        "date": "2024-08-20T06:44:00.000Z",
        "voteCount": 2,
        "content": "c for me"
      },
      {
        "date": "2024-08-16T01:49:00.000Z",
        "voteCount": 1,
        "content": "For those who selected C, why would you create ab SCP that will deny any IAM user from creating another IAM when the question clearly states only the research team shouldn't be able to create an IAM user? the deny policy will restrict only the Research Administrator permission set, which is what we want."
      },
      {
        "date": "2024-07-30T06:03:00.000Z",
        "voteCount": 3,
        "content": "i go for C\njust make sure no one can create account\nscp also can create with exception as mentioned by @CloudHell"
      },
      {
        "date": "2024-07-18T08:11:00.000Z",
        "voteCount": 4,
        "content": "I'll go for A as the question says:\n\"The cloud team must ensure that no one on the research team can create IAM users.\"\n\nC will block everybody (not just the research team)"
      },
      {
        "date": "2024-07-18T08:17:00.000Z",
        "voteCount": 1,
        "content": "even thoguh  xdkonorek2 has a valid point.\njust flip a coin if you get this question in the exam"
      },
      {
        "date": "2024-07-05T03:59:00.000Z",
        "voteCount": 4,
        "content": "C,\nA is not enough due research team still could create iam role with that allows him to create iam user and e.g. invoke lambda that does it for him\nobviously unwanted implication is that no one in this account can create IAM users even admins, but still it fulfills the requirements"
      },
      {
        "date": "2024-05-17T06:25:00.000Z",
        "voteCount": 3,
        "content": "A, only the research team shouldn't be able to create IAM users."
      },
      {
        "date": "2024-05-02T07:00:00.000Z",
        "voteCount": 3,
        "content": "C for me"
      },
      {
        "date": "2024-04-26T12:31:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html"
      },
      {
        "date": "2024-04-03T19:42:00.000Z",
        "voteCount": 5,
        "content": "C is the answer.   IAM policy is not as scalable or centralized as using an SCP.   \nYou can attach an SCP to the organization root, to an organizational unit (OU), or directly to an account\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html"
      },
      {
        "date": "2024-03-30T03:56:00.000Z",
        "voteCount": 1,
        "content": "A is the correct option since you can not apply SCP directly to an AWS Account (need to be OU)"
      },
      {
        "date": "2024-04-11T02:05:00.000Z",
        "voteCount": 4,
        "content": "You can attach an SCP to the organization root, to an organizational unit (OU), or directly to an account.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html"
      },
      {
        "date": "2024-03-28T01:08:00.000Z",
        "voteCount": 3,
        "content": "SCP can be applied to an OU. Therefore, the answer is A."
      },
      {
        "date": "2024-04-11T02:05:00.000Z",
        "voteCount": 3,
        "content": "You can attach an SCP to the organization root, to an organizational unit (OU), or directly to an account.\nhttps://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_attach.html"
      },
      {
        "date": "2024-03-27T09:52:00.000Z",
        "voteCount": 1,
        "content": "Its A, when you attach the SCP no one will be able to create new user not just the team"
      },
      {
        "date": "2024-04-26T12:31:00.000Z",
        "voteCount": 1,
        "content": "isn't that what is required?"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 216,
    "url": "https://www.examtopics.com/discussions/amazon/view/137361-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company releases a new application in a new AWS account. The application includes an AWS Lambda function that processes messages from an Amazon Simple Queue Service (Amazon SQS) standard queue. The Lambda function stores the results in an Amazon S3 bucket for further downstream processing. The Lambda function needs to process the messages within a specific period of time after the messages are published. The Lambda function has a batch size of 10 messages and takes a few seconds to process a batch of messages.<br><br>As load increases on the application's first day of service, messages in the queue accumulate at a greater rate than the Lambda function can process the messages. Some messages miss the required processing timelines. The logs show that many messages in the queue have data that is not valid. The company needs to meet the timeline requirements for messages that have valid data.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the Lambda function's batch size. Change the SQS standard queue to an SQS FIFO queue. Request a Lambda concurrency increase in the AWS Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the Lambda function's batch size. Increase the SQS message throughput quota. Request a Lambda concurrency increase in the AWS Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the Lambda function's batch size. Configure S3 Transfer Acceleration on the S3 bucket. Configure an SQS dead-letter queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the Lambda function's batch size the same. Configure the Lambda function to report failed batch items. Configure an SQS dead-letter queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-02T07:00:00.000Z",
        "voteCount": 1,
        "content": "D for me"
      },
      {
        "date": "2024-04-26T12:38:00.000Z",
        "voteCount": 3,
        "content": "Configure a dead-letter queue to avoid creating a snowball anti-pattern in your serverless application\u2019s architecture. For more information, see the Avoiding snowball anti-patterns section of this guide.\n\nConfigure your Lambda function event source mapping to make only the failed messages visible. To do this, you must include the value ReportBatchItemFailures in the FunctionResponseTypes list when configuring your event source mapping. https://docs.aws.amazon.com/prescriptive-guidance/latest/lambda-event-filtering-partial-batch-responses-for-sqs/best-practices-partial-batch-responses.html"
      },
      {
        "date": "2024-04-13T01:06:00.000Z",
        "voteCount": 2,
        "content": "answer D seems more approprite"
      },
      {
        "date": "2024-04-06T02:26:00.000Z",
        "voteCount": 1,
        "content": "I am torn between option A or D"
      },
      {
        "date": "2024-03-30T03:50:00.000Z",
        "voteCount": 3,
        "content": "D,\nhttps://docs.aws.amazon.com/prescriptive-guidance/latest/lambda-event-filtering-partial-batch-responses-for-sqs/best-practices-partial-batch-responses.html"
      },
      {
        "date": "2024-03-27T09:58:00.000Z",
        "voteCount": 2,
        "content": "its D, 100%"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 217,
    "url": "https://www.examtopics.com/discussions/amazon/view/137340-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an application that runs on AWS Lambda and sends logs to Amazon CloudWatch Logs. An Amazon Kinesis data stream is subscribed to the log groups in CloudWatch Logs. A single consumer Lambda function processes the logs from the data stream and stores the logs in an Amazon S3 bucket.<br><br>The company\u2019s DevOps team has noticed high latency during the processing and ingestion of some logs.<br><br>Which combination of steps will reduce the latency? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a data stream consumer with enhanced fan-out. Set the Lambda function that processes the logs as the consumer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the ParallelizationFactor setting in the Lambda event source mapping.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure reserved concurrency for the Lambda function that processes the logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the batch size in the Kinesis data stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off the ReportBatchItemFailures setting in the Lambda event source mapping.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards in the Kinesis data stream.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ABF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABF",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-17T01:29:00.000Z",
        "voteCount": 1,
        "content": "A: Kinesis Enhanced fan-out is an Amazon Kinesis Data Streams feature that enables consumers to receive records from a data stream with dedicated throughput of up to 2 MB of data per second per shard. A consumer that uses enhanced fan-out doesn't have to contend with other consumers that are receiving data from the stream.\n\nB: Reserved concurrency \u2013 This represents the maximum number of concurrent instances allocated to your function. When a function has reserved concurrency, no other function can use that concurrency. Reserved concurrency is useful for ensuring that your most critical functions always have enough concurrency to handle incoming requests.\n\nF: The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards."
      },
      {
        "date": "2024-05-02T07:01:00.000Z",
        "voteCount": 1,
        "content": "ABF for me"
      },
      {
        "date": "2024-04-26T12:48:00.000Z",
        "voteCount": 3,
        "content": "https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"
      },
      {
        "date": "2024-04-06T03:08:00.000Z",
        "voteCount": 1,
        "content": "ABF or ACF"
      },
      {
        "date": "2024-03-30T03:45:00.000Z",
        "voteCount": 2,
        "content": "A,B,F,\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html"
      },
      {
        "date": "2024-03-27T01:38:00.000Z",
        "voteCount": 3,
        "content": "https://aws.amazon.com/about-aws/whats-new/2019/11/aws-lambda-supports-parallelization-factor-for-kinesis-and-dynamodb-event-sources/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 218,
    "url": "https://www.examtopics.com/discussions/amazon/view/137362-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company operates sensitive workloads across the AWS accounts that are in the company's organization in AWS Organizations. The company uses an IP address range to delegate IP addresses for Amazon VPC CIDR blocks and all non-cloud hardware.<br><br>The company needs a solution that prevents principals that are outside the company\u2019s IP address range from performing AWS actions in the organization's accounts.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Firewall Manager for the organization. Create an AWS Network Firewall policy that allows only source traffic from the company's IP address range. Set the policy scope to all accounts in the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Organizations, create an SCP that denies source IP addresses that are outside of the company\u2019s IP address range. Attach the SCP to the organization's root.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon GuardDuty for the organization. Create a GuardDuty trusted IP address list for the company's IP range. Activate the trusted IP list for the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Organizations, create an SCP that allows source IP addresses that are inside of the company\u2019s IP address range. Attach the SCP to the organization's root."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-02T07:01:00.000Z",
        "voteCount": 1,
        "content": "B 100%"
      },
      {
        "date": "2024-04-26T12:54:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-ip.html"
      },
      {
        "date": "2024-04-13T01:29:00.000Z",
        "voteCount": 2,
        "content": "answer b\nuses an SCP within AWS Organizations to deny source IP addresses that are outside of the company\u2019s IP address range, providing a centralized and organization-wide control over AWS actions based on source IP addresses for all accounts and resources within the organization."
      },
      {
        "date": "2024-03-30T03:39:00.000Z",
        "voteCount": 3,
        "content": "B\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny-ip.html"
      },
      {
        "date": "2024-03-27T10:09:00.000Z",
        "voteCount": 1,
        "content": "its B, 100%"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 219,
    "url": "https://www.examtopics.com/discussions/amazon/view/137363-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company deploys an application in two AWS Regions. The application currently uses an Amazon S3 bucket in the primary Region to store data.<br><br>A DevOps engineer needs to ensure that the application is highly available in both Regions. The DevOps engineer has created a new S3 bucket in the secondary Region. All existing and new objects must be in both S3 buckets. The application must fail over between the Regions with no data loss.<br><br>Which combination of steps will meet these requirements with the MOST operational efficiency? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new IAM role that allows the Amazon S3 and S3 Batch Operations service principals to assume the role that has the necessary permissions for S3 replication.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new IAM role that allows the AWS Batch service principal to assume the role that has the necessary permissions for S3 replication.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 Cross-Region Replication (CRR) rule on the source S3 bucket. Configure the rule to use the IAM role for Amazon S3 to replicate to the target S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a two-way replication rule on the source S3 bucket. Configure the rule to use the IAM role for Amazon S3 to replicate to the target S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Batch job that has an AWS Fargate orchestration type. Configure the job to use the IAM role for AWS Batch. Specify a Bash command to use the AWS CLI to synchronize the contents of the source S3 bucket and the target S3 bucket",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an operation in S3 Batch Operations to replicate the contents of the source S3 bucket to the target S3 bucket. Configure the operation to use the IAM role for Amazon S3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "ACF",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-12T00:26:00.000Z",
        "voteCount": 1,
        "content": "Not D because it states creating the two-way replication on the source bucket and you need to configure it on both to work:\n\nWhen two-way replication is set up, a replication rule from the source bucket (DOC-EXAMPLE-BUCKET-1) to the bucket containing the replicas (DOC-EXAMPLE-BUCKET-2) is created. Then, a second replication rule from the bucket containing the replicas (DOC-EXAMPLE-BUCKET-2) to the source bucket (DOC-EXAMPLE-BUCKET-1) is created."
      },
      {
        "date": "2024-08-20T06:50:00.000Z",
        "voteCount": 1,
        "content": "ADF here, because there is no mention of two way replication in C"
      },
      {
        "date": "2024-08-16T15:47:00.000Z",
        "voteCount": 2,
        "content": "I think it's ADF check out this:\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/mrap-create-two-way-replication-rules.html"
      },
      {
        "date": "2024-08-13T00:00:00.000Z",
        "voteCount": 2,
        "content": "Two-way replication is possible using CRR.\n\n\"Replication is configured via rules. There is no rule for bi-directional replication. You will however setup a rule to replicate from the S3 bucket in the east AWS region to the west bucket, and you will setup a second rule to replicate going the opposite direction. These two rules will enable bi-directional replication across AWS regions.\"\n\n- https://catalog.workshops.aws/well-architected-reliability/en-US/4-failure-management/1-backup/20-bidirectional-replication-for-s3/2-configure-replication"
      },
      {
        "date": "2024-07-31T23:12:00.000Z",
        "voteCount": 1,
        "content": "there is nothing called  (Create a two-way replication rule on the source S3 bucket), the two-way replication is configured separately in each region per each bucket, that's why option D is incorrect."
      },
      {
        "date": "2024-07-30T06:18:00.000Z",
        "voteCount": 2,
        "content": "Option D: two-way replication required"
      },
      {
        "date": "2024-07-26T08:21:00.000Z",
        "voteCount": 3,
        "content": "D - for failover between regions. Any data stored on secondary bucket post failover operations needs to be replicated as well"
      },
      {
        "date": "2024-07-21T22:55:00.000Z",
        "voteCount": 4,
        "content": "Note: Application deployed to both regions, bi-directional replication will be required"
      },
      {
        "date": "2024-07-05T03:55:00.000Z",
        "voteCount": 3,
        "content": "\"The application must fail over between the Regions with no data loss.\"\nC is not enough, because if we failover to region B and then to A application couldn't access data that was created in region B in the meantime"
      },
      {
        "date": "2024-07-01T23:06:00.000Z",
        "voteCount": 2,
        "content": "Two-Way Replication Rule is bidirectional, meaning objects are replicated from bucket A to bucket B and from bucket B to bucket A. This ensures that both buckets always contain the same data.\nS3 Cross-Region Replication (CRR) is unidirectional, meaning it replicates objects from a source bucket to a destination bucket. Changes made in the destination bucket do not propagate back to the source bucket.\nSo D, not C"
      },
      {
        "date": "2024-05-17T06:34:00.000Z",
        "voteCount": 4,
        "content": "ADF, \"All existing and new objects must be in BOTH S3 buckets.\" this requires two-way replication."
      },
      {
        "date": "2024-05-02T07:02:00.000Z",
        "voteCount": 1,
        "content": "ACF for me"
      },
      {
        "date": "2024-04-30T19:02:00.000Z",
        "voteCount": 3,
        "content": "ADF\nThe secondary also needs to replicate to the primary."
      },
      {
        "date": "2024-05-13T02:18:00.000Z",
        "voteCount": 2,
        "content": "I agree. Does anyone have any reason why it wouldn't be B?"
      },
      {
        "date": "2024-04-13T01:38:00.000Z",
        "voteCount": 2,
        "content": "answer acf"
      },
      {
        "date": "2024-04-07T04:38:00.000Z",
        "voteCount": 2,
        "content": "ACF for sure. A - we need a replication role with principles for S3 Batch Operation, replicate job, and S3. C - will replicate all new objects, F - will replicate existing objects"
      },
      {
        "date": "2024-03-27T10:14:00.000Z",
        "voteCount": 2,
        "content": "its ACF for me"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 220,
    "url": "https://www.examtopics.com/discussions/amazon/view/137364-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company needs an automated process across all AWS accounts to isolate any compromised Amazon EC2 instances when the instances receive a specific tag.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation StackSets to deploy the CloudFormation stacks in all AWS accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP that has a Deny statement for the ec2:* action with a condition of \"aws:RequestTag/isolation\": false.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach the SCP to the root of the organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that creates an EC2 instance role that has no IAM policies attached. Configure the template to have a security group that has an explicit Deny rule on all traffic. Use the CloudFormation template to create an AWS Lambda function that attaches the IAM role to instances. Configure the Lambda function to add a network ACL. Set up an Amazon EventBridge rule to invoke the Lambda function when a specific tag is applied to a compromised EC2 instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudFormation template that creates an EC2 instance role that has no IAM policies attached. Configure the template to have a security group that has no inbound rules or outbound rules. Use the CloudFormation template to create an AWS Lambda function that attaches the IAM role to instances. Configure the Lambda function to replace any existing security groups with the new security group. Set up an Amazon EventBridge rule to invoke the Lambda function when a specific tag is applied to a compromised EC2 instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-09T10:17:00.000Z",
        "voteCount": 1,
        "content": "The B + C means no actions allowed on the tagged EC2 for all accounts in Organizations, but the asking was the needs of the isolation(implying the network isolation) on the tagged EC2; hence, A + E is a good option here.."
      },
      {
        "date": "2024-08-20T10:58:00.000Z",
        "voteCount": 1,
        "content": "vote for AE"
      },
      {
        "date": "2024-07-30T06:24:00.000Z",
        "voteCount": 2,
        "content": "I go for AE\nisolating the instance should be mean block traffic"
      },
      {
        "date": "2024-07-11T04:47:00.000Z",
        "voteCount": 3,
        "content": "This CloudFormation template creates the necessary resources:\n\nAn EC2 instance role with no IAM policies, ensuring the instance cannot perform any actions.\nA security group with no inbound or outbound rules, effectively isolating the instance from all network traffic.\nA Lambda function that will be triggered by an EventBridge rule when a specific tag is applied to an EC2 instance. This function will attach the isolated security group to the compromised instance, ensuring it is isolated from any network communication.\nCombining these steps will provide an automated and consistent approach to isolate compromised EC2 instances across all AWS accounts in the organization."
      },
      {
        "date": "2024-06-22T13:14:00.000Z",
        "voteCount": 2,
        "content": "BD is wrong \nisolating the instance doesn't mean \"don't touch it\" with aws actions but to block traffic from and to it"
      },
      {
        "date": "2024-05-13T02:23:00.000Z",
        "voteCount": 4,
        "content": "What a weirdly worded question. I tend to agree with A &amp; E. We need to isolate an EC2 that has a certain tag."
      },
      {
        "date": "2024-05-02T07:06:00.000Z",
        "voteCount": 1,
        "content": "BC for me"
      },
      {
        "date": "2024-05-25T02:55:00.000Z",
        "voteCount": 3,
        "content": "so funny, how to isolate incoming traffic. B,C means deny action with EC2"
      },
      {
        "date": "2024-05-25T02:55:00.000Z",
        "voteCount": 2,
        "content": "Answer is A, E"
      },
      {
        "date": "2024-05-02T07:06:00.000Z",
        "voteCount": 1,
        "content": "BC for me"
      },
      {
        "date": "2024-04-13T01:53:00.000Z",
        "voteCount": 3,
        "content": "ill go with AE"
      },
      {
        "date": "2024-04-11T23:13:00.000Z",
        "voteCount": 1,
        "content": "CE for me.\nOption D is wrong because we can not use Security Group for an explicit deny rule. \nOption B is quite misleading with the resourceTagIsolation set to False instead of True."
      },
      {
        "date": "2024-04-07T04:44:00.000Z",
        "voteCount": 3,
        "content": "in my opinion it could not be AE because we would need a mechanism to apply this template to the right EC2 - I would vote for BC"
      },
      {
        "date": "2024-08-29T00:52:00.000Z",
        "voteCount": 1,
        "content": "BC does not automate the isolation of instance. What it does is preventive measure that stops EC2 from perform action, but ultimately, it is still connected. \n\nYou will need security groups to cut off access to and from the compromised EC2 instance.\n\nTo have a complete solution, AE automates isolation based on tagging and deploy them to all EC2 instance, BC prevents any action by the EC2 using SCP. For this specific question, it is asking for the automation to isolate the EC2 instance so AE is the correct choice."
      },
      {
        "date": "2024-03-27T10:17:00.000Z",
        "voteCount": 3,
        "content": "A,E for me"
      },
      {
        "date": "2024-05-09T09:55:00.000Z",
        "voteCount": 3,
        "content": "AE\nThe question says isolate. What does isolate mean? Prevent outgoing and incoming traffic."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 221,
    "url": "https://www.examtopics.com/discussions/amazon/view/137342-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company manages multiple AWS accounts by using AWS Organizations with OUs for the different business divisions. The company is updating their corporate network to use new IP address ranges. The company has 10 Amazon S3 buckets in different AWS accounts. The S3 buckets store reports for the different divisions. The S3 bucket configurations allow only private corporate network IP addresses to access the S3 buckets.<br><br>A DevOps engineer needs to change the range of IP addresses that have permission to access the contents of the S3 buckets. The DevOps engineer also needs to revoke the permissions of two OUs in the company.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new SCP that has two statements, one that allows access to the new range of IP addresses for all the S3 buckets and one that denies access to the old range of IP addresses for all the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new SCP that has a statement that allows only the new range of IP addresses to access the S3 buckets. Create another SCP that denies access to the S3 buckets. Attach the second SCP to the two OUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn all the S3 buckets, configure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Create a new SCP that denies access to the S3 buckets. Attach the SCP to the two OUs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn all the S3 buckets, configure resource-based policies that allow only the new range of IP addresses to access the S3 buckets. Set a permissions boundary for the OrganizationAccountAccessRole role in the two OUs to deny access to the S3 buckets."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-02T07:07:00.000Z",
        "voteCount": 2,
        "content": "C for me"
      },
      {
        "date": "2024-04-13T02:42:00.000Z",
        "voteCount": 3,
        "content": "answer c"
      },
      {
        "date": "2024-04-11T23:27:00.000Z",
        "voteCount": 1,
        "content": "C.\nUse bucket policy to allow or deny access to a range of IP addresses or VPC endpoints to an S3 resource. Restriction to OUs is best done using SCP."
      },
      {
        "date": "2024-03-27T10:26:00.000Z",
        "voteCount": 3,
        "content": "C for me"
      },
      {
        "date": "2024-03-27T02:10:00.000Z",
        "voteCount": 3,
        "content": "restrict access to S3 bucket using specific VPC endpoints or IP addresses:\nhttps://repost.aws/knowledge-center/block-s3-traffic-vpc-ip"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 222,
    "url": "https://www.examtopics.com/discussions/amazon/view/137341-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has started using AWS across several teams. Each team has multiple accounts and unique security profiles. The company manages the accounts in an organization in AWS Organizations. Each account has its own configuration and security controls.<br><br>The company's DevOps team wants to use preventive and detective controls to govern all accounts. The DevOps team needs to ensure the security of accounts now and in the future as the company creates new accounts in the organization.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Organizations to create OUs that have appropriate SCPs attached for each team. Place team accounts in the appropriate OUs to apply security controls. Create any new team accounts in the appropriate OUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Control Tower landing zone. Configure OUs and appropriate controls in AWS Control Tower for the existing teams. Configure trusted access for AWS Control Tower. Enroll the existing accounts in the appropriate OUs that match the appropriate security policies for each team. Use AWS Control Tower to provision any new accounts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate AWS CloudFormation stack sets in the organization's management account. Configure a stack set that deploys AWS Config with configuration rules and remediation actions for all controls to each account in the organization. Update the stack sets to deploy to new accounts as the accounts are created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Config to manage the AWS Config rules across all AWS accounts in the organization. Deploy conformance packs that provide AWS Config rules and remediation actions across the organization."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-25T23:27:00.000Z",
        "voteCount": 4,
        "content": "A control is a high-level rule that provides ongoing governance for your overall AWS environment. It's expressed in plain language. AWS Control Tower implements preventive, detective, and proactive controls that help you govern your resources and monitor compliance across groups of AWS accounts. https://docs.aws.amazon.com/controltower/latest/controlreference/controls.html"
      },
      {
        "date": "2024-04-13T03:04:00.000Z",
        "voteCount": 2,
        "content": "answer B"
      },
      {
        "date": "2024-04-11T23:34:00.000Z",
        "voteCount": 2,
        "content": "Option B.\n\nKeyword: Preventive and detective controls to govern all accounts. This service is provided by guardrails as part of AWS Control Tower."
      },
      {
        "date": "2024-03-30T02:49:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/controltower/latest/userguide/controls.html"
      },
      {
        "date": "2024-03-27T10:29:00.000Z",
        "voteCount": 2,
        "content": "its B for me"
      },
      {
        "date": "2024-03-27T02:04:00.000Z",
        "voteCount": 4,
        "content": "About controls in AWS Control Tower:\nhttps://docs.aws.amazon.com/controltower/latest/userguide/controls.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 223,
    "url": "https://www.examtopics.com/discussions/amazon/view/142973-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an AWS CodeCommit repository to store its source code and corresponding unit tests. The company has configured an AWS CodePipeline pipeline that includes an AWS CodeBuild project that runs when code is merged to the main branch of the repository.<br><br>The company wants the CodeBuild project to run the unit tests. If the unit tests pass, the CodeBuild project must tag the most recent commit.<br><br>How should the company configure the CodeBuild project to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the CodeBuild project to use native Git to done the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use native Git to create a tag and to push the Git tag to the repository if the code passes the unit tests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the CodeBuild projed to use native Git to done the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new repository tag in the repository if the code passes the unit tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the CodeBuild project to use AWS CLI commands to copy the code from the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new Git tag in the repository if the code passes the unit tests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the CodeBuild project to use AWS CLI commands to copy the code from the CodeCommit repository. Configure the project to run the unit tests. Configure the project to use AWS CLI commands to create a new repository tag in the repository if the code passes the unit tests."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T19:18:00.000Z",
        "voteCount": 1,
        "content": "Option A effectively meets the requirements by using standard Git operations for cloning and tagging, ensuring an efficient and clear workflow for managing the repository in AWS CodePipeline."
      },
      {
        "date": "2024-07-15T03:15:00.000Z",
        "voteCount": 1,
        "content": "---&gt; A"
      },
      {
        "date": "2024-07-14T14:02:00.000Z",
        "voteCount": 1,
        "content": "A\nhttps://docs.aws.amazon.com/codecommit/latest/userguide/getting-started.html#getting-started-create-commit"
      },
      {
        "date": "2024-07-11T07:55:00.000Z",
        "voteCount": 2,
        "content": "Using Native Git: By configuring CodeBuild to use native Git, you ensure the repository is cloned in a way that supports all Git operations, including tagging and pushing changes.\n\nRunning Unit Tests: CodeBuild can be set up to run the unit tests using the build specification file (buildspec.yml), ensuring that the tests are executed before any further actions are taken.\n\nCreating and Pushing Tags: \npost_build:\n    commands:\n      - echo Unit tests passed, tagging commit...\n      - git tag -a v1.0.0 -m \"Tagging commit after successful unit tests\"  \n      - git push origin v1.0.0"
      },
      {
        "date": "2024-06-27T11:43:00.000Z",
        "voteCount": 1,
        "content": "A is the Answer."
      },
      {
        "date": "2024-06-27T02:57:00.000Z",
        "voteCount": 1,
        "content": "A is the Answer."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 224,
    "url": "https://www.examtopics.com/discussions/amazon/view/142990-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer manages a company's Amazon Elastic Container Service (Amazon ECS) cluster. The cluster runs on several Amazon EC2 instances that are in an Auto Scaling group. The DevOps engineer must implement a solution that logs and reviews all stopped tasks for errors.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule to capture task state changes. Send the event to Amazon CloudWatch Logs. Use CloudWatch Logs Insights to investigate stopped tasks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure tasks to write log data in the embedded metric format. Store the logs in Amazon CloudWatch Logs. Monitor the ContainerInstanceCount metric for changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the EC2 instances to store logs in Amazon CloudWatch Logs. Create a CloudWatch Contributor Insights rule that uses the EC2 instance log data. Use the Contributor Insights rule to investigate stopped tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an EC2 Auto Scaling lifecycle hook for the EC2_INSTANCE_TERMINATING scale-in event. Write the SystemEventLog file to Amazon S3. Use Amazon Athena to query the log file for errors."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-02T01:30:00.000Z",
        "voteCount": 6,
        "content": "By using Amazon EventBridge to capture ECS task state changes and sending these events to CloudWatch Logs, combined with the analytical capabilities of CloudWatch Logs Insights, option A provides a comprehensive and straightforward solution for logging and investigating stopped tasks for errors."
      },
      {
        "date": "2024-07-15T03:15:00.000Z",
        "voteCount": 1,
        "content": "---&gt; A"
      },
      {
        "date": "2024-07-11T08:00:00.000Z",
        "voteCount": 2,
        "content": "Just A"
      },
      {
        "date": "2024-06-27T11:46:00.000Z",
        "voteCount": 1,
        "content": "CloudWatch Contributor Insights"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 225,
    "url": "https://www.examtopics.com/discussions/amazon/view/143725-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company wants to deploy a workload on several hundred Amazon EC2 instances. The company will provision the EC2 instances in an Auto Scaling group by using a launch template.<br><br>The workload will pull files from an Amazon S3 bucket, process the data, and put the results into a different S3 bucket. The EC2 instances must have least-privilege permissions and must use temporary security credentials.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that has the appropriate permissions for S3 buckets Add the IAM role to an instance profile.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the launch template to include the IAM instance profile.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user that has the appropriate permissions for Amazon S3 Generate a secret key and token.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trust anchor and profile Attach the IAM role to the profile.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the launch template Modify the user data to use the new secret key and token."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-22T09:49:00.000Z",
        "voteCount": 1,
        "content": "AB for me"
      },
      {
        "date": "2024-07-15T03:16:00.000Z",
        "voteCount": 2,
        "content": "---&gt; AB"
      },
      {
        "date": "2024-07-11T08:07:00.000Z",
        "voteCount": 4,
        "content": "A. This step ensures that the EC2 instances have the necessary permissions to access the S3 buckets. The IAM role should have policies attached that allow it to pull files from one S3 bucket and put results into another S3 bucket. By using an instance profile, the role can be associated with the EC2 instances.\nB. This step ensures that the EC2 instances launched by the Auto Scaling group will automatically use the instance profile (and thus the IAM role) with the appropriate permissions. \nC. This approach uses long-term credentials\nD. The term \"trust anchor\" is more relevant to AWS IAM Identity Center (formerly AWS Single Sign-On) or AWS Organizations. It is not directly applicable to setting up permissions for EC2 instances via Auto Scaling.\nE. Storing and using secret keys and tokens in user data scripts is insecure and not recommended."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 226,
    "url": "https://www.examtopics.com/discussions/amazon/view/142991-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is using AWS CodeDeploy to automate software deployment. The deployment must meet these requirements:<br><br>\u2022 A number of instances must be available to serve traffic during the deployment. Traffic must be balanced across those instances, and the instances must automatically heal in the event of failure. \u2022 A new fleet of instances must be launched for deploying a new revision automatically, with no manual provisioning.<br>\u2022 Traffic must be rerouted to the new environment to half of the new instances at a time. The deployment should succeed if traffic is rerouted to at least half of the instances: otherwise, it should fail.<br>\u2022 Before routing traffic to the new fleet of instances, the temporary files generated during the deployment process must be deleted.<br>\u2022 At the end of a successful deployment, the original instances in the deployment group must be deleted immediately to reduce costs.<br><br>How can a DevOps engineer meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.OneAtAtime as the deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the AllowTraffic hook within appspec.yml to delete the temporary files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, create a custom deployment configuration with minimum healthy hosts defined as 50%, and assign the configuration to the deployment group. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BeforeBlockTraffic hook within appspec.yml to delete the temporary files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Application Load Balancer and a blue/green deployment. Associate the Auto Scaling group and the Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.HalfAtAtime as the deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BeforeAllowTraffic hook within appspec.yml to delete the temporary files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Application Load Balancer and an in-place deployment. Associate the Auto Scaling group and Application Load Balancer target group with the deployment group. Use the Automatically copy Auto Scaling group option, and use CodeDeployDefault.AllatOnce as a deployment configuration. Instruct AWS CodeDeploy to terminate the original instances in the deployment group, and use the BlockTraffic hook within appspec.yml to delete the temporary files."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T19:40:00.000Z",
        "voteCount": 2,
        "content": "Option C is the best choice as it meets all the given requirements by utilizing a blue/green deployment strategy, correct traffic routing, proper temporary file cleanup timing, and automatic instance termination to manage costs.\n\nOption B is wrong due to: \n- Allows for custom deployment configuration, ensuring at least 50% of the instances must be healthy. However, \"reroute to half\" is somewhat ambiguous, as custom deployment with 50% only ensures half remain healthy but doesn't explicitly guarantee half reroute.\n- Uses BeforeBlockTraffic hook, which is not the correct timing for cleaning up temporary files because it should occur after instance provisioning but before allowing traffic."
      },
      {
        "date": "2024-07-15T03:17:00.000Z",
        "voteCount": 1,
        "content": "---&gt; C"
      },
      {
        "date": "2024-07-11T17:56:00.000Z",
        "voteCount": 2,
        "content": "Automatically Copy Auto Scaling Group:\nThis option allows launching a new fleet of instances for each deployment automatically.\n\nCodeDeployDefault.HalfAtAtime Deployment Configuration:\nThis configuration meets the requirement of rerouting traffic to half of the new instances at a time. It ensures that the deployment succeeds if traffic is rerouted to at least half of the instances, otherwise, it fails."
      },
      {
        "date": "2024-07-02T01:44:00.000Z",
        "voteCount": 3,
        "content": "B: blue/green deployment with CodeDeployDefault.HalfAtAtime"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 227,
    "url": "https://www.examtopics.com/discussions/amazon/view/142992-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company needs to adopt a multi-account strategy to deploy its applications and the associated CI/CD infrastructure. The company has created an organization in AWS Organizations that has all features enabled. The company has configured AWS Control Tower and has set up a landing zone.<br><br>The company needs to use AWS Control Tower controls (guardrails) in all AWS accounts in the organization. The company must create the accounts for a multi-environment application and must ensure that all accounts are configured to an initial baseline.<br><br>Which solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Control Tower Account Factory Customization (AFC) blueprint that uses the baseline configuration. Use AWS Control Tower Account Factory to provision a dedicated AWS account for each environment and a CI/CD account by using the blueprint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Control Tower Account Factory to provision a dedicated AWS account for each environment and a CI/CD account. Use AWS CloudFormation StackSets to apply the baseline configuration to the new accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Organizations to provision a multi-environment AWS account and a CI/CD account. In the Organizations management account, create an AWS Lambda function that assumes the Organizations access role to apply the baseline configuration to the new accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Organizations to provision a dedicated AWS account for each environment, an audit account, and a CI/CD account. Use AWS CloudFormation StackSets to apply the baseline configuration to the new accounts."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-25T19:00:00.000Z",
        "voteCount": 1,
        "content": "Answer is B\nUse AWS Control Tower Account Factory to provision dedicated AWS accounts for each environment and a CI/CD account. Then, use AWS CloudFormation StackSets to apply the baseline configuration to the new accounts.\nThis approach simplifies account provisioning and ensures consistency across environments while minimizing manual effort."
      },
      {
        "date": "2024-07-30T19:44:00.000Z",
        "voteCount": 3,
        "content": "keywords: AWS Control Tower Account Factory Customization (AFC)\nOption A is the best choice as it meets all the requirements with the least operational overhead by leveraging AWS Control Tower\u2019s Account Factory and Customization features. This option provides an automated, compliant, and consistent approach to account provisioning and baseline configuration."
      },
      {
        "date": "2024-07-15T03:17:00.000Z",
        "voteCount": 1,
        "content": "---&gt; A"
      },
      {
        "date": "2024-07-11T18:12:00.000Z",
        "voteCount": 1,
        "content": "All of these options are possible. But A is the LEAST operational overhead"
      },
      {
        "date": "2024-06-27T11:53:00.000Z",
        "voteCount": 2,
        "content": "AWS Control Tower Account Factory Customization (AFC)"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 228,
    "url": "https://www.examtopics.com/discussions/amazon/view/143354-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps team has created a Custom Lambda rule in AWS Config. The rule monitors Amazon Elastic Container Repository (Amazon ECR) policy statements for ecr:* actions. When a noncompliant repository is detected, Amazon EventBridge uses Amazon Simple Notification Service (Amazon SNS) to route the notification to a security team.<br><br>When the custom AWS Config rule is evaluated, the AWS Lambda function fails to run.<br><br>Which solution will resolve the issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Lambda function's resource policy to grant AWS Config permission to invoke the function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the SNS topic policy to include configuration changes for EventBridge to publish to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Lambda function's execution role to include configuration changes for custom AWS Config rules.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify all the ECR repository policies to grant AWS Config access to the necessary ECR API actions."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-29T03:17:00.000Z",
        "voteCount": 2,
        "content": "When you create a custom AWS Config rule that uses a Lambda function, AWS Config needs permission to invoke it. This is done by adding a resource-based policy to the Lambda function that explicitly permits AWS Config to invoke it. Without this permission, AWS Config cannot trigger the Lambda function, leading to the function failing to run."
      },
      {
        "date": "2024-07-30T19:47:00.000Z",
        "voteCount": 1,
        "content": "Option A is the best choice to resolve the issue. By modifying the Lambda function's resource policy to grant AWS Config permission to invoke the function, we address the root cause of the invocation failure. This ensures that AWS Config can successfully execute the custom rule using the Lambda function."
      },
      {
        "date": "2024-07-22T00:29:00.000Z",
        "voteCount": 2,
        "content": "Resource policy should allow Config invocation"
      },
      {
        "date": "2024-07-21T05:45:00.000Z",
        "voteCount": 2,
        "content": "A. Modify the Lambda function's resource policy to grant AWS Config permission to invoke the function.                                                                                                                                   \n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"config.amazonaws.com\"\n      },\n      \"Action\": \"lambda:InvokeFunction\",\n      \"Resource\": \"arn:aws:lambda:region:account-id:function:function-name\"\n    }\n  ]\n}"
      },
      {
        "date": "2024-07-15T03:17:00.000Z",
        "voteCount": 4,
        "content": "---&gt; A"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 229,
    "url": "https://www.examtopics.com/discussions/amazon/view/143876-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A developer is creating a proof of concept for a new software as a service (SaaS) application. The application is in a shared development AWS account that is part of an organization in AWS Organizations.<br><br>The developer needs to create service-linked IAM roles for the AWS services that are being considered for the proof of concept. The solution needs to give the developer the ability to create and configure the service-linked roles only.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user for the developer in the organization's management account. Configure a cross-account role in the development account for the developer to use. Limit the scope of the cross-account role to common services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the developer to an IAM group. Attach the PowerUserAccess managed policy to the IAM group. Enforce multi-factor authentication (MFA) on the user account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an SCP to the development account in Organizations. Configure the SCP with a Deny rule for iam:* to limit the developer's access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that has the necessary IAM access to allow the developer to create policies and roles. Create and attach a permissions boundary to the role. Grant the developer access to assume the role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-15T03:18:00.000Z",
        "voteCount": 1,
        "content": "---&gt; D"
      },
      {
        "date": "2024-07-13T23:41:00.000Z",
        "voteCount": 3,
        "content": "D - is more granular since it provides the right balance of granting necessary permissions while maintaining security and following the principle of least privilege. It allows the developer to create and configure service-linked roles as needed for the proof of concept, while the permissions boundary ensures that they can't exceed their intended level of access."
      },
      {
        "date": "2024-07-13T15:50:00.000Z",
        "voteCount": 3,
        "content": "A. This approach involves creating a user in the management account and setting up cross-account roles, which adds unnecessary complexity and potential security risks. \nB. PowerUserAccess managed policy provides broad permissions that go beyond just creating and configuring service-linked roles. This approach does not meet the requirement to restrict the developer's capabilities specifically to service-linked role management.\nC. SCPs are used to set permission guardrails at the organizational or account level, but they do not grant permissions. They are used to restrict actions, and configuring an SCP with a deny rule for iam:* would likely prevent the developer from performing necessary actions\n\nD effectively meets the requirements"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 230,
    "url": "https://www.examtopics.com/discussions/amazon/view/142993-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Organizations to manage its AWS accounts. The company wants its monitoring system to receive an alert when a root user logs in. The company also needs a dashboard to display any log activity that the root user generates.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Config with a multi-account aggregator. Configure log forwarding to Amazon CloudWatch Logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon QuickSight dashboard that uses an Amazon CloudWatch Logs query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Logs metric filter to match root user login events. Configure a CloudWatch alarm and an Amazon Simple Notification Service (Amazon SNS) topic to send alerts to the company's monitoring system.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch Logs subscription filter to match root user login events. Configure the filter to forward events to an Amazon Simple Notification Service (Amazon SNS) topic. Configure the SNS topic to send alerts to the company's monitoring system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CloudTrail organization trail. Configure the organization trail to send events to Amazon CloudWatch Logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch dashboard that uses a CloudWatch Logs Insights query.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CEF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CEF",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T19:55:00.000Z",
        "voteCount": 1,
        "content": "Option E: Use CloudTrail to capture and forward root user activities.\nOption C: Set up metric filters and alarms to alert on root user login events.\nOption F: Create a CloudWatch dashboard for visualizing root user activities.\n\nAdditional Note:\nCloudWatch Logs Subscription Filter:\n- Real-time processing of log events, but typically used for streaming log data to other services like AWS Lambda or Elasticsearch.\n- Not necessary for the specific task of alerting on root user login events.\n\nAWS Config is not directly relevant to capturing and forwarding root user login events to CloudWatch Logs."
      },
      {
        "date": "2024-07-15T03:20:00.000Z",
        "voteCount": 1,
        "content": "---&gt; CEF"
      },
      {
        "date": "2024-07-13T23:48:00.000Z",
        "voteCount": 2,
        "content": "E- AWS CloudTrail will log all activities, including root user logins, across all accounts in the organisation. Sending these logs to CloudWatch Logs enables further processing and analysis.\n\nC- Creating a metric filter to detect root user login events will allow you to trigger a CloudWatch alarm. The alarm can then send notifications via SNS to the company's monitoring system, ensuring real-time alerts for root user logins.\n\nF- Using CloudWatch Logs Insights, you can create queries to extract and visualise log data related to root user activity. This data can be displayed on a CloudWatch dashboard, providing a centralised view of root user actions."
      },
      {
        "date": "2024-07-13T15:56:00.000Z",
        "voteCount": 2,
        "content": "E first, then C, and the last is F\n\nE ensures that all events, including root user login events, are captured across all accounts in the organization. By sending these events to CloudWatch Logs, you centralize the logging data, making it accessible for further processing.\nC creating a metric filter in CloudWatch Logs to detect specific patterns in the log data, such as root user login events. \nF creating a CloudWatch dashboard that utilizes CloudWatch Logs Insights to query and visualize the log data. This dashboard can be used to display detailed information about root user login activity and other relevant log events."
      },
      {
        "date": "2024-06-27T12:00:00.000Z",
        "voteCount": 4,
        "content": "Correct answer."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 231,
    "url": "https://www.examtopics.com/discussions/amazon/view/142994-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Organizations to manage its AWS accounts. A DevOps engineer must ensure that all users who access the AWS Management Console are authenticated through the company\u2019s corporate identity provider (IdP).<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon GuardDuty with a delegated administrator account Use GuardDuty to enforce denial of IAM user logins.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS IAM Identity Center to configure identity federation with SAML 2.0.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a permissions boundary in AWS IAM Identity Center to deny password logins for IAM users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate IAM groups in the Organizations management account to apply consistent permissions for all IAM users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an SCP in Organizations to deny password creation for IAM users.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-25T19:23:00.000Z",
        "voteCount": 1,
        "content": "Use AWS IAM Identity Center to configure identity federation with SAML 2.0:\nConfigure SAML-based federation between your corporate IdP and AWS IAM.\nThis allows users to authenticate via your corporate identity provider when accessing the AWS Management Console.\n\nCreate a permissions boundary in AWS IAM Identity Center:\nSet up a permissions boundary to deny password logins for IAM users.\nThis ensures that users must authenticate through the corporate IdP rather than using IAM user credentials."
      },
      {
        "date": "2024-07-30T20:01:00.000Z",
        "voteCount": 1,
        "content": "Option B: Configure identity federation with SAML 2.0 using AWS IAM Identity Center.\nOption E: Implement an SCP to deny password creation for IAM users, enforcing IdP authentication.\n\nIncorrect for C - Permissions Boundaries \n- Permissions boundaries in AWS IAM Identity Center define the maximum permissions an IAM entity can have but are not used to control login methods or deny password logins.\n- Permissions boundaries do not restrict authentication methods or enforce federation.\n- Permissions boundaries are not applicable for denying IAM user logins."
      },
      {
        "date": "2024-07-15T03:21:00.000Z",
        "voteCount": 1,
        "content": "---&gt; BE"
      },
      {
        "date": "2024-07-13T16:01:00.000Z",
        "voteCount": 2,
        "content": "of course B.\nE enforce that users cannot log in directly with IAM credentials. Instead, they must use the SSO setup provided by AWS IAM Identity Center, ensuring compliance with the requirement to authenticate through the corporate IdP."
      },
      {
        "date": "2024-06-27T12:12:00.000Z",
        "voteCount": 3,
        "content": "BE is answer\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Deny\",\n      \"Action\": [\n        \"iam:CreateLoginProfile\",\n        \"iam:UpdateLoginProfile\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 232,
    "url": "https://www.examtopics.com/discussions/amazon/view/143368-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has deployed a new platform that runs on Amazon Elastic Kubernetes Service (Amazon EKS). The new platform hosts web applications that users frequently update. The application developers build the Docker images for the applications and deploy the Docker images manually to the platform.<br><br>The platform usage has increased to more than 500 users every day. Frequent updates, building the updated Docker images for the applications, and deploying the Docker images on the platform manually have all become difficult to manage.<br><br>The company needs to receive an Amazon Simple Notification Service (Amazon SNS) notification if Docker image scanning returns any HIGH or CRITICAL findings for operating system or programming language package vulnerabilities.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files. Create a pipeline in AWS CodePipeline. Use an Amazon S3 event to invoke the pipeline when a newer version of the Dockerfile is committed. Add a step to the pipeline to initiate the AWS CodeBuild project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files. Create a pipeline in AWS CodePipeline. Use an Amazon EventBridge event to invoke the pipeline when a newer version of the Dockerfile is committed. Add a step to the pipeline to initiate the AWS CodeBuild project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Turn on basic scanning for the ECR repository. Create an Amazon EventBridge rule that monitors Amazon GuardDuty events. Configure the EventBridge rule to send an event to an SNS topic when the finding-severity-counts parameter is more than 0 at a CRITICAL or HIGH level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository. Turn on enhanced scanning for the ECR repository. Create an Amazon EventBridge rule that monitors ECR image scan events. Configure the EventBridge rule to send an event to an SNS topic when the finding-severity-counts parameter is more than 0 at a CRITICAL or HIGH level.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeBuild project that scans the Dockerfile. Configure the project to build the Docker images and store the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository if the scan is successful. Configure an SNS topic to provide notification if the scan returns any vulnerabilities."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-25T19:29:00.000Z",
        "voteCount": 1,
        "content": "The answer is BD"
      },
      {
        "date": "2024-07-30T20:08:00.000Z",
        "voteCount": 2,
        "content": "Option B: \n- AWS CodeCommit repository to store the Dockerfile and Kubernetes deployment files. \n- Amazon EventBridge event to invoke the pipeline when a newer version of the Dockerfile is committed. \n\nOption D: \n- AWS CodeBuild project that builds the Docker images and stores the Docker images in an Amazon Elastic Container Registry (Amazon ECR) repository.\n- enhanced scanning for the ECR repository."
      },
      {
        "date": "2024-07-15T03:23:00.000Z",
        "voteCount": 1,
        "content": "---&gt; BD"
      },
      {
        "date": "2024-07-13T16:15:00.000Z",
        "voteCount": 3,
        "content": "B sets up a CI/CD pipeline with AWS CodePipeline triggered by changes in the AWS CodeCommit repository. Using Amazon EventBridge ensures that the pipeline is invoked whenever there is a new commit, automating the build and deployment process.\n\nD ensures that Docker images are built and pushed to ECR, where enhanced scanning is enabled. Enhanced scanning provides detailed vulnerability information. An EventBridge rule is configured to monitor scan events and trigger notifications via SNS when HIGH or CRITICAL vulnerabilities are found."
      },
      {
        "date": "2024-07-08T10:06:00.000Z",
        "voteCount": 2,
        "content": "Agree with B,D"
      },
      {
        "date": "2024-07-07T12:10:00.000Z",
        "voteCount": 1,
        "content": "--&gt; B D"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 233,
    "url": "https://www.examtopics.com/discussions/amazon/view/143365-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company groups its AWS accounts in OUs in an organization in AWS Organizations. The company has deployed a set of Amazon API Gateway APIs in one of the Organizations accounts. The APIs are bound to the account's VPC and have no existing authentication mechanism. Only principals in a specific OU can have permissions to invoke the APIs.<br><br>The company applies the following policy to the API Gateway interface VPC endpoint:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image21.png\"><br><br>The company also updates the API Gateway resource policies to deny invocations that do not come through the interface VPC endpoint. After the updates, the following error message appears during attempts to use the interface VPC endpoint URL to invoke an API: \"User: anonymous is not authorized.\"<br><br>Which combination of steps will solve this problem? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable IAM authentication on all API methods by setting AWS JAM as the authorization method.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a token-based AWS Lambda authorizer that passes the caller's identity in a bearer token.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a request parameter-based AWS Lambda authorizer that passes the caller's identity in a combination of headers, query string parameters, stage variables, and $cortext variables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Cognito user pools as the authorizer to control access to the API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify the identity of the requester by using Signature Version 4 to sign client requests by using AWS credentials.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-25T19:45:00.000Z",
        "voteCount": 1,
        "content": "Option A\nEnable IAM authentication on all API methods:\nSet AWS IAM as the authorization method for all API methods.\nThis ensures that authentication is required for invoking the APIs1.\n\nOption B\nCreate a token-based AWS Lambda authorizer:\nImplement a custom Lambda authorizer that validates bearer tokens.\nPass the caller\u2019s identity in the token to authorize API requests"
      },
      {
        "date": "2024-08-22T09:53:00.000Z",
        "voteCount": 2,
        "content": "vote for AE"
      },
      {
        "date": "2024-08-19T06:58:00.000Z",
        "voteCount": 3,
        "content": "You can enable IAM authorization for HTTP API routes. When IAM authorization is enabled, clients must use Signature Version 4 (SigV4) to sign their requests with AWS credentials. API Gateway invokes your API route only if the client has execute-api permission for the route."
      },
      {
        "date": "2024-07-30T20:14:00.000Z",
        "voteCount": 3,
        "content": "Hope is Typo for the Option A,  AWS JAM = AWS IAM\n\nOption A. Enable IAM authentication on all API methods by setting AWS IAM as the authorization method.\n- This ensures that all requests to the API must be authenticated using IAM credentials, directly addressing the anonymous access issue.\n\nOption E. Verify the identity of the requester by using Signature Version 4 to sign client requests by using AWS credentials.\n- By using AWS Signature Version 4, requests are authenticated, ensuring they are authorized according to IAM policies linked to the specific Organizational Unit."
      },
      {
        "date": "2024-07-22T05:20:00.000Z",
        "voteCount": 3,
        "content": "JAM= IAM"
      },
      {
        "date": "2024-07-15T03:38:00.000Z",
        "voteCount": 1,
        "content": "---&gt; A E (assuming there's a typo in AWS JAM)\nIf there's no typo in AWS JAM, I'd go for B &amp; E"
      },
      {
        "date": "2024-07-12T17:14:00.000Z",
        "voteCount": 1,
        "content": "Anser:B,E"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 234,
    "url": "https://www.examtopics.com/discussions/amazon/view/143877-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company wants to decrease the time it takes to develop new features. The company uses AWS CodeBuild and AWS CodeDeploy to build and deploy its applications. The company uses AWS CodePipeline to deploy each microservice with its own CI/CD pipeline.<br><br>The company needs more visibility into the average time between the release of new features and the average time to recover after a failed deployment.<br><br>Which solution will provide this visibility with the LEAST configuration effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProgram an AWS Lambda function that creates Amazon CloudWatch custom metrics with information about successful runs and failed runs for each pipeline. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. Use the metrics to build a CloudWatch dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProgram an AWS Lambda function that creates Amazon CloudWatch custom metrics with information about successful runs and failed runs for each pipeline. Create an Amazon EventBridge rule to invoke the Lambda function after every successful run and after every failed run. Use the metrics to build a CloudWatch dashboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProgram an AWS Lambda function that writes information about successful runs and failed runs to Amazon DynamoDB. Create an Amazon EventBridge rule to invoke the Lambda function after every successful run and after every failed run. Build an Amazon QuickSight dashboard to show the information from DynamoDB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProgram an AWS Lambda function that writes information about successful runs and failed runs to Amazon DynamoDB. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. Build an Amazon QuickSight dashboard to show the information from DynamoDB."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-25T19:49:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2024-07-30T20:41:00.000Z",
        "voteCount": 2,
        "content": "B is most simple and direct with LEAST configuration effort."
      },
      {
        "date": "2024-07-15T03:41:00.000Z",
        "voteCount": 1,
        "content": "---&gt; B"
      },
      {
        "date": "2024-07-13T16:53:00.000Z",
        "voteCount": 3,
        "content": "A. Invoking the Lambda function every 5 minutes is less efficient compared to event-driven invocation\nB. provides the needed visibility with minimal configuration effort\nC &amp; D. Using DynamoDB and QuickSight involves more configuration"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 235,
    "url": "https://www.examtopics.com/discussions/amazon/view/143364-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has developed a static website hosted on an Amazon S3 bucket. The website is deployed using AWS CloudFormation. The CloudFormation template defines an S3 bucket and a custom resource that copies content into the bucket from a source location.<br><br>The company has decided that it needs to move the website to a new location, so the existing CloudFormation stack must be deleted and re-created. However, CloudFormation reports that the stack could not be deleted cleanly.<br><br>What is the MOST likely cause and how can the DevOps engineer mitigate this problem for this and future versions of the website?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeletion has failed because the S3 bucket has an active website configuration. Modify the CloudFormation template to remove the WebsiteConfiguration property from the S3 bucket resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeletion has failed because the S3 bucket is not empty. Modify the custom resource's AWS Lambda function code to recursively empty the bucket when RequestType is Delete.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeletion has failed because the custom resource does not define a deletion policy. Add a DeletionPolicy property to the custom resource definition with a value of RemoveOnDeletion.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeletion has failed because the S3 bucket is not empty. Modify the S3 bucket resource in the CloudFormation template to add a DeletionPolicy property with a value of Empty."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-22T09:54:00.000Z",
        "voteCount": 1,
        "content": "vote for B"
      },
      {
        "date": "2024-08-19T07:11:00.000Z",
        "voteCount": 2,
        "content": "By default cloudformation can't delete an S3 bucket that's not empty. If the bucket still contains objects when the stack deletion is attempted, the stack deletion will fail. \n\nAlthough the Q doesn't specify that the custom resource uses Lambda. I think it's safe to assume here that since a custom resource is responsible for copying content into the bucket, it can also be used to handle the cleanup process."
      },
      {
        "date": "2024-07-15T03:43:00.000Z",
        "voteCount": 1,
        "content": "---&gt; B"
      },
      {
        "date": "2024-07-13T16:55:00.000Z",
        "voteCount": 2,
        "content": "of course B"
      },
      {
        "date": "2024-07-12T03:59:00.000Z",
        "voteCount": 2,
        "content": "Agree B"
      },
      {
        "date": "2024-07-11T19:17:00.000Z",
        "voteCount": 2,
        "content": "Definitely B"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 236,
    "url": "https://www.examtopics.com/discussions/amazon/view/143475-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses Amazon EC2 as its primary compute platform. A DevOps team wants to audit the company's EC2 instances to check whether any prohibited applications have been installed on the EC2 instances.<br><br>Which solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Systems Manager on each instance. Use AWS Systems Manager Inventory. Use Systems Manager resource data sync to synchronize and store findings in an Amazon S3 bucket. Create an AWS Lambda function that runs when new objects are added to the S3 bucket. Configure the Lambda function to identify prohibited applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Systems Manager on each instance. Use Systems Manager Inventory Create AWS Config rules that monitor changes from Systems Manager Inventory to identify prohibited applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Systems Manager on each instance. Use Systems Manager Inventory. Filter a trail in AWS CloudTrail for Systems Manager Inventory events to identify prohibited applications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDesignate Amazon CloudWatch Logs as the log destination for all application instances. Run an automated script across all instances to create an inventory of installed applications. Configure the script to forward the results to CloudWatch Logs. Create a CloudWatch alarm that uses filter patterns to search log data to identify prohibited applications."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-25T19:55:00.000Z",
        "voteCount": 1,
        "content": "Option B: Configure AWS Systems Manager on each instance. Use Systems Manager Inventory and create AWS Config rules that monitor changes from Systems Manager Inventory to identify prohibited applications.\nThis approach leverages Systems Manager Inventory and AWS Config to efficiently track and identify prohibited applications while minimizing operational overhead."
      },
      {
        "date": "2024-08-20T06:52:00.000Z",
        "voteCount": 2,
        "content": "B fir nme"
      },
      {
        "date": "2024-08-04T20:15:00.000Z",
        "voteCount": 3,
        "content": "keywords: AWS Systems Manage, Systems Manager Inventory, AWS Config rule"
      },
      {
        "date": "2024-07-15T03:47:00.000Z",
        "voteCount": 1,
        "content": "---&gt; B"
      },
      {
        "date": "2024-07-06T14:32:00.000Z",
        "voteCount": 2,
        "content": "B\nhttps://aws.amazon.com/blogs/mt/preventing-blacklisted-applications-with-aws-systems-manager-and-aws-config/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 237,
    "url": "https://www.examtopics.com/discussions/amazon/view/143363-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an event-driven JavaScript application. The application uses decoupled AWS managed services that publish, consume, and route events. During application testing, events are not delivered to the target that is specified by an Amazon EventBridge rule.<br><br>A DevOps team must provide application testers with additional functionality to view, troubleshoot, and prevent the loss of events without redeployment of the application.<br><br>Which combination of steps should the DevOps team take to meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch AWS Device Farm with a standard test environment and project to run a specific build of the application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 bucket. Enable AWS CloudTrail. Create a CloudTrail trail that specifies the S3 bucket as the storage location.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the EventBridge rule to use an Amazon Simple Queue Service (Amazon SQS) standard queue as a dead-letter queue.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the EventBridge rule to use an Amazon Simple Queue Service (Amazon SQS) FIFO queue as a dead-letter queue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a log group in Amazon CloudWatch Logs Specify the log group as an additional target of the EventBridge rule.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the application code base to use the AWS X-Ray SDK tracing feature to instrument the code with support for the X-Amzn-Trace-Id header."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "CEF",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "CDF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T06:59:00.000Z",
        "voteCount": 1,
        "content": "BCE, without redeployment."
      },
      {
        "date": "2024-07-30T22:45:00.000Z",
        "voteCount": 2,
        "content": "Keywords: without redeployment of the application.\nOption B: Enabling CloudTrail will allow testers to track event activities and interactions with AWS services, aiding in troubleshooting.\nOption C: Using a standard SQS queue as a DLQ ensures that failed events are captured and can be analyzed or retried.\nOption E: Adding CloudWatch Logs as a target provides immediate logging of event processing, aiding in real-time monitoring and troubleshooting."
      },
      {
        "date": "2024-07-22T12:47:00.000Z",
        "voteCount": 1,
        "content": "I will go with CEF"
      },
      {
        "date": "2024-07-18T15:40:00.000Z",
        "voteCount": 1,
        "content": "CEF, X-ray enables you to debug distributed applications to troubleshoot the root cause of performance issues and errors"
      },
      {
        "date": "2024-07-17T00:04:00.000Z",
        "voteCount": 2,
        "content": "without redeployment of the application.\n\nB,C,E"
      },
      {
        "date": "2024-07-15T04:16:00.000Z",
        "voteCount": 1,
        "content": "---&gt; CEF\nI thought E its not possible, but --&gt; https://repost.aws/knowledge-center/cloudwatch-log-group-eventbridge"
      },
      {
        "date": "2024-07-18T04:32:00.000Z",
        "voteCount": 2,
        "content": "good catch with \"without redeployment of the application.\"\nB, C, E"
      },
      {
        "date": "2024-07-13T22:57:00.000Z",
        "voteCount": 1,
        "content": "C allows you to capture events that could not be delivered to the specified target\nE capture detailed logs of the events that are processed\nF end-to-end tracing capabilities\n\nB is wrong because while CloudTrail provides logging for AWS API calls, it is not specifically designed for capturing and troubleshooting event flows in EventBridge"
      },
      {
        "date": "2024-07-11T19:16:00.000Z",
        "voteCount": 2,
        "content": "BCE..."
      },
      {
        "date": "2024-07-05T08:07:00.000Z",
        "voteCount": 1,
        "content": "CEF\nC &gt; D - because SQS FIFO is not supported by Dead Letter Queues in event bridge"
      },
      {
        "date": "2024-07-05T08:06:00.000Z",
        "voteCount": 1,
        "content": "CDF\nC &gt; D - because SQS FIFO is not supported by Dead Letter Queues in event bridge"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 238,
    "url": "https://www.examtopics.com/discussions/amazon/view/143361-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is migrating its container-based workloads to an AWS Organizations multi-account environment. The environment consists of application workload accounts that the company uses to deploy and run the containerized workloads. The company has also provisioned a shared services account for shared workloads in the organization.<br><br>The company must follow strict compliance regulations. All container images must receive security scanning before they are deployed to any environment. Images can be consumed by downstream deployment mechanisms after the images pass a scan with no critical vulnerabilities. Pre-scan and post-scan images must be isolated from one another so that a deployment can never use pre-scan images.<br><br>A DevOps engineer needs to create a strategy to centralize this process.<br><br>Which combination of steps will meet these requirements with the LEAST administrative overhead? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon Elastic Container Registry (Amazon ECR) repositories in the shared services account: one repository for each pre-scan image and one repository for each post-scan image. Configure Amazon ECR image scanning to run on new image pushes to the pre-scan repositories. Use resource-based policies to grant the organization write access to the pre-scan repositories and read access to the post-scan repositories.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate pre-scan Amazon Elastic Container Registry (Amazon ECR) repositories in each account that publishes container images. Create repositories for post-scan images in the shared services account. Configure Amazon ECR image scanning to run on new image pushes to the pre-scan repositories. Use resource-based policies to grant the organization read access to the post-scan repositories.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure image replication for each image from the image's pre-scan repository to the image's post-scan repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline in AWS CodePipeline for each pre-scan repository. Create a source stage that runs when new images are pushed to the pre-scan repositories. Create a stage that uses AWS CodeBuild as the action provider. Write a buildspec.yaml definition that determines the image scanning status and pushes images without critical vulnerabilities to the post-scan repositories.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function. Create an Amazon EventBridge rule that reacts to image scanning completed events and invokes the Lambda function. Write function code that determines the image scanning status and pushes images without critical vulnerabilities to the post-scan repositories.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-12T23:26:00.000Z",
        "voteCount": 2,
        "content": "Lambda is not meant to work with Docker"
      },
      {
        "date": "2024-08-25T20:12:00.000Z",
        "voteCount": 2,
        "content": "AD gives the least administrative overhead"
      },
      {
        "date": "2024-08-07T05:55:00.000Z",
        "voteCount": 1,
        "content": "Why E is not optimal - https://stackoverflow.com/questions/51158595/build-and-push-docker-image-to-aws-ecr-using-lambda"
      },
      {
        "date": "2024-08-07T05:52:00.000Z",
        "voteCount": 2,
        "content": "Why not E - To push images to the post-scan repo, you need a custom lambda container to run docker pull and push commands which is more complicated than Option D"
      },
      {
        "date": "2024-07-30T22:53:00.000Z",
        "voteCount": 2,
        "content": "keywords: LEAST Administrative overhead\n\nOption A centralizes the repository management in the shared services account, simplifying access control and configuration management. Pre-scan and post-scan repositories are clearly separated, ensuring that only post-scan images are deployed.\n\nOption E uses event-driven automation to handle the scanning results and image promotion, reducing manual intervention and ensuring that only images that pass the security scan are moved to the post-scan repositories. This approach is efficient and minimizes administrative overhead compared to manually setting up pipelines or replication mechanisms."
      },
      {
        "date": "2024-07-15T04:24:00.000Z",
        "voteCount": 1,
        "content": "---&gt; AE"
      },
      {
        "date": "2024-07-14T01:07:00.000Z",
        "voteCount": 3,
        "content": "LEAST administrative overhead:\n=&gt; Should create ECR repositories in the shared services account =&gt; A\nAnd should create only 1 Lambda function =&gt; E\n\nD wrong because it involves creating and managing multiple pipelines, which increases administrative overhead significantly"
      },
      {
        "date": "2024-07-05T07:47:00.000Z",
        "voteCount": 2,
        "content": "E &gt; D for LEAST administrative overhead"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 239,
    "url": "https://www.examtopics.com/discussions/amazon/view/143360-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to deploy its web applications on containers. The web applications contain confidential data that cannot be decrypted without specific credentials.<br><br>A DevOps engineer has stored the credentials in AWS Secrets Manager. The secrets are encrypted by an AWS Key Management Service (AWS KMS) customer managed key. A Kubernetes service account for a third-party tool makes the secrets available to the applications. The service account assumes an IAM role that the company created to access the secrets.<br><br>The service account receives an Access Denied (403 Forbidden) error while trying to retrieve the secrets from Secrets Manager.<br><br>What is the root cause of this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IAM role that is attached to the EKS cluster does not have access to retrieve the secrets from Secrets Manager.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe key policy for the customer managed key does not allow the Kubernetes service account IAM role to use the key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe key policy for the customer managed key does not allow the EKS cluster IAM role to use the key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IAM role that is assumed by the Kubernetes service account does not have permission to access the EKS cluster."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T22:59:00.000Z",
        "voteCount": 1,
        "content": "When a service account in Amazon EKS tries to access secrets in AWS Secrets Manager, it does so by assuming an IAM role. The permissions required to access these secrets include:\n- Secrets Manager permissions: The IAM role must have the necessary permissions to retrieve the secrets from AWS Secrets Manager.\n- KMS key permissions: The IAM role must also have permissions to use the AWS KMS key that encrypts the secrets."
      },
      {
        "date": "2024-07-30T22:59:00.000Z",
        "voteCount": 1,
        "content": "If the IAM role has the correct permissions to access Secrets Manager but still receives an \"Access Denied\" error, the issue is likely related to the KMS key policy. Specifically, the key policy needs to explicitly allow the IAM role to use the key for decrypting the secrets.\n\nSo, the error message indicates that the key policy for the customer-managed KMS key does not include the necessary permissions for the IAM role assumed by the Kubernetes service account. Adjusting the key policy to grant the required permissions should resolve the issue."
      },
      {
        "date": "2024-07-15T04:42:00.000Z",
        "voteCount": 1,
        "content": "---&gt; B"
      },
      {
        "date": "2024-07-14T01:13:00.000Z",
        "voteCount": 1,
        "content": "The IAM role assumed by the Kubernetes service account, not the EKS cluster IAM role =&gt; C is wrong"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 240,
    "url": "https://www.examtopics.com/discussions/amazon/view/143359-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is migrating its product development teams from an on-premises data center to a hybrid environment. The new environment will add four AWS Regions and will give the developers the ability to use the Region that is geographically closest to them.<br><br>All the development teams use a shared set of Linux applications. The on-premises data center stores the applications on a NetApp ONTAP storage device. The storage volume is mounted read-only on the development on-premises VMs. The company updates the applications on the shared volume once a week.<br><br>A DevOps engineer needs to replicate the data to all the new Regions. The DevOps engineer must ensure that the data is always up to date with deduplication. The data also must not be dependent on the availability of the on-premises storage device.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon S3 File Gateway in the on-premises data center. Create S3 buckets in each Region. Set up a cron job to copy the data from the storage device to the S3 File Gateway. Set up S3 Cross-Region Replication (CRR) to the S3 buckets in each Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon FSx File Gateway in one Region. Create file servers in Amazon FSx for Windows File Server in each Region. Set up a cron job to copy the data from the storage device to the FSx File Gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Multi-AZ Amazon FSx for NetApp ONTAP instances and volumes in each Region. Configure a scheduled SnapMirror relationship between the on-premises storage device and the FSx for ONTAP instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic File System (Amazon EFS) file system in each Region. Deploy an AWS DataSync agent in the on-premises data center. Configure a schedule for DataSync to copy the data to Amazon EFS daily."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-25T20:21:00.000Z",
        "voteCount": 1,
        "content": "Option C does involve using Amazon FSx for NetApp ONTAP, it doesn\u2019t address the deduplication requirement or the independence from the availability of the on-premises storage device. Additionally, SnapMirror relationships are typically used for data replication within the same storage system rather than across multiple Regions.\n\nFor the specific requirements of deduplication, independence, and multi-Region replication, Option D (using Amazon EFS with AWS DataSync) is a more suitable solution."
      },
      {
        "date": "2024-08-08T00:52:00.000Z",
        "voteCount": 2,
        "content": "Amazon FSx for NetApp ONTAP provides a managed NetApp ONTAP experience in the cloud. By creating Multi-AZ FSx for ONTAP instances in each Region, you can replicate data with high availability and redundancy.\n\nCheckout cheaper contributor access here: https://exammatter.net/\n\nSnapMirror is a replication technology provided by NetApp that allows for efficient and reliable data replication. Configuring SnapMirror relationships between your on-premises NetApp storage device and the FSx for ONTAP instances will ensure that your data is consistently replicated across all AWS Regions."
      },
      {
        "date": "2024-07-30T23:08:00.000Z",
        "voteCount": 2,
        "content": "Amazon FSx for NetApp ONTAP provides a managed NetApp ONTAP experience in the cloud. By creating Multi-AZ FSx for ONTAP instances in each Region, you can replicate data with high availability and redundancy.\n\nSnapMirror is a replication technology provided by NetApp that allows for efficient and reliable data replication. Configuring SnapMirror relationships between your on-premises NetApp storage device and the FSx for ONTAP instances will ensure that your data is consistently replicated across all AWS Regions."
      },
      {
        "date": "2024-07-15T04:45:00.000Z",
        "voteCount": 1,
        "content": "---&gt; C"
      },
      {
        "date": "2024-07-14T01:29:00.000Z",
        "voteCount": 2,
        "content": "C\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/migrating-fsx-ontap-snapmirror.html"
      },
      {
        "date": "2024-07-06T14:41:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://aws.amazon.com/blogs/storage/cross-region-disaster-recovery-with-amazon-fsx-for-netapp-ontap/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 241,
    "url": "https://www.examtopics.com/discussions/amazon/view/143375-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an application that stores data that includes personally identifiable information (PII) in an Amazon S3 bucket. All data is encrypted with AWS Key Management Service (AWS KMS) customer managed keys. All AWS resources are deployed from an AWS CloudFormation template.<br><br>A DevOps engineer needs to set up a development environment for the application in a different AWS account. The data in the development environment's S3 bucket needs to be updated once a week from the production environment's S3 bucket.<br><br>The company must not move PII from the production environment without anonymizing the PII first. The data in each environment must be encrypted with different KMS customer managed keys.<br><br>Which combination of steps should the DevOps engineer take to meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate Amazon Macie on the S3 bucket in the production account. Create an AWS Step Functions state machine to initiate a discovery job and redact all PII before copying files to the S3 bucket in the development account. Give the state machine tasks decrypt permissions on the KMS key in the production account. Give the state machine tasks encrypt permissions on the KMS key in the development account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up S3 replication between the production S3 bucket and the development S3 bucket. Activate Amazon Macie on the development S3 bucket. Create an AWS Step Functions state machine to initiate a discovery job and redact all PII as the files are copied to the development S3 bucket. Give the state machine tasks encrypt and decrypt permissions on the KMS key in the development account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up an S3 Batch Operations job to copy files from the production S3 bucket to the development S3 bucket. In the development account, configure an AWS Lambda function to redact ail PII. Configure S3 Object Lambda to use the Lambda function for S3 GET requests. Give the Lambda function's IAM role encrypt and decrypt permissions on the KMS key in the development account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a development environment from the CloudFormation template in the development account. Schedule an Amazon EventBridge rule to start the AWS Step Functions state machine once a week.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a development environment from the CloudFormation template in the development account. Schedule a cron job on an Amazon EC2 instance to run once a week to start the S3 Batch Operations job."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T23:19:00.000Z",
        "voteCount": 3,
        "content": "Option A addresses the need to anonymize PII before moving data to the development environment. By using Amazon Macie, you can identify PII in the production S3 bucket. AWS Step Functions can orchestrate a workflow to redact this PII before transferring the data. This ensures compliance with data protection requirements. You need to provide the necessary KMS key permissions for decrypting and encrypting data as it moves between accounts.\n\nOption D ensures that the data update process is automated and scheduled. Using Amazon EventBridge to trigger the AWS Step Functions state machine on a weekly basis automates the data transfer and anonymization process."
      },
      {
        "date": "2024-07-18T23:58:00.000Z",
        "voteCount": 2,
        "content": "---&gt; A D"
      },
      {
        "date": "2024-07-14T01:36:00.000Z",
        "voteCount": 2,
        "content": "A. Anonymizing PII in the Production Account\nD. Automating the Weekly Data Transfer \n\nB suggests replicating the data before redacting PII, which violates the requirement \nC does not ensure that the PII is redacted before the data is stored in the development environment\nE introduces additional infrastructure management and costs"
      },
      {
        "date": "2024-07-06T14:49:00.000Z",
        "voteCount": 1,
        "content": "redact should be done before"
      },
      {
        "date": "2024-07-06T14:48:00.000Z",
        "voteCount": 1,
        "content": "A &amp; D\nhttps://aws.amazon.com/blogs/security/how-to-use-amazon-macie-to-preview-sensitive-data-in-s3-buckets/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 242,
    "url": "https://www.examtopics.com/discussions/amazon/view/143374-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an Amazon Elastic Kubernetes Service (Amazon EKS) cluster to host its machine learning (ML) application. As the ML model and the container image size grow, the time that new pods take to start up has increased to several minutes.<br><br>A DevOps engineer needs to reduce the startup time to seconds. The solution must also reduce the startup time to seconds when the pod runs on nodes that were recently added to the cluster.<br><br>The DevOps engineer creates an Amazon EventBridge rule that invokes an automation in AWS Systems Manager. The automation prefetches the container images from an Amazon Elastic Container Registry (Amazon ECR) repository when new images are pushed to the repository. The DevOps engineer also configures tags to be applied to the cluster and the node groups.<br><br>What should the DevOps engineer do next to meet the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's control plane nodes. Create a Systems Manager State Manager association that uses the control plane nodes' tags to prefetch corresponding container images.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's nodes. Create a Systems Manager State Manager association that uses the nodes' machine size to prefetch corresponding container images.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's nodes. Create a Systems Manager State Manager association that uses the nodes' tags to prefetch corresponding container images.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that has a policy that allows EventBridge to use Systems Manager to run commands in the EKS cluster's control plane nodes. Create a Systems Manager State Manager association that uses the nodes' tags to prefetch corresponding container images."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-18T23:59:00.000Z",
        "voteCount": 2,
        "content": "---&gt; C"
      },
      {
        "date": "2024-07-14T01:44:00.000Z",
        "voteCount": 4,
        "content": "The control plane manages the Kubernetes cluster but does not run the application containers =&gt; A &amp; D wrong\nMachine size is not a practical or flexible approach to determining where images should be prefetched. Should be Tag =&gt;B Wrong"
      },
      {
        "date": "2024-07-06T14:56:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://aws.amazon.com/blogs/containers/start-pods-faster-by-prefetching-images/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 243,
    "url": "https://www.examtopics.com/discussions/amazon/view/143373-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's application has an API that retrieves workload metrics. The company needs to audit, analyze, and visualize these metrics from the application to detect issues at scale.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge schedule to invoke an AWS Lambda function that calls the API to retrieve workload metrics. Store the workload metric data in an Amazon S3 bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge schedule to invoke an AWS Lambda function that calls the API to retrieve workload metrics. Store the workload metric data in an Amazon DynamoDB table that has a DynamoDB stream enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Glue crawler to catalog the workload metric data in the Amazon S3 bucket. Create views in Amazon Athena for the cataloged data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect an AWS Glue crawler to the Amazon DynamoDB stream to catalog the workload metric data. Create views in Amazon Athena for the cataloged data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Amazon QuickSight datasets from the Amazon Athena views. Create a QuickSight analysis to visualize the workload metric data as a dashboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon CloudWatch dashboard that has custom widgets that invoke AWS Lambda functions. Configure the Lambda functions to query the workload metrics data from the Amazon Athena views."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T23:26:00.000Z",
        "voteCount": 3,
        "content": "Option A: Using Amazon EventBridge to schedule an AWS Lambda function that retrieves workload metrics from the API and stores the data in Amazon S3 provides a scalable and automated way to collect and store the data.\n\nOption C: AWS Glue can be used to catalog the data stored in Amazon S3, making it queryable using Amazon Athena. This step prepares the data for analysis by creating a schema and making it available for querying.\n\nOption E: Amazon QuickSight can be used to create datasets from the Athena views and then visualize the data in dashboards. This provides the capability to analyze and visualize workload metrics at scale.\n\nhttps://aws.amazon.com/blogs/mt/analyzing-amazon-cloudwatch-internet-monitor-measurement-logs-using-amazon-athena-amazon-quicksight/"
      },
      {
        "date": "2024-07-30T23:30:00.000Z",
        "voteCount": 1,
        "content": "Additional Note:\nNot Option B: Storing workload metric data in an Amazon DynamoDB table with DynamoDB Streams enabled is an option, but it\u2019s not ideal for large-scale metrics and querying. DynamoDB is better suited for high-speed key-value access and doesn\u2019t provide the same level of querying capabilities for large datasets compared to Amazon S3 with Athena.\n\nDynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for up to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real time."
      },
      {
        "date": "2024-07-19T00:00:00.000Z",
        "voteCount": 2,
        "content": "---&gt; A C E"
      },
      {
        "date": "2024-07-14T01:51:00.000Z",
        "voteCount": 3,
        "content": "Data Collection and Storage: EventBridge Schedule + Lambda + S3\nData Cataloging and Querying: Glue Crawler + Athena\nData Visualization: QuickSight"
      },
      {
        "date": "2024-07-06T15:04:00.000Z",
        "voteCount": 1,
        "content": "ACE\nhttps://aws.amazon.com/blogs/mt/analyzing-amazon-cloudwatch-internet-monitor-measurement-logs-using-amazon-athena-amazon-quicksight/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 244,
    "url": "https://www.examtopics.com/discussions/amazon/view/143068-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is building the infrastructure for an application. The application needs to run on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that includes Amazon EC2 instances. The EC2 instances need to use an Amazon Elastic File System (Amazon EFS) file system as a storage backend. The Amazon EFS Container Storage Interface (CSI) driver is installed on the EKS cluster.<br><br>When the DevOps engineer starts the application, the EC2 instances do not mount the EFS file system.<br><br>Which solutions will fix the problem? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch the EKS nodes from Amazon EC2 to AWS Fargate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an inbound rule to the EFS file system\u2019s security group to allow NFS traffic from the EKS cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM role that allows the Amazon EFS CSI driver to interact with the file system\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS DataSync to configure file transfer between the EFS file system and the EKS nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a mount target for the EFS file system in the subnet of the EKS nodes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable encryption or the EFS file system."
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T23:32:00.000Z",
        "voteCount": 2,
        "content": "B: The EFS file system\u2019s security group must allow inbound traffic on the NFS port (2049) from the EC2 instances in the EKS cluster. Without this rule, the EC2 instances won't be able to communicate with the EFS file system.\n\nC: The EFS CSI driver needs permissions to interact with the EFS file system. This involves creating an IAM role with the necessary permissions and associating it with the EFS CSI driver.\n\nE: EFS requires a mount target in each subnet where the EC2 instances reside. This mount target facilitates the network connectivity between the EFS file system and the EC2 instances."
      },
      {
        "date": "2024-07-30T23:33:00.000Z",
        "voteCount": 1,
        "content": "Why Not for options below,\nNot A: Switching from EC2 to AWS Fargate would not directly address the issue with EFS mounting. AWS Fargate does not support mounting EFS file systems natively.\n\nNot D: AWS DataSync is used for data transfer tasks and is not required for mounting EFS file systems in EKS. It is not relevant to solving the problem of mounting EFS.\n\nNot F: Disabling encryption is not necessary and might compromise security. Encryption of EFS file systems should not interfere with mounting unless there is a configuration issue, which is unlikely to be resolved by disabling encryption."
      },
      {
        "date": "2024-07-19T00:02:00.000Z",
        "voteCount": 1,
        "content": "---&gt; B C E"
      },
      {
        "date": "2024-07-14T02:00:00.000Z",
        "voteCount": 1,
        "content": "B. EFS file system\u2019s security group must allow inbound NFS traffic (typically on port 2049) from the security group or IP range of the EKS cluster nodes.\nC. Ensure that the EFS CSI driver has the necessary IAM permissions to interact with the EFS file system, such as \"elasticfilesystem:DescribeFileSystems\", \"elasticfilesystem:DescribeMountTargets\", and other relevant permissions."
      },
      {
        "date": "2024-06-29T06:48:00.000Z",
        "voteCount": 2,
        "content": "So, the correct solutions are:\n\nB. Add an inbound rule to the EFS file system\u2019s security group to allow NFS traffic from the EKS cluster.\nC. Create an IAM role that allows the Amazon EFS CSI driver to interact with the file system.\nE. Create a mount target for the EFS file system in the subnet of the EKS nodes."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 245,
    "url": "https://www.examtopics.com/discussions/amazon/view/143070-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company deploys an application on on-premises devices in the company\u2019s on-premises data center. The company uses an AWS Direct Connect connection between the data center and the company's AWS account. During initial setup of the on-premises devices and during application updates, the application needs to retrieve configuration files from an Amazon Elastic File System (Amazon EFS) file system.<br><br>All traffic from the on-premises devices to Amazon EFS must remain private and encrypted. The on-premises devices must follow the principle of least privilege for AWS access. The company's DevOps team needs the ability to revoke access from a single device without affecting the access of the other devices.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user that has an access key and a secret key for each device. Attach the AmazonElasticFileSystemFullAccess policy to all IAM users. Configure the AWS CLI on the on-premises devices to use the IAM user's access key and secret key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate certificates for each on-premises device in AWS Private Certificate Authority. Create a trust anchor in IAM Roles Anywhere that references an AWS Private CA. Create an IAM role that trust IAM Roles Anywhere. Attach the AmazonElasticFileSystemClientReadWriteAccess to the role. Create an IAM Roles Anywhere profile for the IAM role. Configure the AWS CLI on the on-premises devices to use the aws_signing_helper command to obtain credentials.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM user that has an access key and a secret key for all devices. Attach the AmazonElasticFileSystemClientReadWriteAccess policy to the IAM user. Configure the AWS CLI on the on-premises devices to use the IAM user's access key and secret key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the amazon-efs-utils package to mount the EFS file system.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the native Linux NFS client to mount the EFS file system."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-19T00:05:00.000Z",
        "voteCount": 1,
        "content": "---&gt; B D"
      },
      {
        "date": "2024-07-14T02:10:00.000Z",
        "voteCount": 2,
        "content": "A. Creating individual IAM users with full access does not follow the principle of least privilege =&gt; Wrong\nC. Using a single IAM user for all devices does not allow the ability to revoke access from a single device without affecting others =&gt; Wrong\nE. Technically feasible, but it does not inherently provide encryption in transit"
      },
      {
        "date": "2024-07-06T15:07:00.000Z",
        "voteCount": 3,
        "content": "BD:\nhttps://aws.amazon.com/blogs/aws/amazon-efs-update-on-premises-access-via-direct-connect-vpc/"
      },
      {
        "date": "2024-06-29T06:49:00.000Z",
        "voteCount": 4,
        "content": "B. Generate certificates for each on-premises device in AWS Private Certificate Authority. Create a trust anchor in IAM Roles Anywhere that references an AWS Private CA. Create an IAM role that trusts IAM Roles Anywhere. Attach the AmazonElasticFileSystemClientReadWriteAccess policy to the role. Create an IAM Roles Anywhere profile for the IAM role. Configure the AWS CLI on the on-premises devices to use the aws_signing_helper command to obtain credentials.\nD. Use the amazon-efs-utils package to mount the EFS file system."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 246,
    "url": "https://www.examtopics.com/discussions/amazon/view/143372-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer is setting up an Amazon Elastic Container Service (Amazon ECS) blue/green deployment for an application by using AWS CodeDeploy and AWS CloudFormation. During the deployment window, the application must be highly available and CodeDeploy must shift 10% of traffic to a new version of the application every minute until all traffic is shifted.<br><br>Which configuration should the DevOps engineer add in the CloudFormation template to meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an AppSpec file with the CodeDeployDefault.ECSLinearl OPercentEveryl Minutes deployment configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the AWS::CodeDeployBlueGreen transform and the AWS::CodeDeploy::BlueGreen hook parameter with the CodeDeployDefault.ECSLinear10PercentEvery1Minutes deployment configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an AppSpec file with the ECSCanary10Percent5Minutes deployment configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the AWS::CodeDeployBlueGreen transform and the AWS::CodeDepioy::BlueGreen hook parameter with the ECSCanary10Percent5Minutes deployment configuration."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T07:02:00.000Z",
        "voteCount": 2,
        "content": "B for me"
      },
      {
        "date": "2024-07-14T02:11:00.000Z",
        "voteCount": 3,
        "content": "obviously B"
      },
      {
        "date": "2024-07-07T12:30:00.000Z",
        "voteCount": 1,
        "content": "--&gt; BB"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 247,
    "url": "https://www.examtopics.com/discussions/amazon/view/143371-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an organization in AWS Organizations to manage its AWS accounts. The company's DevOps team has developed an AWS Lambda function that calls the Organizations API to create new AWS accounts.<br><br>The Lambda function runs in the organization's management account. The DevOps team needs to move the Lambda function from the management account to a dedicated AWS account. The DevOps team must ensure that the Lambda function has the ability to create new AWS accounts only in Organizations before the team deploys the Lambda function to the new account.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the management account, create a new IAM role that has the necessary permission to create new accounts in Organizations. Allow the role to be assumed by the Lambda execution role in the new AWS account. Update the Lambda function code to assume the role when the Lambda function creates new AWS accounts. Update the Lambda execution role to ensure that it has permission to assume the new role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the management account, turn on delegated administration for Organizations. Create a new delegation policy that grants the new AWS account permission to create new AWS accounts in Organizations. Ensure that the Lambda execution role has the organizations:CreateAccount permission.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the management account, create a new IAM role that has the necessary permission to create new accounts in Organizations. Allow the role to be assumed by the Lambda service principal. Update the Lambda function code to assume the role when the Lambda function creates new AWS accounts. Update the Lambda execution role to ensure that it has permission to assume the new role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the management account, enable AWS Control Tower. Turn on delegated administration for AWS Control Tower. Create a resource policy that allows the new AWS account to create new AWS accounts in AWS Control Tower. Update the Lambda function code to use the AWS Control Tower API in the new AWS account. Ensure that the Lambda execution role has the controltower:CreateManagedAccount permission."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T07:03:00.000Z",
        "voteCount": 2,
        "content": "A for me"
      },
      {
        "date": "2024-07-30T23:48:00.000Z",
        "voteCount": 4,
        "content": "Create an IAM Role with Necessary Permissions: \n- In the management account, create an IAM role with permissions to call the AWS Organizations API for creating new accounts.\n\nAllow Role Assumption: - Configure this IAM role to be assumable by the Lambda execution role in the new AWS account. This way, the Lambda function in the new account can assume the role to gain the necessary permissions.\n\nUpdate Lambda Function and Execution Role: \n- Modify the Lambda function code in the new account to assume the role created in the management account when it needs to create new AWS accounts. Also, ensure the Lambda execution role in the new account has the permissions required to assume the role in the management account."
      },
      {
        "date": "2024-07-30T23:49:00.000Z",
        "voteCount": 3,
        "content": "Why not following options:\nB: Delegated administration in AWS Organizations typically refers to giving permissions to manage AWS Organizations itself, rather than delegating permissions to create new accounts. Creating new accounts via the Organizations API requires specific IAM permissions, not just a delegation policy.\n\nC: Allowing the Lambda service principal to assume an IAM role is not a valid approach for cross-account role assumption. Lambda functions assume roles that are explicitly allowed by their execution role, not service principals.\n\nD: AWS Control Tower manages accounts and governance but requires different permissions and APIs compared to AWS Organizations for creating new accounts. Control Tower also does not directly handle account creation in the way described; instead, it manages accounts and governance at a higher level."
      },
      {
        "date": "2024-07-14T02:18:00.000Z",
        "voteCount": 3,
        "content": "- Create IAM Role in Management Account: include actions like \"organizations:CreateAccount\"\n- Allow Role Assumption: specifying the ARN of the Lambda execution role in the new account in the trust policy of the IAM role.\n- Using the AWS SDK to assume the role and get temporary credentials in Lambda's code\n- Ensure that the Lambda execution role in the new account has the necessary permissions to assume the IAM role created in the management account."
      },
      {
        "date": "2024-07-07T12:32:00.000Z",
        "voteCount": 1,
        "content": "---&gt; A"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 248,
    "url": "https://www.examtopics.com/discussions/amazon/view/143518-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has deployed an application in a single AWS Region. The application backend uses Amazon DynamoDB tables and Amazon S3 buckets.<br><br>The company wants to deploy the application in a secondary Region. The company must ensure that the data in the DynamoDB tables and the S3 buckets persists across both Regions. The data must also immediately propagate across Regions.<br><br>Which solution will meet these requirements with the MOST operational efficiency?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement two-way S3 bucket replication between the primary Region's S3 buckets and the secondary Region\u2019s S3 buckets. Convert the DynamoDB tables into global tables. Set the secondary Region as the additional Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement S3 Batch Operations copy jobs between the primary Region and the secondary Region for all S3 buckets. Convert the DynamoDB tables into global tables. Set the secondary Region as the additional Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement two-way S3 bucket replication between the primary Region's S3 buckets and the secondary Region's S3 buckets. Enable DynamoDB streams on the DynamoDB tables in both Regions. In each Region, create an AWS Lambda function that subscribes to the DynamoDB streams. Configure the Lambda function to copy new records to the DynamoDB tables in the other Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement S3 Batch Operations copy jobs between the primary Region and the secondary Region for all S3 buckets. Enable DynamoDB streams on the DynamoDB tables in both Regions. In each Region, create an AWS Lambda function that subscribes to the DynamoDB streams. Configure the Lambda function to copy new records to the DynamoDB tables in the other Region."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T07:03:00.000Z",
        "voteCount": 2,
        "content": "A fo rme"
      },
      {
        "date": "2024-07-30T23:52:00.000Z",
        "voteCount": 3,
        "content": "Two-Way S3 Bucket Replication: \n- While two-way replication is not typically needed for most scenarios (one-way replication is generally sufficient), for this requirement, if both regions need to have copies of data and keep them synchronized, you would implement replication rules to ensure data consistency across S3 buckets in different regions.\n\nGlobal DynamoDB Tables: \n- DynamoDB global tables are designed specifically for multi-Region, fully replicated tables. When you convert your DynamoDB tables into global tables and add the secondary Region, DynamoDB handles the replication of data across Regions automatically and immediately. This provides efficient and consistent data replication without requiring custom solutions."
      },
      {
        "date": "2024-07-30T23:53:00.000Z",
        "voteCount": 1,
        "content": "why not following options:\nB: S3 Batch Operations are used for bulk operations and are not suitable for continuous synchronization. DynamoDB streams and Lambda functions are also not necessary when using DynamoDB global tables, as global tables automatically manage replication.\n\nC: Two-way S3 bucket replication is complex and typically unnecessary. Using DynamoDB streams and Lambda functions for replication can be operationally intensive and error-prone compared to using global tables.\n\nD: Similar to option C, using S3 Batch Operations and DynamoDB streams with Lambda functions involves more operational overhead and complexity compared to using DynamoDB global tables."
      },
      {
        "date": "2024-07-30T23:54:00.000Z",
        "voteCount": 1,
        "content": "keywords: \n- data must also immediately propagate across Regions.\n- MOST operational efficiency?"
      },
      {
        "date": "2024-07-14T02:21:00.000Z",
        "voteCount": 4,
        "content": "B &amp; D. S3 Batch Operations copy jobs are not immediate and are typically used for bulk copying of data. They do not provide the immediacy required for data propagation across regions.\nC. DynamoDB streams with AWS Lambda functions to replicate data introduces additional complexity and operational overhead."
      },
      {
        "date": "2024-07-07T12:33:00.000Z",
        "voteCount": 1,
        "content": "---&gt; A"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 249,
    "url": "https://www.examtopics.com/discussions/amazon/view/143519-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has configured Amazon RDS storage autoscaling for its RDS DB instances. A DevOps team needs to visualize the autoscaling events on an Amazon CloudWatch dashboard.<br><br>Which solution will meet this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to RDS storage autoscaling events from RDS events. Create an AWS Lambda function that publishes a CloudWatch custom metric. Configure the EventBridge rule to invoke the Lambda function. Visualize the custom metric by using the CloudWatch dashboard.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trail by using AWS CloudTrail with management events configured. Configure the trail to send the management events to Amazon CloudWatch Logs. Create a metric filter in CloudWatch Logs to match the RDS storage autoscaling events. Visualize the metric filter by using the CloudWatch dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that reacts to RDS storage autoscaling events from the RDS events. Create a CloudWatch alarm. Configure the EventBridge rule to change the status of the CloudWatch alarm. Visualize the alarm status by using the CloudWatch dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a trail by using AWS CloudTrail with data events configured. Configure the trail to send the data events to Amazon CloudWatch Logs. Create a metric filter in CloudWatch Logs to match the RDS storage autoscaling events. Visualize the metric filter by using the CloudWatch dashboard."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-19T07:46:00.000Z",
        "voteCount": 2,
        "content": "This question is tricky as both A and B are correct\nIf it said in near real time then I would have chosen A, but it didn't so I am saying B as it is less complex and doesn't require writing a lambda function to use custom metrics.\n\nHonestly on this question flip a coin and choose one or the other"
      },
      {
        "date": "2024-08-20T07:03:00.000Z",
        "voteCount": 2,
        "content": "Voting for A"
      },
      {
        "date": "2024-08-20T07:03:00.000Z",
        "voteCount": 1,
        "content": "A for me"
      },
      {
        "date": "2024-07-30T23:57:00.000Z",
        "voteCount": 3,
        "content": "keywords: \n- Amazon EventBridge rule \n- CloudWatch custom metric"
      },
      {
        "date": "2024-07-14T02:25:00.000Z",
        "voteCount": 3,
        "content": "While CloudTrail can capture RDS events and send them to CloudWatch Logs, creating a metric filter in CloudWatch Logs is more complex and indirect compared to using EventBridge and a Lambda function to publish custom metrics directly to CloudWatch."
      },
      {
        "date": "2024-07-07T12:35:00.000Z",
        "voteCount": 1,
        "content": "---&gt; A"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 250,
    "url": "https://www.examtopics.com/discussions/amazon/view/143370-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses containers for its applications. The company learns that some container images are missing required security configurations.<br><br>A DevOps engineer needs to implement a solution to create a standard base image. The solution must publish the base image weekly to the us-west-2 Region, us-east-2 Region, and eu-central-1 Region.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an EC2 Image Builder pipeline that uses a container recipe to build the image. Configure the pipeline to distribute the image to an Amazon Elastic Container Registry (Amazon ECR) repository in us-west-2. Configure ECR replication from us-west-2 to us-east-2 and from us-east-2 to eu-central-1. Configure the pipeline to run weekly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodePipeline pipeline that uses an AWS CodeBuild project to build the image. Use AWS CodeDeploy to publish the image to an Amazon Elastic Container Registry (Amazon ECR) repository in us-west-2. Configure ECR replication from us-west-2 to us-east-2 and from us-east-2 to eu-central-1. Configure the pipeline to run weekly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an EC2 Image Builder pipeline that uses a container recipe to build the image. Configure the pipeline to distribute the image to Amazon Elastic Container Registry (Amazon ECR) repositories in all three Regions. Configure the pipeline to run weekly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodePipeline pipeline that uses an AWS CodeBuild project to build the image. Use AWS CodeDeploy to publish the image to Amazon Elastic Container Registry (Amazon ECR) repositories in all three Regions. Configure the pipeline to run weekly."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-27T02:14:00.000Z",
        "voteCount": 1,
        "content": "EC2 Image Builder is for AMIs not container images that go to ECR...\n\n\"A replication action only occurs once per image push. For example, if you configured cross-Region replication from us-west-2 to us-east-1 and from us-east-1 to us-east-2, an image pushed to us-west-2 replicates to only us-east-1, it doesn't replicate again to us-east-2. This behavior applies to both cross-Region and cross-account replication.\"\n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/replication.html"
      },
      {
        "date": "2024-09-27T02:18:00.000Z",
        "voteCount": 1,
        "content": "Ehh nvm it might be C"
      },
      {
        "date": "2024-07-31T00:01:00.000Z",
        "voteCount": 2,
        "content": "EC2 Image Builder: \n- This service is designed to automate the creation and distribution of container images. It supports defining container recipes and automating the build process.\nDirect Distribution: \n- EC2 Image Builder can be configured to distribute the images directly to multiple ECR repositories across different regions. This aligns with the requirement of publishing the base image to the us-west-2, us-east-2, and eu-central-1 regions.\nWeekly Schedule: \n- EC2 Image Builder can be scheduled to run on a weekly basis, meeting the requirement for regular updates."
      },
      {
        "date": "2024-07-31T00:02:00.000Z",
        "voteCount": 1,
        "content": "Why not following Options:\nOptions A and B involve using ECR replication, which adds extra complexity and is not as streamlined as direct distribution through EC2 Image Builder. \nOption D suggests using AWS CodePipeline and AWS CodeDeploy, which is less specialized for container image building and distribution compared to EC2 Image Builder."
      },
      {
        "date": "2024-07-15T06:22:00.000Z",
        "voteCount": 4,
        "content": "---&gt; C\nYou can distributes container image to Amazon ECR repository in multiple regions.\n---\nhttps://docs.aws.amazon.com/imagebuilder/latest/userguide/cr-upd-container-distribution-settings.html"
      },
      {
        "date": "2024-07-11T19:26:00.000Z",
        "voteCount": 1,
        "content": "VOTE A"
      },
      {
        "date": "2024-07-11T19:21:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/ko_kr/imagebuilder/latest/userguide/manage-distribution-settings.html"
      },
      {
        "date": "2024-07-10T10:32:00.000Z",
        "voteCount": 2,
        "content": "A replication action only occurs once per image push. For example, if you configured cross-Region replication from us-west-2 to us-east-1 and from us-east-1 to us-east-2, an image pushed to us-west-2 replicates to only us-east-1, it doesn't replicate again to us-east-2. This behavior applies to both cross-Region and cross-account replication."
      },
      {
        "date": "2024-07-06T15:14:00.000Z",
        "voteCount": 1,
        "content": "B\nhttps://aws.amazon.com/blogs/containers/cross-region-replication-in-amazon-ecr-has-landed/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 251,
    "url": "https://www.examtopics.com/discussions/amazon/view/143395-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer needs to implement a solution to install antivirus software on all the Amazon EC2 instances in an AWS account. The EC2 instances run the most recent version of Amazon Linux.<br><br>The solution must detect all instances and must use an AWS Systems Manager document to install the software if the software is not present.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an association in Systems Manager State Manager. Target all the managed nodes. Include the software in the association. Configure the association to use the Systems Manager document.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up AWS Config to record all the resources in the account. Create an AWS Config custom rule to determine if the software is installed on all the EC2 instances. Configure an automatic remediation action that uses the Systems Manager document for noncompliant EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivate Amazon EC2 scanning on Amazon Inspector to determine if the software is installed on all the EC2 instances. Associate the findings with the Systems Manager document.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that uses AWS CloudTrail to detect the Runinstances API call. Configure inventory collection in Systems Manager Inventory to determine if the software is installed on the EC2 instances. Associate the Systems Manager inventory with the Systems Manager document."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-26T16:40:00.000Z",
        "voteCount": 1,
        "content": "Given the requirement to detect instances and use an SSM document for installation, Option B seems most appropriate. It combines AWS Config for detection and Systems Manager for remediation."
      },
      {
        "date": "2024-07-31T00:36:00.000Z",
        "voteCount": 1,
        "content": "AWS Systems Manager State Manager:\nAutomatic Detection: \n- State Manager allows you to manage the desired state of your AWS resources, including EC2 instances. By targeting all managed nodes, you ensure that every EC2 instance under Systems Manager's management is included in the scope.\nSoftware Installation: \n- You can specify a Systems Manager document (SSM document) to define the steps required to install the antivirus software. The association will ensure that the software is installed on any instances where it is missing.\nContinuous Compliance: \n- State Manager can continuously enforce the desired state, which means it will periodically check for the presence of the software and reapply the document if necessary."
      },
      {
        "date": "2024-07-31T00:36:00.000Z",
        "voteCount": 1,
        "content": "Use Case Alignment:\nManaged Nodes Targeting: \n- This allows for broad application across all instances, ensuring that no instances are missed, as long as they are configured as managed instances.\nEase of Configuration: \n- Setting up an association in State Manager is straightforward and integrates well with the existing AWS Systems Manager services, making it a robust choice for managing configurations across instances."
      },
      {
        "date": "2024-07-24T01:50:00.000Z",
        "voteCount": 1,
        "content": "State Manager associations\nA State Manager association is a configuration that you assign to your AWS resources. The configuration defines the state that you want to maintain on your resources. For example, an association can specify that antivirus software must be installed and running on a managed node, or that certain ports must be closed.\n\nAn association specifies a schedule for when to apply the configuration and the targets for the association. For example, an association for antivirus software might run once a day on all managed nodes in an AWS account. If the software isn't installed on a node, then the association could instruct State Manager to install it. If the software is installed, but the service isn't running, then the association could instruct State Manager to start the service."
      },
      {
        "date": "2024-07-17T06:29:00.000Z",
        "voteCount": 2,
        "content": "By creating an association, you can ensure that all instances have the antivirus software installed and kept up-to-date."
      },
      {
        "date": "2024-07-16T23:42:00.000Z",
        "voteCount": 1,
        "content": "---&gt; I'm between A &amp; D\nNot 100% sure about this but here are my 2 cents about DETECTING the instances that don't have the software installed:\n\nA - it's a bit tricky because it states that it targets all managed nodes - but what if there are other nodes that are not managed? It just assumes that all instances are managed by AWS Systems Manager \n\nB - How can Config determine if the software is installed?\n\nC - Amazon Inspector is focused on security assessments and compliance checks, not on ensuring software is installed. It would require additional setup and is not designed for direct software installation.\n\nD - it ensures that all instances are detected. It ensures that the installed software is tracked by using the AWS Systems Manager Inventory (which is designed for this kind of things). I'm not 100% sure about the phrase \"Associate the Systems Manager inventory with the Systems Manager document.\" which I don't believe its technically possible"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 252,
    "url": "https://www.examtopics.com/discussions/amazon/view/143880-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company needs to increase the security of the container images that run in its production environment. The company wants to integrate operating system scanning and programming language package vulnerability scanning for the containers in its CI/CD pipeline. The CI/CD pipeline is an AWS CodePipeline pipeline that includes an AWS CodeBuild build project, AWS CodeDeploy actions, and an Amazon Elastic Container Registry (Amazon ECR) repository.<br><br>A DevOps engineer needs to add an image scan to the CI/CD pipeline. The CI/CD pipeline must deploy only images without CRITICAL and HIGH findings into production.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ECR basic scanning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon ECR enhanced scanning.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon ECR to submit a Rejected status to the CI/CD pipeline when the image scan returns CRITICAL or HIGH findings.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge rule to invoke an AWS Lambda function when the image scan is completed. Configure the Lambda function to consume the Amazon Inspector scan status and to submit an Approved or Rejected status to the CI/CD pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Amazon EventBridge rule to invoke an AWS Lambda function when the image scan is completed. Configure the Lambda function to consume the Clair scan status and to submit an Approved or Rejected status to the CI/CD pipeline."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T00:44:00.000Z",
        "voteCount": 1,
        "content": "B. Use Amazon ECR Enhanced Scanning\n- Comprehensive Vulnerability Checks: Amazon ECR enhanced scanning is integrated with Amazon Inspector, providing thorough security checks on container images. It scans for both operating system vulnerabilities and application-level vulnerabilities in programming language packages, which basic scanning does not support.\n- Integration with Amazon Inspector: Enhanced scanning leverages Amazon Inspector for deeper vulnerability analysis, ensuring the images are secure before deployment.\n- CRITICAL and HIGH Severity Detection: The enhanced scanning option specifically identifies CRITICAL and HIGH vulnerabilities, aligning with the requirement to only deploy images that do not have these issues."
      },
      {
        "date": "2024-07-24T01:44:00.000Z",
        "voteCount": 1,
        "content": "All images pushed to Amazon ECR after enhanced scanning is turned on are continually scanned for the configured duration."
      },
      {
        "date": "2024-07-19T05:00:00.000Z",
        "voteCount": 3,
        "content": "---&gt; B D\n\nAs per documentation, basic scanning use CVEs from the open-source Clair project. Enhanced scanning is an integration with Amazon Inspector. This suggests both options use different database/scanners.\n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-enhanced.html\n\nhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning-basic.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 253,
    "url": "https://www.examtopics.com/discussions/amazon/view/143394-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's DevOps team manages a set of AWS accounts that are in an organization in AWS Organizations.<br><br>The company needs a solution that ensures that all Amazon EC2 instances use approved AM Is that the DevOps team manages. The solution also must remediate the usage of AMIs that are not approved. The individual account administrators must not be able to remove the restriction to use approved AMIs.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation StackSets to deploy an Amazon EventBridge rule to each account. Configure the rule to react to AWS CloudTrail events for Amazon EC2 and to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps team to the SNS topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS CloudFormation StackSets to deploy the approved-amis-by-id AWS Config managed rule to each account. Configure the rule with the list of approved AMIs. Configure the rule to run the AWS-StopEC2Instance AWS Systems Manager Automation runbook for the noncompliant EC2 instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that processes AWS CloudTrail events for Amazon EC2. Configure the Lambda function to send a notification to an Amazon Simple Notification Service (Amazon SNS) topic. Subscribe the DevOps team to the SNS topic. Deploy the Lambda function in each account in the organization. Create an Amazon EventBridge rule in each account. Configure the EventBridge rules to react to AWS CloudTrail events for Amazon EC2 and to invoke the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable AWS Config across the organization. Create a conformance pack that uses the approved-amis-by-id AWS Config managed rule with the list of approved AMIs. Deploy the conformance pack across the organization. Configure the rule to run the AWS-StopEC2lnstance AWS Systems Manager Automation runbook for the noncompliant EC2 instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-19T05:27:00.000Z",
        "voteCount": 3,
        "content": "---&gt; D"
      },
      {
        "date": "2024-07-14T02:49:00.000Z",
        "voteCount": 4,
        "content": "A &amp; C. only alert, not automatically remediate noncompliant instances\nB. deploy via CloudFormation StackSets to individual accounts can still allow account administrators to modify or remove the rules."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 254,
    "url": "https://www.examtopics.com/discussions/amazon/view/143393-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company gives its employees limited rights to AWS. DevOps engineers have the ability to assume an administrator role. For tracking purposes, the security team wants to receive a near-real-time notification when the administrator role is assumed.<br><br>How should this be accomplished?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure AWS Config to publish logs to an Amazon S3 bucket. Use Amazon Athena to query the logs and send a notification to the security team when the administrator role is assumed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Amazon GuardDuty to monitor when the administrator role is assumed and send a notification to the security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge event rule using an AWS Management Console sign-in events event pattern that publishes a message to an Amazon SNS topic if the administrator role is assumed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge events rule using an AWS API call that uses an AWS CloudTrail event pattern to invoke an AWS Lambda function that publishes a message to an Amazon SNS topic if the administrator role is assumed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T00:54:00.000Z",
        "voteCount": 1,
        "content": "Option D provides a robust and effective approach to tracking and alerting on the assumption of the administrator role by leveraging the power of AWS CloudTrail, Amazon EventBridge, AWS Lambda, and Amazon SNS.\n\nNot Option C as Incorrect Event Pattern: This option specifies monitoring AWS Management Console sign-in events, which are unrelated to the AssumeRole API call used when assuming a role programmatically. It wouldn't detect role assumptions made through CLI or SDKs."
      },
      {
        "date": "2024-07-26T23:05:00.000Z",
        "voteCount": 2,
        "content": "Vote D. \nA is not near-real-time solution. \nB. GuardDuty is designed for threat detection. not for monitoring role assuming. \nC. while C use the EventBridge, it monitoring console sign-in event only. rather than API call for assuming roles."
      },
      {
        "date": "2024-07-15T12:15:00.000Z",
        "voteCount": 1,
        "content": "---&gt; D"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 255,
    "url": "https://www.examtopics.com/discussions/amazon/view/143392-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company needs a strategy for failover and disaster recovery of its data and application. The application uses a MySQL database and Amazon EC2 instances. The company requires a maximum RPO of 2 hours and a maximum RTO of 10 minutes for its data and application at all times.<br><br>Which combination of deployment strategies will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora Single-AZ cluster in multiple AWS Regions as the data store. Use Aurora's automatic recovery capabilities in the event of a disaster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora global database in two AWS Regions as the data store. In the event of a failure, promote the secondary Region to the primary for the application. Update the application to use the Aurora cluster endpoint in the secondary Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Aurora cluster in multiple AWS Regions as the data store. Use a Network Load Balancer to balance the database traffic in different Regions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the application in two AWS Regions. Use Amazon Route 53 failover routing that points to Application Load Balancers in both Regions. Use health checks and Auto Scaling groups in each Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the application in two AWS Regions. Configure AWS Global Accelerator to point to Application Load Balancers (ALBs) in both Regions. Add both ALBs to a single endpoint group. Use health checks and Auto Scaling groups in each Region."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T00:56:00.000Z",
        "voteCount": 1,
        "content": "keywords: Amazon Aurora global database , Route 53"
      },
      {
        "date": "2024-07-15T12:12:00.000Z",
        "voteCount": 1,
        "content": "---&gt; BD"
      },
      {
        "date": "2024-07-14T02:54:00.000Z",
        "voteCount": 1,
        "content": "Only BD meets the requirements"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 256,
    "url": "https://www.examtopics.com/discussions/amazon/view/143391-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A developer is using the AWS Serverless Application Model (AWS SAM) to create a prototype for an AWS Lambda function. The AWS SAM template contains an AWS::Serverless::Function resource that has the CodeUri property that points to an Amazon S3 location. The developer wants to identify the correct commands for deployment before creating a CI/CD pipeline.<br><br>The developer creates an archive of the Lambda function code named package.zip. The developer uploads the .zip file archive to the S3 location specified in the CodeUri property. The developer runs the sam deploy command and deploys the Lambda function. The developer updates the Lambda function code and uses the same steps to deploy the new version of the Lambda function. The sam deploy command fails and returns an error of no changes to deploy.<br><br>Which solutions will deploy the new version? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the aws cloudformation update-stack command instead of the sam deploy command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the aws cloudformation update-stack-instances command instead of the sam deploy command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeUri property to reference the local application code folder. Use the sam deploy command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeUri property to reference the local application code folder. Use the aws cloudformation create-change-set command and the aws cloudformation execute-change-set command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeUri property to reference the local application code folder. Use the aws cloudformation package command and the aws cloudformation deploy command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T01:16:00.000Z",
        "voteCount": 1,
        "content": "Both Option C and Option E provide efficient and reliable methods to deploy updated Lambda function code using AWS SAM and CloudFormation. They address the deployment issue by ensuring that changes are recognized and appropriately handled, facilitating successful code updates in a CI/CD context.\n\nBy updating the CodeUri to reference the local folder, both approaches ensure that SAM or CloudFormation acknowledges code changes, effectively resolving the \"no changes to deploy\" error and enabling seamless deployments."
      },
      {
        "date": "2024-07-31T01:18:00.000Z",
        "voteCount": 1,
        "content": "Not A as Misfit for Code Changes:\n- aws cloudformation update-stack is primarily used for updating CloudFormation stack configurations, not for detecting code changes in deployment packages.\n- It relies on an already packaged and uploaded S3 file specified in the template. Since no template changes are detected, the command would not recognize code updates.\n\nNot B as Incorrect Command Context:\n- aws cloudformation update-stack-instances is designed for AWS CloudFormation StackSets, which are used to manage resources across multiple AWS accounts and regions, not for deploying Lambda functions or single stack updates.\n- This command is irrelevant to the deployment of a single Lambda function and won't address the issue at hand."
      },
      {
        "date": "2024-07-23T05:16:00.000Z",
        "voteCount": 2,
        "content": "The two correct solutions to deploy the new version of the Lambda function code when sam deploy reports no changes are:\n\nA. Use the aws cloudformation update-stack command instead of the sam deploy command.\nC. Update the CodeUri property to reference the local application code folder. Use the sam deploy command.\nThese approaches ensure that changes to your Lambda function code are correctly identified and deployed without encountering the \"no changes to deploy\" error."
      },
      {
        "date": "2024-07-14T03:02:00.000Z",
        "voteCount": 2,
        "content": "C. Update the CodeUri property to reference the local application code folder, AWS SAM will handle packaging and uploading the code to S3 during the \"sam deploy\" command execution.\nE.\n- \"aws cloudformation package\" command packages the local artifacts (such as Lambda function code) and uploads them to an S3 bucket. It then generates a CloudFormation template that references these artifacts.\n- \"aws cloudformation deploy\" command deploys the generated CloudFormation template.\n\nA. \"aws cloudformation update-stack\": without the packaging step, it won't recognize changes in the Lambda function code\nB. used for stack set instances\nD. without proper packaging of the local code, it may not detect changes correctly."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 257,
    "url": "https://www.examtopics.com/discussions/amazon/view/143792-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs its container workloads in AWS App Runner. A DevOps engineer manages the company's container repository in Amazon Elastic Container Registry (Amazon ECR).<br><br>The DevOps engineer must implement a solution that continuously monitors the container repository. The solution must create a new container image when the solution detects an operating system vulnerability or language package vulnerability.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse EC2 Image Builder to create a container image pipeline. Use Amazon ECR as the target repository. Turn on enhanced scanning on the ECR repository. Create an Amazon EventBridge rule to capture an Inspector? finding event. Use the event to invoke the image pipeline. Re-upload the container to the repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse EC2 Image Builder to create a container image pipeline. Use Amazon ECR as the target repository. Enable Amazon GuardDuty Malware Protection on the container workload. Create an Amazon EventBridge rule to capture a GuardDuty finding event. Use the event to invoke the image pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeBuild project to create a container image. Use Amazon ECR as the target repository. Turn on basic scanning on the repository. Create an Amazon EventBridge rule to capture an ECR image action event. Use the event to invoke the CodeBuild project. Re-upload the container to the repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS CodeBuild project to create a container image. Use Amazon ECR as the target repository. Configure AWS Systems Manager Compliance to scan all managed nodes. Create an Amazon EventBridge rule to capture a configuration compliance state change event. Use the event to invoke the CodeBuild project."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T01:20:00.000Z",
        "voteCount": 3,
        "content": "Keywords: Enhanced scanning, Amazon ECR, Amazon Inspector, vulnerabilities"
      },
      {
        "date": "2024-07-15T12:11:00.000Z",
        "voteCount": 3,
        "content": "---&gt; A"
      },
      {
        "date": "2024-07-14T03:05:00.000Z",
        "voteCount": 3,
        "content": "Enhanced scanning provides deep and comprehensive scanning for vulnerabilities in container images using Amazon Inspector."
      },
      {
        "date": "2024-07-12T06:28:00.000Z",
        "voteCount": 4,
        "content": "Turn on enhanced scanning in the Amazon ECR repository settings. This enables Amazon Inspector to scan images for vulnerabilities."
      },
      {
        "date": "2024-07-12T06:29:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/inspector/latest/user/scanning-ecr.html#:~:text=To%20configure%20your%20enhanced%20scanning%20settings&amp;text=Open%20the%20Amazon%20ECR%20console,registry%2C%20and%20then%20choose%20Settings."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 258,
    "url": "https://www.examtopics.com/discussions/amazon/view/143389-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company wants to use AWS Systems Manager documents to bootstrap physical laptops for developers. The bootstrap code is stored in GitHub. A DevOps engineer has already created a Systems Manager activation, installed the Systems Manager agent with the registration code, and installed an activation ID on all the laptops.<br><br>Which set of steps should be taken next?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Systems Manager document to use the AWS-RunShellScript command to copy the files from GitHub to Amazon S3, then use the aws-downloadContent plugin with a sourceType of S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Systems Manager document to use the aws-configurePackage plugin with an install action and point to the Git repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Systems Manager document to use the aws-downloadContent plugin with a sourceType of GitHub and sourceInfo with the repository details.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Systems Manager document to use the aws:softwareInventory plugin and run the script from the Git repository."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T01:24:00.000Z",
        "voteCount": 3,
        "content": "aws:downloadContent\n(Schema version 2.0 or later) Download SSM documents and scripts from remote locations. GitHub Enterprise repositories are not supported. This plugin is supported on Linux and Windows Server operating systems."
      },
      {
        "date": "2024-07-16T23:53:00.000Z",
        "voteCount": 3,
        "content": "---&gt; C"
      },
      {
        "date": "2024-07-14T08:52:00.000Z",
        "voteCount": 4,
        "content": "The aws-downloadContent plugin is specifically designed to download content from various sources, including GitHub.\nSetting the sourceType to GitHub and providing the repository details in sourceInfo to directly download the bootstrap code from GitHub to the laptops."
      },
      {
        "date": "2024-07-06T15:36:00.000Z",
        "voteCount": 1,
        "content": "c\nhttps://docs.aws.amazon.com/systems-manager/latest/userguide/documents-running-remote-github-s3.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 259,
    "url": "https://www.examtopics.com/discussions/amazon/view/143378-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's development team uses AWS CloudFormation to deploy its application resources. The team must use CloudFormation for all changes to the environment. The team cannot use the AWS Management Console or the AWS CLI to make manual changes directly.<br><br>The team uses a developer IAM role to access the environment. The role is configured with the AdministratorAccess managed IAM policy. The company has created a new CloudFormationDeployment IAM role that has the following policy attached:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image22.png\"><br><br>The company wants to ensure that only CloudFormation can use the new role. The development team cannot make any manual changes to the deployed resources.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to use the CloudFormationDeployment role as a CloudFormation service role when the developers deploy new stacks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the trust policy of the CloudFormationDeployment role to allow the developer IAM role to assume the CloudFormationDeployment role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the developer IAM role to be able to get and pass the CloudFormationDeployment role if iam:PassedToService equals . Configure the CloudFormationDeployment role to allow all cloudformation actions for all resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the trust policy of the CloudFormationDeployment role to allow the cloudformation.amazonaws.com AWS principal to perform the iam:AssumeRole action.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to assume the CloudFormationDeployment role when the developers deploy new stacks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an IAM policy to the CloudFormationDeployment role to allow cloudformation:* on all resources. Add a policy that allows the iam:PassRole action for the ARN of the CloudFormationDeployment role if iam:PassedToService equals cloudformation.amazonaws.com.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "ABD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-26T18:08:00.000Z",
        "voteCount": 1,
        "content": "While Option F seems reasonable at first glance, it has a potential issue. By allowing cloudformation:* on all resources, you grant broad permissions to CloudFormation actions, which may not align with the goal of restricting manual changes. It\u2019s essential to strike a balance between security and functionality.\n\nThe combination of Options A, B, and D ensures that only CloudFormation can assume the CloudFormationDeployment role, and the ReadOnlyAccess policy for developers prevents unintended modifications. This approach maintains a more controlled and secure environment."
      },
      {
        "date": "2024-07-31T01:31:00.000Z",
        "voteCount": 1,
        "content": "A. Remove the AdministratorAccess policy. Assign the ReadOnlyAccess managed IAM policy to the developer role. Instruct the developers to use the CloudFormationDeployment role as a CloudFormation service role when the developers deploy new stacks.\n\nD. Update the trust policy of the CloudFormationDeployment role to allow the cloudformation.amazonaws.com AWS principal to perform the iam:AssumeRole action.\n\nF. Add an IAM policy to the CloudFormationDeployment role to allow cloudformation:* on all resources. Add a policy that allows the iam:PassRole action for the ARN of the CloudFormationDeployment role if iam:PassedToService equals cloudformation.amazonaws.com."
      },
      {
        "date": "2024-07-31T01:32:00.000Z",
        "voteCount": 2,
        "content": "Why Other Options Are Less Suitable:\nNot Option B: Trust Policy with Developer IAM Role\n- Misaligned Trust Policy: Allowing the developer IAM role to assume the CloudFormationDeployment role would directly enable developers to assume the role, contradicting the requirement of not allowing them to make manual changes. This option bypasses the control we want to establish by having CloudFormation handle the role assumption.\n\nNot Option C: Conditional Role Passing\n- Incorrect Logic: While this option attempts to create a condition for passing the role, it does not align with using CloudFormation as the sole entity allowed to assume the role. It implies a developer-driven role assumption rather than a service-driven one."
      },
      {
        "date": "2024-07-31T01:32:00.000Z",
        "voteCount": 1,
        "content": "Not Option E: Developer Assumption of CloudFormationDeployment Role\n- Manual Role Assumption: Similar to option B, this option would allow developers to directly assume the CloudFormationDeployment role. It introduces the risk of developers bypassing CloudFormation for changes, which violates the requirement to prevent manual modifications."
      },
      {
        "date": "2024-07-16T23:58:00.000Z",
        "voteCount": 1,
        "content": "---&gt; A D F"
      },
      {
        "date": "2024-07-14T09:01:00.000Z",
        "voteCount": 2,
        "content": "A. ensures that developers cannot make manual changes to the environment.\nD.  ensures that only CloudFormation can assume this role.\nF. ensures that the role can only be passed to CloudFormation, not to any other service or user."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 260,
    "url": "https://www.examtopics.com/discussions/amazon/view/143377-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is developing a web application's infrastructure using AWS CloudFormation. The database engineering team maintains the database resources in a CloudFormation template, and the software development team maintains the web application resources in a separate CloudFormation template. As the scope of the application grows, the software development team needs to use resources maintained by the database engineering team. However, both teams have their own review and lifecycle management processes that they want to keep. Both teams also require resource-level change-set reviews. The software development team would like to deploy changes to this template using their CI/CD pipeline.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation nested stack to make cross-stack resource references and parameters available in both stacks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudFormation stack set to make cross-stack resource references and parameters available in both stacks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate input parameters in the web application CloudFormation template and pass resource names and IDs from the database stack."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T01:39:00.000Z",
        "voteCount": 2,
        "content": "A. Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template.\n- Decoupled Management: Each team retains its management process, aligning with their specific workflows.\n- Cross-Stack Referencing: Utilizes CloudFormation's Exports and Fn::ImportValue to reference resources between stacks efficiently.\n- Resource-Level Change Sets: Supports detailed change-set reviews, enabling teams to preview changes before deployment.\n- CI/CD Pipeline Compatibility: Works seamlessly with CI/CD pipelines by allowing modular updates to stacks without direct dependencies."
      },
      {
        "date": "2024-07-14T09:29:00.000Z",
        "voteCount": 4,
        "content": "B. Nested stacks combine all stacks into a single stack, which can complicate the independent lifecycle and review processes for each team.\nC. Stack sets are typically used for deploying stacks across multiple AWS accounts and Regions\nD. could work but require manual intervention to update the parameter values whenever there is a change in the database stack resources."
      },
      {
        "date": "2024-07-05T12:09:00.000Z",
        "voteCount": 3,
        "content": "A. Create a stack export from the database CloudFormation template and import those references into the web application CloudFormation template."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 261,
    "url": "https://www.examtopics.com/discussions/amazon/view/143893-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an organization in AWS Organizations. A DevOps engineer needs to maintain multiple AWS accounts that belong to different OUs in the organization. All resources, including IAM policies and Amazon S3 policies within an account, are deployed through AWS CloudFormation. All templates and code are maintained in an AWS CodeCommit repository. Recently, some developers have not been able to access an S3 bucket from some accounts in the organization.<br><br>The following policy is attached to the S3 bucket:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image23.png\"><br><br>What should the DevOps engineer do to resolve this access issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the S3 bucket policy. Turn off the S3 Block Public Access setting on the S3 bucket. In the S3 policy, add the aws:SourceAccount condition. Add the AWS account IDs of all developers who are experiencing the issue.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that no IAM permissions boundaries are denying developers access to the S3 bucket. Make the necessary changes to IAM permissions boundaries. Use an AWS Config recorder in the individual developer accounts that are experiencing the issue to revert any changes that are blocking access. Commit the fix back into the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an SCP that stops anyone from modifying IAM resources in developer OUs. In the S3 policy, add the aws:SourceAccount condition. Add the AWS account IDs of all developers who are experiencing the issue. Commit the fix back into the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that no SCP is blocking access for developers to the S3 bucket. Ensure that no IAM policy permissions boundaries are denying access to developer IAM users. Make the necessary changes to the SCP and IAM policy permissions boundaries in the CodeCommit repository. Invoke deployment through CloudFormation to apply the changes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T01:45:00.000Z",
        "voteCount": 3,
        "content": "- Comprehensive approach: Reviews both SCPs and IAM permissions boundaries that could block access.\n- Changes are committed to CodeCommit and deployed through CloudFormation, maintaining the required deployment pipeline.\n- By checking both SCPs and permissions boundaries, this solution covers potential organizational and account-level restrictions that could impact access."
      },
      {
        "date": "2024-07-15T12:08:00.000Z",
        "voteCount": 3,
        "content": "---&gt; D"
      },
      {
        "date": "2024-07-14T09:37:00.000Z",
        "voteCount": 4,
        "content": "Option D is the most comprehensive and aligns with the requirements:\n- It ensures that both SCPs and IAM policies are correctly configured.\n- It adheres to the use of CloudFormation for all changes.\n- It addresses the immediate issue while providing a scalable and manageable approach."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 262,
    "url": "https://www.examtopics.com/discussions/amazon/view/143894-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an organization in AWS Organizations for its multi-account environment. A DevOps engineer is developing an AWS CodeArtifact based strategy for application package management across the organization. Each application team at the company has its own account in the organization. Each application team also has limited access to a centralized shared services account.<br><br>Each application team needs full access to download, publish, and grant access to its own packages. Some common library packages that the application teams use must also be shared with the entire organization.<br><br><br>Which combination of steps will meet these requirements with the LEAST administrative overhead? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a domain in each application team's account. Grant each application team's account full read access and write access to the application team's domain.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a domain in the shared services account. Grant the organization read access and CreateRepository access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a repository in each application team\u2019s account. Grant each application team\u2019s account full read access and write access to its own repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a repository in the shared services account. Grant the organization read access to the repository in the shared services account Set the repository as the upstream repository in each application team's repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor teams that require shared packages, create resource-based policies that allow read access to the repository from other application teams' accounts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the other application teams' repositories as upstream repositories."
    ],
    "answer": "BCD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCD",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "BDE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-26T18:27:00.000Z",
        "voteCount": 1,
        "content": "While Option D is a valid approach, it introduces additional complexity by requiring each application team to set up their repositories with the shared services account as an upstream repository. This can lead to more administrative overhead and potential misconfigurations.\n\nIn contrast, Options B, C, and E provide a simpler and more direct way to achieve the desired outcome. By centralizing the domain in the shared services account, granting organization-wide access, and allowing resource-based policies for shared packages, you can efficiently manage package distribution without relying on individual repository configurations."
      },
      {
        "date": "2024-07-31T01:49:00.000Z",
        "voteCount": 3,
        "content": "B: Establish a centralized domain in the shared services account and provide organizational access to common libraries.\nD: Create a repository for common libraries in the shared services account, allow organization-wide read access, and configure upstream repositories.\nC: Create individual repositories in each team\u2019s account and grant full access to manage their own packages."
      },
      {
        "date": "2024-07-31T01:50:00.000Z",
        "voteCount": 1,
        "content": "Keywords: LEAST Administrative overhead\nWhy not following options: \nA: (Optional) Creating a domain in each team\u2019s account if they need isolated domains (generally, a centralized domain in the shared services account might be more efficient).\nE: (Optional) Configure resource-based policies for cross-account access if specific repositories need access by other teams' accounts beyond the shared services domain.\nF: (Optional) Set other repositories as upstream if required, though this may be redundant if the shared services account repository is already upstream."
      },
      {
        "date": "2024-07-17T00:28:00.000Z",
        "voteCount": 3,
        "content": "---&gt; BCD"
      },
      {
        "date": "2024-07-16T13:06:00.000Z",
        "voteCount": 1,
        "content": "I will go with BDE"
      },
      {
        "date": "2024-07-14T09:53:00.000Z",
        "voteCount": 3,
        "content": "B allows for centralized control and management of common packages, and the organization can easily access and create repositories within this domain.\nC ensures that each team has full control over their packages\nD allows all teams to access common packages"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 263,
    "url": "https://www.examtopics.com/discussions/amazon/view/143406-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company deploys an application to Amazon EC2 instances. The application runs Amazon Linux 2 and uses AWS CodeDeploy. The application has the following file structure for its code repository:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image24.png\"><br><br>The appspec.yml file has the following contents in the files section:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image25.png\"><br><br>What will the result be for the deployment of the config.txt file?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe config.txt file will be deployed to only /var/www/html/config/config.txt.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/config/config.txt.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe config.txt file will be deployed to only /usr/local/src/config.txt.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/application/web/config.txt."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T02:01:00.000Z",
        "voteCount": 3,
        "content": "The config/config.txt file will be copied to /usr/local/src/config.txt based on the first mapping.\nSince the entire source directory (/) is also mapped to /var/www/html, config/config.txt will also be copied to /var/www/html/config/config.txt."
      },
      {
        "date": "2024-07-20T08:22:00.000Z",
        "voteCount": 3,
        "content": "First Entry:\n\nsource: config/config.txt\ndestination: /usr/local/src/config.txt\nThis rule specifically copies the config/config.txt file from the source repository to /usr/local/src/config.txt on the EC2 instance.\n\nSecond Entry:\n\nsource: /\ndestination: /var/www/html\nThis rule copies the entire contents of the root directory of the repository to /var/www/html on the EC2 instance. This includes the config/config.txt file, so it will be copied to /var/www/html/config/config.txt.\n\nGiven these two rules:\n\nThe config/config.txt file will be copied to /usr/local/src/config.txt by the first rule.\nThe same file will also be copied to /var/www/html/config/config.txt due to the second rule.\nSo, the correct answer is indeed B. The config.txt file will be deployed to /usr/local/src/config.txt and to /var/www/html/config/config.txt."
      },
      {
        "date": "2024-07-18T04:39:00.000Z",
        "voteCount": 3,
        "content": "---&gt; B"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 264,
    "url": "https://www.examtopics.com/discussions/amazon/view/143405-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has set up AWS CodeArtifact repositories with public upstream repositories. The company's development team consumes open source dependencies from the repositories in the company's internal network.<br><br>The company's security team recently discovered a critical vulnerability in the most recent version of a package that the development team consumes. The security team has produced a patched version to fix the vulnerability. The company needs to prevent the vulnerable version from being downloaded. The company also needs to allow the security team to publish the patched version.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the status of the affected CodeArtifact package version to unlisted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the status of the affected CodeArtifact package version to deleted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the status of the affected CodeArtifact package version to archived.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeArtifact package origin control settings to allow direct publishing and to block upstream operations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeArtifact package origin control settings to block direct publishing and to allow upstream operations."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-13T23:01:00.000Z",
        "voteCount": 1,
        "content": "There is no delete version status - https://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status"
      },
      {
        "date": "2024-08-29T08:09:00.000Z",
        "voteCount": 1,
        "content": "you have to delete it not archive it"
      },
      {
        "date": "2024-08-26T18:48:00.000Z",
        "voteCount": 1,
        "content": "Option B: Update the status of the affected CodeArtifact package version to deleted. This action will prevent the vulnerable version from being accessible.\nOption D: Update the CodeArtifact package origin control settings to allow direct publishing and block upstream operations. This ensures that only the security team can publish the patched version directly."
      },
      {
        "date": "2024-08-20T07:08:00.000Z",
        "voteCount": 1,
        "content": "CD for me"
      },
      {
        "date": "2024-07-31T02:16:00.000Z",
        "voteCount": 2,
        "content": "C. Update the status of the affected CodeArtifact package version to archived.\n- Reason: Setting the package version status to Archived will prevent it from being downloaded while still retaining its metadata. This ensures that the vulnerable version cannot be accessed or used but allows you to track or potentially restore it later if needed.\n\nD. Update the CodeArtifact package origin control settings to allow direct publishing and to block upstream operations.\n- Reason: Allowing direct publishing and blocking upstream operations will enable the security team to publish the patched version directly to your repository without being blocked by upstream restrictions. This ensures that the patched version can be made available while preventing any interference from upstream repositories."
      },
      {
        "date": "2024-07-31T02:17:00.000Z",
        "voteCount": 1,
        "content": "Why not B as \"deleted\" is not a valid code artifact package version status\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status"
      },
      {
        "date": "2024-07-29T16:06:00.000Z",
        "voteCount": 3,
        "content": "A - unlisted does not prevent download\nB - deleted is not a valid code artifact package version status\nC- archived will prevent download\n\nhttps://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status"
      },
      {
        "date": "2024-07-19T03:39:00.000Z",
        "voteCount": 4,
        "content": "I had this question in my exam and checking what was the correct option for the package version led me here. C - archived seems to be the right one.\nA - unlisted will only remove the package version from the list of versions returned to package managers, but it WILL NOT prevent the download.\nB - deleted - it's not a valid package version status (https://docs.aws.amazon.com/codeartifact/latest/ug/packages-overview.html#package-version-status)\nC - archived - will block the package version download.\nD - Allow direct publishing will give the internal team permissions to upload the new version of the package \nE - block direct publishing means the package version are updated from external (public) repos \nMore on the packages origin control settings here: https://docs.aws.amazon.com/codeartifact/latest/ug/package-origin-controls.html"
      },
      {
        "date": "2024-07-15T07:12:00.000Z",
        "voteCount": 1,
        "content": "---&gt; BD"
      },
      {
        "date": "2024-07-14T10:19:00.000Z",
        "voteCount": 1,
        "content": "By allowing direct publishing, the security team can publish the patched version directly to the CodeArtifact repository. Blocking upstream operations ensures that only the patched version is available and prevents the vulnerable version from being pulled from the upstream repository."
      },
      {
        "date": "2024-07-13T12:33:00.000Z",
        "voteCount": 1,
        "content": "-----&gt; B,D"
      },
      {
        "date": "2024-07-11T21:09:00.000Z",
        "voteCount": 1,
        "content": "VOTE B,D"
      },
      {
        "date": "2024-07-06T15:52:00.000Z",
        "voteCount": 1,
        "content": "BE\nhttps://aws.amazon.com/blogs/devops/tighten-your-package-security-with-codeartifact-package-origin-control-toolkit/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 265,
    "url": "https://www.examtopics.com/discussions/amazon/view/143912-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is running a custom-built application that processes records. All the components run on Amazon EC2 instances that run in an Auto Scaling group. Each record's processing is a multistep sequential action that is compute-intensive. Each step is always completed in 5 minutes or less.<br><br>A limitation of the current system is that if any steps fail, the application has to reprocess the record from the beginning. The company wants to update the architecture so that the application must reprocess only the failed steps.<br><br>What is the MOST operationally efficient solution that meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a web application to write records to Amazon S3. Use S3 Event Notifications to publish to an Amazon Simple Notification Service (Amazon SNS) topic. Use an EC2 instance to poll Amazon SNS and start processing. Save intermediate results to Amazon S3 to pass on to the next step.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform the processing steps by using logic in the application. Convert the application code to run in a container. Use AWS Fargate to manage the container instances. Configure the container to invoke itself to pass the state from one step to the next.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a web application to pass records to an Amazon Kinesis data stream. Decouple the processing by using the Kinesis data stream and AWS Lambda functions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a web application to pass records to AWS Step Functions. Decouple the processing into Step Functions tasks and AWS Lambda functions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T02:27:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html"
      },
      {
        "date": "2024-07-31T02:28:00.000Z",
        "voteCount": 1,
        "content": "State Management and Fault Tolerance:\n- AWS Step Functions is a service that allows you to coordinate multiple AWS services into serverless workflows. It manages the state and execution of your tasks, ensuring that each step is completed in sequence.\nIf a step fails, Step Functions can retry the failed step or handle errors based on the defined retry policies or error catchers. This means that only the failed steps will be reprocessed, rather than starting from the beginning.\n\nDecoupling and Scalability:\n- Using AWS Lambda functions in combination with Step Functions allows you to run each step of the process as a separate Lambda function. This provides scalability and makes it easier to handle compute-intensive tasks as they can be distributed across multiple Lambda invocations."
      },
      {
        "date": "2024-07-31T02:28:00.000Z",
        "voteCount": 1,
        "content": "Operational Efficiency:\n- AWS Step Functions abstracts much of the complexity involved in managing the state and flow of tasks. This leads to a more operationally efficient solution, as you don't have to manually handle retries, error states, or intermediate results."
      },
      {
        "date": "2024-07-19T06:18:00.000Z",
        "voteCount": 1,
        "content": "---&gt; D"
      },
      {
        "date": "2024-07-14T19:45:00.000Z",
        "voteCount": 2,
        "content": "Step Functions and Lambda:\n- Decoupling Tasks\n- Error Handling and Retry Logic\n- State Management"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 266,
    "url": "https://www.examtopics.com/discussions/amazon/view/143404-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company is migrating its on-premises Windows applications and Linux applications to AWS. The company will use automation to launch Amazon EC2 instances to mirror the on-premises configurations. The migrated applications require access to shared storage that uses SMB for Windows and NFS for Linux.<br><br>The company is also creating a pilot light disaster recovery (DR) environment in another AWS Region. The company will use automation to launch and configure the EC2 instances in the DR Region. The company needs to replicate the storage to the DR Region.<br><br>Which storage solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 for the application storage. Create an S3 bucket in the primary Region and an S3 bucket in the DR Region. Configure S3 Cross-Region Replication (CRR) from the primary Region to the DR Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Elastic Block Store (Amazon EBS) for the application storage. Create a backup plan in AWS Backup that creates snapshots of the EBS volumes that are in the primary Region and replicates the snapshots to the DR Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Volume Gateway in AWS Storage Gateway for the application storage. Configure Cross-Region Replication (CRR) of the Volume Gateway from the primary Region to the DR Region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon FSx for NetApp ONTAP for the application storage. Create an FSx for ONTAP instance in the DR Region. Configure NetApp SnapMirror replication from the primary Region to the DR Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T02:35:00.000Z",
        "voteCount": 2,
        "content": "Amazon FSx for NetApp ONTAP support Multi-protocol access to data using the Network File System (NFS), Server Message Block (SMB), Internet Small Computer Systems Interface (iSCSI), and Non-Volatile Memory Express (NVMe) protocols\nhttps://docs.aws.amazon.com/fsx/latest/ONTAPGuide/what-is-fsx-ontap.html#features-overview"
      },
      {
        "date": "2024-07-19T06:18:00.000Z",
        "voteCount": 2,
        "content": "---&gt; D"
      },
      {
        "date": "2024-07-14T19:50:00.000Z",
        "voteCount": 3,
        "content": "NetApp ONTAP:\nMulti-Protocol Support:\n- SMB for Windows: Fully supports the SMB protocol required by your Windows applications.\n- NFS for Linux: Fully supports the NFS protocol required by your Linux applications.\n\nCross-Region Replication:\nNetApp SnapMirror: Provides efficient and reliable replication of data between ONTAP instances in different regions"
      },
      {
        "date": "2024-07-10T14:26:00.000Z",
        "voteCount": 3,
        "content": "Amazon FSx for NetApp ONTAP supports both SMB and NFS protocols, making it suitable for both Windows and Linux applications"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 267,
    "url": "https://www.examtopics.com/discussions/amazon/view/143403-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's application uses a fleet of Amazon EC2 On-Demand Instances to analyze and process data. The EC2 instances are in an Auto Scaling group. The Auto Scaling group is a target group for an Application Load Balancer (ALB). The application analyzes critical data that cannot tolerate interruption. The application also analyzes noncritical data that can withstand interruption.<br><br>The critical data analysis requires quick scalability in response to real-time application demand. The noncritical data analysis involves memory consumption. A DevOps engineer must implement a solution that reduces scale-out latency for the critical data. The solution also must process the noncritical data.<br><br>Which combination of steps will meet these requirements? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Define the warm pool size. Create a new version of the launch template that has detailed monitoring enabled. Use Spot Instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the critical data, modify the existing Auto Scaling group. Create a warm pool instance in the stopped state. Define the warm pool size. Create a new version of the launch template that has detailed monitoring enabled. Use On-Demand Instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the critical data, modify the existing Auto Scaling group. Create a lifecycle hook to ensure that bootstrap scripts are completed successfully. Ensure that the application on the instances is ready to accept traffic before the instances are registered. Create a new version of the launch template that has detailed monitoring enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the noncritical data, create a second Auto Scaling group that uses a launch template. Configure the launch template to install the unified Amazon CloudWatch agent and to configure the CloudWatch agent with a custom memory utilization metric. Use Spot Instances. Add the new Auto Scaling group as the target group for the ALB. Modify the application to use two target groups for critical data and noncritical data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the noncritical data, create a second Auto Scaling group. Choose the predefined memory utilization metric type for the target tracking scaling policy. Use Spot Instances. Add the new Auto Scaling group as the target group for the ALB. Modify the application to use two target groups for critical data and noncritical data."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T02:41:00.000Z",
        "voteCount": 3,
        "content": "Option B (For critical data): Creates a warm pool and ensures quick scaling with On-Demand Instances, addressing the need for low latency in scaling.\nOption D (For noncritical data): Uses Spot Instances with memory-based scaling policies to handle noncritical data efficiently."
      },
      {
        "date": "2024-08-05T00:25:00.000Z",
        "voteCount": 1,
        "content": "For D, Spot Instance, Using Cloudwatch with Custom Memory Utilization Metric\nhttps://aws.amazon.com/blogs/mt/create-amazon-ec2-auto-scaling-policy-memory-utilization-metric-linux/\n\nNot E as Auto Scaling does not provide predefined memory utilization."
      },
      {
        "date": "2024-07-19T06:19:00.000Z",
        "voteCount": 3,
        "content": "---&gt; B D"
      },
      {
        "date": "2024-07-14T20:00:00.000Z",
        "voteCount": 4,
        "content": "AWS Auto Scaling does not provide a predefined memory utilization metric type"
      },
      {
        "date": "2024-07-10T14:37:00.000Z",
        "voteCount": 1,
        "content": "On-Demand Instances: For critical data that cannot tolerate interruption, On-Demand Instances are reliable and provide the required stability without the risk of termination\n\nSpot Instances: Utilising Spot Instances for noncritical data processing can significantly reduce costs since these workloads can tolerate interruptions.\n\nThis combination ensures that the critical data analysis benefits from reduced scale-out latency and reliability, while noncritical data processing leverages cost-effective Spot Instances and is scaled based on memory usage."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 268,
    "url": "https://www.examtopics.com/discussions/amazon/view/143402-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company recently migrated its application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that uses Amazon EC2 instances. The company configured the application to automatically scale based on CPU utilization.<br><br>The application produces memory errors when it experiences heavy loads. The application also does not scale out enough to handle the increased load. The company needs to collect and analyze memory metrics for the application over time.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach the CloudWatchAgentServerPolicy managed IAM policy to the IAM instance profile that the cluster uses.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach the CloudWatchAgentServerPolicy managed IAM policy to a service account role for the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect performance metrics by deploying the unified Amazon CloudWatch agent to the existing EC2 instances in the cluster. Add the agent to the AMI for any new EC2 instances that are added to the cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCollect performance logs by deploying the AWS Distro for OpenTelemetry collector as a DaemonSet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAnalyze the pod_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the Service dimension.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAnalyze the node_memory_utilization Amazon CloudWatch metric in the ContainerInsights namespace by using the ClusterName dimension."
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CEF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-14T20:12:00.000Z",
        "voteCount": 7,
        "content": "A. This policy grants the necessary permissions for the Amazon CloudWatch agent to collect and publish metrics from the EC2 instances.\nC. The unified Amazon CloudWatch agent can collect both CPU and memory utilization metrics. Deploying it ensures you capture memory metrics across all EC2 instances in the EKS cluster.\nE. pod_memory_utilization metric provides detailed insights into memory usage at the pod level\n\nB. service account role is more relevant for applications running within Kubernetes pods needing AWS permissions.\nD irrelevant\nF Node-level metrics do not provide the granularity needed to diagnose pod-level memory issues effectively"
      },
      {
        "date": "2024-08-20T10:38:00.000Z",
        "voteCount": 2,
        "content": "vote for ACE"
      },
      {
        "date": "2024-07-31T03:01:00.000Z",
        "voteCount": 3,
        "content": "After check, feel Option A better\n- provides necessary permissions at the EC2 instance level, which is where the CloudWatch agent runs.\n- is directly suitable for metrics collection because it ensures that EC2 instances can send metrics to CloudWatch. The CloudWatch agent on the EC2 instances needs the IAM policy to push metrics and logs to CloudWatch.\n\nHope someone can explain further if choose option B instead of A."
      },
      {
        "date": "2024-08-06T19:47:00.000Z",
        "voteCount": 1,
        "content": "it seen BCE better as option B \n- attached the policy to EKS service account role for better Granular Control, Scalability and Management\n- this policy targets permissions at the Kubernetes level, granting specific pods or services within the cluster the ability to collect and send metrics."
      },
      {
        "date": "2024-07-17T19:18:00.000Z",
        "voteCount": 1,
        "content": "B - control permission with service account\nC - cloudwatch agent on k8s worker nodes\nE - monitoring with k8s service (pods)"
      },
      {
        "date": "2024-07-16T13:50:00.000Z",
        "voteCount": 1,
        "content": "I will go with B C E"
      },
      {
        "date": "2024-07-16T13:53:00.000Z",
        "voteCount": 1,
        "content": "B- Necessary permissions\nC- Cloud watch agent installed\nE - understanding performance and scaling of the application within Kubernetes Enviro"
      },
      {
        "date": "2024-07-12T17:14:00.000Z",
        "voteCount": 1,
        "content": "Answer : C E F"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 269,
    "url": "https://www.examtopics.com/discussions/amazon/view/143401-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company's video streaming platform usage has increased from 10,000 users each day to 50,000 users each day in multiple countries. The company deploys the streaming platform on Amazon Elastic Kubernetes Service (Amazon EKS). The EKS workload scales up to thousands of nodes during peak viewing time.<br><br>The company's users report occurrences of unauthorized logins. Users also report sudden interruptions and logouts from the platform.<br><br>The company wants additional security measures for the entire platform. The company also needs a summarized view of the resource behaviors and interactions across the company's entire AWS environment. The summarized view must show login attempts, API calls, and network traffic. The solution must permit network traffic analysis while minimizing the overhead of managing logs. The solution must also quickly investigate any potential malicious behavior that is associated with the EKS workload.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon GuardDuty for EKS Audit Log Monitoring. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log files in an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon Detective in the company's AWS account. Enable EKS audit logs from optional source packages in Detective.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon CloudWatch Container Insights. Enable AWS CloudTrail logs. Store the EKS audit logs and CloudTrail log files in an Amazon S3 bucket. Use Amazon Athena to create an external table. Use Amazon QuickSight to create a dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon GuardDuty for EKS Audit Log Monitoring. Enable Amazon CloudWatch Container Insights and VPC Flow Logs. Enable AWS CloudTrail logs."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T03:05:00.000Z",
        "voteCount": 1,
        "content": "Amazon Detective helps you quickly analyze and investigate security events across one or more AWS accounts by generating data visualizations that represent the ways your resources behave and interact over time. Detective creates visualizations of GuardDuty findings.\nhttps://docs.aws.amazon.com/guardduty/latest/ug/detective-integration.html\n\nAmazon EKS audit logs is an optional data source package that can be added to your Detective behavior graph.\nhttps://docs.aws.amazon.com/detective/latest/userguide/source-data-types-EKS.html"
      },
      {
        "date": "2024-07-16T14:02:00.000Z",
        "voteCount": 1,
        "content": "B- Guardduty any potential malicious behavior and Amazon Detective summarised view must show login attempts, API calls, and network traffic"
      },
      {
        "date": "2024-07-14T20:34:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/guardduty/latest/ug/detective-integration.html\nhttps://docs.aws.amazon.com/detective/latest/userguide/source-data-types-EKS.html"
      },
      {
        "date": "2024-07-11T20:59:00.000Z",
        "voteCount": 1,
        "content": "vote B"
      },
      {
        "date": "2024-07-06T16:04:00.000Z",
        "voteCount": 2,
        "content": "D\nhttps://aws.amazon.com/blogs/security/how-to-use-new-amazon-guardduty-eks-protection-findings/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 270,
    "url": "https://www.examtopics.com/discussions/amazon/view/143399-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Organizations to manage hundreds of AWS accounts. The company has a team that is responsible for AWS Identity and Access Management (IAM).<br><br>The IAM team wants to implement AWS IAM Identity Center (AWS Single Sign-On). The IAM team must have only the minimum needed permissions to manage IAM Identity Center. The IAM team must not be able to gain unneeded access to the Organizations management account. The IAM team must be able to provision new IAM Identity Center permission sets and assignments for existing and new member accounts.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS account for the IAM team. In the new account, enable IAM Identity Center. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new AWS account for the IAM team. In the Organizations management account, enable IAM Identity Center. In the Organizations management account, register the new account as a delegated administrator for IAM Identity Center.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set. Attach the AWSSSODirectoryAdministrator managed IAM policy to the group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set. Attach the AWSSSOMemberAccountAdministrator managed IAM policy to the group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the permission set to the Organizations management account. Allow the IAM team group to use the permission set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the permission set to the new AWS account. Allow the IAM team group to use the permission set.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADF",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "BCF",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "BDF",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-14T20:44:00.000Z",
        "voteCount": 5,
        "content": "A  ensures that the IAM team operates within their own account, isolating their permissions and activities from the Organizations management account.\nD provides the IAM team with the necessary permissions to manage IAM Identity Center across member accounts, without granting broader access.\n*Note that AWSSSODirectoryAdministrator policy grants broader permissions than necessary\nF. ensures that the IAM team has the necessary permissions within their designated account\n\nB. \"The IAM team must not be able to gain unneeded access to the Organizations management account\" =&gt; So B is wrong\nC contradicting the principle of least privilege.\nE should be avoided to prevent the IAM team from gaining unneeded access."
      },
      {
        "date": "2024-09-27T10:28:00.000Z",
        "voteCount": 1,
        "content": "C for sure.\n\nThe AWSSSOMemberAccountAdministrator policy provides required administrative actions to principals. The policy is intended for principals who perform the job role of an IAM Identity Center administrator.\n\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/security-iam-awsmanpol.html#security-iam-awsmanpol-AWSSSOMemberAccountAdministrator"
      },
      {
        "date": "2024-09-27T10:29:00.000Z",
        "voteCount": 1,
        "content": "This policy grants administrative permissions over IAM Identity Center users and groups. Principals with this policy attached can make any updates to IAM Identity Center users and groups."
      },
      {
        "date": "2024-09-27T10:30:00.000Z",
        "voteCount": 1,
        "content": "AWSSSODirectoryAdministrator is the policy described above"
      },
      {
        "date": "2024-09-12T14:04:00.000Z",
        "voteCount": 2,
        "content": "For B see this: https://aws.amazon.com/blogs/security/getting-started-with-aws-sso-delegated-administration/\nFor D: compare the two policies AWSSSODirectoryAdministrator does not grant managment of permission sets\nF: makes sure that the IAM team has the necessary permissions within their designated account"
      },
      {
        "date": "2024-08-26T19:20:00.000Z",
        "voteCount": 2,
        "content": "Option B: Create a new AWS account for the IAM team. In the Organizations management account, enable IAM Identity Center. Register the new account as a delegated administrator for IAM Identity Center. This ensures that the IAM team can manage IAM Identity Center without gaining access to the Organizations management account.\nOption C: In IAM Identity Center, create users and a group for the IAM team. Add the users to the group. Create a new permission set and attach the AWSSSODirectoryAdministrator managed IAM policy to the group. This allows the IAM team to provision new permission sets and assignments for member accounts.\nOption F: Assign the permission set to the new AWS account. Allow the IAM team group to use the permission set. This ensures that the IAM team can effectively manage IAM Identity Center in their dedicated account."
      },
      {
        "date": "2024-08-20T10:40:00.000Z",
        "voteCount": 2,
        "content": "Vote for ADF"
      },
      {
        "date": "2024-07-31T03:17:00.000Z",
        "voteCount": 3,
        "content": "I go for ADF\nOption A: This option creates a new account for IAM Identity Center management, separating it from the Organizations management account. This helps in maintaining the principle of least privilege and ensures that IAM Identity Center management is handled without direct access to broader organizational settings.\n\nOption D: The AWSSSOMemberAccountAdministrator policy provides comprehensive IAM Identity Center permissions needed for provisioning new permission sets and assignments across member accounts. This policy aligns well with the requirement to manage IAM Identity Center with full administrative capabilities. (Require to manage new and all member accounts)"
      },
      {
        "date": "2024-08-06T19:10:00.000Z",
        "voteCount": 1,
        "content": "keywords: \n- minimum needed permissions to manage IAM Identity Center. \n- unneeded access to the Organizations management account."
      },
      {
        "date": "2024-07-23T10:07:00.000Z",
        "voteCount": 2,
        "content": "AWSSSODirectoryAdministrator policy better\nAWSSSOMasterAccountAdministrator gives too much permissions\nhttps://docs.aws.amazon.com/singlesignon/latest/userguide/security-iam-awsmanpol.html\n\nB also because you want to enabled AWS IAM IDC in the management account and delegate administration to the IAM account"
      },
      {
        "date": "2024-07-15T07:02:00.000Z",
        "voteCount": 4,
        "content": "---&gt; ADF"
      },
      {
        "date": "2024-07-12T04:09:00.000Z",
        "voteCount": 3,
        "content": "B - This step is correct because it enables IAM Identity Center in the management account (which is necessary) and then delegates administration to a separate account for the IAM team. This approach follows the principle of least privilege by not giving the IAM team unnecessary access to the management account.\n\nC - This step is correct because it sets up the necessary users and groups in IAM Identity Center and assigns the appropriate permissions. The AWSSSODirectoryAdministrator policy provides the necessary permissions to manage IAM Identity Center without granting excessive privileges.\n\nF - This step completes the setup by assigning the permission set to the new account created for the IAM team, allowing them to perform their duties within that account rather than in the management account."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 271,
    "url": "https://www.examtopics.com/discussions/amazon/view/143435-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an organization in AWS Organizations that has all features enabled. The company uses AWS Backup in a primary account and uses an AWS Key Management Service (AWS KMS) key to encrypt the backups.<br><br>The company needs to automate a cross-account backup of the resources that AWS Backup backs up in the primary account. The company configures cross-account backup in the Organizations management account. The company creates a new AWS account in the organization and configures an AWS Backup backup vault in the new account. The company creates a KMS key in the new account to encrypt the backups. Finally, the company configures a new backup plan in the primary account. The destination for the new backup plan is the backup vault in the new account.<br><br>When the AWS Backup job in the primary account is invoked, the job creates backups in the primary account. However, the backups are not copied to the new account's backup vault.<br><br>Which combination of steps must the company take so that backups can be copied to the new account's backup vault? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the backup vault access policy in the new account to allow access to the primary account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the backup vault access policy in the primary account to allow access to the new account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the backup vault access policy in the primary account to allow access to the KMS key in the new account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the key policy of the KMS key in the primary account to share the key with the new account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the key policy of the KMS key in the new account to share the key with the primary account."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-27T16:25:00.000Z",
        "voteCount": 1,
        "content": "Option A: Edit the backup vault access policy in the new account to allow access to the primary account. This step ensures that the primary account has the necessary permissions to copy backups into the new account\u2019s backup vault.\n\nOption E: Edit the key policy of the KMS key in the new account to share the key with the primary account. This step allows the primary account to use the KMS key in the new account for encryption during the backup copy process"
      },
      {
        "date": "2024-08-20T10:45:00.000Z",
        "voteCount": 2,
        "content": "vote for AD"
      },
      {
        "date": "2024-07-31T10:56:00.000Z",
        "voteCount": 4,
        "content": "https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html\n\nIn your destination account, you must create a backup vault. Then, you assign a customer managed key to encrypt backups in the destination account, and a resource-based access policy to allow AWS Backup to access the resources you would like to copy. In the source account, if your resources are encrypted with a customer managed key, you must share this customer managed key with the destination account. You can then create a backup plan and choose a destination account that is part of your organizational unit in AWS Organizations."
      },
      {
        "date": "2024-07-31T03:35:00.000Z",
        "voteCount": 2,
        "content": "I prefer AE as\n1. the company need cross-account backup but not cross-account copy. \n2. And the KMS key created in new account for backup encryption. \nhighlighted keys: \n- The company configures cross-account backup in the Organizations management account. \n- The company creates a new AWS account in the organization and configures an AWS Backup backup vault in the new account. \n- The company creates a KMS key in the new account to encrypt the backups. \n- Finally, the company configures a new backup plan in the primary account. \n- The destination for the new backup plan is the backup vault in the new account."
      },
      {
        "date": "2024-07-31T03:45:00.000Z",
        "voteCount": 1,
        "content": "A. Edit the backup vault access policy in the new account to allow access from the primary account.\nE. Edit the key policy of the KMS key in the new account to share the key with the primary account.\n\nBackup Plan and Resource located in Management Account.\nBackup Vault and KMS Key located in new account. \n\nBased on URLs below, still confusing as the KMS key in new account (Destination) already\nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html\nhttps://repost.aws/knowledge-center/backup-troubleshoot-cross-account-copy\n\nHope someone choose option D can explain further why option D but not E."
      },
      {
        "date": "2024-08-05T01:33:00.000Z",
        "voteCount": 1,
        "content": "Seen like D correct \n- For the resources that aren't fully managed by AWS Backup, the backups use the same KMS key as the source resource.\n- For the resources that are fully managed by AWS Backup, the backups are encrypted with encryption key of the backup vault.\nhttps://repost.aws/knowledge-center/backup-troubleshoot-cross-account-copy"
      },
      {
        "date": "2024-07-23T10:34:00.000Z",
        "voteCount": 3,
        "content": "D - https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html#backup-cab-encryption\nDuring a cross-account copy, the source account KMS key policy must allow the destination account on the KMS key policy."
      },
      {
        "date": "2024-07-18T07:05:00.000Z",
        "voteCount": 3,
        "content": "A, D \nhttps://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html"
      },
      {
        "date": "2024-07-14T21:11:00.000Z",
        "voteCount": 4,
        "content": "A: Ensures the primary account can access the backup vault in the new account.\nE: Ensures the primary account can use the KMS key in the new account for encryption."
      },
      {
        "date": "2024-07-11T21:21:00.000Z",
        "voteCount": 1,
        "content": "VOTE AE"
      },
      {
        "date": "2024-07-06T10:16:00.000Z",
        "voteCount": 4,
        "content": "backup a backup using aws backup to backup account :) \nAD\nsecond paragraph: https://docs.aws.amazon.com/aws-backup/latest/devguide/create-cross-account-backup.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 272,
    "url": "https://www.examtopics.com/discussions/amazon/view/143418-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs an application that uses an Amazon S3 bucket to store images. A DevOps engineer needs to implement a multi-Region strategy for the objects that are stored in the S3 bucket. The company needs to be able to fail over to an S3 bucket in another AWS Region. When an image is added to either S3 bucket, the image must be replicated to the other S3 bucket within 15 minutes.<br><br>The DevOps engineer enables two-way replication between the S3 buckets.<br><br>Which combination of steps should the DevOps engineer take next to meet the requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable S3 Replication Time Control (S3 RTC) on each replication rule.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3 Multi-Region Access Point in an active-passive configuration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall the SubmitMultiRegionAccessPointRoutes operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable S3 Transfer Acceleration on both S3 buckets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a routing control in Amazon Route 53 Recovery Controller. Add the S3 buckets in an active-passive configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall the UpdateRoutingControlStates operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region."
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T06:32:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/getting-started/hands-on/getting-started-with-amazon-s3-multi-region-access-points/"
      },
      {
        "date": "2024-07-14T21:23:00.000Z",
        "voteCount": 2,
        "content": "UpdateRoutingControlStates is for Route53"
      },
      {
        "date": "2024-07-10T14:02:00.000Z",
        "voteCount": 2,
        "content": "A. Enable S3 Replication Time Control (S3 RTC) on each replication rule. This ensures that 99.99% of objects are replicated within 15 minutes.\n\nB. Create an S3 Multi-Region Access Point in an active-passive configuration. This is crucial for managing access to data across multiple S3 buckets in different AWS Regions and facilitating failover.\n\nC. Call the SubmitMultiRegionAccessPointRoutes operation in the AWS API when the company needs to fail over to the S3 bucket in the other Region. This operation updates the routes to direct traffic to the designated S3 bucket, enabling failover."
      },
      {
        "date": "2024-07-06T16:22:00.000Z",
        "voteCount": 3,
        "content": "ABC\nhttps://aws.amazon.com/getting-started/hands-on/getting-started-with-amazon-s3-multi-region-access-points/?ref=docs_gateway/amazons3/MultiRegionAccessPoints.html"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 273,
    "url": "https://www.examtopics.com/discussions/amazon/view/143414-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses the AWS Cloud Development Kit (AWS CDK) to define its application. The company uses a pipeline that consists of AWS CodePipeline and AWS CodeBuild to deploy the CDK application.<br><br>The company wants to introduce unit tests to the pipeline to test various infrastructure components. The company wants to ensure that a deployment proceeds if no unit tests result in a failure.<br><br>Which combination of steps will enforce the testing requirement in the pipeline? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeBuild build phase commands to run the tests then to deploy the application. Set the OnFailure phase property to ABORT.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeBuild build phase commands to run the tests then to deploy the application. Add the --rollback true flag to the cdk deploy command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the CodeBuild build phase commands to run the tests then to deploy the application. Add the --require-approval any-change flag to the cdk deploy command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a test that uses the AWS CDK assertions module. Use the template.hasResourceProperties assertion to test that resources have the expected properties.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a test that uses the cdk diff command. Configure the test to fail if any resources have changed."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-06T20:18:00.000Z",
        "voteCount": 2,
        "content": "Option A: &lt;OnFailure phase&gt;\n- This option involves configuring the build phase in CodeBuild to run tests before deployment. Setting the OnFailure phase property to ABORT ensures that if any test fails, the build process is stopped, and the deployment doesn't proceed.\n- This ensures that the build and deployment process only proceeds if the tests are successful, which is crucial for maintaining application integrity.\n\nOption D: &lt;CDK assertions&gt;\n- This option involves writing unit tests using the AWS CDK assertions module. The template.hasResourceProperties assertion checks if CloudFormation templates have resources with expected properties, ensuring the infrastructure's logical correctness.\n- This enforces unit testing by checking that the CloudFormation templates have the expected properties, ensuring the application\u2019s infrastructure is as intended."
      },
      {
        "date": "2024-07-14T21:31:00.000Z",
        "voteCount": 2,
        "content": "I'm not sure but A D seems right"
      },
      {
        "date": "2024-07-10T14:06:00.000Z",
        "voteCount": 2,
        "content": "A. This step is crucial because:\n\nIt integrates the unit tests into the build phase of CodeBuild.\nBy setting the OnFailure property to ABORT, it ensures that the pipeline stops if any tests fail, preventing deployment of potentially faulty infrastructure.\nIf all tests pass, the deployment will proceed as normal.\n\nD. This step is important because:\n\nIt utilizes the AWS CDK assertions module, which is specifically designed for testing CDK applications.\nThe template.hasResourceProperties assertion allows you to verify that the resources defined in your CDK code have the expected properties.\nThis type of test can catch issues in your infrastructure definition before deployment."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 274,
    "url": "https://www.examtopics.com/discussions/amazon/view/143413-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company has an application that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The EC2 instances are in multiple Availability Zones. The application was misconfigured in a single Availability Zone, which caused a partial outage of the application.<br><br>A DevOps engineer made changes to ensure that the unhealthy EC2 instances in one Availability Zone do not affect the healthy EC2 instances in the other Availability Zones. The DevOps engineer needs to test the application's failover and shift where the ALB sends traffic. During failover, the ALB must avoid sending traffic to the Availability Zone where the failure has occurred.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off cross-zone load balancing on the ALB. Use Amazon Route 53 Application Recovery Controller to start a zonal shift away from the Availability Zone.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off cross-zone load balancing on the ALB\u2019s target group. Use Amazon Route 53 Application Recovery Controller to start a zonal shift away from the Availability Zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Route 53 Application Recovery Controller resource set that uses the DNS hostname of the ALB. Start a zonal shift for the resource set away from the Availability Zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Route 53 Application Recovery Controller resource set that uses the ARN of the ALB\u2019s target group. Create a readiness check that uses the ElbV2TargetGroupsCanServeTraffic rule."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-16T05:51:00.000Z",
        "voteCount": 5,
        "content": "For me the correct answer is A:\"Note that the Elastic Load Balancing resources must have cross-zone load balancing turned off to use this capability.\"\nhttps://docs.aws.amazon.com/r53recovery/latest/dg/arc-zonal-shift.html"
      },
      {
        "date": "2024-07-28T08:35:00.000Z",
        "voteCount": 1,
        "content": "\"With Application Load Balancers, cross-zone load balancing is always turned on at the load balancer level, and cannot be turned off. For target groups, the default is to use the load balancer setting, but you can override the default by explicitly turning cross-zone load balancing off at the target group level.\"\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/application/disable-cross-zone.html#:~:text=The%20nodes%20for%20your%20load,in%20all%20registered%20Availability%20Zones."
      },
      {
        "date": "2024-09-14T00:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is B\nYou can not turn of cross-zone load balancing on an ALB: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html#cross-zone-load-balancing\n\nYou can turn it off at the Target Group: https://aws.amazon.com/about-aws/whats-new/2022/11/application-load-balancers-turning-off-cross-zone-load-balancing-per-target-group/\n\nzonal shift:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#cross-zone-load-balancing#:~:text=zonal%20shift"
      },
      {
        "date": "2024-08-27T16:45:00.000Z",
        "voteCount": 1,
        "content": "A is the most appropriate answer due to the reasons below;\n\nOption A: Turning off cross-zone load balancing on the ALB and using Amazon Route 53 Application Recovery Controller to start a zonal shift away from the affected Availability Zone ensures that traffic is not sent to the unhealthy instances in the problematic zone. This directly addresses the need to avoid sending traffic to the Availability Zone where the failure has occurred.\n\nOption D: While creating a resource set and readiness check with Amazon Route 53 Application Recovery Controller is useful for monitoring and ensuring traffic is routed to healthy instances, it doesn\u2019t explicitly mention turning off cross-zone load balancing, which is crucial for isolating the affected Availability Zone."
      },
      {
        "date": "2024-08-26T01:25:00.000Z",
        "voteCount": 1,
        "content": "Turning off cross-zone load balancing at the target group level ensures that each target group handles traffic independently for its specific Availability Zone. Why specifically target group is because cross-zone load balancing is always turned on and cannot be turned off for a ALB. But, if using target group, the default is to use the load balancer setting, which you can override the default by explicitly turning cross-zone load balancing off at the target group level.\n\nThe next part us to move away from the affected region by using Amazon Route 53 Application Recovery Controller. \n\nOption A works well for a ELB, but for ALB, you can't turn off cross-zone load balancing. \n\nOption C is good to handle DNS failover, but doesn't stop traffic from going to the affected zone. \n\nOption D doesn't handle moving of traffic away from the affected zone and add extra complexity by introducing a readiness check."
      },
      {
        "date": "2024-07-31T07:00:00.000Z",
        "voteCount": 1,
        "content": "- Turning off cross-zone load balancing on the ALB ensures that each Availability Zone only handles traffic directed to its own healthy instances, allowing for granular control over traffic distribution. This prevents the ALB from sending traffic to the unhealthy instances across all zones, thereby isolating the problem to the affected zone.\n\n- Amazon Route 53 Application Recovery Controller's Zonal Shifts is used to direct traffic away from a specific AZ that experiences a failure, allowing the ALB to reroute requests to healthy AZs automatically.\n\nAmazon Route 53 Application Recovery Controller currently supports the following resources for zonal shift and zonal autoshift:\n- Network Load Balancers with cross-zone load balancing disabled\n- Application Load Balancers with cross-zone load balancing disabled\nhttps://docs.aws.amazon.com/r53recovery/latest/dg/arc-zonal-shift.resource-types.html"
      },
      {
        "date": "2024-07-31T07:01:00.000Z",
        "voteCount": 1,
        "content": "B Incorrect: This option mentions turning off cross-zone load balancing on the ALB's target group rather than the ALB itself. This does not apply because cross-zone load balancing is a property of the ALB, not the target group. This makes the solution nonviable because the configuration settings described do not exist on target groups."
      },
      {
        "date": "2024-07-14T21:38:00.000Z",
        "voteCount": 2,
        "content": "A&amp;B Turn off cross-zone load balancing is a bad idea\nD involves creating a readiness check rule (ElbV2TargetGroupsCanServeTraffic) which checks the ability of the ALB\u2019s target groups to serve traffic. However, this does not directly control traffic routing based on Availability Zone health.\n\nC Route 53 Application Recovery Controller: It directly manages traffic based on health checks and allows for zonal shifts\nZonal Shift: Specifically addresses the requirement to avoid sending traffic to an Availability Zone experiencing issues"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 275,
    "url": "https://www.examtopics.com/discussions/amazon/view/143412-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company sends its AWS Network Firewall flow logs to an Amazon S3 bucket. The company then analyzes the flow logs by using Amazon Athena.<br><br>The company needs to transform the flow logs and add additional data before the flow logs are delivered to the existing S3 bucket.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function to transform the data and to write a new object to the existing S3 bucket. Configure the Lambda function with an S3 trigger for the existing S3 bucket. Specify all object create events for the event type. Acknowledge the recursive invocation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Amazon EventBridge notifications on the existing S3 bucket. Create a custom EventBridge event bus. Create an EventBridge rule that is associated with the custom event bus. Configure the rule to react to all object create events for the existing S3 bucket and to invoke an AWS Step Functions workflow. Configure a Step Functions task to transform the data and to write the data into a new S3 bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon EventBridge rule that is associated with the default EventBridge event bus. Configure the rule to react to all object create events for the existing S3 bucket. Define a new S3 bucket as the target for the rule. Create an EventBridge input transformation to customize the event before passing the event to the rule target.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Kinesis Data Firehose delivery stream that is configured with an AWS Lambda transformer. Specify the existing S3 bucket as the destination. Change the Network Firewall logging destination from Amazon S3 to Kinesis Data Firehose.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T07:06:00.000Z",
        "voteCount": 2,
        "content": "Amazon Kinesis Data Firehose:\nKinesis Data Firehose is designed for real-time streaming data delivery and transformation. It can ingest data, process it with a Lambda function, and deliver the transformed data to destinations like Amazon S3, Redshift, or Elasticsearch.\nhttps://aws.amazon.com/firehose/faqs/\n\nAWS Lambda Transformer:\nBy configuring a Lambda function as a transformer within Kinesis Data Firehose, you can implement custom logic to transform the flow logs and add any additional data required before the logs are written to the existing S3 bucket."
      },
      {
        "date": "2024-07-31T07:07:00.000Z",
        "voteCount": 1,
        "content": "keywords: transform, flow logs"
      },
      {
        "date": "2024-07-23T11:11:00.000Z",
        "voteCount": 2,
        "content": "D for me"
      },
      {
        "date": "2024-07-14T21:49:00.000Z",
        "voteCount": 3,
        "content": "D for me"
      },
      {
        "date": "2024-07-06T16:31:00.000Z",
        "voteCount": 2,
        "content": "Dhttps://aws.amazon.com/firehose/faqs/"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 276,
    "url": "https://www.examtopics.com/discussions/amazon/view/143411-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A DevOps engineer needs to implement integration tests into an existing AWS CodePipeline CI/CD workflow for an Amazon Elastic Container Service (Amazon ECS) service. The CI/CD workflow retrieves new application code from an AWS CodeCommit repository and builds a container image. The Cl/CD workflow then uploads the container image to Amazon Elastic Container Registry (Amazon ECR) with a new image tag version.<br><br>The integration tests must ensure that new versions of the service endpoint are reachable and that various API methods return successful response data. The DevOps engineer has already created an ECS cluster to test the service.<br><br>Which combination of steps will meet these requirements with the LEAST management overhead? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a deploy stage to the pipeline. Configure Amazon ECS as the action provider.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a deploy stage to the pipeline. Configure AWS CodeDeploy as the action provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an appspec.yml file to the CodeCommit repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the image build pipeline stage to output an imagedefinitions.json file that references the new image tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an AWS Lambda function that runs connectivity checks and API calls against the service. Integrate the Lambda function with CodePipeline by using a Lambda action stage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a script that runs integration tests against the service. Upload the script to an Amazon S3 bucket. Integrate the script in the S3 bucket with CodePipeline by using an S3 action stage."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-06T05:04:00.000Z",
        "voteCount": 5,
        "content": "ADE\nhttps://docs.aws.amazon.com/codepipeline/latest/userguide/ecs-cd-pipeline.html"
      },
      {
        "date": "2024-08-06T20:25:00.000Z",
        "voteCount": 2,
        "content": "keywords: LEAST management overhead\n\nA. Add a deploy stage to the pipeline. Configure Amazon ECS as the action provider.\n- Directly deploys the container image to ECS, ensuring the service is updated with the latest code without unnecessary complexity.\n\nD. Update the image build pipeline stage to output an image definitions.json file that references the new image tag.\n- Necessary for ECS to recognize and deploy the new image version, facilitating automated updates.\n\nE. Create an AWS Lambda function that runs connectivity checks and API calls against the service. Integrate the Lambda function with CodePipeline by using a Lambda action stage.\n- Provides a low-management solution for running integration tests, leveraging AWS Lambda's serverless capabilities."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 277,
    "url": "https://www.examtopics.com/discussions/amazon/view/143410-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company runs applications on Windows and Linux Amazon EC2 instances. The instances run across multiple Availability Zones in an AWS Region. The company uses Auto Scaling groups for each application.<br><br>The company needs a durable storage solution for the instances. The solution must use SMB for Windows and must use NFS for Linux. The solution must also have sub-millisecond latencies. All instances will read and write the data.<br><br>Which combination of steps will meet these requirements? (Choose three.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon Elastic File System (Amazon EFS) file system that has targets in multiple Availability Zones.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon FSx for NetApp ONTAP Multi-AZ file system.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume to use for shared storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the user data for each application\u2019s launch template to mount the file system.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform an instance refresh on each Auto Scaling group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the EC2 instances for each application to mount the file system when new instances are launched."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "BDF",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-27T17:07:00.000Z",
        "voteCount": 1,
        "content": "Option B: Create an Amazon FSx for NetApp ONTAP Multi-AZ file system. This provides high-performance storage with support for both SMB and NFS protocols.\n\nOption D: Update the user data for each application\u2019s launch template to mount the file system. This ensures that the file system is automatically mounted when new instances are launched.\n\nOption F: Update the EC2 instances for each application to mount the file system when new instances are launched. This ensures that all instances can read and write data to the file system"
      },
      {
        "date": "2024-08-20T10:36:00.000Z",
        "voteCount": 1,
        "content": "BDE for me"
      },
      {
        "date": "2024-07-31T07:42:00.000Z",
        "voteCount": 2,
        "content": "after review and check, BDE will be better\nOption E focuses on updating existing instances with the new configurations by replacing them with new instances based on the updated launch template. This is particularly useful when you want all instances, including those currently running, to immediately adhere to new configurations.\n\nB. Amazon FSx for NetApp ONTAP Multi-AZ file system.\n- SMB and NFS Support\n- Sub-Millisecond Latency\n- Multi-AZ Availability: ensures HA and fault tolerance, as the data is replicated across multiple Availability Zones, which aligns with the requirement for a durable storage solution.\n\nD. Update the user data for each application\u2019s launch template to mount the file system.\n- Automated Mounting the FSx file system at startup\n- Protocol-Specific Commands: For Windows instances, mount the SMB share, while for Linux instances, mount the NFS share"
      },
      {
        "date": "2024-07-31T07:43:00.000Z",
        "voteCount": 1,
        "content": "Not A. Amazon EFS as supports only NFS, not SMB support for Windows.\n\nNot C. Amazon EBS volume not natively support SMB or NFS, and EBS is block storage devices that attach to individual EC2 instances and do not support being shared directly across multiple instances."
      },
      {
        "date": "2024-07-31T07:47:00.000Z",
        "voteCount": 1,
        "content": "Not Option F \"Update the EC2 instances for each application to mount the file system when new instances are launched.\"\n- because this is more about ensuring that future instances have the correct configuration, without immediately affecting running instances. It sets the stage for consistency moving forward but does not address existing instances.\n- and future instances also take care by Option D"
      },
      {
        "date": "2024-07-31T07:27:00.000Z",
        "voteCount": 1,
        "content": "B. Amazon FSx for NetApp ONTAP Multi-AZ file system.\n- SMB and NFS Support\n- Sub-Millisecond Latency\n- Multi-AZ Availability: ensures HA and fault tolerance, as the data is replicated across multiple Availability Zones, which aligns with the requirement for a durable storage solution.\n\nD. Update the user data for each application\u2019s launch template to mount the file system.\n- Automated Mounting the FSx file system at startup\n- Protocol-Specific Commands: For Windows instances, mount the SMB share, while for Linux instances, mount the NFS share\n\nF. Update the EC2 instances for each application to mount the file system when new instances are launched.\n- Configuration Consistency: ensures that every new instance launched as part of the Auto Scaling group auto mounts the FSx file system.\n- Ease of Management: By automating the mounting process, you reduce the administrative overhead and potential for errors, ensuring a consistent and reliable setup."
      },
      {
        "date": "2024-07-31T07:30:00.000Z",
        "voteCount": 1,
        "content": "Not A. Amazon EFS as supports only NFS, not SMB support for Windows. \n\nNot C. Amazon EBS volume not natively support SMB or NFS, and EBS is block storage devices that attach to individual EC2 instances and do not support being shared directly across multiple instances. \n\nNot E, Perform an instance refresh on each Auto Scaling group.\n- Not Necessary for Mounting: An instance refresh primarily updates instances in the Auto Scaling group to use new configurations or launch templates. While useful in other scenarios, it doesn't directly relate to mounting the file system or affect the requirement for shared storage.\n- Redundant with Updated Launch Templates: If launch templates and user data scripts are updated correctly, new instances will automatically mount the file system, making a manual refresh unnecessary unless other updates are needed."
      },
      {
        "date": "2024-07-31T07:44:00.000Z",
        "voteCount": 1,
        "content": "Hi Moderator, please delete this comment, thank you."
      },
      {
        "date": "2024-07-15T06:23:00.000Z",
        "voteCount": 1,
        "content": "---&gt; BDE"
      },
      {
        "date": "2024-07-14T22:06:00.000Z",
        "voteCount": 2,
        "content": "BDE for me"
      },
      {
        "date": "2024-07-13T22:19:00.000Z",
        "voteCount": 4,
        "content": "To meet the requirements of the scenario, the company should take the following steps:\n\nCreate an Amazon FSx for NetApp ONTAP Multi-AZ file system (Option B): Amazon FSx for NetApp ONTAP supports both SMB (for Windows) and NFS (for Linux), and it provides sub-millisecond latencies. It also supports Multi-AZ configurations for high availability and durability.\nUpdate the user data for each application\u2019s launch template to mount the file system (Option D): This ensures that every new instance launched by the Auto Scaling group will have the file system mounted.\nPerform an instance refresh on each Auto Scaling group (Option E): This will update the existing instances with the new launch template configuration, ensuring that they have the file system mounted."
      },
      {
        "date": "2024-07-11T21:12:00.000Z",
        "voteCount": 2,
        "content": "VOTE BDF"
      },
      {
        "date": "2024-07-06T04:59:00.000Z",
        "voteCount": 2,
        "content": "BDE\nNetApp ONTAP for SMB and NFS at the same time"
      },
      {
        "date": "2024-07-11T12:18:00.000Z",
        "voteCount": 1,
        "content": "why not BDF?"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 278,
    "url": "https://www.examtopics.com/discussions/amazon/view/146254-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses an organization in AWS Organizations that a security team and a DevOps team manage. Both teams access the accounts by using AWS IAM Identity Center.<br><br>A dedicated group has been created for each team. The DevOps team's group has been assigned a permission set named DevOps. The permission set has the AdministratorAccess managed IAM policy attached. The permission set has been applied to all accounts in the organization.<br><br>The security team wants to ensure that the DevOps team does not have access to IAM Identity Center in the organization's management account. The security team has attached the following SCP to the organization root:<br><br><img src=\"https://img.examtopics.com/aws-certified-devops-engineer-professional-dop-c02/image26.png\"><br><br>After implementing the policy, the security team discovers that the DevOps team can still access IAM Identity Center.<br><br>Which solution will fix the problem?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, create a new OU. Move the organization's management account to the new OU. Detach the SCP from the organization root. Attach the SCP to the new OU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the organization's management account, update the SCP condition reference to the ARN of the DevOps team's group role to include the AWS account ID of the organization's management account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn IAM Identity Center, create a new permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. Update the assigned permission set for the DevOps team's group role in the organization's management account. Delete the SCP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn IAM Identity Center, update the DevOps permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. In the Deny statement, add a StringEquals condition that compares the aws:SourceAccount global condition context key with the organization's management account IDelete the SCP.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-29T07:20:00.000Z",
        "voteCount": 1,
        "content": "It's B according to chatGPT"
      },
      {
        "date": "2024-08-27T17:20:00.000Z",
        "voteCount": 1,
        "content": "Option D \n \nIn IAM Identity Center, update the DevOps permission set. Ensure that the assigned policy has full access but explicitly denies permission for the sso:* action and the sso-directory:* action. In the Deny statement, add a StringEquals condition that compares the aws:SourceAccount global condition context key with the organization\u2019s management account. Delete the SCP.\n\nThis approach ensures that the DevOps team retains necessary permissions while explicitly denying access to IAM Identity Center actions in the management account. Adding the StringEquals condition ensures that the policy is applied specifically to the management account, effectively preventing access."
      },
      {
        "date": "2024-08-25T22:41:00.000Z",
        "voteCount": 1,
        "content": "vote D"
      },
      {
        "date": "2024-08-21T07:20:00.000Z",
        "voteCount": 1,
        "content": "The right answer is A"
      },
      {
        "date": "2024-09-02T00:41:00.000Z",
        "voteCount": 1,
        "content": "Sorry, It's D"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 279,
    "url": "https://www.examtopics.com/discussions/amazon/view/146391-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "An Amazon EC2 Auto Scaling group manages EC2 instances that were created from an AMI. The AMI has the AWS Systems Manager Agent installed. When an EC2 instance is launched into the Auto Scaling group, tags are applied to the EC2 instance.<br><br>EC2 instances that are launched by the Auto Scaling group must have the correct operating system configuration.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Systems Manager Run Command document that configures the desired instance configuration. Set up Systems Manager Compliance to invoke the Run Command document when the EC2 instances are not in compliance with the most recent patches.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that runs immediately.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Systems Manager Run Command task that specifies the desired instance configuration. Create a maintenance window in Systems Manager Maintenance Windows that runs daily. Register the Run Command task against the maintenance window. Designate the targets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Systems Manager Patch Manager patch baseline and a patch group that use the same tags that the Auto Scaling group applies. Register the patch group with the patch baseline. Define a Systems Manager command document to patch the instances Invoke the document by using Systems Manager Run Command."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-27T17:26:00.000Z",
        "voteCount": 1,
        "content": "Option B: Create a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that runs immediately.\n\nHere\u2019s why:\n\nState Manager allows you to define and maintain consistent configuration of your instances. By creating an association that links to the command document, you can ensure that the desired configuration is applied as soon as the instances are launched.\nUsing a tag query ensures that the configuration is applied to the correct instances based on their tags, which are applied by the Auto Scaling group."
      },
      {
        "date": "2024-08-25T23:43:00.000Z",
        "voteCount": 1,
        "content": "State Manager ensures that all instances launched by the Auto Scaling group automatically receive the desired configuration which immediately applies when instances are launched.\n\nWhile the primary focus of Option D is on patch management (applying updates and patches) rather than configuring the overall state of the operating system. Using Run Command is very AD-hoc which means you need to time the launching of the EC2 instance with the command and might not provide the continuous assurance that the configuration is maintained."
      },
      {
        "date": "2024-08-24T08:19:00.000Z",
        "voteCount": 1,
        "content": "B.&nbsp;Create a Systems Manager State Manager association that links to the Systems Manager command document. Create a tag query that runs immediately."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 280,
    "url": "https://www.examtopics.com/discussions/amazon/view/146260-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company uses AWS Organizations to manage its AWS accounts. The organization root has a child OU that is named Department. The Department OU has a child OU that is named Engineering. The default FullAWSAccess policy is attached to the root, the Department OU, and the Engineering OU.<br><br>The company has many AWS accounts in the Engineering OU. Each account has an administrative IAM role with the AdministratorAccess IAM policy attached. The default FullAWSAccessPolicy is also attached to each account.<br><br>A DevOps engineer plans to remove the FullAWSAccess policy from the Department OU. The DevOps engineer will replace the policy with a policy that contains an Allow statement for all Amazon EC2 API operations.<br><br>What will happen to the permissions of the administrative 1AM roles as a result of this change?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll API actions on all resources will be allowed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll API actions on EC2 resources will be allowed. All other API actions will be denied.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll API actions on all resources will be denied.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll API actions on EC2 resources will be denied. All other API actions will be allowed."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-14T00:10:00.000Z",
        "voteCount": 1,
        "content": "The default FullAWSAccess policy is attached to the root, the Department OU, and the Engineering OU. So even if it is removed from the Department OU, it is still attached on the Engineering OU."
      },
      {
        "date": "2024-08-29T07:23:00.000Z",
        "voteCount": 1,
        "content": "I'ts B"
      },
      {
        "date": "2024-08-25T22:34:00.000Z",
        "voteCount": 1,
        "content": "vote B.."
      },
      {
        "date": "2024-08-25T18:19:00.000Z",
        "voteCount": 2,
        "content": "When the FullAWSAccess policy is replaced with a policy that allows only EC2 actions, this new SCP will act as a boundary. Even if an IAM role or user within the account has a broader permission set (like AdministratorAccess), the SCP limits what can be done."
      },
      {
        "date": "2024-08-21T08:03:00.000Z",
        "voteCount": 1,
        "content": "The answer is A \nStill, the root has attached a full access policy."
      },
      {
        "date": "2024-08-25T18:20:00.000Z",
        "voteCount": 1,
        "content": "Sorry the Answer: B\nWhen the FullAWSAccess policy is replaced with a policy that allows only EC2 actions, this new SCP will act as a boundary. Even if an IAM role or user within the account has a broader permission set (like AdministratorAccess), the SCP limits what can be done."
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  },
  {
    "topic": 1,
    "index": 281,
    "url": "https://www.examtopics.com/discussions/amazon/view/146217-exam-aws-certified-devops-engineer-professional-dop-c02/",
    "body": "A company manages AWS accounts in AWS Organizations. The company needs a solution to send Amazon CloudWatch Logs data to an Amazon S3 bucket in a dedicated AWS account. The solution must support all existing and future CloudWatch Logs log groups.<br><br>Which solution will meet these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Organizations backup policies to back up all log groups to a dedicated S3 bucket. Add an S3 bucket policy that allows access from all accounts that belong to the company.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all CloudWatch Logs log group resources to the backup plan. Create resource assignments in the backup plan for all accounts that belong to the company.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a backup plan in AWS Backup. Specify a dedicated S3 bucket as a backup vault. Assign all existing log groups to the backup plan. Create resource assignments in the backup plan for all accounts that belong to the company. Create an AWS Systems Manager Automation runbook to assign log groups to a backup plan. Create an AWS Config rule that has an automatic remediation action for all noncompliant log groups. Specify the runbook as the rule's target.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a CloudWatch Logs destination and an Amazon Kinesis Data Firehose delivery stream in the dedicated AWS account. Specify the S3 bucket as the destination of the delivery stream. Create subscription filters for all existing log groups in all accounts. Create an AWS Lambda function to call the CloudWatch Logs PutSubscriptionFilter API operation. Create an Amazon EventBridge rule to invoke the Lambda function when a CreateLogGroup event occurs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-14T00:21:00.000Z",
        "voteCount": 1,
        "content": "C seems like the most elegant solution, but I was not able to find any documentation that supports it.\n\nFor option D I found this - https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/SubscriptionFilters.html#FirehoseExample"
      },
      {
        "date": "2024-08-21T07:08:00.000Z",
        "voteCount": 1,
        "content": "Correct answer D"
      },
      {
        "date": "2024-08-21T01:23:00.000Z",
        "voteCount": 1,
        "content": "Correct answer D"
      }
    ],
    "examNameCode": "aws-certified-devops-engineer-professional-dop-c02",
    "topicNumber": "notopic"
  }
]