[
  {
    "topic": 2,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/amazon/view/4092-exam-aws-certified-big-data-specialty-topic-2-question-1/",
    "body": "An Amazon Redshift Database is encrypted using KMS. A data engineer needs to use the AWS CLI to create a KMS encrypted snapshot of the database in another AWS region.<br>Which three steps should the data engineer take to accomplish this task? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new KMS key in the destination region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the existing KMS key to the destination region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key from the source region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the source region, enable cross-region replication and specify the name of the copy grant created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the destination region, enable cross-region replication and specify the name of the copy grant created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse CreateSnapshotCopyGrant to allow Amazon Redshift to use the KMS key created in the destination region. ADF"
    ],
    "answer": "Explanation",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-23T23:02:00.000Z",
        "voteCount": 6,
        "content": "answer ADF"
      },
      {
        "date": "2021-10-30T10:30:00.000Z",
        "voteCount": 1,
        "content": "ADF. All steps are here\n\nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html#configure-snapshot-copy-grant"
      },
      {
        "date": "2021-10-30T05:44:00.000Z",
        "voteCount": 1,
        "content": "ABD is the right answer"
      },
      {
        "date": "2021-10-23T02:42:00.000Z",
        "voteCount": 2,
        "content": "According to https://docs.aws.amazon.com/redshift/latest/APIReference/API_CreateSnapshotCopyGrant.html  answer is ADF"
      },
      {
        "date": "2021-10-17T19:05:00.000Z",
        "voteCount": 2,
        "content": "ADF is correct Answer https://docs.aws.amazon.com/redshift/latest/mgmt/managing-snapshots-console.html#xregioncopy-kms-encrypted-snapshot"
      },
      {
        "date": "2021-10-16T10:50:00.000Z",
        "voteCount": 2,
        "content": "looks its ABD."
      },
      {
        "date": "2021-10-11T21:56:00.000Z",
        "voteCount": 2,
        "content": "my selection ADF"
      },
      {
        "date": "2021-10-07T18:42:00.000Z",
        "voteCount": 1,
        "content": "looks its ABD. any thoughts \nhttps://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html"
      },
      {
        "date": "2021-10-05T16:26:00.000Z",
        "voteCount": 1,
        "content": "find the step by step AWS Redshift cross-region copy snapshot through the console (Question ask for CLI but both has same steps)\nhttps://www.youtube.com/watch?v=9DepoiBOe6o"
      },
      {
        "date": "2021-09-28T12:55:00.000Z",
        "voteCount": 2,
        "content": "I choose ACD."
      },
      {
        "date": "2021-10-01T04:48:00.000Z",
        "voteCount": 4,
        "content": "ADF is correct"
      },
      {
        "date": "2021-10-01T15:32:00.000Z",
        "voteCount": 1,
        "content": "I choose ACD as well."
      },
      {
        "date": "2021-10-28T18:57:00.000Z",
        "voteCount": 1,
        "content": "It's F not C, because the snapshot in the destination region should be encrypted with a key that exists in that region."
      },
      {
        "date": "2021-09-26T18:16:00.000Z",
        "voteCount": 2,
        "content": "Same question is on https://www.examtopics.com/exams/amazon/aws-certified-big-data-specialty/view/8/ Question#36. This is the correct question and answer"
      },
      {
        "date": "2021-10-07T01:37:00.000Z",
        "voteCount": 2,
        "content": "You asked ACD in that question )\nWhy ADF here?"
      },
      {
        "date": "2021-10-29T16:36:00.000Z",
        "voteCount": 1,
        "content": "Question#36 has a bug since F is missing and C is closest to F That's why mattyb123 went for ACD there I suppose. ADF is correct."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/amazon/view/4093-exam-aws-certified-big-data-specialty-topic-2-question-2/",
    "body": "How should an Administrator BEST architect a large multi-layer Long Short-Term Memory (LSTM) recurrent neural network (RNN) running with MXNET on<br>Amazon EC2? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse data parallelism to partition the workload over multiple devices and balance the workload within the GPUs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse compute-optimized EC2 instances with an attached elastic GPU.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse general purpose GPU computing instances such as G3 and P3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse processing parallelism to partition the workload over multiple storage devices and balance the workload within the GPUs."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-29T02:21:00.000Z",
        "voteCount": 3,
        "content": "my selection AC"
      },
      {
        "date": "2021-10-24T17:41:00.000Z",
        "voteCount": 1,
        "content": "Anyone got any further thoughts on this one?"
      },
      {
        "date": "2021-10-27T14:59:00.000Z",
        "voteCount": 8,
        "content": "Question is missing an answer which states Model Parallelism. Answer is Its Model Parallelism and the instance type G3 and P3."
      },
      {
        "date": "2021-09-28T03:06:00.000Z",
        "voteCount": 1,
        "content": "answer is correct. https://aws.amazon.com/blogs/machine-learning/parallelizing-across-multiple-cpu-gpus-to-speed-up-deep-learning-inference-at-the-edge/"
      },
      {
        "date": "2021-10-01T16:34:00.000Z",
        "voteCount": 1,
        "content": "https://mxnet.incubator.apache.org/versions/master/faq/distributed_training.html"
      },
      {
        "date": "2021-10-02T02:50:00.000Z",
        "voteCount": 2,
        "content": "Data Parallelism vs Model Parallelism\nBy default, MXNet uses data parallelism to partition the workload over multiple devices. Assume there are n devices. Then each one will receive a copy of the complete model and train it on 1/n of the data. The results such as gradients and updated model are communicated across these devices.\nMXNet also supports model parallelism. In this approach, each device holds onto only part of the model. This proves useful when the model is too large to fit onto a single device.\nAnswer doesn't list Model Parallelism as that would be correct when using large models maybe this is a typo?"
      },
      {
        "date": "2021-11-03T18:59:00.000Z",
        "voteCount": 1,
        "content": "but data or model parallelism which is better is still a hot debate in ML, if both options appear then I really don't know which one is better..."
      },
      {
        "date": "2021-10-07T21:49:00.000Z",
        "voteCount": 1,
        "content": "https://aws.amazon.com/blogs/machine-learning/reducing-deep-learning-inference-cost-with-mxnet-and-amazon-elastic-inference/ Mentions increased performance with EI elastic GPUs on compute ec2 instances. However answer doesn't refer to Amazon Elastic Inference."
      },
      {
        "date": "2021-10-14T14:13:00.000Z",
        "voteCount": 1,
        "content": "@mattyb123: r u sitting in exam any time soon?"
      },
      {
        "date": "2021-10-21T00:31:00.000Z",
        "voteCount": 2,
        "content": "Yes, sitting it very soon again. Hence i want some feedback to talk through answers as this exam is hard compared to the CSA one."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/amazon/view/4807-exam-aws-certified-big-data-specialty-topic-2-question-3/",
    "body": "An organization is soliciting public feedback through a web portal that has been deployed to track the number of requests and other important data. As part of reporting and visualization, AmazonQuickSight connects to an Amazon RDS database to virtualize data. Management wants to understand some important metrics about feedback and how the feedback has changed over the last four weeks in a visual representation.<br>What would be the MOST effective way to represent multiple iterations of an analysis in Amazon QuickSight that would show how the data has changed over the last four weeks?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the analysis option for data captured in each week and view the data by a date range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a pivot table as a visual option to display measured values and weekly aggregate data as a row dimension.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a dashboard option to create an analysis of the data for each week and apply filters to visualize the data change.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a story option to preserve multiple iterations of an analysis and play the iterations sequentially."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-04T01:42:00.000Z",
        "voteCount": 2,
        "content": "D pretty cool!\nhttps://docs.aws.amazon.com/quicksight/latest/user/working-with-stories.html"
      },
      {
        "date": "2021-10-09T06:14:00.000Z",
        "voteCount": 1,
        "content": "my selection C"
      },
      {
        "date": "2021-10-16T16:11:00.000Z",
        "voteCount": 1,
        "content": "Your other selection is more appropriate."
      },
      {
        "date": "2021-10-31T21:35:00.000Z",
        "voteCount": 5,
        "content": "I really like this bot..."
      },
      {
        "date": "2021-10-08T01:24:00.000Z",
        "voteCount": 3,
        "content": "my selection D"
      },
      {
        "date": "2021-10-01T01:02:00.000Z",
        "voteCount": 4,
        "content": "Answer is right. https://docs.aws.amazon.com/quicksight/latest/user/working-with-stories.html"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/amazon/view/7724-exam-aws-certified-big-data-specialty-topic-2-question-4/",
    "body": "An organization is setting up a data catalog and metadata management environment for their numerous data stores currently running on AWS. The data catalog will be used to determine the structure and other attributes of data in the data stores. The data stores are composed of Amazon RDS databases, Amazon<br>Redshift, and CSV files residing on Amazon S3. The catalog should be populated on a scheduled basis, and minimal administration is required to manage the catalog.<br>How can this be accomplished?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Amazon DynamoDB as the data catalog and run a scheduled AWS Lambda function that connects to data sources to populate the DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Amazon database as the data catalog and run a scheduled AWS Lambda function that connects to data sources to populate the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS Glue Data Catalog as the data catalog and schedule crawlers that connect to data sources to populate the catalog.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Apache Hive metastore on an Amazon EC2 instance and run a scheduled bash script that connects to data sources to populate the metastore."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-11T13:43:00.000Z",
        "voteCount": 3,
        "content": "my selection C"
      },
      {
        "date": "2021-10-04T13:29:00.000Z",
        "voteCount": 1,
        "content": "Does these kind of straight forward questions appear in the exam ? Anyone who have wrote the exam can confirm. Thanks."
      },
      {
        "date": "2021-09-29T12:53:00.000Z",
        "voteCount": 2,
        "content": "C is correct https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html"
      },
      {
        "date": "2021-09-20T07:18:00.000Z",
        "voteCount": 4,
        "content": "C is correct"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/amazon/view/4074-exam-aws-certified-big-data-specialty-topic-2-question-5/",
    "body": "An organization is currently using an Amazon EMR long-running cluster with the latest Amazon EMR release for analytic jobs and is storing data as external tables on Amazon S3.<br>The company needs to launch multiple transient EMR clusters to access the same tables concurrently, but the metadata about the Amazon S3 external tables are defined and stored on the long-running cluster.<br>Which solution will expose the Hive metastore with the LEAST operational effort?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport Hive metastore information to Amazon DynamoDB hive-site classification to point to the Amazon DynamoDB table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport Hive metastore information to a MySQL table on Amazon RDS and configure the Amazon EMR hive-site classification to point to the Amazon RDS database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an Amazon EC2 instance, install and configure Apache Derby, and export the Hive metastore information to derby.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and configure an AWS Glue Data Catalog as a Hive metastore for Amazon EMR."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-27T20:34:00.000Z",
        "voteCount": 8,
        "content": "It is D.\nThe key is \"multiple transient EMR clusters to access the same tables concurrently\".\nFor External RDS metastore, it is not recommended to write concurrently.(https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-external.html)\nFor Glue, it have 1 to 10 concurrent access.(https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html)"
      },
      {
        "date": "2021-11-05T02:18:00.000Z",
        "voteCount": 1,
        "content": "D. Should be the right answer, because Amazon recommends use this configuration when you require a persistent metastore or a metastore shared by different clusters, services, applications, or AWS account. \nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html"
      },
      {
        "date": "2021-11-04T20:08:00.000Z",
        "voteCount": 1,
        "content": "answer is D. Glue crawlers can update tables when schema changes, thus requiring the LEAST operational effort."
      },
      {
        "date": "2021-10-31T05:23:00.000Z",
        "voteCount": 2,
        "content": "Answer is D https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html"
      },
      {
        "date": "2021-10-23T22:42:00.000Z",
        "voteCount": 2,
        "content": "my selection B"
      },
      {
        "date": "2021-10-16T03:04:00.000Z",
        "voteCount": 1,
        "content": "D is correct; the question is about \"which solution will expose the Hive metastore, ... defined on the long-running cluster\""
      },
      {
        "date": "2021-10-10T04:50:00.000Z",
        "voteCount": 4,
        "content": "well long-running cluster is already operating Hive catalog on in MySQL DB on Master Node so I think moving existing database to RDS and switching is more easy than using Glue Crawler, so B"
      },
      {
        "date": "2021-10-10T16:59:00.000Z",
        "voteCount": 1,
        "content": "Here we go...\nConfiguring an External Metastore for Hive\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-metastore-external-hive.html\nUsing the AWS Glue Data Catalog as the Metastore for Hive\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html\n(Important point: If another cluster needs to access the table, it fails unless it has adequate permissions to the cluster that created the table. Furthermore, because HDFS storage is transient, if the cluster terminates, the table data is lost, and the table must be recreated)\nUsing an External MySQL Database or Amazon Aurora\nhttps://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-external.html\n(Your Hive cluster runs using the metastore located in Amazon RDS. Launch all additional Hive clusters that share this metastore by specifying the metastore location.)"
      },
      {
        "date": "2021-10-13T05:41:00.000Z",
        "voteCount": 1,
        "content": "thanks. the metadata for S3 external tables is still defined on long running clusters. So shouldn't that point to using Glue? (D as option)"
      },
      {
        "date": "2021-10-10T03:21:00.000Z",
        "voteCount": 1,
        "content": "D is least operational effort"
      },
      {
        "date": "2021-10-09T10:49:00.000Z",
        "voteCount": 1,
        "content": "B and D both are correct but looks like Glue will be easily configurable so it would be D"
      },
      {
        "date": "2021-09-23T08:03:00.000Z",
        "voteCount": 1,
        "content": "Isn't it D. LEAST operational effort."
      },
      {
        "date": "2021-09-25T21:50:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-metastore-glue.html. Using Amazon EMR version 5.8.0 or later, you can configure Hive to use the AWS Glue Data Catalog as its metastore. We recommend this configuration when you require a persistent metastore or a metastore shared by different clusters, services, applications, or AWS accounts."
      },
      {
        "date": "2021-09-26T00:49:00.000Z",
        "voteCount": 1,
        "content": "Only reason it could be B. Is due to glue needing to be setup as the metadata store before launching the EMR cluster. Anyone else have some thoughts on this?"
      },
      {
        "date": "2021-09-26T11:49:00.000Z",
        "voteCount": 1,
        "content": "Thoughts anyone?"
      },
      {
        "date": "2021-10-03T20:11:00.000Z",
        "voteCount": 1,
        "content": "Make sense to seleted \"D\"\n but again, I have not try 'handon' create a AWS Glue yet."
      },
      {
        "date": "2021-10-05T02:11:00.000Z",
        "voteCount": 1,
        "content": "Looks like D is correct. Storing external data on S3.\nhttps://docs.aws.amazon.com/glue/latest/dg/add-crawler.html. \nWhen you define an Amazon S3 data store to crawl, you can choose whether to crawl a path in your account or another account. The output of the crawler is one or more metadata tables defined in the AWS Glue Data Catalog. A table is created for one or more files found in your data store. If all the Amazon S3 files in a folder have the same schema, the crawler creates one table. Also, if the Amazon S3 object is partitioned, only one metadata table is created."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/amazon/view/4281-exam-aws-certified-big-data-specialty-topic-2-question-6/",
    "body": "An organization is using Amazon Kinesis Data Streams to collect data generated from thousands of temperature devices and is using AWS Lambda to process the data. Devices generate 10 to 12 million records every day, but Lambda is processing only around 450 thousand records. Amazon CloudWatch indicates that throttling on Lambda is not occurring.<br>What should be done to ensure that all data is processed? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the BatchSize value on the EventSource, and increase the memory allocated to the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the BatchSize value on the EventSource, and increase the memory allocated to the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple Lambda functions that will consume the same Amazon Kinesis stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of vCores allocated for the Lambda function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards on the Amazon Kinesis stream."
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-04T06:54:00.000Z",
        "voteCount": 6,
        "content": "There is no throttling on Lambda so it is due to the number of shards (throughput limit of 2MB/shard) = increasing number of shards E is correct.\n\nGiven that Lambda is only processing .5m records, the number of shards to be increased would be at least 20 folds. Which is likely to cause throttling on Lambda, thus increasing the batch size and RAM amount for Lambda to prepare for this would make sense. =&gt; A\n\nOther answers don't make sense anyway."
      },
      {
        "date": "2021-11-01T18:42:00.000Z",
        "voteCount": 1,
        "content": "Why not C, E?"
      },
      {
        "date": "2021-10-20T19:59:00.000Z",
        "voteCount": 1,
        "content": "A, E\nhttps://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html\n...To increase the speed at which your function processes records, add shards to your data stream...."
      },
      {
        "date": "2021-10-15T16:36:00.000Z",
        "voteCount": 3,
        "content": "A-\tBecause Lambda\u2019s capacity is underutilized so increasing the batch size will enable the function to process more records in a single invocation. However it will also need more memory to process the increased # of records.\nE- The reason why Lambda is processing only 450 records per day even though the Kinesis Streams is collecting 10 to 12 million records per day is due to lower Kinesis throughput. To increase the throughput ( Bytes/sec read from Kinesis stream) we would need to increase the # of shards."
      },
      {
        "date": "2021-10-11T00:27:00.000Z",
        "voteCount": 3,
        "content": "my selection AE"
      },
      {
        "date": "2021-09-20T11:16:00.000Z",
        "voteCount": 2,
        "content": "Correct. As mentioned previously  via this link \nhttps://tech.trivago.com/2018/07/13/aws-kinesis-with-lambdas-lessons-learned/"
      },
      {
        "date": "2021-09-23T07:33:00.000Z",
        "voteCount": 1,
        "content": "don't get it, the post says increase batch size and lambda memory can let each invocation handle more data to reduce invocation rate, but in the question has specifically indicate there is no throttle."
      },
      {
        "date": "2021-10-03T09:03:00.000Z",
        "voteCount": 2,
        "content": "A - Lambda has no throttle - so increasing batch size will better utilize the memory allocation.\nE - Increase the shards,so more throughput for Lambda consumption - thus utilize Lambda effectively."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/amazon/view/4075-exam-aws-certified-big-data-specialty-topic-2-question-7/",
    "body": "An Operations team continuously monitors the number of visitors to a website to identify any potential system problems. The number of website visitors varies throughout the day. The site is more popular in the middle of the day and less popular at night.<br>Which type of dashboard display would be the MOST useful to allow staff to quickly and correctly identify system problems?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA vertical stacked bar chart showing today's website visitors and the historical average number of website visitors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn overlay line chart showing today's website visitors at one-minute intervals and also the historical average number of website visitors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA single KPI metric showing the statistical variance between the current number of website visitors and the historical number of website visitors for the current time of day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA scatter plot showing today's website visitors on the X-axis and the historical average number of website visitors on the Y-axis."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-31T02:43:00.000Z",
        "voteCount": 1,
        "content": "It should be C, \n\"With KPI charts, you can present a single aggregated value from a measure, and also comparisons against another measure or over a time period\"\nhttps://aws.amazon.com/blogs/big-data/amazon-quicksight-spring-announcement-kpi-charts-export-to-csv-ad-connector-and-more/"
      },
      {
        "date": "2021-11-01T22:51:00.000Z",
        "voteCount": 1,
        "content": "Change my mind, I think B is correct, historical trend is the key of this answer"
      },
      {
        "date": "2021-10-28T10:35:00.000Z",
        "voteCount": 3,
        "content": "B is the right answer.\nC is just one number that changes every minute. You cannot see the historical trend."
      },
      {
        "date": "2021-10-21T18:23:00.000Z",
        "voteCount": 3,
        "content": "my selection B"
      },
      {
        "date": "2021-10-13T17:21:00.000Z",
        "voteCount": 1,
        "content": "Isn't C just B with much much less information and no time sesries means no information regarding trend?"
      },
      {
        "date": "2021-10-01T22:35:00.000Z",
        "voteCount": 1,
        "content": "Wouldn't this be C? Would make the dashboard display the MOST useful."
      },
      {
        "date": "2021-10-08T17:42:00.000Z",
        "voteCount": 3,
        "content": "Use a KPI to visualize a comparison between a key value and its target value. A KPI displays a value comparison, the two values being compared, and a progress bar. For example, the following KPI shows how closely revenue is meeting its forecast.\nhttps://docs.aws.amazon.com/quicksight/latest/user/kpi.html"
      },
      {
        "date": "2021-10-09T06:38:00.000Z",
        "voteCount": 1,
        "content": "will choose C, too. A and B have same problem that the beginning of each minute will show misleading info."
      },
      {
        "date": "2021-10-10T06:32:00.000Z",
        "voteCount": 2,
        "content": "Differnces between current number of visitors and historic number cannot indicate system problem, we also need the trends of the current number of vistors"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/amazon/view/5556-exam-aws-certified-big-data-specialty-topic-2-question-8/",
    "body": "An organization would like to run analytics on their Elastic Load Balancing logs stored in Amazon S3 and join this data with other tables in Amazon S3. The users are currently using a BI tool connecting with JDBC and would like to keep using this BI tool.<br>Which solution would result in the LEAST operational overhead?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrigger a Lambda function when a new log file is added to the bucket to transform and load it into Amazon Redshift. Run the VACUUM command on the Amazon Redshift cluster every night.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a long-running Amazon EMR cluster that continuously downloads and transforms new files from Amazon S3 into its HDFS storage. Use Presto to expose the data through JDBC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrigger a Lambda function when a new log file is added to the bucket to transform and move it to another bucket with an optimized data structure. Use Amazon Athena to query the optimized bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a transient Amazon EMR cluster every night that transforms new log files and loads them into Amazon Redshift."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-24T02:45:00.000Z",
        "voteCount": 13,
        "content": "C is correct ... \"Q: Does Athena support other BI Tools and SQL Clients?\nYes. Amazon Athena comes with an ODBC and JDBC driver that you can use with other business intelligence tools and SQL clients. Learn more about using an ODBC or JDBC driver with Athena.\". This satisfies the requirement."
      },
      {
        "date": "2021-11-05T13:18:00.000Z",
        "voteCount": 4,
        "content": "C is LEAST operational overhead. Others may require existing BI tool to change"
      },
      {
        "date": "2021-11-04T21:38:00.000Z",
        "voteCount": 3,
        "content": "my selection C"
      },
      {
        "date": "2021-10-20T20:43:00.000Z",
        "voteCount": 4,
        "content": "C is LEAST operational overhead"
      },
      {
        "date": "2021-09-28T20:10:00.000Z",
        "voteCount": 1,
        "content": "Why not D, you can use JDBC to connect to Redshift"
      },
      {
        "date": "2021-10-25T01:47:00.000Z",
        "voteCount": 1,
        "content": "Vote D as well"
      },
      {
        "date": "2021-11-06T21:19:00.000Z",
        "voteCount": 3,
        "content": "Cause imagine you were a manager, and your employee would create a transient EMR cluster + a Amazon Redshift database just for some log file viewing in a BI tool, you'd be quite pissed with him too ;) admittedly, technically it would work tho"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/amazon/view/4320-exam-aws-certified-big-data-specialty-topic-2-question-9/",
    "body": "An organization has added a clickstream to their website to analyze traffic. The website is sending each page request with the PutRecord API call to an Amazon<br>Kinesis stream by using the page name as the partition key. During peak spikes in website traffic, a support engineer notices many events in the application logs.<br>ProvisionedThroughputExcededException<br>What should be done to resolve the issue in the MOST cost-effective way?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple Amazon Kinesis streams for page requests to increase the concurrency of the clickstream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shards on the Kinesis stream to allow for more throughput to meet the peak spikes in traffic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the application to use on the Kinesis Producer Library to aggregate requests before sending them to the Kinesis stream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach more consumers to the Kinesis stream to process records in parallel, improving the performance on the stream. B"
    ],
    "answer": "Explanation",
    "answerDescription": "Reference:<br>https://aws.amazon.com/kinesis/data-streams/faqs/",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-28T07:57:00.000Z",
        "voteCount": 8,
        "content": "I'm also stumbled upon this questions - it looks very obvious that they are not aggregating because they use PutRecord API. Furthermore they have only peak spikes, so if they increase number of shards they would have constantly higher costs for only occasional spikes. It's definitely C, additionally it would also be possible to implement compression using KPL."
      },
      {
        "date": "2021-09-21T03:27:00.000Z",
        "voteCount": 6,
        "content": "But is B a most cost-effective way?.. the price we pay depends on the shards .. can it be C?"
      },
      {
        "date": "2021-11-06T12:46:00.000Z",
        "voteCount": 1,
        "content": "C - this is how you avoid ProvisionedThroughputExcededException"
      },
      {
        "date": "2021-11-05T19:22:00.000Z",
        "voteCount": 2,
        "content": "It should be C, because batching increase throughput and decrease cost"
      },
      {
        "date": "2021-11-05T17:45:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer"
      },
      {
        "date": "2021-10-31T23:20:00.000Z",
        "voteCount": 1,
        "content": "B or C?"
      },
      {
        "date": "2021-10-27T03:23:00.000Z",
        "voteCount": 1,
        "content": "The request rate for the stream is too high, or the requested data is too large for the available throughput. Reduce the frequency or size of your requests. For more information,\nhttps://docs.aws.amazon.com/kinesis/latest/APIReference/API_PutRecords.html\nSo using the producer library will decrease the frequency"
      },
      {
        "date": "2021-10-22T01:04:00.000Z",
        "voteCount": 4,
        "content": "Answer is C: \nBatching (both turned on by default) increase throughput, decrease\ncost:\n\u2022Collect Records and Write to multiple shards in the same PutRecords API call\n\u2022Aggregate increased latency\n\u2022Capability to store multiple records in one record (go over 1000 records per second limit)\n\u2022Increase payload size and improve throughput (maximize 1MB/s limit)"
      },
      {
        "date": "2021-10-17T12:56:00.000Z",
        "voteCount": 1,
        "content": "agreed B. input data increased so to increase shards.\nYour data blob, partition key, and data stream name are required parameters of a PutRecord or PutRecords call. The size of your data blob (before Base64 encoding) and partition key will be counted against the data throughput of your Amazon Kinesis data stream, which is determined by the number of shards within the data stream."
      },
      {
        "date": "2021-10-17T02:50:00.000Z",
        "voteCount": 6,
        "content": "my selection C"
      },
      {
        "date": "2021-10-16T02:39:00.000Z",
        "voteCount": 1,
        "content": "C ! https://docs.aws.amazon.com/streams/latest/dev/kinesis-kpl-concepts.html"
      },
      {
        "date": "2021-10-14T01:44:00.000Z",
        "voteCount": 3,
        "content": "C is correct; KPL supports Aggregation, storing multiple records within a single Kinesis Data Streams record and with this feature, we can go beyond 1000 records per second per shard."
      },
      {
        "date": "2021-10-10T10:40:00.000Z",
        "voteCount": 1,
        "content": "I will go with B\nhttps://aws.amazon.com/kinesis/data-streams/faqs/"
      },
      {
        "date": "2021-10-12T11:54:00.000Z",
        "voteCount": 3,
        "content": "B may not work because partition key is page name. Even if you increase the number of shards still there may be a hot partition because of single page sending more puts. Since this information is not provided, C may be right answer."
      },
      {
        "date": "2021-10-05T16:19:00.000Z",
        "voteCount": 1,
        "content": "Due to frequent checkpointing its giving the errors. When the data will be aggregated the checkpointing will get reduced and in turn solve the problem in the most effective way."
      },
      {
        "date": "2021-09-29T18:54:00.000Z",
        "voteCount": 2,
        "content": "C I think. https://docs.aws.amazon.com/streams/latest/dev/developing-producers-with-kpl.html\n\"The KPL is an easy-to-use, highly configurable library that helps you write to a Kinesis data stream. It acts as an intermediary between your producer application code and the Kinesis Data Streams API actions. The KPL performs the following primary tasks:\n\nWrites to one or more Kinesis data streams with an automatic and configurable retry mechanism\n\nCollects records and uses PutRecords to write multiple records to multiple shards per request\n\nAggregates user records to increase payload size and improve throughput\".\n\nPlease, anyone on this thread who passed this exam already? That way we can all be on same page with such, to know which answers are correct"
      },
      {
        "date": "2021-09-24T06:32:00.000Z",
        "voteCount": 3,
        "content": "My choice is C. \nWhile B is a way to resolve the throughput issue, aggregation proposed by C would be more cost-effective."
      },
      {
        "date": "2021-09-27T18:06:00.000Z",
        "voteCount": 4,
        "content": "You also get provisionedthroughputexceeded for volume of data also. So aggregation won't solve the issue. Answer is B"
      },
      {
        "date": "2021-09-28T07:43:00.000Z",
        "voteCount": 3,
        "content": "Aggregation solves volume issues, Batching solves throughput issues.. seems C."
      },
      {
        "date": "2021-09-19T23:48:00.000Z",
        "voteCount": 2,
        "content": "B ? Thoughts"
      },
      {
        "date": "2021-09-20T14:55:00.000Z",
        "voteCount": 5,
        "content": "Correct. \nhttps://aws.amazon.com/kinesis/data-streams/faqs/\nQ: What happens if the capacity limits of an Amazon Kinesis data stream are exceeded while the data producer adds data to the data stream?\nThe capacity limits of an Amazon Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of PUT records. While the capacity limits are exceeded, the put data call will be rejected with a ProvisionedThroughputExceeded exception. If this is due to a temporary rise of the data stream\u2019s input data rate, retry by the data producer will eventually lead to completion of the requests. If this is due to a sustained rise of the data stream\u2019s input data rate, you should increase the number of shards within your data stream to provide enough capacity for the put data calls to consistently succeed. In both cases, Amazon CloudWatch metrics allow you to learn about the change of the data stream\u2019s input data rate and the occurrence of ProvisionedThroughputExceeded exceptions."
      },
      {
        "date": "2021-10-06T19:32:00.000Z",
        "voteCount": 1,
        "content": "B is the answer\nMore on \"ProvisionedThroughputExceededException\"...\nhttps://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html\nhttps://docs.aws.amazon.com/streams/latest/dev/kinesis-low-latency.html\nhttps://any-api.com/amazonaws_com/kinesis/docs/Definitions/ProvisionedThroughputExceededException"
      },
      {
        "date": "2021-11-04T08:27:00.000Z",
        "voteCount": 5,
        "content": "C is correct.\nProvisionedThroughputExceededException can be caused either by too large data volume or too many requests. Since PutRecord API is used, each record gets sent on its own, making too many requests highly probable. Thus, C should help and it is obviously more cost effective than B."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/amazon/view/4076-exam-aws-certified-big-data-specialty-topic-2-question-10/",
    "body": "An organization currently runs a large Hadoop environment in their data center and is in the process of creating an alternative Hadoop environment on AWS, using Amazon EMR.<br>They generate around 20 TB of data on a monthly basis. Also on a monthly basis, files need to be grouped and copied to Amazon S3 to be used for the Amazon<br>EMR environment. They have multiple S3 buckets across AWS accounts to which data needs to be copied. There is a 10G AWS Direct Connect setup between their data center and AWS, and the network team has agreed to allocate 50% of AWS Direct Connect bandwidth to data transfer. The data transfer cannot take more than two days.<br>What would be the MOST efficient approach to transfer data to AWS on a monthly basis?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an offline copy method, such as an AWS Snowball device, to copy and transfer data to Amazon S3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a multipart upload for Amazon S3 on AWS Java SDK to transfer data over AWS Direct Connect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon S3 transfer acceleration capability to transfer data to Amazon S3 over AWS Direct Connect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup S3DistCop tool on the on-premises Hadoop environment to transfer data to Amazon S3 over AWS Direct Connect."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-06T12:48:00.000Z",
        "voteCount": 1,
        "content": "Just attempted exam, This is typo. In the exam, s3distcp was properly mentioned. \nI selected D"
      },
      {
        "date": "2021-10-27T12:05:00.000Z",
        "voteCount": 1,
        "content": "There are no S3DistCOP tools (It is rather called S3DistCp), so the answer is B"
      },
      {
        "date": "2021-10-24T07:44:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2021-10-19T11:09:00.000Z",
        "voteCount": 1,
        "content": "D. since the transfer is from on-premise hadoop S3DistCp would work from hadoop to S3."
      },
      {
        "date": "2021-10-17T19:14:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://forums.aws.amazon.com/thread.jspa?threadID=120522\nhttps://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf"
      },
      {
        "date": "2021-10-07T17:55:00.000Z",
        "voteCount": 3,
        "content": "Answer D: \nhttps://blog.ippon.tech/aws-white-paper-in-5-minutes-or-less-best-practices-for-amazon-emr/"
      },
      {
        "date": "2021-10-06T19:22:00.000Z",
        "voteCount": 2,
        "content": "B makes senses. not D  since S3DistCp is used to copy large amounts of data from Amazon S3 into HDFS. the issue here is from on-premise to S3"
      },
      {
        "date": "2021-11-07T02:45:00.000Z",
        "voteCount": 1,
        "content": "bot true: \nYou can also use S3DistCp to copy data from HDFS to Amazon S3. \nS3DistCp is more scalable and efficient for parallel copying large numbers of objects across buckets and across AWS accounts.\nS3DistCp is the same as the Hadoop binary DistCp, except it takes advantage of multi-part upload to S3 for larger files. Hadoop is optimized for large file blocks, so it is usually best to use S3DistCp for copying HDFS files from an external data center or local disk to S3 to take advantage of this optimization."
      },
      {
        "date": "2021-09-28T19:39:00.000Z",
        "voteCount": 3,
        "content": "my selection D"
      },
      {
        "date": "2021-09-28T03:51:00.000Z",
        "voteCount": 4,
        "content": "D. S3DistCop is the right answer and it's used for moving big data from Hadoop to s3 , S3 to hadopp or from s3 to s3 and at backend it uses map reduce job. Multipart upload is not the best choice here"
      },
      {
        "date": "2021-09-25T12:13:00.000Z",
        "voteCount": 4,
        "content": "Vote D for now"
      },
      {
        "date": "2021-09-27T14:38:00.000Z",
        "voteCount": 5,
        "content": "Answer is D. There is a typo."
      },
      {
        "date": "2021-09-19T12:41:00.000Z",
        "voteCount": 1,
        "content": "Confirmed. \n1.https://toolstud.io/data/filesize.php?speed=5&amp;speed_unit=Gbps&amp;duration=12&amp;duration_unit=hours&amp;compare=harddisk\n2.https://aws.amazon.com/snowball/faqs/#Q.3A_How_long_does_it_take_to_transfer_my_data.3F: Q: When should I consider using Snowball instead of AWS Direct Connect?\nAWS Direct Connect provides you with dedicated, fast connections from your premises to the AWS network. If you need to transfer large quantities of data to AWS on an ongoing basis, AWS Direct Connect might be the right choice."
      },
      {
        "date": "2021-09-19T16:57:00.000Z",
        "voteCount": 1,
        "content": "Anyone disagree with B. I did select this answer last time but the numbers seem to add up?"
      },
      {
        "date": "2021-09-20T01:26:00.000Z",
        "voteCount": 2,
        "content": "By reviewing https://d0.awsstatic.com/whitepapers/aws-amazon-emr-best-practices.pdf looks like D is the correct answer."
      },
      {
        "date": "2021-10-04T11:11:00.000Z",
        "voteCount": 2,
        "content": "More info: http://www.thecloudxperts.co.uk/moving-large-amounts-of-data-from-hdfs-data-center-to-amazon-s3-using-s3distcp\n\nTwo Important tools to move data\u2014S3DistCp and DistCp\u2014can help you move data stored on your local (data center) HDFS storage to Amazon S3."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/amazon/view/4077-exam-aws-certified-big-data-specialty-topic-2-question-11/",
    "body": "An organization is developing a mobile social application and needs to collect logs from all devices on which it is installed. The organization is evaluating the<br>Amazon Kinesis Data Streams to push logs and Amazon EMR to process data. They want to store data on HDFS using the default replication factor to replicate data among the cluster, but they are concerned about the durability of the data. Currently, they are producing 300 GB of raw data daily, with additional spikes during special events. They will need to scale out the Amazon EMR cluster to match the increase in streamed data.<br>Which solution prevents data loss and matches compute demand?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse multiple Amazon EBS volumes on Amazon EMR to store processed data and scale out the Amazon EMR cluster as needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the EMR File System and Amazon S3 to store processed data and scale out the Amazon EMR cluster as needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB to store processed data and scale out the Amazon EMR cluster as needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tuse Amazon Kinesis Data Firehose and, instead of using Amazon EMR, stream logs directly into Amazon Elasticsearch Service."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-06T02:21:00.000Z",
        "voteCount": 6,
        "content": "my selection B"
      },
      {
        "date": "2021-10-28T19:54:00.000Z",
        "voteCount": 1,
        "content": "I would say B without no doubt"
      },
      {
        "date": "2021-11-05T12:57:00.000Z",
        "voteCount": 2,
        "content": "Agreed, B is the correct answer."
      },
      {
        "date": "2021-09-27T17:46:00.000Z",
        "voteCount": 2,
        "content": "B, anyone?"
      },
      {
        "date": "2021-10-07T20:32:00.000Z",
        "voteCount": 4,
        "content": "I went with B"
      },
      {
        "date": "2021-10-27T09:22:00.000Z",
        "voteCount": 2,
        "content": "I vote B"
      },
      {
        "date": "2021-09-21T08:12:00.000Z",
        "voteCount": 2,
        "content": "Thoughts on B? Prevents data loss (durable data store S3 with S3 versioning) Compute demand (EMR)"
      },
      {
        "date": "2021-10-01T14:47:00.000Z",
        "voteCount": 3,
        "content": "Keyword for EMR is usually spike or spikey so i am assuming EMR is required in the answer"
      },
      {
        "date": "2021-10-03T13:16:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/elasticsearch-service/latest/developerguide/sizing-domains.html refers to a lot smaller reference data then what the question states. When searching for compute resources on AWS elasticsearch isnt a service that appears."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/amazon/view/4078-exam-aws-certified-big-data-specialty-topic-2-question-12/",
    "body": "An advertising organization uses an application to process a stream of events that are received from clients in multiple unstructured formats.<br>The application does the following:<br>\u2711 Transforms the events into a single structured format and streams them to Amazon Kinesis for real-time analysis.<br>\u2711 Stores the unstructured raw events from the log files on local hard drivers that are rotated and uploaded to Amazon S3.<br>The organization wants to extract campaign performance reporting using an existing Amazon redshift cluster.<br>Which solution will provide the performance data with the LEAST number of operations?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the Amazon Kinesis Data Firehose agent on the application servers and use it to stream the log files directly to Amazon Redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external table in Amazon Redshift and point it to the S3 bucket where the unstructured raw events are stored.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an AWS Lambda function that triggers every hour to load the new log files already in S3 to Amazon redshift.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect Amazon Kinesis Data Firehose to the existing Amazon Kinesis stream and use it to stream the event directly to Amazon Redshift."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-08T21:26:00.000Z",
        "voteCount": 6,
        "content": "Not A \u2013 No use loading unstructured data in multiple formats to RedShift via Kinesis Firehouse agent.\nNot B- Creating External table using RedShift Spectrum will be an issue against unstructured data in multiple formats.\nNot C -  Not a good choice. Never seen Lambda talking to RedShift and why would you use it when KFH directly connect to RedShift.\nCorrect Option is D- Because it loads structured data in a single format to RedShift."
      },
      {
        "date": "2021-11-05T22:49:00.000Z",
        "voteCount": 1,
        "content": "Correct D: least  number of operations"
      },
      {
        "date": "2021-10-22T09:08:00.000Z",
        "voteCount": 2,
        "content": "D seems more reasonable. I go with D"
      },
      {
        "date": "2021-10-15T08:06:00.000Z",
        "voteCount": 1,
        "content": "Option A also talks about shipping structured data from the application using Kinesis Firehouse agent to Redshift. So using the same Agent it's possible to stream the data to both Kinesis Data Streams for analyiss and KFH to deliver it to Redshift. It seems like the most direct option."
      },
      {
        "date": "2021-10-19T05:43:00.000Z",
        "voteCount": 1,
        "content": "A is about log files which are unstructured.  Not a good idea to move the log files to Redshift."
      },
      {
        "date": "2021-10-07T16:14:00.000Z",
        "voteCount": 1,
        "content": "B. the question is asking how to consume the data in Redshift not how to get/input the data which is already in place"
      },
      {
        "date": "2021-10-06T09:23:00.000Z",
        "voteCount": 1,
        "content": "The unstructured date is already transformed to single dtructured format prior to putting into Kinesis. So I will go with D for LEAST number of operations. B = spectrum is not needed"
      },
      {
        "date": "2021-10-06T09:12:00.000Z",
        "voteCount": 2,
        "content": "To handle the unstructured data structure, Kinesis Data Firehose can invoke Lambda function to do data transformation and format conversion, so it's D"
      },
      {
        "date": "2021-10-05T19:26:00.000Z",
        "voteCount": 2,
        "content": "my selection D"
      },
      {
        "date": "2021-10-05T04:32:00.000Z",
        "voteCount": 3,
        "content": "answer is D. The key here is multiple unstructured formats. You can't define an external table with multiple source formats."
      },
      {
        "date": "2021-10-04T05:10:00.000Z",
        "voteCount": 1,
        "content": "A. FH to Redshift is direct..."
      },
      {
        "date": "2021-10-03T18:07:00.000Z",
        "voteCount": 4,
        "content": "For unstructured data combine Redshift with S3 is basic. Because Redshift is not for unstructured data."
      },
      {
        "date": "2021-10-29T06:24:00.000Z",
        "voteCount": 1,
        "content": "Transforms the events into a single structured format and streams them to Amazon Kinesis for real-time analysis."
      },
      {
        "date": "2021-09-26T10:34:00.000Z",
        "voteCount": 2,
        "content": "I go with D. Fire Hose can read the Structured data from Kinesis Stream and store it in Redshift."
      },
      {
        "date": "2021-09-25T15:21:00.000Z",
        "voteCount": 1,
        "content": "The problem with B is fine if the data was structured since we could use redshidt spectrum to create external tables pointing to S3 . For this I'd go with D. At least as solution it is correct"
      },
      {
        "date": "2021-09-23T03:24:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2021-09-29T05:58:00.000Z",
        "voteCount": 3,
        "content": "How can B be the answer when is says 'point the table to the unstructured data'? The answer is D."
      },
      {
        "date": "2021-10-03T05:38:00.000Z",
        "voteCount": 1,
        "content": "refereed FAQ, unstructured data in s3 could be the external table of redshift, So it is B"
      },
      {
        "date": "2021-10-29T14:25:00.000Z",
        "voteCount": 1,
        "content": "Transforms the events into a single structured format and streams them to Amazon Kinesis for real-time analysis."
      },
      {
        "date": "2021-09-20T08:37:00.000Z",
        "voteCount": 4,
        "content": "Thoughts on D?"
      },
      {
        "date": "2021-09-20T14:25:00.000Z",
        "voteCount": 2,
        "content": "Amazon Redshift Spectrum uses external tables to query data that is stored in Amazon S3. You can query an external table using the same SELECT syntax you use with other Amazon Redshift tables. External tables are read-only. You can't write to an external table.\n1.https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_EXTERNAL_TABLE.html 2.https://blog.openbridge.com/10-simple-tips-that-help-you-quickly-find-success-adopting-amazon-redshift-spectrum-810db089abbe\n3.https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-external-tables.html\n4.https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_EXTERNAL_TABLE.html"
      },
      {
        "date": "2021-10-15T08:36:00.000Z",
        "voteCount": 1,
        "content": "Amazon Redshift now supports writing to external tables in Amazon S3 : \nhttps://aws.amazon.com/about-aws/whats-new/2020/06/amazon-redshift-now-supports-writing-to-external-tables-in-amazon-s3/"
      },
      {
        "date": "2021-09-21T04:45:00.000Z",
        "voteCount": 4,
        "content": "I think its D due to FH being able to automatically copy/write the data to redshift. Where if you were using redshift spectrum you can only create read only external tables and you would need to write the SQL to create the external table."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/amazon/view/4079-exam-aws-certified-big-data-specialty-topic-2-question-13/",
    "body": "An organization needs to store sensitive information on Amazon S3 and process it through Amazon EMR. Data must be encrypted on Amazon S3 and Amazon<br>EMR at rest and in transit. Using Thrift Server, the Data Analysis team users HIVE to interact with this data. The organization would like to grant access to only specific databases and tables, giving permission only to the SELECT statement.<br>Which solution will protect the data and limit user access to the SELECT statement on a specific portion of data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Transparent Data Encryption on Amazon EMR. Create an Amazon EC2 instance and install Apache Ranger. Configure the authorization on the cluster to use Apache Ranger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure data encryption at rest for EMR File System (EMRFS) on Amazon S3. Configure data encryption in transit for traffic between Amazon S3 and EMRFS. Configure storage and SQL base authorization on HiveServer2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS KMS for encryption of data. Configure and attach multiple roles with different permissions based on the different user needs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Security Group on Amazon EMR. Create an Amazon VPC endpoint for Amazon S3. Configure HiveServer2 to use Kerberos authentication on the cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-19T10:40:00.000Z",
        "voteCount": 5,
        "content": "It's A. \nhttps://aws.amazon.com/blogs/big-data/implementing-authorization-and-auditing-using-apache-ranger-on-amazon-emr/"
      },
      {
        "date": "2021-10-01T11:28:00.000Z",
        "voteCount": 1,
        "content": "Transparent Data Encryption is for HDFS not s3, B may be the correct answer https://poonamkucheriya.wordpress.com/2019/01/11/how-to-implement-sql-standard-based-hive-authorization-in-emr-hive/"
      },
      {
        "date": "2021-10-02T00:22:00.000Z",
        "voteCount": 1,
        "content": "Transparent Data Encryption is for HDFS:  https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-data-encryption-options.html"
      },
      {
        "date": "2022-06-23T03:08:00.000Z",
        "voteCount": 1,
        "content": "Every is saying B \nbut https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hive-differences.html\nit says \"Hive authorization : \nAmazon EMR supports  Hive authorization for HDFS but not for EMRFS and Amazon S3. Amazon EMR clusters run with authorization disabled by default.\""
      },
      {
        "date": "2021-11-06T13:58:00.000Z",
        "voteCount": 1,
        "content": "Answer A:\nTransparent Encryption - Security configurations offer settings to enable security for data in-transit and data at-rest in Amazon Elastic Block Store (Amazon EBS) storage volumes and EMRFS data in Amazon S3.\nRanger - EMR steps are used to perform the following: Install and configure Ranger HDFS and Hive plugins"
      },
      {
        "date": "2021-11-05T17:41:00.000Z",
        "voteCount": 1,
        "content": "it's A..\n\nApache Ranger has the following goals:\n\nCentralized security administration to manage all security related tasks in a central UI or using REST APIs.\nFine grained authorization to do a specific action and/or operation with Hadoop component/tool and managed through a central administration tool\nStandardize authorization method across all Hadoop components.\nEnhanced support for different authorization methods - Role based access control, attribute based access control etc.\nCentralize auditing of user access and administrative actions (security related) within all the components of Hadoop."
      },
      {
        "date": "2021-11-02T00:28:00.000Z",
        "voteCount": 2,
        "content": "B is the only one to address \"SQL base authorization\":\nhttps://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization\nWhich is basically the right to execute a SELECT statement, as required by the question.\nAnswer A is tempting, but why installing an additional ec2 instance if it goes without doing so?"
      },
      {
        "date": "2021-10-30T15:46:00.000Z",
        "voteCount": 1,
        "content": "B my choice\nhttps://aws.amazon.com/fr/blogs/big-data/encrypt-data-at-rest-and-in-flight-on-amazon-emr-with-security-configurations/"
      },
      {
        "date": "2021-10-28T20:31:00.000Z",
        "voteCount": 2,
        "content": "B.\nA: didn\u2019t mention data encryption on S3.\nC: KMS is a key management service to managed the encryption keys. No matter what encryption you use, you will can always KMS to manage the keys. so you can say use KMS to manage encryption keys, but not use KMS for encryption of data.\nD: security group is not helping in giving permission to select statement."
      },
      {
        "date": "2021-10-28T01:33:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2021-10-16T09:29:00.000Z",
        "voteCount": 3,
        "content": "This  is a good example which confirms that A is the answer-&gt; https://noise.getoto.net/2016/12/02/implementing-authorization-and-auditing-using-apache-ranger-on-amazon-emr/.\nThe key is that the question is about EMR HDFS but some of the choices offered talk about EMRFS which may lead to picking up a wrong answer. As this is AWS EMR, we are looking at an HDFS cluster that needs to be protected. Transparent Data Encryption meets the at rest and in transit requirements. So now authorizing access to Hive tables is best offered using Apache ranger as shown in the blog at the above link."
      },
      {
        "date": "2021-10-22T09:24:00.000Z",
        "voteCount": 2,
        "content": "B is correct.\n\"store sensitive information on Amazon S3 and process it through Amazon EMR\" &lt;- this is EMRFS at its best. Option A does not help with encrypting data in S3, nor in transit when it gets loaded from S3 to EMR, which the question clearly asks for."
      },
      {
        "date": "2021-10-16T00:21:00.000Z",
        "voteCount": 1,
        "content": "I dont think B is correct - Amazon EMR supports Hive Authorization(Storage Based Authorization, SQL Standards Based Authorization in HiveServer2) for HDFS but not for EMRFS and Amazon S3. Amazon EMR clusters run with authorization disabled by default."
      },
      {
        "date": "2021-10-27T09:44:00.000Z",
        "voteCount": 1,
        "content": "This article written by AWS employees indicates otherwise stating \"The EMRFS authorization feature specifically applies to access by using HiveServer2.\" https://idk.dev/best-practices-for-securing-amazon-emr/"
      },
      {
        "date": "2021-10-12T18:52:00.000Z",
        "voteCount": 2,
        "content": "my selection B"
      },
      {
        "date": "2021-10-12T12:53:00.000Z",
        "voteCount": 2,
        "content": "It's B\nhttps://cwiki.apache.org/confluence/display/Hive/LanguageManual+Authorization"
      },
      {
        "date": "2021-10-11T20:43:00.000Z",
        "voteCount": 1,
        "content": "Why not C? Is it wrong?"
      },
      {
        "date": "2021-10-08T18:31:00.000Z",
        "voteCount": 1,
        "content": "why nobody has a think about  C?"
      },
      {
        "date": "2021-10-21T05:34:00.000Z",
        "voteCount": 1,
        "content": "Yes, C is the answer. Using KMS IAM roles to control data access is a good pattern that control access to users using the data."
      },
      {
        "date": "2021-11-06T10:08:00.000Z",
        "voteCount": 1,
        "content": "WRONG: IAM roles are not for protecting data in transit"
      },
      {
        "date": "2021-10-05T01:25:00.000Z",
        "voteCount": 2,
        "content": "A is wrong. https://aws.amazon.com/blogs/aws/new-at-rest-and-in-transit-encryption-for-amazon-emr/ \"We already offer several data encryption options for EMR including server and client side encryption for Amazon S3 with EMRFS and Transparent Data Encryption for HDFS. While these solutions do a good job of protecting data at rest, they do not address data stored in temporary files or data that is in flight, moving between job steps. Each of these encryption options must be individually enabled and configured, making the process of implementing encryption more tedious that it need be\""
      },
      {
        "date": "2021-10-08T12:29:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer, as Kerberos authentication in D will not limit access to SELECT statements, it's simply an authentication mechanism"
      },
      {
        "date": "2021-10-12T02:36:00.000Z",
        "voteCount": 1,
        "content": "First of all\n\"transit for traffic between Amazon S3 and EMRFS.\" is Invalid, because EMRFS is already on S3\nSecondly, you can use different IAM roles for EMRFS requests to Amazon S3 based on cluster users, groups, or the location of EMRFS data in Amazon S3. (invalid - authorization on HiveServer2)\nhttps://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-fs.html\nTherefore the correct answer is C"
      },
      {
        "date": "2021-10-02T01:41:00.000Z",
        "voteCount": 2,
        "content": "I choose B"
      },
      {
        "date": "2021-09-22T21:34:00.000Z",
        "voteCount": 1,
        "content": "Apache Ranger is not AWS product, might not a right choice"
      },
      {
        "date": "2021-09-24T14:19:00.000Z",
        "voteCount": 1,
        "content": "Please view the big data exam preparation course on aws. It is mentioned quite heavily and the use case matches https://www.aws.training/Details/Curriculum?id=21332"
      },
      {
        "date": "2021-09-26T09:31:00.000Z",
        "voteCount": 1,
        "content": "https://www.aws.training/Details/Curriculum?id=21332 -&gt; can not open for some reason, login with credential"
      },
      {
        "date": "2021-10-01T04:51:00.000Z",
        "voteCount": 1,
        "content": "It's a free course. You just need to sign in with your amazon/aws account or APN account to access the training."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/amazon/view/4775-exam-aws-certified-big-data-specialty-topic-2-question-14/",
    "body": "Multiple rows in an Amazon Redshift table were accidentally deleted. A System Administrator is restoring the table from the most recent snapshot. The snapshot contains all rows that were in the table before the deletion.<br>What is the SIMPLEST solution to restore the table without impacting users?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore the snapshot to a new Amazon Redshift cluster, then UNLOAD the table to Amazon S3. In the original cluster, TRUNCATE the table, then load the data from Amazon S3 by using a COPY command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Restore Table from a Snapshot command and specify a new table name DROP the original table, then RENAME the new table to the original table name.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestore the snapshot to a new Amazon Redshift cluster. Create a DBLINK between the two clusters in the original cluster, TRUNCATE the destination table, then use an INSERT command to copy the data from the new cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the ALTER TABLE REVERT command and specify a time stamp of immediately before the data deletion. Specify the Amazon Resource Name of the snapshot as the SOURCE and use the OVERWRITE REPLACE option."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-20T20:16:00.000Z",
        "voteCount": 4,
        "content": "B. https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html#working-with-snapshot-restore-table-from-snapshot"
      },
      {
        "date": "2021-10-10T16:32:00.000Z",
        "voteCount": 3,
        "content": "my selection B"
      },
      {
        "date": "2021-10-10T07:22:00.000Z",
        "voteCount": 3,
        "content": "Its B\nhttps://aws.amazon.com/about-aws/whats-new/2016/03/amazon-redshift-now-supports-table-level-restore/"
      },
      {
        "date": "2021-10-08T22:31:00.000Z",
        "voteCount": 1,
        "content": "I think B is the simplest solution here"
      },
      {
        "date": "2021-09-21T11:54:00.000Z",
        "voteCount": 1,
        "content": "I went with A. As the question states  without impacting users."
      },
      {
        "date": "2021-10-02T14:18:00.000Z",
        "voteCount": 3,
        "content": "B is correct. Truncate and load takes time which will impact user hence not A. When are you planing to take the test?"
      },
      {
        "date": "2021-10-02T21:40:00.000Z",
        "voteCount": 1,
        "content": "using google mail"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/amazon/view/4080-exam-aws-certified-big-data-specialty-topic-2-question-15/",
    "body": "An organization's data warehouse contains sales data for reporting purposes. data governance policies prohibit staff from accessing the customers' credit card numbers.<br>How can these policies be adhered to and still allow a Data Scientist to group transactions that use the same credit card number?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore a cryptographic hash of the credit card number.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the credit card number with a symmetric encryption key, and give the key only to the authorized Data Scientist.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMask the credit card numbers to only show the last four digits of the credit card number.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt the credit card number with an asymmetric encryption key and give the decryption key only to the authorized Data Scientist."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-27T02:11:00.000Z",
        "voteCount": 10,
        "content": "Its A, as hash will always be same for a given number. So they can easily group by it. Moreover hashing is one way they can't decrypt."
      },
      {
        "date": "2021-10-27T15:23:00.000Z",
        "voteCount": 3,
        "content": "Agree, name + 4 last digit might work but is not as good as using a hash function.\n- same number will produce the same result\n- can't revert\nhttps://en.wikipedia.org/wiki/Cryptographic_hash_function"
      },
      {
        "date": "2021-11-02T16:18:00.000Z",
        "voteCount": 1,
        "content": "A - is the most suitable \nB &amp; D are wrong - no one should have access to data\nC - will not work with only 4 digits since there may be duplication between different customers"
      },
      {
        "date": "2021-10-31T09:23:00.000Z",
        "voteCount": 1,
        "content": "A... A hash function with the same input always generates the same output, so if there are 10 records of credit card 12345 the hash will generate xyzabc for all those 10 records."
      },
      {
        "date": "2021-10-30T13:08:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer. Its more secure and KMS or HSM can be easily used to achieve this."
      },
      {
        "date": "2021-10-29T00:45:00.000Z",
        "voteCount": 2,
        "content": "my selection A"
      },
      {
        "date": "2021-10-29T02:06:00.000Z",
        "voteCount": 2,
        "content": "Hashing credit card numbers is the most unsafe application practice as per several of the blogposts. It could also be easily hacked using a simple powershell script. I will go with C."
      },
      {
        "date": "2021-10-20T00:41:00.000Z",
        "voteCount": 2,
        "content": "I will go with A"
      },
      {
        "date": "2021-10-17T21:58:00.000Z",
        "voteCount": 3,
        "content": "A CC has 16 digits... if you have only the last 4 you will most likely group incorrect transactions that were made using different CCs. A seems to be correct."
      },
      {
        "date": "2021-10-16T02:12:00.000Z",
        "voteCount": 3,
        "content": "I choose C. By last 4 digit it is possible to group the transactions for a customer. Assuming a customer do not have more than one same last-4 digit cc numbers. The data is needed for data scientist (may be to perform ML). You need to have un-encrypted data to do the modeling. Encrypting/Decrypting for ach Data scientist may not be a viable solution."
      },
      {
        "date": "2021-10-25T06:25:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2021-10-15T17:58:00.000Z",
        "voteCount": 1,
        "content": "so what's the right answer , any conclusion ?"
      },
      {
        "date": "2021-10-12T15:30:00.000Z",
        "voteCount": 4,
        "content": "The last 4 digits are NOT unique... they may duplicate.. so, I think A is correct"
      },
      {
        "date": "2021-10-27T07:09:00.000Z",
        "voteCount": 1,
        "content": "Combine last four digits with CC holder name, so it will be unique to short out.\nSo Option C is correct"
      },
      {
        "date": "2021-10-30T00:52:00.000Z",
        "voteCount": 2,
        "content": "1. You don't know from the question that the data scientist has access to the full name.\n2. 4 digits give you 10k options. The name \"James Smith\" exists more than 38k times in the world. So uniqueness would still be questionable."
      },
      {
        "date": "2021-09-29T00:54:00.000Z",
        "voteCount": 1,
        "content": "Thoughts on this one? I couldn't find any blog posts on this. I was thinking either A or C."
      },
      {
        "date": "2021-10-07T03:47:00.000Z",
        "voteCount": 2,
        "content": "https://security.stackexchange.com/questions/19860/minimum-requirements-for-storing-last-4-digits-of-credit-card-number. Seems you can get away with storing the last 4 digits."
      },
      {
        "date": "2021-10-21T13:25:00.000Z",
        "voteCount": 4,
        "content": "I'd say C is correct. The question said nothing about \"names\", so grouping with name and last 4 digits will help even if 2 customers have same last 4 digits"
      },
      {
        "date": "2021-10-26T23:05:00.000Z",
        "voteCount": 2,
        "content": "A should be correct"
      },
      {
        "date": "2021-11-02T14:01:00.000Z",
        "voteCount": 1,
        "content": "A credit card contains 16 digits, many customers will have the last 4 digits identical with the remaining 12 digits different, this goes against the requirement."
      },
      {
        "date": "2021-10-07T18:41:00.000Z",
        "voteCount": 1,
        "content": "maybe A, how to perform Group while masking and storing the last 4 digits?"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/amazon/view/4081-exam-aws-certified-big-data-specialty-topic-2-question-16/",
    "body": "A real-time bidding company is rebuilding their monolithic application and is focusing on serving real-time data. A large number of reads and writes are generated from thousands of concurrent users who follow items and bid on the company's sale offers.<br>The company is experiencing high latency during special event spikes, with millions of concurrent users.<br>The company needs to analyze and aggregate a part of the data in near real time to feed an internal dashboard.<br>What is the BEST approach for serving and analyzing data, considering the constraint of the row latency on the highly demanded data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Aurora with Multi Availability Zone and read replicas. Use Amazon ElastiCache in front of the read replicas to serve read-only content quickly. Use the same database as datasource for the dashboard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon DynamoDB to store real-time data with Amazon DynamoDB. Accelerator to serve content quickly. use Amazon DynamoDB Streams to replay all changes to the table, process and stream to Amazon Elasti search Service with AWS Lambda.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon RDS with Multi Availability Zone. Provisioned IOPS EBS volume for storage. Enable up to five read replicas to serve read-only content quickly. Use Amazon EMR with Sqoop to import Amazon RDS data into HDFS for analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Redshift with a DC2 node type and a multi-mode cluster. Create an Amazon EC2 instance with pgpoo1 installed. Create an Amazon ElastiCache cluster and route read requests through pgpoo1, and use Amazon Redshift for analysis. D"
    ],
    "answer": "Explanation",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-07T00:32:00.000Z",
        "voteCount": 1,
        "content": "A wrong -&gt; Use Amazon ElastiCache in front of the read replicas?\nC wrong -&gt; why five read replicas?\nD wrong -&gt; 1. Pgpool can run in an Amazon EC2 instance for dev and test and a fleet of EC2 instances with Elastic Load Balancing and Auto Scaling in production however, we strongly recommend that you test pgpool with your PostgreSQL client before making any changes to your architecture. 2. The company needs to analyze and aggregate a part of the data in near real time to feed an internal dashboard. - for Redshift it will not be near real time - For Amazon Redshift destination, Amazon Kinesis Data Firehose delivers data to your Amazon S3 bucket first and then issues Redshift COPY command to load data from your S3 bucket to your Redshift cluster. The frequency of data delivery to Amazon S3 - buffer interval 60 to 900 seconds\nB should be correct"
      },
      {
        "date": "2021-11-04T11:58:00.000Z",
        "voteCount": 1,
        "content": "B for sure"
      },
      {
        "date": "2021-11-03T01:00:00.000Z",
        "voteCount": 1,
        "content": "B looks to be best solution"
      },
      {
        "date": "2021-11-01T12:49:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2021-11-01T04:13:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      },
      {
        "date": "2021-10-31T06:02:00.000Z",
        "voteCount": 1,
        "content": "my selection B"
      },
      {
        "date": "2021-10-27T16:30:00.000Z",
        "voteCount": 1,
        "content": "\u201cThe question is the answer.\u201d\n\u2015 Thomas Vato, Questology\nWhat is the BEST approach for serving and analyzing data, considering the constraint of the low latency on the highly demanded data?\nFirstly, Question asking for analyzing data (The company needs to analyze and aggregate a part of the data in near real time to feed an internal dashboard.), So all about OLAP\nSecondly, low latency on the highly demanded data (The company is experiencing high latency during special event spikes, with millions of concurrent users.)- Sort of Caching solution\nFinally, D is the correct answer"
      },
      {
        "date": "2021-10-27T19:45:00.000Z",
        "voteCount": 4,
        "content": "agregate and analyze data in real-time with Redshift?\nmillions of users and transactions with Redshift?\nnice quote, doesn't work"
      },
      {
        "date": "2021-11-06T11:05:00.000Z",
        "voteCount": 1,
        "content": "AdamSmith - you are 100% right - Redshift is not intended for such workload"
      },
      {
        "date": "2021-10-25T16:47:00.000Z",
        "voteCount": 2,
        "content": "Support B. Real-time bidding is a perfect use case of DynamoDB accelerator."
      },
      {
        "date": "2021-09-20T07:50:00.000Z",
        "voteCount": 1,
        "content": "D's similar setup explained here https://aws.amazon.com/blogs/big-data/using-pgpool-and-amazon-elasticache-for-query-caching-with-amazon-redshift/. Just keen to hear other thoughts selected this option last time and didnt score well in storage section. I thought best practice was to use Redshift for OLAP instead of OLTP. Keen to get everyone else's thoughts?"
      },
      {
        "date": "2021-09-23T16:28:00.000Z",
        "voteCount": 2,
        "content": "Could be as simple as B due to DynamoDB"
      },
      {
        "date": "2021-09-30T01:30:00.000Z",
        "voteCount": 1,
        "content": "Since the question refers to monolithic i am now assuming it is referring to architecture design tiers using Relational Databases.This link https://blog.acolyer.org/2019/03/25/amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases/  discusses the improvements on using Aurora."
      },
      {
        "date": "2021-10-07T13:08:00.000Z",
        "voteCount": 1,
        "content": "A? selected?"
      },
      {
        "date": "2021-10-08T01:28:00.000Z",
        "voteCount": 1,
        "content": "What are your thoughts on D again? AWS blog post shows how it can be done and AWS want users to view and consult the AWS blog posts to assist in creating solutions.  As the question asks 'near real time' and 'BEST approach for serving and analyzing data' redshift can do SQL queries the fastest and with the caching engine attached with pgpool this can be done."
      },
      {
        "date": "2021-10-09T12:29:00.000Z",
        "voteCount": 1,
        "content": "I dont think Amazon Elastisearch Service is the right use case here."
      },
      {
        "date": "2021-10-16T00:06:00.000Z",
        "voteCount": 10,
        "content": "millions of concurrent users, low latency =&gt; DynamoDB and DAX seems correct.\nreal time dash board =&gt; DynamoDB streams helps also elasticsearch can do aggregation."
      },
      {
        "date": "2021-10-16T15:38:00.000Z",
        "voteCount": 1,
        "content": "Thanks for the correction @ranabhay"
      },
      {
        "date": "2021-10-16T01:23:00.000Z",
        "voteCount": 23,
        "content": "Apologies. Have done more research on this question. I think the answer is B not D. The reason for D being incorrect is AWS big data blog post mentions only having 6 to 10 users for the use case. B for the following reasons DynamoDB scales well for the millions of users, DynamoDB streams can be used to aggregate data, Lambda function to push to Elasticsearch. Elasticsearch can be used for application monitoring and Analyzing product usage data. Has Kibana visualisations. Lastly question doesn't mention anything about querying\n1.https://rockset.com/blog/live-dashboards-dynamodb-streams-lambda-elasticache/\n2.https://aws.amazon.com/blogs/compute/indexing-amazon-dynamodb-content-with-amazon-elasticsearch-service-using-aws-lambda/\n3. https://aws.amazon.com/blogs/startups/combining-dynamodb-and-amazon-elasticsearch-with-lambda/\n4.https://d1.awsstatic.com/whitepapers/Big_Data_Analytics_Options_on_AWS.pdf?did=wp_card&amp;trk=wp_card"
      },
      {
        "date": "2021-10-20T20:06:00.000Z",
        "voteCount": 6,
        "content": "Good catch, B +1"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/amazon/view/4082-exam-aws-certified-big-data-specialty-topic-2-question-17/",
    "body": "A gas company needs to monitor gas pressure in their pipelines. Pressure data is streamed from sensors placed throughout the pipelines to monitor the data in real time. When an anomaly is detected, the system must send a notification to open valve. An Amazon Kinesis stream collects the data from the sensors and an anomaly Kinesis stream triggers an AWS Lambda function to open the appropriate valve.<br>Which solution is the MOST cost-effective for responding to anomalies in real time?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach a Kinesis Firehose to the stream and persist the sensor data in an Amazon S3 bucket. Schedule an AWS Lambda function to run a query in Amazon Athena against the data in Amazon S3 to identify anomalies. When a change is detected, the Lambda function sends a message to the anomaly stream to open the valve.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch an Amazon EMR cluster that uses Spark Streaming to connect to the Kinesis stream and Spark machine learning to detect anomalies. When a change is detected, the Spark application sends a message to the anomaly stream to open the valve.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLaunch a fleet of Amazon EC2 instances with a Kinesis Client Library application that consumes the stream and aggregates sensor data over time to identify anomalies. When an anomaly is detected, the application sends a message to the anomaly stream to open the valve.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Kinesis Analytics application by using the RANDOM_CUT_FOREST function to detect an anomaly. When the anomaly score that is returned from the function is outside of an acceptable range, a message is sent to the anomaly stream to open the valve."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-22T06:38:00.000Z",
        "voteCount": 6,
        "content": "Its D.\n1. https://sagemaker-workshop.com/builtin/rcf.html\n2. https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html"
      },
      {
        "date": "2021-11-04T07:35:00.000Z",
        "voteCount": 1,
        "content": "Monitor data in realtime + Random cut forect term --&gt; Inclines to Kinesis data analytics"
      },
      {
        "date": "2021-10-27T10:07:00.000Z",
        "voteCount": 3,
        "content": "my selection D"
      },
      {
        "date": "2021-10-23T10:34:00.000Z",
        "voteCount": 3,
        "content": "whenever see anomaly, think Random cut forest"
      },
      {
        "date": "2021-10-09T20:40:00.000Z",
        "voteCount": 2,
        "content": "D for now after reading url link from below"
      },
      {
        "date": "2021-10-18T16:54:00.000Z",
        "voteCount": 2,
        "content": "Also very good description in the big data exam preparation course on AWS. https://www.aws.training/Details/Curriculum?id=21332"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/amazon/view/4083-exam-aws-certified-big-data-specialty-topic-2-question-18/",
    "body": "A gaming organization is developing a new game and would like to offer real-time competition to their users. The data architecture has the following characteristics:<br>\u2711 The game application is writing events directly to Amazon DynamoDB from the user's mobile device.<br>\u2711 Users from the website can access their statistics directly from DynamoDB.<br>\u2711 The game servers are accessing DynamoDB to update the user's information.<br>\u2711 The data science team extracts data from DynamoDB for various applications.<br>The engineering team has already agreed to the IAM roles and policies to use for the data science team and the application.<br>Which actions will provide the MOST security, while maintaining the necessary access to the website and game application? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Amazon Cognito user pool to authenticate to both the website and the game application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse IAM identity federation to authenticate to both the website and the game application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy with PUT permission for both the website and the game application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy with fine-grained permission for both the website and the game application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IAM policy with PUT permission for the game application and an IAM policy with GET permission for the website."
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-17T15:36:00.000Z",
        "voteCount": 8,
        "content": "Answer : A, D\nA-\tMobile app integrating with an application hosted on AWS. So Cognito is a default choice allowing user to use their social media account to login and assume temporary credentials to login to the org application backing the mobile app or the website server backing the website assuming there are 2 different backend application for the mobile and web app.\nB-\tNot a good choice for mobile users. Works for internal users.\nC-\tThis is wrong as the website needs the ability to do a GET as well.\nD-\tRefer to this link -&gt; https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html and go down to the Limiting User access section where you will see how both the mobile app and the website users can be restricted using fine grained IAM policy using their AWS user ID or their google or facebook userID.\nE-\tNot a good choice because we want users to access only their statistical information and to be able to update their own profiles."
      },
      {
        "date": "2021-10-19T16:10:00.000Z",
        "voteCount": 1,
        "content": "AD looks correct. D is a very clear choice based on the link that you provided."
      },
      {
        "date": "2021-10-27T21:45:00.000Z",
        "voteCount": 1,
        "content": "A,D - exactly, for example, the game app preceding limits access in this way so that users can only access game data that is associated with their user ID."
      },
      {
        "date": "2021-11-05T02:54:00.000Z",
        "voteCount": 1,
        "content": "How would D work when website requires read permissions whereas the gaming application requires write access. Can the same IAM policy have a read permission for website and write permission for gaming app? Having one policy for each permission is cleaner in my opinion, going with A and E."
      },
      {
        "date": "2021-11-06T17:54:00.000Z",
        "voteCount": 1,
        "content": "Can the same IAM policy have a read permission for website and write permission for gaming app? -&gt; YES can have"
      },
      {
        "date": "2021-09-20T14:56:00.000Z",
        "voteCount": 5,
        "content": "Thoughts on A &amp; D?\nhttps://docs.amazonaws.cn/en_us/IAM/latest/UserGuide/id_credentials_temp.html\nhttps://aws.amazon.com/iam/\nhttps://aws.amazon.com/blogs/security/create-fine-grained-session-permissions-using-iam-managed-policies/"
      },
      {
        "date": "2021-11-04T09:52:00.000Z",
        "voteCount": 1,
        "content": "It is BD, A is wrong since if you need the app or website to access to ddb, you need Cognito Identity Pool, but not only User Pool."
      },
      {
        "date": "2021-11-07T04:45:00.000Z",
        "voteCount": 1,
        "content": "Hailiang is 100% correct - Identity pool use cases - Give your users access to AWS resources, such as an Amazon DynamoDB table. \nBUT\nWhen you create User Pool you will be forced to create at least one Identity Poll :)\nSince a gaming organization is developing a !!new game!! the option A&amp;D would fit best;\n- Use Amazon Cognito user pool to authenticate to both the website and the game application.\n- Create an IAM policy with fine-grained permission for both the website and the game application."
      },
      {
        "date": "2021-11-02T16:44:00.000Z",
        "voteCount": 1,
        "content": "A and D. I thought A and E at first sight, but fine grained is necessary, as you don't want to allow the website itself, but a user coming from that website.\nAlso, it is possible to have multiple statements in a policy, as some thought that this is not possible:\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_statement.html"
      },
      {
        "date": "2021-11-02T04:01:00.000Z",
        "voteCount": 1,
        "content": "A + D\n\nhttps://aws.amazon.com/blogs/aws/fine-grained-access-control-for-amazon-dynamodb/"
      },
      {
        "date": "2021-10-12T09:33:00.000Z",
        "voteCount": 1,
        "content": "Why not B and E\n\nE is more finegrained as it is only allowing read for website and write for mobile"
      },
      {
        "date": "2021-10-11T11:54:00.000Z",
        "voteCount": 2,
        "content": "my selection AE"
      },
      {
        "date": "2021-10-09T19:43:00.000Z",
        "voteCount": 1,
        "content": "A is obvious.\nD sounds right but the catch is using a single IAM policy for both the web server and the app, which is pretty bad.\nE satisfies the requirements.\nStill a hard choice but I'd go with E."
      },
      {
        "date": "2021-10-05T09:08:00.000Z",
        "voteCount": 1,
        "content": "Option B\nIdentity Providers and Federation\nIt is also useful if you are creating a mobile app or web application that requires access to AWS resources.\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers.html"
      },
      {
        "date": "2021-09-28T13:22:00.000Z",
        "voteCount": 1,
        "content": "A is correct for sure.\nNot sure whether D or E should be the 2nd answer"
      },
      {
        "date": "2021-10-02T07:23:00.000Z",
        "voteCount": 4,
        "content": "A&amp;E; D is incorrect because we need different policies for game application role &amp; user role from mobile devices."
      },
      {
        "date": "2021-10-05T01:09:00.000Z",
        "voteCount": 1,
        "content": "in d it just says fine grained control but nothing about same or diff policies, so d is good"
      },
      {
        "date": "2021-09-26T08:56:00.000Z",
        "voteCount": 1,
        "content": "what's the right answer  , any conclusion ?"
      },
      {
        "date": "2021-09-27T08:33:00.000Z",
        "voteCount": 1,
        "content": "I'm sure about A but not sure between D and E. My issue with D is that is states \"create one IAM policy\" - not sure if it can accommodate both user types (mobile, web) which need different permissions. E seems straightforward - PUT for mobile and GET for web..."
      },
      {
        "date": "2021-10-04T14:45:00.000Z",
        "voteCount": 1,
        "content": "Option D&amp;E both says \"an IAM policy\", so it doesn't mean same permission for both mobile and web. Answer still seems to be AD"
      },
      {
        "date": "2021-09-24T03:16:00.000Z",
        "voteCount": 2,
        "content": "Hi,\nNeed your input.\nAre these question really on actual exam? all of them?\nHave you scheduled/taken your exam how did you perform ?"
      },
      {
        "date": "2021-09-25T18:31:00.000Z",
        "voteCount": 5,
        "content": "Yes, @ranabhay majority of these questions were on my exam. But as you have noticed some of the selected answers are incorrect which is why i have been so active to discuss the reasons why for certain answers. As you can tell with these questions they aren't worded very well on purpose to make you either over or under think the solution."
      },
      {
        "date": "2021-09-22T22:34:00.000Z",
        "voteCount": 3,
        "content": "A=cognito, -&gt; mobile device\nD= fine grain IAM\nmake sense,"
      },
      {
        "date": "2021-10-04T00:25:00.000Z",
        "voteCount": 2,
        "content": "question doesn't say mobile app. I think BE is correct"
      },
      {
        "date": "2021-10-04T07:43:00.000Z",
        "voteCount": 2,
        "content": "\"The game application is writing events directly to Amazon DynamoDB from the user\u00e2\u20ac\u2122s \"mobile device\" ...so it's a mobile app. Also fined grain IAM access doesn't mean single policy for both. AD seems right"
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/amazon/view/4084-exam-aws-certified-big-data-specialty-topic-2-question-19/",
    "body": "An organization has 10,000 devices that generate 100 GB of telemetry data per day, with each record size around 10 KB. Each record has 100 fields, and one field consists of unstructured log data with a \"String\" data type in the English language. Some fields are required for the real-time dashboard, but all fields must be available for long-term generation.<br>The organization also has 10 PB of previously cleaned and structured data, partitioned by Date, in a SAN that must be migrated to AWS within one month.<br>Currently, the organization does not have any real-time capabilities in their solution. Because of storage limitations in the on-premises data warehouse, selective data is loaded while generating the long-term trend with ANSI SQL queries through JDBC for visualization. In addition to the one-time data loading, the organization needs a cost-effective and real-time solution.<br>How can these requirements be met? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tuse AWS IoT to send data from devices to an Amazon SQS queue, create a set of workers in an Auto Scaling group and read records in batch from the queue to process and save the data. Fan out to an Amazon SNS queue attached with an AWS Lambda function to filter the request dataset and save it to Amazon Elasticsearch Service for real-time analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Direct Connect connection between AWS and the on-premises data center and copy the data to Amazon S3 using S3 Acceleration. Use Amazon Athena to query the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS IoT to send the data from devices to Amazon Kinesis Data Streams with the IoT rules engine. Use one Kinesis Data Firehose stream attached to a Kinesis stream to batch and stream the data partitioned by date. Use another Kinesis Firehose stream attached to the same Kinesis stream to filter out the required fields to ingest into Elasticsearch for real-time analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse AWS IoT to send the data from devices to Amazon Kinesis Data Streams with the IoT rules engine. Use one Kinesis Data Firehose stream attached to a Kinesis stream to stream the data into an Amazon S3 bucket partitioned by date. Attach an AWS Lambda function with the same Kinesis stream to filter out the required fields for ingestion into Amazon DynamoDB for real-time analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tuse multiple AWS Snowball Edge devices to transfer data to Amazon S3, and use Amazon Athena to query the data."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-10T14:23:00.000Z",
        "voteCount": 8,
        "content": "A is too slow.\nSo between B and E for the transportation problem.\nQuick math, 10G bit  ~ 1G byte/sec ~ 73 TB/day ~ 2 PB/month\nYou should remember this because you will need this math in many other exams too.\nSo E.\n\nBetween C and D for the storage and analysis.\nSee unstructured text =&gt; Think DynamoDB\n\nConclusion D, E"
      },
      {
        "date": "2021-11-01T16:21:00.000Z",
        "voteCount": 1,
        "content": "you don't really need the math to choose between B or E for two obvious reasons. Direct connect would take around two months to be established. 2. The question states \"one time upload\". You don't need a Direct connect for a one off use."
      },
      {
        "date": "2021-11-04T10:21:00.000Z",
        "voteCount": 1,
        "content": "DynamoDB is not for real-time analytics....D is wrong. It must be C,E."
      },
      {
        "date": "2021-09-24T12:25:00.000Z",
        "voteCount": 7,
        "content": "It's D and E ... SQS can not hold more than 256B for a value.. but the question says \"each record is 10KB\", so SQS is out, so A is out. To copy the 10PB of data, you need multiple Snowball Edge, so E is in. Direct Connect (B) and stream data partitioned by date (C) are also not valid. So, the answer should be D and E."
      },
      {
        "date": "2021-09-25T17:23:00.000Z",
        "voteCount": 3,
        "content": "sorry.. SQS limit is 256 KB, and not Bytes."
      },
      {
        "date": "2021-09-26T01:53:00.000Z",
        "voteCount": 4,
        "content": "agreed, for realtime on new data need \"stream\" not \"batch\" (D), for existing data need to use Snowball (E)."
      },
      {
        "date": "2021-09-28T19:07:00.000Z",
        "voteCount": 1,
        "content": "Option D says partition bucket by date. Only 100 buckets allowed for an AWS Account. So the answer should be C &amp; E"
      },
      {
        "date": "2021-11-05T12:02:00.000Z",
        "voteCount": 1,
        "content": "A is wrong - set of workers in autoscaling group? Exactly what will happen there? (\"read records in batch from the queue\")\nB is wrong - will take more than 1 month to get established and working \nC is correct - with Kinesis Firehose you can transform data (using Lambda)\nD is wrong - DynamoDB is not used for Real time Analytics which typically involve large range scans and complex operations such as grouping and aggregation (\"selective data is loaded while generating the long-term trend with ANSI SQL queries through JDBC for visualization\")\nE is correct"
      },
      {
        "date": "2021-11-04T09:50:00.000Z",
        "voteCount": 1,
        "content": "D\nE\nhttps://aws.amazon.com/fr/getting-started/hands-on/build-serverless-real-time-data-processing-app-lambda-kinesis-s3-dynamodb-cognito-athena/\nLook at the architecture ... inspiring"
      },
      {
        "date": "2021-10-22T12:34:00.000Z",
        "voteCount": 2,
        "content": "C look good if store in s3, as data in firehose expire in 24hours.\nfor D, DynamoDB for real-time analytics, weird."
      },
      {
        "date": "2021-10-31T05:04:00.000Z",
        "voteCount": 2,
        "content": "C states \"Use Firehose to ingest into Elasticsearch for real-time analytics\" -&gt; Firehose can only do near-realtime and thus C must be false."
      },
      {
        "date": "2021-10-18T18:34:00.000Z",
        "voteCount": 1,
        "content": "E for transferring data.\nBoth C and D have inconsistency regarding real-time analytics. FH has 60 seconds delay and DynamoDB is not suited for real-time analytics. So in this scenario i prefer dealing with FH not being truly real-time than having real-time analytics over DynamoDB table. Then C and E."
      },
      {
        "date": "2021-10-21T08:25:00.000Z",
        "voteCount": 1,
        "content": "Agreed that DynamoDB is not suited for real-time analytics by itself. However, you may build real-time dashboards based on the content of a DynamoDB table, see for example here: https://aws.amazon.com/blogs/startups/building-dynamic-dashboards-using-lambda-and-dynamodb-streams-part-1/"
      },
      {
        "date": "2021-10-18T09:23:00.000Z",
        "voteCount": 5,
        "content": "D is the answer for real-time analytics;None of the other options offer real-time analytics. ElasticSearch is for near real-time analytics.\nE is the other answer for transfering PB data."
      },
      {
        "date": "2021-10-17T23:51:00.000Z",
        "voteCount": 1,
        "content": "DE\nA-sqs is typically for a decouple scenario, not for big data\nB- far from real-time\nC-firehose has 60 seconds latency, again not real-time"
      },
      {
        "date": "2021-10-15T09:56:00.000Z",
        "voteCount": 1,
        "content": "my selection DE"
      },
      {
        "date": "2021-09-27T10:29:00.000Z",
        "voteCount": 2,
        "content": "Option D says partition bucket by date. Only 100 buckets allowed for an AWS Account. So the answer should be C &amp; E"
      },
      {
        "date": "2021-09-29T18:17:00.000Z",
        "voteCount": 4,
        "content": "even with 1 bucket you can still do the partitioning - just with the object keys"
      },
      {
        "date": "2021-09-29T22:54:00.000Z",
        "voteCount": 1,
        "content": "Kinesis stream can't filter out the required fields"
      },
      {
        "date": "2021-09-24T04:13:00.000Z",
        "voteCount": 6,
        "content": "Should be D, E. E for large data transfer. D for real-time dashboard as C is near real-time"
      },
      {
        "date": "2021-10-07T07:33:00.000Z",
        "voteCount": 1,
        "content": "\"but all fields must be available for long-term generation\"\nWhat's why C, because Dynamo DB has some limitations for this kind of analysis"
      },
      {
        "date": "2021-09-22T19:46:00.000Z",
        "voteCount": 2,
        "content": "Cost effective and real time, lamba and s3 are good candidates A,D?"
      },
      {
        "date": "2021-09-23T21:29:00.000Z",
        "voteCount": 4,
        "content": "Im going to go with C &amp; E. I selected A,D last time. C as FH allows batching and custom transformations to obtain the date data to connect to the near real time dashboard. Also C uses multiple FH streams. E as its a PB data transport solution so you can load data from the SAN into S3 within a couple of days and will allow queries from Athena."
      },
      {
        "date": "2021-09-19T16:55:00.000Z",
        "voteCount": 1,
        "content": "Thoughts on B and D? B due to Athena is mentioned with ANSI SQL queries through JDBC."
      },
      {
        "date": "2021-09-20T16:50:00.000Z",
        "voteCount": 2,
        "content": "Apologies B &amp; C. has visualization aspect from Kibana in elastic search."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/amazon/view/4403-exam-aws-certified-big-data-specialty-topic-2-question-20/",
    "body": "An organization is designing a public web application and has a requirement that states all application users must be centrally authenticated before any operations are permitted. The organization will need to create a user table with fast data lookup for the application in which a user can read only his or her own data. All users already have an account with amazon.com.<br>How can these requirements be met?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS Aurora table, with Amazon_ID as the primary key. The application uses amazon.com web identity federation to get a token that is used to assume an IAM role from AWS STS. Use IAM database authentication by using the rds:db-tag IAM authentication policy and GRANT Amazon RDS row-level read permission per user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon RDS Aurora table, with Amazon_ID as the primary key for each user. The application uses amazon.com web identity federation to get a token that is used to assume an IAM role. Use IAM database authentication by using rds:db-tag IAM authentication policy and GRANT Amazon RDS row- level read permission per user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DynamoDB table, with Amazon_ID as the partition key. The application uses amazon.com web identity federation to get a token that is used to assume an IAM role from AWS STS in the Role, use IAM condition context key dynamodb:LeadingKeys with IAM substitution variables $ and allow the required DynamoDB API operations in IAM JSON policy Action element for reading the records. {www.amazon.com:user_id}",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Amazon DynamoDB table, with Amazon_ID as the partition key. The application uses amazon.com web identity federation to assume an IAM role from AWS STS in the Role, use IAM condition context key dynamodb:LeadingKeys with IAM substitution variables $ {www.amazon.com:user_id} and allow the required DynamoDB API operations in IAM JSON policy Action element for reading the records."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-05T13:45:00.000Z",
        "voteCount": 6,
        "content": "For the question on Dynamo DB that @mattyb123 has posted here, the answer is A and not B. You should always use IAM role and attach an IAM policy to that role on EC2 instance to be able to access both Dynamo DB and DAX."
      },
      {
        "date": "2021-11-06T00:00:00.000Z",
        "voteCount": 1,
        "content": "\"A\" - Because , Dynamo blends more with DAX cannot think of elasticache. Role is Must"
      },
      {
        "date": "2021-11-03T05:55:00.000Z",
        "voteCount": 1,
        "content": "Thank you for helping me and everyone"
      },
      {
        "date": "2021-10-30T22:30:00.000Z",
        "voteCount": 2,
        "content": "my selection C"
      },
      {
        "date": "2021-10-23T06:27:00.000Z",
        "voteCount": 1,
        "content": "The issues boil down to IAM user VS  IAM role.\nAnd  IAM user on the EC2 instance is an invalid option\nTherefore A is the correct answer."
      },
      {
        "date": "2021-10-15T16:51:00.000Z",
        "voteCount": 1,
        "content": "What's the answer finally?"
      },
      {
        "date": "2021-10-18T18:05:00.000Z",
        "voteCount": 2,
        "content": "C should be the correct one."
      },
      {
        "date": "2021-10-13T05:28:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer  : as token is mentioned and it's very important while it's missing in option D. see below article\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WIF.html"
      },
      {
        "date": "2021-10-04T01:08:00.000Z",
        "voteCount": 1,
        "content": "C is correct for question 20 as per the following link:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WIF.html\nreview the Web Identity Federation Overview section."
      },
      {
        "date": "2021-10-07T06:13:00.000Z",
        "voteCount": 1,
        "content": "https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html additional reading"
      },
      {
        "date": "2021-10-02T10:15:00.000Z",
        "voteCount": 3,
        "content": "@mattyb123, i took the exam recently and it looks most of the questions are from the thread, and the answer i was provided is the one which is specified in the comment section. even though i passed some answers are wrong. i request you take some more time to review each questions before you guys go with final exam."
      },
      {
        "date": "2021-10-03T05:33:00.000Z",
        "voteCount": 1,
        "content": "@revs1610 thanks for the feedback. A few of us have been working on the correct solutions by writing the reasons why in the comments throughout the past 17 pages, as we have also noticed mistakes with the answers. If you can spare some time we would appreciate you take a look at some of the comments and assist where we may be wrong."
      },
      {
        "date": "2021-09-23T22:24:00.000Z",
        "voteCount": 2,
        "content": "with the requirement \"Minimal changes to application code to improve performance using write-through cache\", the answer could be B instead of D. base on the https://www.quora.com/What-is-the-difference-between-ElastiCache-and-DynamoDB-Accelerator-in-AWS"
      },
      {
        "date": "2021-09-29T01:30:00.000Z",
        "voteCount": 1,
        "content": "thanks @revs1610 i think you are right with B after reviewing that link. Answer has to be either B or D due to the RCU/WCU calculation.\n1.https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.client.create-user-policy.html Mentions providing ec2 user permissions to use DAX application client to secure the the ec2 instance instead of assuming a role or service role which makes sense for the answer to be B. Only reason i didn't think of DAX was the millisecond requirement which Elasticache performs."
      },
      {
        "date": "2021-09-20T14:46:00.000Z",
        "voteCount": 3,
        "content": "An organization is designing an Amazon DynamoDB table for an application that must meet the following requirements:\n- item size is 40 KB\n- Read/Write ratio 2000/500 sustained, respectively\n- Heavily read-oriented and requires low latencies in the order of milliseconds\n- The application runs on an Amazon Ec2 instance\n- Access to the DynamoDB table must be secure within the VPC\n- Minimal changes to application code to improve performance using write-through cache\nWhich design options will BEST meet these requirements?\nA. Size the DynamoDB table with 10000 RCU/20000 WCUs, implement the DynamoDB Accelerator (DAX) for read performance, use VPC endpoints for DynamoDB, and implement an IAM role on the EC2 Instance to secure DynamoDB access.\nB. Size the DynamoDB table with 20000 RCU/20000 WCUs, implement the DynamoDB Accelerator (DAX) for read performance, leverage VPC endpoints for DynamoDB, and implement an IAM user on the EC2 Instance to secure DynamoDB access."
      },
      {
        "date": "2021-09-21T00:19:00.000Z",
        "voteCount": 2,
        "content": "C. Size the DynamoDB table with 10000 RCU/20000 WCUs, implement Amazon ElastiCache for read performance, set up a NAT gateway on VPC for the EC2 instance to access DynamoDB, and implement an IAM role on the EC2 Instance to secure DynamoDB access.\nD. Size the DynamoDB table with 20000 RCU/20000 WCUs, implement Amazon ElastiCache for read performance, leverage VPC endpoints for DynamoDB, and implement an IAM user on the EC2 Instance to secure DynamoDB access."
      },
      {
        "date": "2021-10-14T07:40:00.000Z",
        "voteCount": 7,
        "content": "Default dynamo behavious (DAX included) is eventual consistent reads. Question does not mention strongly consistent reads. RCU should be 10k not 20k. DAX will work in this scenario. EC2 should use IAM role not user. Answer A seems correct as dynamo needs VPC endpoint."
      },
      {
        "date": "2021-10-28T01:59:00.000Z",
        "voteCount": 3,
        "content": "agree,\nnever select an answer that give a user to EC2 instead of a role.\nAlso always think DAX when seeing milliseconds.\nAlso VPC so there must be an Endpoint.\nThe only answer that satisfies all of the above is A."
      },
      {
        "date": "2021-11-06T06:24:00.000Z",
        "voteCount": 1,
        "content": "A. Table with 10000 RCU/20000 WCU implies eventually consistent model which has a lower latency compared to strongly consistent (https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html). And of course DAX integrates better with DynamoDB with millisecond to microsecond latency."
      },
      {
        "date": "2021-09-22T02:23:00.000Z",
        "voteCount": 1,
        "content": "I think answer is D due to. \n1 provides milliseconds, \n2. provides write through cache \n3. RCU 40/4=10 10*2000 = 20000\n4. WCU 40*500= 20000\nOnly part i dont like is the ec2 instance implementing a user not a role."
      },
      {
        "date": "2021-10-11T18:29:00.000Z",
        "voteCount": 1,
        "content": "If you want to write through cache to dynamodb.. don't we need DAX? I think the answer is B for this question. I don't think you can update dynamo-db through ElastiCache. Please correct me if I am wrong."
      }
    ],
    "examNameCode": "aws-certified-big-data-specialty",
    "topicNumber": "2"
  }
]