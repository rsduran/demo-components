[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/databricks/view/107311-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following describes the Spark driver?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Spark driver is responsible for performing all execution in all execution modes \u2013 it is the entire Spark application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Spare driver is fault tolerant \u2013 if it fails, it will recover the entire Spark application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Spark driver is the coarsest level of the Spark execution hierarchy \u2013 it is synonymous with the Spark application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Spark driver is the program space in which the Spark application\u2019s main method runs coordinating the Spark entire application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Spark driver is horizontally scaled to increase overall processing throughput of a Spark application."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-14T10:11:00.000Z",
        "voteCount": 1,
        "content": "The Spark driver is responsible for orchestrating the execution of a Spark application, managing the SparkContext, and coordinating the execution of tasks across the Spark cluster. It does not perform the execution of tasks itself but rather schedules tasks on the worker nodes. The Spark driver is not fault-tolerant in the sense that if it fails, the entire Spark application usually fails. It also does not scale horizontally; only the executors (worker nodes) do that."
      },
      {
        "date": "2024-06-24T03:32:00.000Z",
        "voteCount": 1,
        "content": "ignore my previous comment.\nE is the answer as per the sample exam in Databricks site."
      },
      {
        "date": "2024-06-24T03:30:00.000Z",
        "voteCount": 1,
        "content": "B is the answer as per the sample exam in Databricks site."
      },
      {
        "date": "2024-05-24T22:36:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2024-04-21T19:12:00.000Z",
        "voteCount": 1,
        "content": "Answer -D"
      },
      {
        "date": "2023-10-20T14:34:00.000Z",
        "voteCount": 1,
        "content": "Please let me know what are these dumps used for? \nScala - Spark or Python Spark?"
      },
      {
        "date": "2023-07-31T00:47:00.000Z",
        "voteCount": 4,
        "content": "Answer: D\nReceives the user's code and breaks it into tasks for execution.\nOrchestrates the execution plan and optimizes the Spark job.\nCoordinates with cluster managers to allocate resources for tasks.\nCollects and aggregates results from distributed workers.\nMaintains the metadata and state of the Spark application during its execution."
      },
      {
        "date": "2023-06-17T08:44:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is D. The Spark driver is the program space in which the Spark application's main method runs, coordinating the entire Spark application.\n\nExplanation: The Spark driver is a program that runs the main method of a Spark application and coordinates the execution of the entire application. It is responsible for defining the SparkContext, which is the entry point for any Spark functionality. The driver program is responsible for dividing the Spark application into tasks, scheduling them on the cluster, and managing the overall execution. The driver communicates with the cluster manager to allocate resources and coordinate the distribution of tasks to the worker nodes. It also maintains the overall control and monitoring of the application. Horizontal scaling, fault tolerance, and execution modes are not directly related to the Spark driver."
      },
      {
        "date": "2023-04-24T06:14:00.000Z",
        "voteCount": 3,
        "content": "D. The Spark driver is the program space in which the Spark application\u2019s main method runs coordinating the Spark entire application.\n\nThe Spark driver is responsible for coordinating the execution of a Spark application, and it runs in the program space where the Spark application's main method runs. It manages the scheduling, distribution, and monitoring of tasks across the cluster, and it communicates with the cluster manager to acquire resources and allocate them to the application. The driver also maintains the state of the application and collects results. It is not responsible for performing all execution in all execution modes, nor is it fault-tolerant or horizontally scalable."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/databricks/view/104977-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following describes the relationship between nodes and executors?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutors and nodes are not related.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAnode is a processing engine running on an executor.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn executor is a processing engine running on a node.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are always the same number of executors and nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are always more nodes than executors."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-03T08:01:00.000Z",
        "voteCount": 6,
        "content": "In a Spark cluster, each node typically has multiple executors, which are responsible for executing tasks on that node. An executor is a separate process that runs on a node and is responsible for executing tasks assigned to it by the driver. Therefore, a node can have multiple executors running on it. The number of executors on a node depends on the resources available on that node and the configuration settings of the Spark application. So, option C is the correct answer."
      },
      {
        "date": "2024-05-24T22:37:00.000Z",
        "voteCount": 3,
        "content": "Executor is a process inside a node. Hence answer is C"
      },
      {
        "date": "2024-03-12T02:09:00.000Z",
        "voteCount": 3,
        "content": "A) is C. nodes are the physical machines in the Spark cluster, while executors are the worker processes running on these nodes, responsible for executing tasks and processing data as part of a Spark application."
      },
      {
        "date": "2024-02-03T22:53:00.000Z",
        "voteCount": 1,
        "content": "this is so wrong, it is option C"
      },
      {
        "date": "2024-02-03T08:30:00.000Z",
        "voteCount": 1,
        "content": "It is C, this solution is so wrong.\nExecutors are the processes running on the Nodes/Workers/Machines."
      },
      {
        "date": "2024-01-24T05:58:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2023-10-29T10:19:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-09-20T01:56:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C. The other (currently selected as correct )answer D is also wrong the total number of nodes is always larger than the executor nodes by 1 sins we need one of the Cluster nodes to run the Driver."
      },
      {
        "date": "2023-08-03T08:05:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      },
      {
        "date": "2023-08-01T15:08:00.000Z",
        "voteCount": 1,
        "content": "Which one to accept as an answer community answer or a Suggested answer from exam topics? Are the examtopics answers trustworthy?"
      },
      {
        "date": "2023-07-31T00:59:00.000Z",
        "voteCount": 1,
        "content": "C. Spark executors run on worker nodes and are responsible for executing tasks and storing intermediate data during data processing. The nodes represent the physical machines that provide computing resources to run the Spark application."
      },
      {
        "date": "2023-06-28T20:07:00.000Z",
        "voteCount": 1,
        "content": "An executor is a process that is launched for a Spark application on a worker node."
      },
      {
        "date": "2023-06-17T08:47:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C. An executor is a processing engine running on a node.\n\nExplanation: In Apache Spark, a node refers to a physical or virtual machine in a cluster that is part of the Spark cluster. Each node can have one or more executors running on it. An executor is a Spark component responsible for executing tasks and storing data in memory or on disk. It is a worker process that runs on a node and performs the actual computation and data processing tasks assigned to it by the driver program. Executors are created and managed by the cluster manager, and they are responsible for executing the tasks and managing the data partitions assigned to them."
      },
      {
        "date": "2023-04-15T17:10:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C. The executor runs in a node."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/databricks/view/104978-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following will occur if there are more slots than there are tasks?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Spark job will likely not run as efficiently as possible.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Spark application will fail \u2013 there must be at least as many tasks as there are slots.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSome executors will shut down and allocate all slots on larger executors first.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMore tasks will be automatically generated to ensure all slots are being used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Spark job will use just one single slot to perform all tasks."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-03T08:02:00.000Z",
        "voteCount": 5,
        "content": "Slots are the basic unit of parallelism in Spark, and represent a unit of resource allocation on a single executor. If there are more slots than there are tasks, it means that some of the slots will be idle and not being used to execute any tasks, leading to inefficient resource utilization. In this scenario, the Spark job will likely not run as efficiently as possible. However, it is still possible for the Spark job to complete successfully. Therefore, option A is the correct answer."
      },
      {
        "date": "2024-08-14T10:21:00.000Z",
        "voteCount": 1,
        "content": "If there are more slots (i.e., available cores) than tasks, some of the slots will remain idle, leading to underutilization of resources. This can result in less efficient execution because the available resources are not being fully utilized."
      },
      {
        "date": "2024-06-24T04:57:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-05-24T22:38:00.000Z",
        "voteCount": 1,
        "content": "Answer - A"
      },
      {
        "date": "2024-03-25T00:20:00.000Z",
        "voteCount": 1,
        "content": "it is A"
      },
      {
        "date": "2023-12-29T18:19:00.000Z",
        "voteCount": 2,
        "content": "C. Some executors will shut down and allocate all slots on larger executors first.\n\nexplanation :If there are more slots than there are tasks in Apache Spark, some executors may shut down, and the available slots will be allocated to larger executors first. This process is part of the dynamic resource allocation mechanism in Spark, where resources are adjusted based on the workload. It helps in efficient resource utilization by shutting down unnecessary executors and allocating resources to larger executors to perform tasks more efficiently."
      },
      {
        "date": "2024-07-16T03:46:00.000Z",
        "voteCount": 1,
        "content": "there is dynamic property : spark.dynamicAllocation.executorIdleTimeout which reallocate executors when they are idle"
      },
      {
        "date": "2023-12-27T21:39:00.000Z",
        "voteCount": 1,
        "content": "E , When there are more available slots than tasks, Spark will use a single slot to perform all tasks, which may result in inefficient use of resources."
      },
      {
        "date": "2023-12-27T21:38:00.000Z",
        "voteCount": 1,
        "content": "E es Correct"
      },
      {
        "date": "2023-12-17T05:08:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-08-03T08:06:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-07-31T01:04:00.000Z",
        "voteCount": 1,
        "content": "A.  IF there are more slots than there are tasks, the extra slots will not be utilized, and they will remain idle, resulting in some resource waste. To maximize resource usage efficiency, it is essential to configure the cluster properly and adjust the number of tasks and slots based on the workload demands. Dynamic resource allocation features in cluster managers can also help improve resource utilization by adjusting the cluster size dynamically based on the task requirements."
      },
      {
        "date": "2023-04-24T06:16:00.000Z",
        "voteCount": 4,
        "content": "A. The Spark job will likely not run as efficiently as possible.\n\nIn Spark, a slot represents a unit of processing capacity that an executor can offer to run a task. If there are more slots than there are tasks, some of the slots will remain unused, and the Spark job will likely not run as efficiently as possible. Spark automatically assigns tasks to slots, and if there are more slots than necessary, some of them may remain idle, resulting in wasted resources and slower job execution. However, the job will not fail as long as there are enough resources to execute the tasks, and Spark will not generate more tasks than needed. Also, executors will not shut down because there are unused slots. They will remain active until the end of the job or until explicitly terminated."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/databricks/view/106305-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following is the most granular level of the Spark execution hierarchy?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTask\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJob",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlot"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-31T01:05:00.000Z",
        "voteCount": 1,
        "content": "A. Task"
      },
      {
        "date": "2023-06-27T05:40:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A. Task."
      },
      {
        "date": "2023-06-17T08:48:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A. Task.\n\nExplanation: In the Spark execution hierarchy, the most granular level is the task. A task represents a unit of work that is executed on a partitioned portion of the data in parallel. Tasks are created by the Spark driver program and assigned to individual executors for execution. Each task operates on a subset of the data and performs a specific operation defined by the Spark application, such as a transformation or an action."
      },
      {
        "date": "2023-04-15T17:12:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/databricks/view/107312-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following statements about Spark jobs is incorrect?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJobs are broken down into stages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are multiple tasks within a single job when a DataFrame has more than one partition.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJobs are collections of tasks that are divided up based on when an action is called.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no way to monitor the progress of a job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJobs are collections of tasks that are divided based on when language variables are defined."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-24T06:29:00.000Z",
        "voteCount": 7,
        "content": "There are two incorrect answers in the original question.\n\nOption D, \"There is no way to monitor the progress of a job,\" is incorrect. As I mentioned earlier, Spark provides various tools and interfaces for monitoring the progress of a job, including the Spark UI, which provides real-time information about the job's stages, tasks, and resource utilization. Other tools, such as the Spark History Server, can be used to view completed job information.\n\nOption E, \"Jobs are collections of tasks that are divided based on when language variables are defined,\" is also incorrect. The division of tasks in a Spark job is not based on when language variables are defined, but rather based on when actions are called."
      },
      {
        "date": "2023-06-17T08:49:00.000Z",
        "voteCount": 3,
        "content": "The incorrect statement is:\n\nD. There is no way to monitor the progress of a job.\n\nExplanation: Spark provides several ways to monitor the progress of a job. The Spark UI (Web UI) provides a graphical interface to monitor the progress of Spark jobs, stages, tasks, and other relevant metrics. It displays information such as job status, task completion, execution time, and resource usage. Additionally, Spark provides programmatic APIs, such as the JobProgressListener interface, which allows developers to implement custom job progress monitoring logic within their Spark applications."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/databricks/view/107314-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations is most likely to result in a shuffle?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.join()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.filter()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.union()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.where()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.drop()"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-17T08:51:00.000Z",
        "voteCount": 2,
        "content": "The most likely operation to result in a shuffle is:\n\nA. DataFrame.join()\n\nExplanation: A shuffle operation in Spark involves redistributing and reorganizing data across partitions. It typically occurs when data needs to be rearranged or merged based on a specific key or condition. DataFrame joins involve combining two DataFrames based on a common key column, and this operation often requires data to be shuffled to ensure that matching records are located on the same executor or partition. The shuffle process involves exchanging data between nodes or executors in the cluster, which can incur significant data movement and network communication overhead."
      },
      {
        "date": "2023-04-24T06:38:00.000Z",
        "voteCount": 2,
        "content": "The operation that is most likely to result in a shuffle is DataFrame.join().\n\nJoin operation requires data to be combined from two different sources based on a common key, and this typically involves a reorganization of the data such that the data with the same keys are co-located in the same executor. This process is known as a shuffle operation, which can be a performance-intensive operation, especially for large datasets.\n\nThe other DataFrame operations such as filter(), union(), where() or drop() do not require data to be shuffled across the nodes."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/databricks/view/111436-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The default value of spark.sql.shuffle.partitions is 200. Which of the following describes what that means?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBy default, all DataFrames in Spark will be spit to perfectly fill the memory of 200 executors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBy default, new DataFrames created by Spark will be split to perfectly fill the memory of 200 executors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBy default, Spark will only read the first 200 partitions of DataFrames to improve speed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBy default, all DataFrames in Spark, including existing DataFrames, will be split into 200 unique segments for parallelization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBy default, DataFrames will be split into 200 unique partitions when data is being shuffled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-31T01:37:00.000Z",
        "voteCount": 1,
        "content": "E is correct."
      },
      {
        "date": "2023-06-17T08:52:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is E. By default, DataFrames will be split into 200 unique partitions when data is being shuffled.\n\nExplanation: The spark.sql.shuffle.partitions configuration parameter in Spark determines the number of partitions to use when shuffling data. When a shuffle operation occurs, such as during DataFrame joins or aggregations, data needs to be redistributed across partitions based on a specific key. The spark.sql.shuffle.partitions value defines the default number of partitions to be used during such shuffling operations."
      },
      {
        "date": "2023-06-07T12:42:00.000Z",
        "voteCount": 2,
        "content": "E. By default, DataFrames will be split into 200 unique partitions when data is being shuffled.\n\nThe spark.sql.shuffle.partitions configuration parameter determines the number of partitions that are used when shuffling data for joins or aggregations. The default value is 200, which means that by default, when a shuffle operation occurs, the data will be divided into 200 partitions. This allows the tasks to be distributed across the cluster and processed in parallel, improving performance.\n\nHowever, the optimal number of shuffle partitions depends on the specific details of your cluster and data. If the number is too small, then each partition will be large, and the tasks may take a long time to run. If the number is too large, then there will be many small tasks, and the overhead of scheduling and processing all these tasks can degrade performance. Therefore, tuning this parameter to match your specific use case can help optimize the performance of your Spark applications."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/databricks/view/107438-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following is the most complete description of lazy evaluation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNone of these options describe lazy evaluation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA process is lazily evaluated if its execution does not start until it is put into action by some type of trigger\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA process is lazily evaluated if its execution does not start until it is forced to display a result to the user",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA process is lazily evaluated if its execution does not start until it reaches a specified date and time",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA process is lazily evaluated if its execution does not start until it is finished compiling"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-17T08:52:00.000Z",
        "voteCount": 4,
        "content": "The most complete description of lazy evaluation is:\n\nB. A process is lazily evaluated if its execution does not start until it is put into action by some type of trigger.\n\nExplanation: Lazy evaluation is a programming language feature that delays the evaluation of expressions or computations until the result is actually needed or requested. In a lazily evaluated system, expressions are not immediately executed or evaluated when they are defined or assigned. Instead, the evaluation is deferred until the value is needed by another part of the program or when an action is triggered."
      },
      {
        "date": "2023-04-25T06:37:00.000Z",
        "voteCount": 2,
        "content": "B. A process is lazily evaluated if its execution does not start until it is put into action by some type of trigger.\n\nLazy evaluation is a programming paradigm that defers the evaluation of an expression until its value is needed. In other words, lazy evaluation delays the computation of a value until the value is actually required by the program. This is in contrast to eager evaluation, where expressions are evaluated as soon as they are bound to a variable.\n\nIn Spark, lazy evaluation is used extensively to optimize the execution of complex data processing pipelines. When a user creates a series of operations to transform a dataset, Spark does not immediately execute those operations. Instead, it builds a logical plan that represents the operations as a set of transformations on the original dataset. The actual execution of the plan is deferred until the user requests the result by triggering an action, such as writing the result to disk or displaying it on the screen. By using lazy evaluation, Spark can optimize the execution plan and avoid unnecessary computations, resulting in faster processing times and more efficient use of cluster resources."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/databricks/view/109255-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following DataFrame operations is classified as an action?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.drop()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.coalesce()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.take()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.join()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.filter()"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-30T16:52:00.000Z",
        "voteCount": 1,
        "content": "It returns first n rows of an array"
      },
      {
        "date": "2023-06-17T08:53:00.000Z",
        "voteCount": 4,
        "content": "The DataFrame operation classified as an action is:\n\nC. DataFrame.take()\n\nExplanation: In Spark, actions are operations that trigger the execution of transformations on a DataFrame and return results or side effects. Actions are evaluated eagerly, meaning they initiate the execution of the computation plan built by transformations. Among the options provided, DataFrame.take() is an action because it returns an array with the first n elements from the DataFrame as an array. It triggers the execution of any pending transformations and collects the resulting data."
      },
      {
        "date": "2023-05-14T23:40:00.000Z",
        "voteCount": 3,
        "content": "DataFrame.take(num: int) \u2192 List[pyspark.sql.types.Row]\nAll the other functions return a dataframe again, which is defined as a transformation. An action returns a result of a computation, which take() does."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/databricks/view/108386-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following DataFrame operations is classified as a wide transformation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.filter()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.join()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.select()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.drop()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.union()"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-03T04:28:00.000Z",
        "voteCount": 5,
        "content": "B. DataFrame.join() is classified as a wide transformation, as it shuffles the data across the network to perform the join operation."
      },
      {
        "date": "2024-09-30T16:52:00.000Z",
        "voteCount": 1,
        "content": "Join takes different partitions and combines into single dataframe."
      },
      {
        "date": "2023-06-17T08:54:00.000Z",
        "voteCount": 3,
        "content": "The DataFrame operation classified as a wide transformation is:\n\nB. DataFrame.join()\n\nExplanation: In Spark, transformations are operations on DataFrames that create a new DataFrame from an existing one. Wide transformations involve shuffling or redistributing data across partitions and typically require data movement across the network. Among the options provided, DataFrame.join() is a wide transformation because it involves combining two DataFrames based on a common key column, which often requires shuffling and redistributing the data across partitions."
      },
      {
        "date": "2023-05-14T23:34:00.000Z",
        "voteCount": 3,
        "content": "None of the other options are wide transformations, they are narrow (logically, they modify the length of a dataframe). Only a join can force shuffling of data between horizontally scaled partitions."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/databricks/view/116693-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following describes the difference between cluster and client execution modes?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cluster execution mode runs the driver on a worker node within a cluster, while the client execution mode runs the driver on the client machine (also known as a gateway machine or edge node).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cluster execution mode is run on a local cluster, while the client execution mode is run in the cloud.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cluster execution mode distributes executors across worker nodes in a cluster, while the client execution mode runs a Spark job entirely on one client machine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cluster execution mode runs the driver on the cluster machine (also known as a gateway machine or edge node), while the client execution mode runs the driver on a worker node within a cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cluster execution mode distributes executors across worker nodes in a cluster, while the client execution mode submits a Spark job from a remote machine to be run on a remote, unconfigurable cluster."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-07-28T11:40:00.000Z",
        "voteCount": 5,
        "content": "Explanation:\nIn cluster mode, the driver runs on the master node, while in client mode, the driver runs on a virtual machine in the cloud.\n\nThis is wrong, since execution modes do not specify whether workloads are run in the cloud or on-premise.\n\nIn cluster mode, each node will launch its own executor, while in client mode, executors will exclusively run on the client machine.\n\nWrong, since in both cases executors run on worker nodes.\n\nIn cluster mode, the driver runs on the edge node, while the client mode runs the driver in a worker node.\n\nWrong C in cluster mode, the driver runs on a worker node. In client mode, the driver runs on the client machine.\n\nIn client mode, the cluster manager runs on the same host as the driver, while in cluster mode, the cluster manager runs on a separate node.\n\nNo. In both modes, the cluster manager is typically on a separate node C not on the same host as the driver. It only runs on the same host as the driver in local execution mode. More info: Learning Spark, 2nd Edition, Chapter 1, and Spark: The Definitive Guide, Chapter 15. ()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/databricks/view/103841-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following statements about Spark\u2019s stability is incorrect?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark is designed to support the loss of any set of worker nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark will rerun any failed tasks due to failed worker nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark will recompute data cached on failed worker nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark will spill data to disk if it does not fit in memory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark will reassign the driver to a worker node if the driver\u2019s node fails.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-17T08:56:00.000Z",
        "voteCount": 5,
        "content": "Option E is incorrect because the driver program in Spark is not reassigned to another worker node if the driver's node fails. The driver program is responsible for the coordination and control of the Spark application and runs on a separate machine, typically the client machine or cluster manager. If the driver's node fails, the Spark application as a whole may fail or need to be restarted, but the driver is not automatically reassigned to another worker node."
      },
      {
        "date": "2024-08-21T06:53:00.000Z",
        "voteCount": 2,
        "content": "In Spark, the driver node is crucial for orchestrating the execution of the Spark application. If the driver node fails, the Spark application fails. Spark does not automatically reassign the driver to another node if the driver fails. This would require the application to be restarted manually or through external high-availability mechanisms."
      },
      {
        "date": "2024-07-22T19:14:00.000Z",
        "voteCount": 1,
        "content": "Considering the word \"any set\" looks like A is not correct either. What if all the worker nodes fail.\nA- \"Spark is designed to support the loss of any set of worker nodes.\""
      },
      {
        "date": "2023-09-18T10:27:00.000Z",
        "voteCount": 2,
        "content": "Option E beacuse spark doesn't assiggned driver if faild"
      },
      {
        "date": "2023-06-17T08:56:00.000Z",
        "voteCount": 2,
        "content": "The incorrect statement about Spark's stability is:\n\nE. Spark will reassign the driver to a worker node if the driver\u2019s node fails.\n\nExplanation:\n\nOption A is correct because Spark is designed to handle the failure of worker nodes. When a worker node fails, Spark redistributes the lost tasks to other available worker nodes to ensure fault tolerance.\n\nOption C is correct because Spark is able to recompute data that was cached on failed worker nodes. Spark maintains lineage information about RDDs (Resilient Distributed Datasets), allowing it to reconstruct lost data partitions in case of failures."
      },
      {
        "date": "2023-05-14T23:31:00.000Z",
        "voteCount": 2,
        "content": "The driver is responsible for maintaining spark context. If it fails, there is no recourse. The driver can mitigate the failure of worker nodes through limited fault tolerance mechanisms."
      },
      {
        "date": "2023-05-03T04:59:00.000Z",
        "voteCount": 2,
        "content": "All of the following statements about Spark's stability are correct except for:\n\nE. Spark will reassign the driver to a worker node if the driver\u2019s node fails.\n\nThe driver is a special process in Spark that is responsible for coordinating tasks and executing the main program. If the driver fails, the entire Spark application fails and cannot be restarted. Therefore, Spark does not reassign the driver to a worker node if the driver's node fails."
      },
      {
        "date": "2023-04-25T10:54:00.000Z",
        "voteCount": 2,
        "content": "The E is only valid when spark-submit is in cluster modes"
      },
      {
        "date": "2024-07-16T08:08:00.000Z",
        "voteCount": 1,
        "content": "And it also depened on Resource Manger of cluser on which spark is running."
      },
      {
        "date": "2023-03-25T03:42:00.000Z",
        "voteCount": 3,
        "content": "If the driver node fails your cluster will fail. If the worker node fails, Databricks will spawn a new worker node to replace the failed node and resumes the workload."
      },
      {
        "date": "2023-04-03T08:33:00.000Z",
        "voteCount": 1,
        "content": "If the node running the driver program fails, Spark's built-in fault-tolerance mechanism can reassign the driver program to run on another node."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/databricks/view/104961-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following cluster configurations is most likely to experience an out-of-memory error in response to data skew in a single partition?<br><img title=\"image1\" src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image1.png\"><br>Note: each configuration has roughly the same compute power using 100 GB of RAM and 200 cores.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #4",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #5",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #6\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMore information is needed to determine an answer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #1"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-17T09:00:00.000Z",
        "voteCount": 11,
        "content": "The most likely scenario to experience an out-of-memory error in response to data skew in a single partition is:\n\nC. Scenario #6: 12.5 GB Worker Node, 12.5 GB Executor. 1 Driver &amp; 8 Executors.\n\nExplanation:\n\nData skew refers to an uneven distribution of data across partitions. When there is significant skew in a single partition, it can lead to increased memory usage for that specific partition, potentially causing out-of-memory errors. The smaller the available memory per executor, the higher the likelihood of encountering such issues.\n\nIn this case, Scenario #6 has the smallest worker node and executor configuration, with only 12.5 GB of RAM available for each executor. With 8 executors, the total available memory is still 100 GB (similar to other scenarios), but the reduced memory per executor increases the risk of encountering out-of-memory errors when handling skewed data in a single partition."
      },
      {
        "date": "2024-03-07T14:38:00.000Z",
        "voteCount": 2,
        "content": "D is correct. even though you have less executor memory in scenario 6, spark will still complete the process , it might take more time to do the shuffle neverthless."
      },
      {
        "date": "2023-06-22T04:45:00.000Z",
        "voteCount": 1,
        "content": "This is the right answer."
      },
      {
        "date": "2023-06-17T09:00:00.000Z",
        "voteCount": 3,
        "content": "Option A, Scenario #4, has larger worker nodes and executors compared to Scenario #6, reducing the likelihood of encountering out-of-memory errors due to data skew.\n\nOption B, Scenario #5, also has larger worker nodes and executors compared to Scenario #6, providing more memory per executor and reducing the risk of out-of-memory errors.\n\nOption D states that more information is needed to determine an answer, but based on the available information, Scenario #6 is the most likely to experience out-of-memory errors due to data skew in a single partition.\n\nOption E, Scenario #1, has larger worker nodes and executors compared to Scenario #6, reducing the likelihood of out-of-memory errors due to data skew."
      },
      {
        "date": "2023-04-25T10:56:00.000Z",
        "voteCount": 3,
        "content": "Data skew is when you have a few partitions oversized. But due to initial partitioning this large datasets needed to be processed by single threads so can cause OOM"
      },
      {
        "date": "2023-04-03T05:30:00.000Z",
        "voteCount": 1,
        "content": "Please explain the answer!!"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/databricks/view/104430-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Of the following situations, in which will it be most advantageous to store DataFrame df at the MEMORY_AND_DISK storage level rather than the MEMORY_ONLY storage level?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen all of the computed data in DataFrame df can fit into memory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the memory is full and it\u2019s faster to recompute all the data in DataFrame df rather than read it from disk.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen it\u2019s faster to recompute all the data in DataFrame df that cannot fit into memory based on its logical plan rather than read it from disk.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen it\u2019s faster to read all the computed data in DataFrame df that cannot fit into memory from disk rather than recompute it based on its logical plan.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe storage level MENORY_ONLY will always be more advantageous because it\u2019s faster to read data from memory than it is to read data from disk."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-29T14:59:00.000Z",
        "voteCount": 9,
        "content": "D. When it\u2019s faster to read all the computed data in DataFrame df that cannot fit into memory from disk rather than recompute it based on its logical plan."
      },
      {
        "date": "2023-06-07T11:08:00.000Z",
        "voteCount": 7,
        "content": "All other explanation is either wrong or misleading. To understand the question, you need to understand the difference between Memory_only and Memory_and_Disk\n1. Memory_and_Disk, which is the default mode for cache ro persist. That means, if the data size is larger than the memory, it will store the extra data in disk. next time when we n eed to read data, we will read data firstly from memory, and then read from disk.\n2. Memory_Only means, if the data size is larger than memory, it will not store the extra data. next time we read data, we will read from memory first and then recompute the extra data which cannot store in memory.\nPS. Mr. 4be8126 is wrong about raising error when out of memory.\nTherefore, the difference/balance between Memory_only and memory_and_disk lay in how they handle the extra data out of memory. which is option D, if it is faster to read data from disk is faster than recompute it, then memory_and_disk."
      },
      {
        "date": "2023-11-05T07:29:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-08-09T04:56:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-07-31T20:52:00.000Z",
        "voteCount": 1,
        "content": "D.  It is faster to read the computed data from disk instead of recomputing it based on its logical plan when the recomputation is costly and time-consuming."
      },
      {
        "date": "2023-05-15T00:06:00.000Z",
        "voteCount": 1,
        "content": "If it's faster to read from memory and can fit in, then there is no reason to use Memory_and_disk, Memory_only is sufficient. Also, if it's faster to compute than read from disk, that's what you would do. The only options is when it's too big to fit in memory and too expensive to recompute, so reading from disk (or rather caching from disk into memory on the fly) is faster."
      },
      {
        "date": "2023-05-03T05:11:00.000Z",
        "voteCount": 1,
        "content": "The most advantageous situation to store a DataFrame at the MEMORY_AND_DISK storage level instead of the MEMORY_ONLY storage level is option D - when it\u2019s faster to read all the computed data in DataFrame df that cannot fit into memory from disk rather than recompute it based on its logical plan.\n\nThis is because the MEMORY_ONLY storage level only stores data in memory, which can result in an out-of-memory error if the data exceeds the available memory. On the other hand, the MEMORY_AND_DISK storage level will spill data to disk if there is not enough memory available, allowing more data to be processed without errors.\n\nIn situations where the computed data can fit entirely into memory, it is best to use the MEMORY_ONLY storage level as it will be faster than reading from disk. However, when there is not enough memory to store all the computed data, it may be necessary to use the MEMORY_AND_DISK storage level."
      },
      {
        "date": "2023-05-03T01:26:00.000Z",
        "voteCount": 1,
        "content": "Yes but what about the link with the question ? I would say B too :)"
      },
      {
        "date": "2023-04-25T10:58:00.000Z",
        "voteCount": 2,
        "content": "Answer is D. This is the whole idea behind caching"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/databricks/view/107464-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "A Spark application has a 128 GB DataFrame A and a 1 GB DataFrame B. If a broadcast join were to be performed on these two DataFrames, which of the following describes which DataFrame should be broadcasted and why?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEither DataFrame can be broadcasted. Their results will be identical in result and efficiency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame A should be broadcasted because it is larger and will eliminate the need for the shuffling of DataFrame B.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of DataFrame A.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame A should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-09T17:48:00.000Z",
        "voteCount": 1,
        "content": "Should be B:  During the join, the intention of the shuffle would be to bring the same keys from both dataframes in same partition. Now, this would ideally require both of them to be shuffled. however, if smaller one is broadcasted, that would mean we have sent the entire smaller dataframe in each partition whereas the bigger one would still undergo a shuffle to get its similar keys in each partition. hence, the re-shuffle of just the smaller one is avoided."
      },
      {
        "date": "2024-08-13T03:07:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is:\n\nD. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of DataFrame A.\n\nExplanation:\n\nIn a broadcast join, the smaller DataFrame (in this case, DataFrame B, which is 1 GB) is broadcasted to all worker nodes. This allows the larger DataFrame (DataFrame A, which is 128 GB) to be joined without shuffling its data across the cluster, which would be computationally expensive.\nBroadcasting the smaller DataFrame reduces the amount of data that needs to be shuffled, improving the efficiency of the join operation."
      },
      {
        "date": "2024-07-10T22:32:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is:\nB. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself.\n\nExplanation:\n\nIn Spark, a broadcast join is a specific type of join where one DataFrame is sent to every node in the cluster to avoid the costly network shuffle that can occur with large datasets in regular joins.\nGenerally, the smaller DataFrame should be broadcasted to optimize performance. This is because broadcasting a smaller DataFrame requires less network bandwidth and memory usage across the cluster.\nBroadcasting DataFrame B (the smaller DataFrame at 1 GB) means that each node will have a local copy of DataFrame B, allowing them to perform the join operation locally with their respective partitions of DataFrame A without needing to shuffle DataFrame B across the network.\nThis approach significantly reduces the amount of data that needs to be shuffled (since only DataFrame A is partitioned across the nodes), thereby improving the performance of the join operation."
      },
      {
        "date": "2024-03-06T13:15:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B. D is wrong. Being the larger dataset Dataframe A (128 GB) will get shuffled being the larger dataset. Dataframe A (1 GB) (if hint is specified in join), will be broadcasted hence it would not get shuffled."
      },
      {
        "date": "2024-02-22T09:17:00.000Z",
        "voteCount": 1,
        "content": "answer D - With broadcast join, Spark broadcast the smaller DataFrame to all executors and the executor keeps this DataFrame in memory and the larger DataFrame is split and distributed across all executors so that Spark can perform a join without shuffling any data from the larger DataFrame as the data required for join colocated on every executor."
      },
      {
        "date": "2023-11-07T08:55:00.000Z",
        "voteCount": 3,
        "content": "It should really be B."
      },
      {
        "date": "2023-09-05T17:24:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of itself. A broadcast join is an optimization technique in the Spark SQL engine that is used to join two DataFrames. This technique is ideal for joining a large DataFrame with a smaller one. With broadcast join, Spark broadcasts the smaller DataFrame to all executors and the executor keeps this DataFrame in memory. The larger DataFrame is split and distributed across all executors so that Spark can perform a join without shuffling any data from the larger DataFrame as the data required for join colocated on every executor."
      },
      {
        "date": "2023-09-05T17:23:00.000Z",
        "voteCount": 2,
        "content": "Option D is incorrect because it states that DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of DataFrame A. However, broadcasting DataFrame B will not eliminate the need for shuffling DataFrame A. Instead, broadcasting DataFrame B will eliminate the need for shuffling itself. In a broadcast join, the smaller DataFrame is broadcasted to all executors and kept in memory. The larger DataFrame is split and distributed across all executors so that Spark can perform a join without shuffling any data from the larger DataFrame as the data required for join colocated on every executor."
      },
      {
        "date": "2023-08-11T02:38:00.000Z",
        "voteCount": 3,
        "content": "https://sparkbyexamples.com/spark/broadcast-join-in-spark/\n\nSpark Broadcast Join is an important part of the Spark SQL execution engine, With broadcast join, Spark broadcast the smaller DataFrame to all executors and the executor keeps this DataFrame in memory and the larger DataFrame is split and distributed across all executors so that Spark can perform a join without shuffling any data from the larger DataFrame as the data required for join colocated on every executor."
      },
      {
        "date": "2023-07-09T08:56:00.000Z",
        "voteCount": 2,
        "content": "D should be correct. Broadcast join happens on smaller DataFrame to prevent the shuffling of larger DataFrame."
      },
      {
        "date": "2023-06-17T09:02:00.000Z",
        "voteCount": 3,
        "content": "Option A is incorrect because not both DataFrames can be broadcasted. Only one of the DataFrames should be broadcasted to minimize shuffling.\n\nOption B is correct because DataFrame B is smaller and broadcasting it will eliminate the shuffling of DataFrame B, improving the join operation's efficiency.\n\nOption C is incorrect because DataFrame A is larger and shuffling DataFrame B is not a concern in this scenario.\n\nOption E is incorrect because DataFrame A is larger, and broadcasting it would not eliminate the shuffling of itself. The larger DataFrame typically undergoes shuffling in a broadcast join.\n\nTherefore, the correct option is D."
      },
      {
        "date": "2023-06-07T11:15:00.000Z",
        "voteCount": 2,
        "content": "B is correct."
      },
      {
        "date": "2023-05-03T05:13:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D. DataFrame B should be broadcasted because it is smaller and will eliminate the need for the shuffling of DataFrame A.\n\nA broadcast join is a technique where the smaller DataFrame is broadcast to all the worker nodes in the cluster, so that it can be joined with the larger DataFrame without requiring any shuffling of the larger DataFrame. This is generally more efficient than a shuffle join, which requires data to be shuffled across the network.\n\nIn this scenario, DataFrame B is much smaller than DataFrame A, so it is more efficient to broadcast DataFrame B to all worker nodes in the cluster. This will eliminate the need for shuffling of DataFrame A, making the join more efficient."
      },
      {
        "date": "2023-04-25T11:01:00.000Z",
        "voteCount": 2,
        "content": "All the ANS are incorrect. The DAG will perform a sort merge join instead of BCJ. The size of a DF needed to be 10MB max for broadcast else it will cause a network overload."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/databricks/view/108391-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations can be used to create a new DataFrame that has 12 partitions from an original DataFrame df that has 8 partitions?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.repartition(12)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.cache()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.partitionBy(1.5)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.coalesce(12)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.partitionBy(12)"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-03T05:15:00.000Z",
        "voteCount": 6,
        "content": "The answer is A. The repartition operation can be used to increase or decrease the number of partitions in a DataFrame. In this case, the number of partitions is being increased from 8 to 12, so we can use the repartition operation with a partition count of 12: df.repartition(12).\n\nOption B, df.cache(), is used to cache a DataFrame in memory for faster access, but it does not change the number of partitions.\n\nOption C, df.partitionBy(1.5), is not a valid operation for partitioning a DataFrame.\n\nOption D, df.coalesce(12), can be used to reduce the number of partitions in a DataFrame, but it cannot be used to increase the number of partitions beyond the current number.\n\nOption E, df.partitionBy(12), is used to partition a DataFrame by a specific column or set of columns, but it does not change the number of partitions."
      },
      {
        "date": "2023-11-06T06:35:00.000Z",
        "voteCount": 1,
        "content": "nice explanation @4be8126"
      },
      {
        "date": "2023-06-17T09:03:00.000Z",
        "voteCount": 1,
        "content": "The operation that can be used to create a new DataFrame with 12 partitions from an original DataFrame df that has 8 partitions is:\n\nD. df.coalesce(12)\n\nExplanation:\n\nThe coalesce() operation in Spark is used to decrease the number of partitions in a DataFrame, and it can be used to create a new DataFrame with a specific number of partitions. In this case, calling df.coalesce(12) on the original DataFrame df with 8 partitions will create a new DataFrame with 12 partitions."
      },
      {
        "date": "2023-05-15T00:10:00.000Z",
        "voteCount": 1,
        "content": "Comprehensive explanation by 4be8126, only using this comment to vote A."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/databricks/view/112464-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following object types cannot be contained within a column of a Spark DataFrame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tString",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tArray",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tnull",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVector"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-31T22:03:00.000Z",
        "voteCount": 3,
        "content": "A. Spark DataFrames do not directly support containing other DataFrames as columns. A DataFrame column can only have one of the supported data types, such as primitive types (e.g., IntegerType, StringType, DoubleType, etc.) or complex types (e.g., ArrayType, MapType, StructType, etc.), but it cannot contain an entire DataFrame as a column."
      },
      {
        "date": "2023-06-17T09:05:00.000Z",
        "voteCount": 2,
        "content": "Spark DataFrames are designed to store structured data, where each column has a specific data type. While DataFrames can contain various data types such as strings (option B), arrays (option C), null values (option D), and vectors (option E), they cannot directly contain other DataFrames (option A) as a column."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/databricks/view/107534-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations can be used to create a DataFrame with a subset of columns from DataFrame storesDF that are specified by name?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.subset()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.select()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.selectColumn()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.drop()"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-26T00:27:00.000Z",
        "voteCount": 5,
        "content": "The operation that can be used to create a DataFrame with a subset of columns from DataFrame storesDF that are specified by name is storesDF.select().\n\nThe select() operation allows you to specify the columns you want to keep in the resulting DataFrame by passing in the column names as arguments. For example, to create a new DataFrame that contains only the columns store_id and store_name from the storesDF \n\nDataFrame, you can use the following code:\n\nnewDF = storesDF.select(\"store_id\", \"store_name\")"
      },
      {
        "date": "2024-07-22T19:46:00.000Z",
        "voteCount": 1,
        "content": "E.storesDF.drop() is also correct. It is just opposite of select. If you have a large number of columns you need to select but a few to drop to meet your requirements, then drop is easier than select."
      },
      {
        "date": "2023-06-17T09:06:00.000Z",
        "voteCount": 2,
        "content": "The select() operation in Spark DataFrame allows you to specify the columns you want to include in the resulting DataFrame. You can provide column names as arguments to the select() operation to create a new DataFrame with only the specified columns."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/databricks/view/107535-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a DataFrame containing all columns from DataFrame storesDF except for column sqft and column customerSatisfaction. Identify the error.<br>Code block:<br>storesDF.drop(sqft, customerSatisfaction)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe drop() operation only works if one column name is called at a time \u2013 there should be two calls in succession like storesDF.drop(\"sqft\").drop(\"customerSatisfaction\").",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe drop() operation only works if column names are wrapped inside the col() function like storesDF.drop(col(sqft), col(customerSatisfaction)).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no drop() operation for storesDF.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe sqft and customerSatisfaction column names should be quoted like \"sqft\" and \"customerSatisfaction\".\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe sqft and customerSatisfaction column names should be subset from the DataFrame storesDF like storesDF.\"sqft\" and storesDF.\"customerSatisfaction\"."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-26T00:29:00.000Z",
        "voteCount": 5,
        "content": "The error in the code block is that the column names sqft and customerSatisfaction should be quoted, like \"sqft\" and \"customerSatisfaction\", since they are strings. The correct code block should be:\n\nstoresDF.drop(\"sqft\", \"customerSatisfaction\")\n\nOption D correctly identifies this error."
      },
      {
        "date": "2023-06-07T11:21:00.000Z",
        "voteCount": 1,
        "content": "The correct one is B:\nstoresDF.drop(\"sqft\").drop(\"customerSatisfaction\")\nFor D, it should be list of column name: storesDF.drop([\"sqft\", \"customerSatisfaction\"])"
      },
      {
        "date": "2023-06-07T11:24:00.000Z",
        "voteCount": 1,
        "content": "The correct one is D, but my explanation is correct"
      },
      {
        "date": "2024-03-06T13:41:00.000Z",
        "voteCount": 1,
        "content": "sorry, Option D is correct"
      },
      {
        "date": "2024-03-06T13:27:00.000Z",
        "voteCount": 1,
        "content": "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html option A is correct, drop expects only one argument, if its more than one, you would have to use as listofcols=['col1','col2'] and drop(*listofcols)"
      },
      {
        "date": "2023-07-30T07:23:00.000Z",
        "voteCount": 1,
        "content": "D is correct, \ndf.drop('id','firstname').show() tested code"
      },
      {
        "date": "2023-06-17T09:06:00.000Z",
        "voteCount": 1,
        "content": "When using the drop() operation in Spark DataFrame, the column names should be specified as strings and enclosed in quotes. In the given code block, the column names \"sqft\" and \"customerSatisfaction\" are not quoted, which results in a syntax error."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/databricks/view/147843-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following statements about Spark DataFrames is incorrect?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark DataFrames are the same as a data frame in Python or R.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark DataFrames are built on top of RDDs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark DataFrames are immutable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark DataFrames are distributed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark DataFrames have common Structured APIs."
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-19T06:23:00.000Z",
        "voteCount": 2,
        "content": "A. Spark DataFrames are the same as a data frame in Python or R.: This statement is incorrect because Spark DataFrames, while conceptually similar to data frames in Python (e.g., pandas) or R, are not the same. Spark DataFrames are distributed, immutable, and built on top of RDDs, designed to handle large-scale data processing across a cluster. In contrast, data frames in Python (pandas) or R are typically in-memory, single-node constructs."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/databricks/view/107537-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 OR the value in column customerSatisfaction is greater than or equal to 30?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(\"sqft\") &lt;= 25000 | col(\"customerSatisfaction\") &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(\"sqft\") &lt;= 25000 or col(\"customerSatisfaction\") &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(sqft &lt;= 25000 or customerSatisfaction &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(sqft) &lt;= 25000 | col(customerSatisfaction) &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter((col(\"sqft\") &lt;= 25000) | (col(\"customerSatisfaction\") &gt;= 30))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-23T07:17:00.000Z",
        "voteCount": 1,
        "content": "Answer: E\nI tried it with the code below, all other options raised an error:\n\n# register UDF with udf function\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 3, 20000, \"A\"),\n        (1, 1, 50000, \"A\"),\n        (2, 2, 70000, \"A\"),\n        (3, 5, 10000, \"B\"),\n        (4, 4, 100000, \"B\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"customerSatisfaction\", \"sqft\", \"division\"])\n\ntry:\n    storesDF.filter(col(\"sqft\") &lt;= 25000 | col(\"customerSatisfaction\") &gt;= 30).show()\nexcept Exception as e:\n    print(e)\n\ntry:\n    storesDF.filter((col(\"sqft\") &lt;= 25000) | (col(\"customerSatisfaction\") &gt;= 30)).show()\nexcept Exception as e:\n    print(e)"
      },
      {
        "date": "2023-06-17T09:09:00.000Z",
        "voteCount": 4,
        "content": "Option E, storesDF.filter((col(\"sqft\") &lt;= 25000) | (col(\"customerSatisfaction\") &gt;= 30)), is the correct option. It uses the filter() operation with the conditions (col(\"sqft\") &lt;= 25000) | (col(\"customerSatisfaction\") &gt;= 30) to filter the rows where the value in column sqft is less than or equal to 25,000 OR the value in column customerSatisfaction is greater than or equal to 30."
      },
      {
        "date": "2023-05-15T00:27:00.000Z",
        "voteCount": 2,
        "content": "E has the right syntax, logic, operator and correct number of parentheses. All of the others falter in one of these respects."
      },
      {
        "date": "2023-04-26T06:31:00.000Z",
        "voteCount": 1,
        "content": "Should be A. Tested it in communitity edition with 2 filters."
      },
      {
        "date": "2023-04-26T06:34:00.000Z",
        "voteCount": 6,
        "content": "sorry, we need 2 paranthesis indeed. So E !"
      },
      {
        "date": "2023-05-03T01:37:00.000Z",
        "voteCount": 2,
        "content": "Yes I agree, it's E"
      },
      {
        "date": "2023-08-26T06:52:00.000Z",
        "voteCount": 4,
        "content": "Congrats man, not everyone goes back to tell they were wrong and corrects them selves. We need more people like this on this platform"
      },
      {
        "date": "2023-04-26T00:38:00.000Z",
        "voteCount": 2,
        "content": "The correct code block to return a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 OR the value in column customerSatisfaction is greater than or equal to 30 is:\n\nstoresDF.filter((col(\"sqft\") &lt;= 25000) | (col(\"customerSatisfaction\") &gt;= 30))\nOption A uses a single pipe (|) instead of the correct syntax of two vertical bars (||) to represent \"OR\" logic, and also uses the wrong syntax for column referencing.\n\nOption B uses the correct or operator, but also uses the wrong syntax for column referencing.\n\nOption C uses the correct operator and syntax for column referencing, but does not use the col() function to reference column names.\n\nOption D uses the col() function, but also uses the wrong syntax for column referencing.\n\nOption E uses the correct syntax for both column referencing and logical operator, and correctly specifies the parentheses to ensure the proper order of operations.\n\nTherefore, the correct answer is E.\n\nstoresDF.filter((col(\"sqft\") &lt;= 25000) | (col(\"customerSatisfaction\") &gt;= 30))"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/databricks/view/107538-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame from DataFrame storesDF where column storeId is of the type string?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeId, cast(col(\"storeId\"), StringType()))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeId, col(\"storeId\").cast(StringType()))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeId, cast(storeId).as(StringType)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeId, col(storeId).cast(StringType)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeId, cast(\"storeId\").as(StringType()))"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-23T07:24:00.000Z",
        "voteCount": 1,
        "content": "Answer is B but with a typo: \nSee code below (Spark 3.5.1):\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, cast\nfrom pyspark.sql.types import StringType\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 3, 20000, \"A\"),\n        (1, 1, 50000, \"A\"),\n        (2, 2, 70000, \"A\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"customerSatisfaction\", \"sqft\", \"division\"])\n\nstoresDF.withColumn(\"storeId\", col(\"storeId\").cast(StringType())).printSchema()\n# root\n#  |-- storeId: string (nullable = true)\n#  |-- customerSatisfaction: long (nullable = true)\n#  |-- sqft: long (nullable = true)\n#  |-- division: string (nullable = true)"
      },
      {
        "date": "2023-10-28T18:21:00.000Z",
        "voteCount": 2,
        "content": "Anwer is B but it has a typo"
      },
      {
        "date": "2023-06-06T10:47:00.000Z",
        "voteCount": 1,
        "content": "cast is a method belongs to class pyspark.sql.column\ntherefore, A C E are wrong. it should be dataframe.column.cast() or col('col_name').cast()\nB is correct, with small typo"
      },
      {
        "date": "2023-05-15T06:25:00.000Z",
        "voteCount": 3,
        "content": "All answers are wrong because the first argument does not have the closing quotes :D, apart from that, it is B"
      },
      {
        "date": "2023-04-26T00:43:00.000Z",
        "voteCount": 2,
        "content": "The correct code block to return a new DataFrame from DataFrame storesDF where column storeId is of the type string is:\n\nstoresDF.withColumn(\"storeId\", col(\"storeId\").cast(StringType()))\nOption A has an extra quotation mark after \"storeId\" and is missing a closing parenthesis for the cast() function.\n\nOption B correctly uses the cast() function to change the data type, but has a typo where \"storeId\" is repeated inside the string argument for the withColumn() function.\n\nOption C is missing the col() function to reference the storeId column, and also has a typo with the closing parentheses for the cast() function.\n\nOption D correctly references the storeId column using col(), but has a typo with the quotation marks and parentheses.\n\nOption E has a syntax error where the cast() function is inside the quotation marks, and is also missing the col() function to reference the storeId column.\n\nTherefore, the correct answer is B.\n\nstoresDF.withColumn(\"storeId\", col(\"storeId\").cast(StringType()))"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/databricks/view/107539-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame with a new column employeesPerSqft that is the quotient of column numberOfEmployees and column sqft, both of which are from DataFrame storesDF? Note that column employeesPerSqft is not in the original DataFrame storesDF.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"employeesPerSqft\", col(\"numberOfEmployees\") / col(\"sqft\"))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"employeesPerSqft\", \"numberOfEmployees\" / \"sqft\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.select(\"employeesPerSqft\", \"numberOfEmployees\" / \"sqft\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.select(\"employeesPerSqft\", col(\"numberOfEmployees\") / col(\"sqft\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(col(\"employeesPerSqft\"), col(\"numberOfEmployees\") / col(\"sqft\"))"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-23T07:31:00.000Z",
        "voteCount": 1,
        "content": "Answer: A\nAll other options do not work\n\nCode:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 3, 20, \"A\"),\n        (1, 1, 50, \"A\"),\n        (2, 2, 70, \"A\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"numberOfEmployees\", \"sqft\", \"division\"])\n\nstoresDF.withColumn(\"employeesPerSqft\", col(\"numberOfEmployees\") / col(\"sqft\")) # A."
      },
      {
        "date": "2023-09-11T05:21:00.000Z",
        "voteCount": 1,
        "content": "Test:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\n# Initializing Spark session (if not already initialized)\nspark = SparkSession.builder.appName(\"databricks_example\").getOrCreate()\n\n# Creating some synthetic data for storesDF\ndata = [\n    {\"storeId\": 1, \"numberOfEmployees\": 10, \"sqft\": 500},\n    {\"storeId\": 2, \"numberOfEmployees\": 15, \"sqft\": 750},\n    {\"storeId\": 3, \"numberOfEmployees\": 8, \"sqft\": 400}\n]\n\nstoresDF = spark.createDataFrame(data)\n\n\n# Option A:\ntry:\n    df_a = storesDF.withColumn(\"employeesPerSqft\", col(\"numberOfEmployees\") / col(\"sqft\"))\n    df_a.show()\n    print(\"Option A works\")\nexcept Exception as e:\n    print(\"Option A doesn't work:\", str(e))"
      },
      {
        "date": "2023-05-15T00:32:00.000Z",
        "voteCount": 2,
        "content": "C, D are wrong as exmployeesPerSqft cannot be selected, it doesn't exist. Also, that is not proper select syntax anyway. B does not select existing columns using col(), and E refers to employeesPerSqft as an existing column; also, it cannot be the first argument for withColumn()."
      },
      {
        "date": "2023-04-26T00:51:00.000Z",
        "voteCount": 1,
        "content": "storesDF.select(\"employeesPerSqft\", col(\"numberOfEmployees\") / col(\"sqft\"))\n\nThis code block selects the columns \"employeesPerSqft\" and the quotient of \"numberOfEmployees\" and \"sqft\" from the DataFrame storesDF. However, since \"employeesPerSqft\" is not a column in the original storesDF, this code block would throw an error.\n\nTo create a new column \"employeesPerSqft\" in the resulting DataFrame, we need to use the withColumn() method instead of select(). Here's the corrected code block:\n\nstoresDF.withColumn(\"employeesPerSqft\", col(\"numberOfEmployees\") / col(\"sqft\"))\n\nThis code block adds a new column \"employeesPerSqft\" to the storesDF DataFrame. The new column is created by dividing the values in column \"numberOfEmployees\" by the values in column \"sqft\"."
      },
      {
        "date": "2023-04-26T00:48:00.000Z",
        "voteCount": 1,
        "content": "The correct code block to return a new DataFrame with a new column employeesPerSqft that is the quotient of column numberOfEmployees and column sqft from DataFrame storesDF is:\n\nstoresDF.withColumn(\"employeesPerSqft\", col(\"numberOfEmployees\") / col(\"sqft\"))\n\nOption A correctly uses the withColumn() function to create a new column employeesPerSqft by dividing column numberOfEmployees by column sqft.\n\nOption B has a syntax error because it uses quotation marks to reference column names instead of the col() function.\n\nOption C also has a syntax error because it uses quotation marks to reference column names instead of the col() function, and also uses the select() function instead of withColumn() to create a new column.\n\nOption D correctly references column names using col() and uses the select() function to return a DataFrame with only the two selected columns.\n\nOption E has a syntax error where col() is used as a first argument instead of a second argument for the withColumn() function.\n\nTherefore, the correct answer is A.\n\nstoresDF.withColumn(\"employeesPerSqft\", col(\"numberOfEmployees\") / col(\"sqft\"))"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/databricks/view/107542-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame from DataFrame storesDF where column modality is the constant string \"PHYSICAL\", Assume DataFrame storesDF is the only defined language variable. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br>Code block:<br>storesDF. _1_(_2_,_3_(_4_))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"modality\"<br>3. col<br>4. \"PHYSICAL\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"modality\"<br>3. lit<br>4. PHYSICAL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"modality\"<br>3. lit<br>4. \"PHYSICAL\"\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"modality\"<br>3. SrtringType<br>4. \"PHYSICAL\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. newColumn<br>2. modality<br>3. SrtringType<br>4. PHYSICAL"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-23T07:37:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nCode:\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import lit\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 3, 20, \"A\"),\n        (1, 1, 50, \"A\"),\n        (2, 2, 70, \"A\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"numberOfEmployees\", \"sqft\", \"division\"])\n\nstoresDF.withColumn(\"modality\", lit(\"PHYSICAL\"))"
      },
      {
        "date": "2023-09-11T05:52:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-04-26T00:58:00.000Z",
        "voteCount": 1,
        "content": "lit and col are two functions in PySpark that are used to create or reference columns in a DataFrame.\n\nlit: This function is used to create a column with a literal value. It returns a Column expression of literal value. For example, lit(2) creates a Column with a value of 2. It can be useful when you want to add a new column to a DataFrame with a constant value for all rows.\n\ncol: This function is used to reference an existing column in a DataFrame. It returns a Column expression that represents a column. For example, col(\"age\") returns a Column expression that represents the \"age\" column in a DataFrame. It can be useful when you want to select, filter or transform an existing column in a DataFrame.\n\nIn short, lit is used to create a new column with a constant value, while col is used to reference an existing column in a DataFrame."
      },
      {
        "date": "2023-04-26T00:57:00.000Z",
        "voteCount": 2,
        "content": "Option C is the correct answer. Here's why:\n\nThe withColumn function is used to add a new column to the DataFrame based on an existing column or a constant value. The first blank (_1_) should be replaced with withColumn to indicate that we want to add a new column.\n\nThe second blank (_2_) should be replaced with the name of the column we want to add. In this case, we want to add a column called modality.\n\nThe third blank (_3_) should be replaced with a function that will create the values for the new column. In this case, we want to create a column that has the constant value \"PHYSICAL\". The lit function can be used to create a column with a literal value.\n\nFinally, the fourth blank (_4_) should be replaced with the actual value we want to use for the new column. Since we want to use the string \"PHYSICAL\", it should be wrapped in quotation marks to indicate that it is a string.\n\nTherefore, option C correctly fills in the blanks to give us the following code block:\n\nstoresDF.withColumn(\"modality\", lit(\"PHYSICAL\"))"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/databricks/view/105644-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame where column storeCategory from DataFrame storesDF is split at the underscore character into column storeValueCategory and column storeSizeCategory?<br>A sample of DataFrame storesDF is displayed below:<br><img title=\"image2\" src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image2.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"storeValueCategory\", split(col(\"storeCategory\"), \"_\")[1])<br>.withColumn(\"storeSizeCategory\", split(col(\"storeCategory\"), \"_\")[2]))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"storeValueCategory\", col(\"storeCategory\").split(\"_\")[0])<br>.withColumn(\"storeSizeCategory\", col(\"storeCategory\").split(\"_\")[1]))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"storeValueCategory\", split(col(\"storeCategory\"), \"_\")[0])<br>.withColumn(\"storeSizeCategory\", split(col(\"storeCategory\"), \"_\")[1]))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"storeValueCategory\", split(\"storeCategory\", \"_\")[0])<br>.withColumn(\"storeSizeCategory\", split(\"storeCategory\", \"_\")[1]))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"storeValueCategory\", col(\"storeCategory\").split(\"_\")[1])<br>.withColumn(\"storeSizeCategory\", col(\"storeCategory\").split(\"_\")[2]))"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-08T23:20:00.000Z",
        "voteCount": 8,
        "content": "Both C or D are correct. Function split accepts both col and str.\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.split.html?highlight=split#pyspark.sql.functions.split"
      },
      {
        "date": "2023-07-01T06:46:00.000Z",
        "voteCount": 2,
        "content": "Both C or D are correct!"
      },
      {
        "date": "2023-04-26T01:11:00.000Z",
        "voteCount": 3,
        "content": "Option D is not correct because the split function should be used with the col function to split the values in a column. In option D, the split function is used with a string literal rather than a column, which will result in an error."
      },
      {
        "date": "2024-07-23T09:05:00.000Z",
        "voteCount": 1,
        "content": "Both C or D work in Spark 3.5.1, but C is probably better for backward compatibility.\nSee code example below:\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, col\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, True, 10020, \"VALUE_MEDIUM\"),\n        (1, True, 10050, \"MAINSTREAM_SMALL\"),\n        (2, False, 10070, \"PREMIUM_LARGE\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"open\", \"openDate\", \"storeCategory\"])\n\n(storesDF.withColumn(\"storeValueCategory\", split(col(\"storeCategory\"), \"_\")[0]).withColumn(\"storeSizeCategory\", split(col(\"storeCategory\"), \"_\")[1])).show()\n(storesDF.withColumn(\"storeValueCategory\", split(\"storeCategory\", \"_\")[0]).withColumn(\"storeSizeCategory\", split(\"storeCategory\", \"_\")[1])).show()"
      },
      {
        "date": "2023-09-11T06:57:00.000Z",
        "voteCount": 2,
        "content": "C\nyou can check, by running the code below: \n\nfrom pyspark.sql import SparkSession\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"split_test\").getOrCreate()\n\n# Create synthetic data\ndata = [\n    {\"storeCategory\": \"value1_size1\"},\n    {\"storeCategory\": \"value2_size2\"},\n    {\"storeCategory\": \"value3_size3\"},\n]\n\nstoresDF = spark.createDataFrame(data)\nstoresDF.show()\n\nfrom pyspark.sql.functions import split, col\n\n# Option C\n\n\nnewDF = (storesDF.withColumn(\"storeValueCategory\", split(col(\"storeCategory\"), \"_\")[0])\n.withColumn(\"storeSizeCategory\", split(col(\"storeCategory\"), \"_\")[1]))\nnewDF.show()"
      },
      {
        "date": "2023-07-30T07:43:00.000Z",
        "voteCount": 1,
        "content": "c is correct"
      },
      {
        "date": "2023-04-26T01:12:00.000Z",
        "voteCount": 2,
        "content": "Option C returns a DataFrame where column storeCategory from DataFrame storesDF is split at the underscore character into column storeValueCategory and column storeSizeCategory.\n\nThe correct code is:\n\n(storesDF.withColumn(\"storeValueCategory\", split(col(\"storeCategory\"), \"_\")[0])\n.withColumn(\"storeSizeCategory\", split(col(\"storeCategory\"), \"_\")[1]))\n\nExplanation:\n\nsplit(col(\"storeCategory\"), \"_\") splits the values in column storeCategory by the \"_\" character and returns an array of strings.\n\n[0] gets the first element of the resulting array and assigns it to the new column storeValueCategory.\n\n[1] gets the second element of the resulting array and assigns it to the new column storeSizeCategory.\n\nwithColumn is used to create the new columns and returns a new DataFrame."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/databricks/view/107544-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame where column productCategories only has one word per row, resulting in a DataFrame with many more rows than DataFrame storesDF?<br>A sample of storesDF is displayed below:<br><img title=\"image3\" src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image3.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"productCategories\", explode(col(\"productCategories\")))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"productCategories\", split(col(\"productCategories\")))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"productCategories\", col(\"productCategories\").explode())",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"productCategories\", col(\"productCategories\").split())",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"productCategories\", explode(\"productCategories\"))"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-24T00:12:00.000Z",
        "voteCount": 1,
        "content": "Both option A and E work with spark 3.5.1.\nBut A is better for backward compatibility.\nSee code example below:\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, explode\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, [\"value 1\", \"value 2\", \"value 3\"]),\n        (1, [\"value 1\", \"value 2\", \"value 3\"]),\n        (2, [\"value 1\", \"value 2\", \"value 3\"]),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"productCategories\"])\n\nstoresDF.withColumn(\"productCategories\", explode(col(\"productCategories\"))).show() # A. \nstoresDF.withColumn(\"productCategories\", explode(\"productCategories\")).show() # E."
      },
      {
        "date": "2024-04-02T23:44:00.000Z",
        "voteCount": 1,
        "content": "A and E are correct"
      },
      {
        "date": "2024-03-07T07:50:00.000Z",
        "voteCount": 1,
        "content": "Both A and E are correct according to the new version"
      },
      {
        "date": "2023-09-11T08:11:00.000Z",
        "voteCount": 3,
        "content": "A is correct, use below code to test:\nfrom pyspark.sql import SparkSession\n\n# Initializing Spark session\nspark = SparkSession.builder.appName(\"test\").getOrCreate()\n\n# 1. Creating DataFrame with an array column\ndata_array = [\n    (1, [\"electronics\", \"clothes\", \"toys\"]),\n    (2, [\"groceries\", \"electronics\"]),\n    (3, [\"books\", \"clothes\"]),\n]\n\nstoresDF = spark.createDataFrame(data_array, [\"ID\", \"productCategories\"])\nstoresDF.show()\n\ndf_array = storesDF.withColumn(\"productCategories\", explode(col(\"productCategories\")))\ndf_array.show()"
      },
      {
        "date": "2023-09-11T08:20:00.000Z",
        "voteCount": 2,
        "content": "But E works as well, sadly. What has to be chosen then?\nfrom pyspark.sql import SparkSession\n\n# Initializing Spark session\nspark = SparkSession.builder.appName(\"test\").getOrCreate()\n\n# 1. Creating DataFrame with an array column\ndata_array = [\n    (1, [\"electronics\", \"clothes\", \"toys\"]),\n    (2, [\"groceries\", \"electronics\"]),\n    (3, [\"books\", \"clothes\"]),\n]\n\nstoresDF = spark.createDataFrame(data_array, [\"ID\", \"productCategories\"])\nstoresDF.show()\n\n#df_array = storesDF.withColumn(\"productCategories\", explode(col(\"productCategories\")))\n#df_array.show()\n\n\n#check E\ndf_array = storesDF.withColumn(\"productCategories\", explode(\"productCategories\"))\ndf_array.show()"
      },
      {
        "date": "2023-11-06T08:42:00.000Z",
        "voteCount": 1,
        "content": "E for 3.0"
      },
      {
        "date": "2023-07-01T06:51:00.000Z",
        "voteCount": 4,
        "content": "Both A and E are correct."
      },
      {
        "date": "2023-05-22T09:19:00.000Z",
        "voteCount": 2,
        "content": "While the Explode function allows for a str or Column input, this requires the col() wrapper because it is used in a withColumn() call, where the 2nd parameter requires the column object.\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html?highlight=withcolumn#pyspark.sql.DataFrame.withColumn"
      },
      {
        "date": "2023-04-26T01:22:00.000Z",
        "voteCount": 1,
        "content": "Option A is correct: storesDF.withColumn(\"productCategories\", explode(col(\"productCategories\"))).\n\nExplanation:\n\nThe explode function is used to transform a column of arrays or maps into multiple rows, one for each element in the array or map. In this case, productCategories is a column with arrays of strings.\n\nThe withColumn function is used to add a new column or update an existing column. The first argument is the name of the new or existing column, and the second argument is the expression that defines the values for the column."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/databricks/view/104986-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame with column storeDescription where the pattern \"Description: \" has been removed from the beginning of column storeDescription in DataFrame storesDF?<br>A sample of DataFrame storesDF is below:<br><img title=\"image4\" src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image4.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeDescription\", regexp_replace(col(\"storeDescription\"), \"^Description: \"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeDescription\", col(\"storeDescription\").regexp_replace(\"^Description: \", \"\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeDescription\", regexp_extract(col(\"storeDescription\"), \"^Description: \", \"\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeDescription\", regexp_replace(\"storeDescription\", \"^Description: \", \"\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeDescription\", regexp_replace(col(\"storeDescription\"), \"^Description: \", \"\"))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-24T00:19:00.000Z",
        "voteCount": 1,
        "content": "Both D and E work with Spark 3.5.1 but E is better for backward compatibility\nSee code below:\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, regexp_replace\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, \"Description: Store 0\"),\n        (1, \"Description: Store 1\"),\n        (2, \"Description: Store 2\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"StoreDescription\"])\n\nstoresDF.withColumn(\"storeDescription\", regexp_replace(col(\"StoreDescription\"), \"Description: \", \"\")).show() \nstoresDF.withColumn(\"storeDescription\", regexp_replace(\"StoreDescription\", \"Description: \", \"\")).show()"
      },
      {
        "date": "2024-03-07T07:53:00.000Z",
        "voteCount": 1,
        "content": "Both D and E are correct according to the new version"
      },
      {
        "date": "2024-02-08T10:35:00.000Z",
        "voteCount": 1,
        "content": "E is most likely correct in this scenario"
      },
      {
        "date": "2023-09-11T08:39:00.000Z",
        "voteCount": 3,
        "content": "Both work: \n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import regexp_replace,regexp_extract, col\nspark = SparkSession.builder.appName(\"test\").getOrCreate()\n\ndata = [\n    (1, \"Description: This is a tech store. Description: This\"),\n    (2, \"Description: This is a grocery store.\"),\n    (3, \"Description: This is a book store.\"),\n]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"storeDescription\"])\nstoresDF.show(truncate=False)\n\n#Case D\nprint (\"Case D\")\nstoresDF = storesDF.withColumn(\"storeDescription\", regexp_replace(\"storeDescription\", \"^Description: \", \"\"))\nstoresDF.show(truncate=False)\n\n\n#Case E\nprint (\"Case E\")\nstoresDF = storesDF.withColumn(\"storeDescription\", regexp_replace(col(\"storeDescription\"), \"^Description: \", \"\")) \nstoresDF.show(truncate=False)"
      },
      {
        "date": "2023-08-01T19:25:00.000Z",
        "voteCount": 2,
        "content": "regexp_replace(str, regexp, rep [, position] )\nThis is what Databricks documentation says. You guys can debate between D and E but actually question clearly says to remove from the begging of the string. And if you take answer D it takes whole only one constant string \u201cstoreDescription\u201d to match pattern and will return empty string after Description for each row. \n\nSo if you have debate between D, E then E is the correct answer."
      },
      {
        "date": "2023-07-30T07:55:00.000Z",
        "voteCount": 2,
        "content": "E is the answer tested"
      },
      {
        "date": "2023-07-01T06:56:00.000Z",
        "voteCount": 1,
        "content": "Both D and E are correct."
      },
      {
        "date": "2023-05-15T03:55:00.000Z",
        "voteCount": 1,
        "content": "It's between D and E, and D is wrong as there is no replacement string expression (which is a required argument/parameter). Thus, E wins as the correct option."
      },
      {
        "date": "2023-06-06T07:31:00.000Z",
        "voteCount": 7,
        "content": "this is completely wrong explanation. Both D and E has replacement expression, the only difference is how they call the replaced column.\nBoth D and E are correct, but D works for Pyspark 2.0. D and E both work Pyspark 3.0+. Period!"
      },
      {
        "date": "2023-06-06T07:32:00.000Z",
        "voteCount": 1,
        "content": "I think what you really mean, \"there is no replacement string expression\", is for option A.\nThe only difference between A and E, is about the claim of replacement string expression"
      },
      {
        "date": "2023-05-03T05:30:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is E indeed\n- According to the pyspark doc, the syntax is regexp_replace(str, pattern, replacement)\n   -&gt; it means that it's not a function of the column object\n- storeDescription is a String field\n\nhttps://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.functions.regexp_replace"
      },
      {
        "date": "2023-04-28T01:34:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D.\nFirst, regexp_replace/regexp_extract are from sql.functions. They cannot be applied directly after a column Object =&gt; B is incorrect.\nSecond, regexp_replace/regexp_extract accept a STRING Object as a first argument to specify the column. Check the documentation there : https://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions =&gt; A, C, E are incorrects."
      },
      {
        "date": "2023-05-03T05:19:00.000Z",
        "voteCount": 2,
        "content": "Almost right but it's not about \"String object\" but \"String value\". So the correct answer is indeed the answer E ;)"
      },
      {
        "date": "2023-04-26T01:31:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is option E: storesDF.withColumn(\"storeDescription\", regexp_replace(col(\"storeDescription\"), \"^Description: \", \"\")).\n\nThis code block uses the withColumn() function to create a new column called storeDescription. It uses the regexp_replace() function to replace the pattern \"^Description: \" at the beginning of the string in the storeDescription column with an empty string. This effectively removes the pattern from the beginning of the string in each row of the column."
      },
      {
        "date": "2023-04-26T01:32:00.000Z",
        "voteCount": 1,
        "content": "The correct code block that returns a new DataFrame with column storeDescription where the pattern \"Description: \" has been removed from the beginning of column storeDescription in DataFrame storesDF is:\n\nA. storesDF.withColumn(\"storeDescription\", regexp_replace(col(\"storeDescription\"), \"^Description: \"))\n\nThis code uses the regexp_replace function to replace the pattern \"^Description: \" (which matches the string \"Description: \" at the beginning of the string) with an empty string in the column storeDescription. The resulting DataFrame will have the modified storeDescription column.\n\nOption B has a syntax error because the regexp_replace function should be called on the column using the dot notation instead of passing it as the second argument.\n\nOption C uses the regexp_extract function, which extracts a substring matching a regular expression pattern. It doesn't remove the pattern from the string.\n\nOption D has a syntax error because the column name is not wrapped in the col function.\n\nOption E is the same as option A, except that it uses the col function unnecessarily."
      },
      {
        "date": "2023-04-26T01:21:00.000Z",
        "voteCount": 1,
        "content": "Option A is correct: storesDF.withColumn(\"productCategories\", explode(col(\"productCategories\"))).\n\nExplanation:\n\nThe explode function is used to transform a column of arrays or maps into multiple rows, one for each element in the array or map. In this case, productCategories is a column with arrays of strings.\n\nThe withColumn function is used to add a new column or update an existing column. The first argument is the name of the new or existing column, and the second argument is the expression that defines the values for the column."
      },
      {
        "date": "2023-05-03T05:11:00.000Z",
        "voteCount": 2,
        "content": "You got the wrong question :\u00b0"
      },
      {
        "date": "2023-04-08T23:37:00.000Z",
        "voteCount": 2,
        "content": "Both D and E are correct answer."
      },
      {
        "date": "2023-04-07T11:01:00.000Z",
        "voteCount": 3,
        "content": "This should actually be D sorry for the wrong answer. refer to this, https://sparkbyexamples.com/pyspark/pyspark-replace-column-values/"
      },
      {
        "date": "2023-04-03T09:06:00.000Z",
        "voteCount": 1,
        "content": "The regexp_replace function is used to remove the pattern \"Description: \" from the beginning of the column storeDescription. The ^ symbol indicates the beginning of the string, and the pattern \"Description: \" is replaced with an empty string. This results in a new DataFrame with column storeDescription where the pattern \"Description: \" has been removed from the beginning of each cell in that column."
      },
      {
        "date": "2023-04-26T01:31:00.000Z",
        "voteCount": 2,
        "content": "Option A is incorrect because the regexp_replace function requires two arguments: the column to be transformed and the regular expression pattern to be replaced. In the given code block, only the regular expression pattern is provided, but not the column to be transformed.\n\nThe correct syntax to use regexp_replace on a DataFrame column is regexp_replace(col(column_name), pattern, replacement), where col(column_name) specifies the DataFrame column to be transformed, pattern specifies the regular expression pattern to be replaced, and replacement specifies the new string to replace the matched pattern.\n\nTherefore, the correct code block to remove the pattern \"Description: \" from the beginning of the storeDescription column in DataFrame storesDF is:\n\nstoresDF.withColumn(\"storeDescription\", regexp_replace(col(\"storeDescription\"), \"^Description: \", \"\"))"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/databricks/view/107543-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame where column division from DataFrame storesDF has been replaced and renamed to column state and column managerName from DataFrame storesDF has been replaced and renamed to column managerFullName?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumnRenamed([\"division\", \"state\"], [\"managerName\", \"managerFullName\"])",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"state\", col(\"division\"))<br>.withColumn(\"managerFullName\", col(\"managerName\")))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"state\", \"division\")<br>.withColumn(\"managerFullName\", \"managerName\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumnRenamed(\"state\", \"division\")<br>.withColumnRenamed(\"managerFullName\", \"managerName\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumnRenamed(\"division\", \"state\")<br>.withColumnRenamed(\"managerName\", \"managerFullName\"))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-26T01:21:00.000Z",
        "voteCount": 7,
        "content": "Option E is the correct answer. The withColumnRenamed function renames an existing column, whereas withColumn creates a new column. To replace the \"division\" column with a new column \"state\" and rename the \"managerName\" column to \"managerFullName\", we need to use withColumnRenamed. So option E is correct, where we first rename \"division\" to \"state\" and then rename \"managerName\" to \"managerFullName\"."
      },
      {
        "date": "2024-02-08T10:40:00.000Z",
        "voteCount": 2,
        "content": "E is the right answer for this question"
      },
      {
        "date": "2023-08-26T08:54:00.000Z",
        "voteCount": 3,
        "content": "If we take in consideration this part of the text .-&gt; \"has been REPLACED and renamed to...\" it means that the columnis not only renames but replaved with the contents of the other clumn. In this case the righr answer is B. \n\nDo you guys thin this insterpretation is correct? Thanks for the feedback."
      },
      {
        "date": "2023-07-30T08:01:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2023-07-01T06:58:00.000Z",
        "voteCount": 2,
        "content": "DataFrame.withColumnRenamed(existing, new)"
      },
      {
        "date": "2023-06-22T08:49:00.000Z",
        "voteCount": 1,
        "content": "this is the right answer."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/databricks/view/104987-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown contains an error. The code block is intended to return a new DataFrame where column sqft from DataFrame storesDF has had its missing values replaced with the value 30,000. Identify the error.<br>A sample of DataFrame storesDF is displayed below:<br><img title=\"image5\" src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image5.png\"><br>Code block:<br>storesDF.na.fill(30000, col(\"sqft\"))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe argument to the subset parameter of fill() should be a string column name or a list of string column names rather than a Column object.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe na.fill() operation does not work and should be replaced by the dropna() operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\the argument to the subset parameter of fill() should be a the numerical position of the column rather than a Column object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe na.fill() operation does not work and should be replaced by the nafill() operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe na.fill() operation does not work and should be replaced by the fillna() operation."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-06T17:11:00.000Z",
        "voteCount": 6,
        "content": "Correct anwser is A.\neven for most updated version, spark 3.4. na.fill() still functioning, it is an alias of fillna()\nMr. 4be8126 , \u4f60\u53ef\u771f\u662f\u5f20\u5634\u5c31\u6765\u554a"
      },
      {
        "date": "2024-07-22T23:59:00.000Z",
        "voteCount": 1,
        "content": "The most correct answer seems to be A:\n\nCode below with Spark 3.5.1.\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.errors import PySparkTypeError\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 43161),\n        (1, 51200),\n        (2, None),\n        (3, 78367),\n        (4, None),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"sqft\"])\n\n# storesDF.show()\n\ntry:\n    storesDF.na.fill(30000, col(\"sqft\"))\nexcept PySparkTypeError as e:\n    print(e)\n    \nstoresDF.na.fill(30000, \"sqft\").show()\nstoresDF.na.fill(30000, [\"sqft\"]).show()\nstoresDF.fillna(30000, [\"sqft\"]).show()\nstoresDF.fillna(30000, \"sqft\").show()\n```"
      },
      {
        "date": "2024-02-08T11:06:00.000Z",
        "voteCount": 1,
        "content": "We don't need any replacement here. A would be correct. \n\nIn PySpark both fillna() and fill() are used to replace missing or null values of a DataFrame. Functionally they both perform same. One can choose either of these based on preference. These are used mainly for handling missing data in PySpark."
      },
      {
        "date": "2023-04-26T01:44:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is either A or E, depending on the version of Spark being used.\n\nIn Spark 2.x, the correct method to replace missing values is na.fill(). Option A is correct in Spark 2.x, as it correctly specifies the column to apply the fill operation to using a Column object.\n\nHowever, in Spark 3.x, the method has been renamed to fillna(). Therefore, in Spark 3.x, the correct answer is E, as it uses the correct method name.\n\nBoth A and E accomplish the same task of replacing missing values in the sqft column with 30,000, so either answer can be considered correct depending on the version of Spark being used."
      },
      {
        "date": "2023-04-13T10:44:00.000Z",
        "voteCount": 2,
        "content": "the answer should be A. See this link for reference\nhttps://sparkbyexamples.com/pyspark/pyspark-fillna-fill-replace-null-values/"
      },
      {
        "date": "2023-04-03T09:11:00.000Z",
        "voteCount": 1,
        "content": "The error in the code block is that the method na.fill() should be replaced by fillna() to fill the missing values in the column \"sqft\" with the value 30,000."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/databricks/view/103969-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations fails to return a DataFrame with no duplicate rows?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.dropDuplicates()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.distinct()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.drop_duplicates()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.drop_duplicates(subset = None)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.drop_duplicates(subset = \"all\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-26T01:52:00.000Z",
        "voteCount": 10,
        "content": "A. DataFrame.dropDuplicates(): This method returns a new DataFrame with distinct rows based on all columns. It should return a DataFrame with no duplicate rows.\n\nB. DataFrame.distinct(): This method returns a new DataFrame with distinct rows based on all columns. It should also return a DataFrame with no duplicate rows.\n\nC. DataFrame.drop_duplicates(): This is an alias for DataFrame.dropDuplicates(). It should also return a DataFrame with no duplicate rows.\n\nD. DataFrame.drop_duplicates(subset=None): This method returns a new DataFrame with distinct rows based on all columns. It should return a DataFrame with no duplicate rows.\n\nE. DataFrame.drop_duplicates(subset=\"all\"): This method attempts to drop duplicates based on all columns but returns an error, because \"all\" is not a valid argument for the subset parameter. So this operation fails to return a DataFrame with no duplicate rows.\n\nTherefore, the correct answer is E."
      },
      {
        "date": "2023-03-26T08:11:00.000Z",
        "voteCount": 10,
        "content": "Option E is incorrect as \"all\" is not a valid value for the subset parameter in the drop_duplicates() method. The correct value should be a column name or a list of column names to be used as the subset to identify duplicate rows.\n\nAll other options (A, B, C, and D) can be used to return a DataFrame with no duplicate rows. The dropDuplicates(), distinct(), and drop_duplicates() methods are all equivalent and return a new DataFrame with distinct rows. The drop_duplicates() method also accepts a subset parameter to specify the columns to use for identifying duplicates, and when the subset parameter is not specified, all columns are used. Therefore, both option A and C are valid, and option D is also valid as it is equivalent to drop_duplicates() with no subset parameter."
      },
      {
        "date": "2024-08-04T05:39:00.000Z",
        "voteCount": 1,
        "content": "bro the question asks ( no duplicate rows ) ,that means the correct answer should be able to return rows with duplication. and  (E) does that so.  Focus on question."
      },
      {
        "date": "2024-07-23T00:08:00.000Z",
        "voteCount": 1,
        "content": "It's E, see code below:\n\n# Drop duplicates in a DataFrame\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.errors import PySparkTypeError\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 43161),\n        (0, 43161),\n        (1, 51200),\n        (2, None),\n        (2, None),\n        (3, 78367),\n        (4, None),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"sqft\"])\n\ntry:\n    storesDF.dropDuplicates().show()\nexcept PySparkTypeError as e:\n    print(e)\n\ntry:\n    storesDF.distinct().show()\nexcept PySparkTypeError as e:\n    print(e)\n\ntry:\n    storesDF.drop_duplicates().show()\nexcept PySparkTypeError as e:\n    print(e)\n\ntry:\n    storesDF.drop_duplicates(subset=None).show()\nexcept PySparkTypeError as e:\n    print(e)\n\ntry:\n    storesDF.drop_duplicates(subset=\"all\").show()\nexcept PySparkTypeError as e:\n    print(e)"
      },
      {
        "date": "2024-04-15T07:42:00.000Z",
        "voteCount": 1,
        "content": "the answer is E"
      },
      {
        "date": "2024-04-09T11:30:00.000Z",
        "voteCount": 1,
        "content": "E\nPySparkTypeError: [NOT_LIST_OR_TUPLE] Argument `subset` should be a list or tuple, got str."
      },
      {
        "date": "2024-03-07T14:14:00.000Z",
        "voteCount": 1,
        "content": "DataFrame.drop_duplicates(subset = \"all\") - this is specific to pandas"
      },
      {
        "date": "2024-03-07T14:13:00.000Z",
        "voteCount": 2,
        "content": "Option E . df.drop_duplicates(subset = \"all\") returns error \nSparkTypeError: [NOT_LIST_OR_TUPLE] Argument `subset` should be a list or tuple, got str."
      },
      {
        "date": "2023-08-02T20:14:00.000Z",
        "voteCount": 1,
        "content": "B is the right one, as TC007 said, the argument for drop_duplicates is a subset of columns: \n\nDataFrame.dropDuplicates(subset: Optional[List[str]] = None) \u2192 pyspark.sql.dataframe.DataFrame[source]\nReturn a new DataFrame with duplicate rows removed, optionally only considering certain columns.\n\nFor a static batch DataFrame, it just drops duplicate rows. For a streaming DataFrame, it will keep all data across triggers as intermediate state to drop duplicates rows. You can use withWatermark() to limit how late the duplicate data can be and the system will accordingly limit the state. In addition, data older than watermark will be dropped to avoid any possibility of duplicates.\n\ndrop_duplicates() is an alias for dropDuplicates().\n\nParameters\nsubsetList of column names, optional\nList of columns to use for duplicate comparison (default All columns)."
      },
      {
        "date": "2023-08-03T06:29:00.000Z",
        "voteCount": 3,
        "content": "OMG, I got it all wrong, the answer is E :)"
      },
      {
        "date": "2023-07-09T05:26:00.000Z",
        "voteCount": 1,
        "content": "the correct answer is E"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/databricks/view/104988-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks will most quickly return an approximation for the number of distinct values in column division in DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.agg(approx_count_distinct(col(\"division\")).alias(\"divisionDistinct\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.agg(approx_count_distinct(col(\"division\"), 0.01).alias(\"divisionDistinct\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.agg(approx_count_distinct(col(\"division\"), 0.15).alias(\"divisionDistinct\"))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.agg(approx_count_distinct(col(\"division\"), 0.0).alias(\"divisionDistinct\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.agg(approx_count_distinct(col(\"division\"), 0.05).alias(\"divisionDistinct\"))"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-26T01:57:00.000Z",
        "voteCount": 5,
        "content": "To quickly return an approximation for the number of distinct values in column division in DataFrame storesDF, the most efficient code block to use would be:\n\nB. storesDF.agg(approx_count_distinct(col(\"division\"), 0.01).alias(\"divisionDistinct\"))\n\nUsing the approx_count_distinct() function allows for an approximate count of the distinct values in the column without scanning the entire DataFrame. The second parameter passed to the function is the maximum estimation error allowed, which in this case is set to 0.01. This is a trade-off between the accuracy of the estimate and the computational cost. Option C may still be efficient but with a larger estimation error of 0.15. Option A and D are not correct as they do not specify the estimation error, which means that the function would use the default value of 0.05. Option E specifies an estimation error of 0.05, but a smaller error of 0.01 is a better choice for a more accurate estimate with less computational cost."
      },
      {
        "date": "2024-06-09T08:00:00.000Z",
        "voteCount": 3,
        "content": "But your answer contradicts the question, they only ask you for the fastest way, while the error value is closer to zero, then it will take more time and resources. 0.15&gt;0.01, that means that option C will be faster, it will have more errors, but it will be the fastest."
      },
      {
        "date": "2023-06-06T10:52:00.000Z",
        "voteCount": 19,
        "content": "I see you reply in a lot of question, barely correct.\nbro, you need to stop comment wrong information here.\nThis question only ask for efficiency, no need to balance between accuracy and efficiency. \nStop posting ChatGPT answer here"
      },
      {
        "date": "2023-11-08T14:26:00.000Z",
        "voteCount": 1,
        "content": "I noticed the same thing with this ID - bro has confidence, I have to triple make sure because he keeps answering wrong thus creating doubts in my head."
      },
      {
        "date": "2024-08-19T06:22:00.000Z",
        "voteCount": 3,
        "content": "C is correct because it will provide the fastest approximate count with a standard deviation of 0.15"
      },
      {
        "date": "2023-12-30T08:09:00.000Z",
        "voteCount": 1,
        "content": "B. storesDF.agg(approx_count_distinct(col(\"division\"), 0.01).alias(\"divisionDistinct\"))\n\nExplanation:\n\napprox_count_distinct(col(\"division\"), 0.01): This uses the approx_count_distinct function to approximate the number of distinct values in the \"division\" column with a relative error of 1%. The smaller the relative error, the more accurate the approximation, but it may require more resources.\n.alias(\"divisionDistinct\"): This renames the result column to \"divisionDistinct\" for better readability.\nSo, the correct answer is:\n\nB. storesDF.agg(approx_count_distinct(col(\"division\"), 0.01).alias(\"divisionDistinct\"))"
      },
      {
        "date": "2024-06-20T23:55:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer.\nC. storesDF.agg(approx_count_distinct(col(\"division\"), 0.15).alias(\"divisionDistinct\"))\n\nThis option uses the largest rsd value (0.15), which means it prioritizes speed over accuracy. the smaller the rsd, the more accurate the result, but the longer it might take to compute. Conversely, a larger rsd value provides a faster result with less accuracy."
      },
      {
        "date": "2024-06-09T07:59:00.000Z",
        "voteCount": 1,
        "content": "But your answer contradicts the question, they only ask you for the fastest way, while the error value is closer to zero, then it will take more time and resources. 0.15&gt;0.01, that means that option C will be faster, it will have more errors, but it will be the fastest."
      },
      {
        "date": "2023-09-05T19:16:00.000Z",
        "voteCount": 4,
        "content": "C\nThe code block that will most quickly return an approximation for the number of distinct values in column `division` in DataFrame `storesDF` is **C**, `storesDF.agg(approx_count_distinct(col(\"division\"), 0.15).alias(\"divisionDistinct\"))`. The `approx_count_distinct` function can be used to quickly estimate the number of distinct values in a column by using a probabilistic data structure. The second parameter of the `approx_count_distinct` function specifies the maximum estimation error allowed, with a smaller value resulting in a more accurate but slower estimation. In this case, an error of 0.15 is specified, which will result in a faster but less accurate estimation than the other options."
      },
      {
        "date": "2023-08-03T06:51:00.000Z",
        "voteCount": 3,
        "content": "C - the less accurate the calculation, the faster it is"
      },
      {
        "date": "2023-08-01T01:19:00.000Z",
        "voteCount": 3,
        "content": "A. https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.approx_count_distinct.html"
      },
      {
        "date": "2023-05-15T06:07:00.000Z",
        "voteCount": 3,
        "content": "While not an option I would use, the question says most quickly (relatively), and this will be the fastest. Note that a 15% error is too high."
      },
      {
        "date": "2023-04-03T09:15:00.000Z",
        "voteCount": 2,
        "content": "The higher the relative error parameter, the less accurate and faster. The lower the relative error parameter, the more accurate and slower."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/databricks/view/107546-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a new DataFrame with the mean of column sqft from DataFrame storesDF in column sqftMean. Identify the error.<br>Code block:<br>storesDF.agg(mean(\"sqft\").alias(\"sqftMean\"))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe argument to the mean() operation should be a Column abject rather than a string column name.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe argument to the mean() operation should not be quoted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe mean() operation is not a standalone function \u2013 it\u2019s a method of the Column object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe agg() operation is not appropriate here \u2013 the withColumn() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe only way to compute a mean of a column is with the mean() method from a DataFrame.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-26T02:02:00.000Z",
        "voteCount": 7,
        "content": "The code block shown is correct and should return a new DataFrame with the mean of column sqft from DataFrame storesDF in column sqftMean. Therefore, the answer is E - none of the options identify a valid error in the code block.\n\nHere's an explanation for each option:\n\nA. The argument to the mean() operation can be either a Column object or a string column name, so there is no error in using a string column name in this case.\n\nE. This option is incorrect because the code block shown is a valid way to compute the mean of a column using PySpark. Another way to compute the mean of a column is with the mean() method from a DataFrame, but that doesn't mean the code block shown is invalid."
      },
      {
        "date": "2023-11-13T07:20:00.000Z",
        "voteCount": 3,
        "content": "wrong! A"
      },
      {
        "date": "2024-10-14T06:29:00.000Z",
        "voteCount": 2,
        "content": "The mean() function expects a Column object as an argument, which can be created using col(\"sqft\"). Simply passing the column name as a string will result in an error."
      },
      {
        "date": "2024-10-12T09:07:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A.&nbsp;The argument to the mean() operation should be a Column object rather than a string column name.\n\nIn Spark DataFrames, the mean() function takes a Column object as its argument, not a string column name. To create a Column object from a string column name, you can use the col() function."
      },
      {
        "date": "2024-03-12T01:22:00.000Z",
        "voteCount": 2,
        "content": "The error in the code is A. The argument to the mean() operation should be a Column object rather than a string column name.\nIn the provided code block, \"sqft\" is passed as a string column name to the mean() function. However, the correct approach is to use a Column object. This can be achieved by referencing the column using the storesDF DataFrame and the col() function. Here's the corrected code:\nstoresDF.agg(mean(col(\"sqft\")).alias(\"sqftMean\"))"
      },
      {
        "date": "2024-03-07T14:23:00.000Z",
        "voteCount": 3,
        "content": "from pyspark.sql.functions import col, mean\n\nstudents =[\n{'rollno':'001','name':'sravan','sqft':23, 'height':5.79,'weight':67,'address':'guntur'},\n{'rollno':'002','name':'ojaswi','sqft':16, 'height':3.79,'weight':34,'address':'hyd'}]\nstoresDF = spark.createDataFrame( students)\nstoresDF.agg(mean('sqft').alias('sqftMean')).show()\n\nthis works as well! not sure which one is wrong then"
      },
      {
        "date": "2024-02-08T11:20:00.000Z",
        "voteCount": 2,
        "content": "A is most like correct here"
      },
      {
        "date": "2023-12-21T06:22:00.000Z",
        "voteCount": 1,
        "content": "A) should be the one considering databricks practice pdf. mean() function should take col object as input."
      },
      {
        "date": "2023-11-08T14:38:00.000Z",
        "voteCount": 1,
        "content": "it appears that there might be some flexibility in how the mean function can be used with either a string column name or a col() function. However, the most accurate and recommended approach is to use the col() function to create a Column object explicitly.\n\nWith this in mind, the best choice is:\n\nA. The argument to the mean() operation should be a Column object rather than a string column name. The mean function takes a Column object as an argument, not a string column name. To fix the error, the code block should be rewritten as storesDF.agg(mean(col(\"sqft\")).alias(\"sqftMean\")), where the col function is used to create a Column object from the string column name \"sqft\".\n\nWhile there might be situations where using a string column name works, following the standard practice of creating a Column object with col() ensures compatibility and clarity in code."
      },
      {
        "date": "2023-10-31T19:18:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A:\n\nfrom pyspark.sql.functions import col, mean\n\nstudents =[\n{'rollno':'001','name':'sravan','sqft':23, 'height':5.79,'weight':67,'address':'guntur'},\n{'rollno':'002','name':'ojaswi','sqft':16, 'height':3.79,'weight':34,'address':'hyd'}]\nstoresDF = spark.createDataFrame( students)\nstoresDF.agg(mean(col('sqft')).alias('sqftMean')).show()"
      },
      {
        "date": "2023-10-19T08:19:00.000Z",
        "voteCount": 1,
        "content": "D\nwithColumn() for new calculated column."
      },
      {
        "date": "2023-09-05T19:20:00.000Z",
        "voteCount": 2,
        "content": "A.\nA\nThe error in the code block is **A**, the argument to the `mean` operation should be a Column object rather than a string column name. The `mean` function takes a Column object as an argument, not a string column name. To fix the error, the code block should be rewritten as `storesDF.agg(mean(col(\"sqft\")).alias(\"sqftMean\"))`, where the `col` function is used to create a Column object from the string column name `\"sqft\"`.\n\nHere is the correct code\nstoresDF.agg(mean(col(\"sqft\")).alias(\"sqftMean\"))"
      },
      {
        "date": "2023-10-19T08:17:00.000Z",
        "voteCount": 1,
        "content": "storesDF.agg(mean(\"Value\").alias(\"sqftMean\")).show() it works"
      },
      {
        "date": "2023-08-13T11:08:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is:\n\nB. The argument to the mean() operation should not be quoted.\n\nIn the context of Apache Spark, the mean function takes a column name as its argument. Therefore, you would write it without quotes. The corrected code line would look something like this:"
      },
      {
        "date": "2023-08-10T08:37:00.000Z",
        "voteCount": 3,
        "content": "There's a similar question in the official Databricks samples and the right answer there is: \nCode block:\nstoresDF.__1__(__2__(__3__).alias(\"sqftMean\"))\nA.\n1. agg\n2. mean\n3. col(\"sqft\")\n\nIf we stick to this logic, the answer is A."
      },
      {
        "date": "2023-07-30T09:21:00.000Z",
        "voteCount": 1,
        "content": "df.agg(mean(\"amountpaid\").alias(\"amountpaid\")).show()\ndf.agg(mean(col(\"amountpaid\")).alias(\"sqftMean\")).show(). Both produces the result"
      },
      {
        "date": "2023-06-23T03:48:00.000Z",
        "voteCount": 3,
        "content": "agg is not required here."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/databricks/view/125645-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations can be used to return the number of rows in a DataFrame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.numberOfRows()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.n()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.sum()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.count()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.countDistinct()"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-23T00:39:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct: DataFrame.count()"
      },
      {
        "date": "2023-11-08T14:42:00.000Z",
        "voteCount": 2,
        "content": "The operation that can be used to return the number of rows in a DataFrame is:\n\nD. DataFrame.count()\n\nThe count() method in Spark DataFrame returns the number of rows in the DataFrame, and it is the standard way to determine the row count. Options A, B, C, and E are not valid methods for counting the number of rows in a DataFrame."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/databricks/view/125646-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations returns a GroupedData object?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.GroupBy()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.cubed()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.group()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.groupBy()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.grouping_id()"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-08T11:32:00.000Z",
        "voteCount": 2,
        "content": ".groupBy() is correct one"
      },
      {
        "date": "2023-11-08T14:43:00.000Z",
        "voteCount": 2,
        "content": "D. DataFrame.groupBy()\n\nThe groupBy() method is used to group the DataFrame based on one or more columns, and it returns a GroupedData object, which can then be used to perform various aggregation operations on the grouped data. Options A, B, C, and E do not return a GroupedData object."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/databricks/view/107565-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a collection of summary statistics for all columns in<br>DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.summary(\"mean\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.describe(all = True)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.describe(\"all\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.summary(\"all\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.describe()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-23T00:45:00.000Z",
        "voteCount": 1,
        "content": "E is the right option.\n\nSee code below with Spark 3.5.1\n# Summary statistics of a DataFrame \nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.errors import PySparkTypeError\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 43161),\n        (1, 51200),\n        (2, None),\n        (3, 78367),\n        (4, None),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"sqft\"])\n    \ntry:\n    storesDF.summary(\"mean\").show()\nexcept Exception as e:\n    print(e)\n\ntry:\n    storesDF.describe(all = True).show()\nexcept Exception as e:\n    print(e)\n\ntry:\n    storesDF.describe(\"all\").show()\nexcept Exception as e:\n    print(e)\n\ntry:\n    storesDF.summary(\"all\").show()\nexcept Exception as e:\n    print(e)\n\ntry:\n    storesDF.describe().show()\nexcept Exception as e:\n    print(e)"
      },
      {
        "date": "2024-04-09T13:02:00.000Z",
        "voteCount": 1,
        "content": "E is the correct one"
      },
      {
        "date": "2024-02-08T11:36:00.000Z",
        "voteCount": 1,
        "content": "E would be correct here"
      },
      {
        "date": "2023-12-31T03:07:00.000Z",
        "voteCount": 2,
        "content": "tested e is the right answer"
      },
      {
        "date": "2023-08-16T05:55:00.000Z",
        "voteCount": 1,
        "content": "E is the correct answer"
      },
      {
        "date": "2023-08-03T07:35:00.000Z",
        "voteCount": 1,
        "content": "check the documentation, mates. both methods receive names of columns as arguments, so E is correct!"
      },
      {
        "date": "2023-07-30T09:24:00.000Z",
        "voteCount": 2,
        "content": "E is correct, it's giving the output."
      },
      {
        "date": "2023-07-22T17:25:00.000Z",
        "voteCount": 1,
        "content": "B is correct. On running the last option it gives error.\n\nTypeError: describe() got an unexpected keyword argument 'all'"
      },
      {
        "date": "2023-08-03T07:37:00.000Z",
        "voteCount": 3,
        "content": "checked it, it gave me the right result, so E is the one"
      },
      {
        "date": "2023-04-26T04:48:00.000Z",
        "voteCount": 1,
        "content": "The answer is B.\n\nExplanation: The describe() method in DataFrame returns a DataFrame with summary statistics for all numeric columns in the input DataFrame. By default, only the count, mean, standard deviation, minimum, and maximum values are returned, but additional statistics can be specified with the percentiles parameter. Setting the all parameter to True will include non-numeric columns in the output as well. Therefore, option B is the correct answer.\n\nOption A is not correct, as the summary() method only returns summary statistics for the specified column(s) and is not a valid option for returning summary statistics for all columns in the DataFrame.\n\nOption C is not correct, as the describe() method does not have an \"all\" option.\n\nOption D is also not correct, as the summary() method only returns summary statistics for the specified column(s) and does not have an \"all\" option.\n\nOption E is not incorrect, but it does not specify whether to include non-numeric columns in the output. Therefore, option B is a better answer."
      },
      {
        "date": "2023-06-06T08:50:00.000Z",
        "voteCount": 6,
        "content": "Did you really try this in pyspark, or look up the document?\nTypeError: describe() got an unexpected keyword argument 'all'"
      },
      {
        "date": "2023-06-30T06:44:00.000Z",
        "voteCount": 5,
        "content": "describe() is correct"
      },
      {
        "date": "2023-07-07T07:07:00.000Z",
        "voteCount": 1,
        "content": "Is you answer from Chat GPT ?"
      },
      {
        "date": "2023-08-03T07:36:00.000Z",
        "voteCount": 3,
        "content": "even chat gpt says E is the correct one :)"
      },
      {
        "date": "2023-10-19T08:25:00.000Z",
        "voteCount": 1,
        "content": "TypeError                                 Traceback (most recent call last)\n&lt;ipython-input-34-5077330dead7&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 storesDF.describe(all = True)\n\nTypeError: DataFrame.describe() got an unexpected keyword argument 'all'"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/databricks/view/114376-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks fails to return a DataFrame reverse sorted alphabetically based on column division?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.orderBy(\"division\", ascending \u2013 False)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.orderBy([\"division\"], ascending = [0])",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.orderBy(col(\"division\").asc())\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sort(\"division\", ascending \u2013 False)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sort(desc(\"division\"))"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-23T00:52:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer. \nSee code below with Spark 3.5.1\n\n# Sort a DataFrame by a column in reverse alphabetical order\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, asc, desc\nfrom pyspark.errors import PySparkTypeError\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 43161, \"A\"),\n        (1, 51200, \"A\"),\n        (2, None, \"B\"),\n        (3, 78367, \"B\"),\n        (4, None, \"C\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"sqft\", \"division\"])\n    \nstoresDF.orderBy(\"division\", ascending = False).show()\nstoresDF.orderBy([\"division\"], ascending = [0]).show()\nstoresDF.orderBy(col(\"division\").asc()).show()\nstoresDF.sort(\"division\", ascending = False).show()\nstoresDF.sort(desc(\"division\")).show()"
      },
      {
        "date": "2024-04-09T13:03:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2024-03-07T14:10:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer because it returns the dataframe in ascending order."
      },
      {
        "date": "2023-12-31T00:02:00.000Z",
        "voteCount": 1,
        "content": "b i tesetd ut"
      },
      {
        "date": "2023-12-10T20:37:00.000Z",
        "voteCount": 1,
        "content": "C. It is the only option \"not returning\" the dataframe in descending(reverse) order.\nAll other formats are returning the descending order. \nIn Oprtion E, if we import the desc function,. it will not throw error and will return the dataframe in descending order."
      },
      {
        "date": "2023-11-08T14:51:00.000Z",
        "voteCount": 1,
        "content": "E. storesDF.sort(desc(\"division\"))\n\nOption E correctly uses the desc function to specify the descending order for sorting. Thank you for providing additional information and clarification."
      },
      {
        "date": "2023-08-01T02:19:00.000Z",
        "voteCount": 1,
        "content": "Option A and D is giving errors. ~ cannot be used in ascending. Right way is to use ascending=False. Most relevant option is C which is sorting the data in ascending order , Option A, D have some typos it should be = instead of ~."
      },
      {
        "date": "2023-08-01T02:12:00.000Z",
        "voteCount": 3,
        "content": "C is the answer. Only C will make the data in ascending order. Tested the code."
      },
      {
        "date": "2023-07-30T09:31:00.000Z",
        "voteCount": 3,
        "content": "E is right"
      },
      {
        "date": "2023-07-09T05:17:00.000Z",
        "voteCount": 1,
        "content": "It's C: storesDF.orderBy(col(\"division\").asc()) =&gt; storesDF.orderBy(col(\"division\").desc())"
      },
      {
        "date": "2023-07-06T21:57:00.000Z",
        "voteCount": 2,
        "content": "Option E is right answer"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/databricks/view/107566-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a 15 percent sample of rows from DataFrame storesDF without replacement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample(fraction = 0.10)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sampleBy(fraction = 0.15)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample(True, fraction = 0.10)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample(fraction = 0.15)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-26T05:14:00.000Z",
        "voteCount": 3,
        "content": "The answer is E.\n\nOption A returns a 10% sample, not a 15% sample as requested.\n\nOption B is incorrect because sampleBy() is used to perform stratified sampling based on a column's values.\n\nOption C is incorrect because the first argument should be set to False to prevent sampling with replacement.\n\nOption D is incorrect because the sample() method without arguments will return a 50% sample of the DataFrame.\n\nOption E is the correct answer as it returns a sample of 15% of the DataFrame without replacement."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/databricks/view/107567-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns all the rows from DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.head()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.collect()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.take()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.show()"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-26T05:15:00.000Z",
        "voteCount": 5,
        "content": "Answer: B\n\nExplanation:\n\nhead() returns the first n rows of the DataFrame. By default, it returns the first 5 rows.\ncollect() returns an array of Row objects that represent the entire DataFrame.\ncount() returns the number of rows in the DataFrame.\ntake(n) returns the first n rows of the DataFrame as an array of Row objects.\nshow() prints the first 20 rows of the DataFrame in a tabular form.\nOnly collect() returns all the rows from the DataFrame."
      },
      {
        "date": "2023-12-31T03:15:00.000Z",
        "voteCount": 1,
        "content": "b correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/databricks/view/107568-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks applies the function assessPerformance() to each row of DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[assessPerformance(row) for row in storesDF.take(3)]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[assessPerformance() for row in storesDF]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.collect().apply(lambda: assessPerformance)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[assessPerformance(row) for row in storesDF.collect()]\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[assessPerformance(row) for row in storesDF]"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-24T04:48:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct. \nSee example code below:\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 43161, \"A\"),\n        (1, 51200, \"A\"),\n        (2, None, \"B\"),\n        (3, 78367, \"B\"),\n        (4, None, \"C\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"sqft\", \"division\"])\n\ndef myFunction(row):\n        return row[0]\n\n[myFunction(row) for row in storesDF.collect()]"
      },
      {
        "date": "2023-06-06T15:57:00.000Z",
        "voteCount": 1,
        "content": "There are many way to apply a function to dataframe.\n1. apply, as shown in option D. but it should be apply(assessPerformance)\n2. list comprehension: for row in df.collect()\n3. foreach\n4. map, but for RDD majorly"
      },
      {
        "date": "2023-04-26T05:17:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D.\n\nExplanation:\n\nOption A uses the take() method to extract three rows from the DataFrame, but it applies the assessPerformance() function to each row outside of the DataFrame context.\n\nOption B attempts to apply the assessPerformance() function to each row, but it doesn't reference the row object in any way.\n\nOption C tries to apply the assessPerformance() function to the entire DataFrame but does so using an incorrect syntax.\n\nOption D correctly applies the assessPerformance() function to each row of the DataFrame using a list comprehension over the result of the collect() method.\n\nOption E is similar to D, but it will iterate over rows individually instead of using the collect() method to retrieve all rows at once. While this is still a valid approach, it may be less efficient."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/databricks/view/108118-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to print the schema of DataFrame storesDF. Identify the error.<br>Code block:<br>storesDF.printSchema",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no printSchema member of DataFrame \u2013 schema and the print() function should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe entire line needs to be a string \u2013 it should be wrapped by str().",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no printSchema member of DataFrame \u2013 the getSchema() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no printSchema member of DataFrame \u2013 the schema() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe printSchema member of DataFrame is an operation and needs to be followed by parentheses.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-23T02:00:00.000Z",
        "voteCount": 1,
        "content": "Option E is correct.\nCode example below:\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, asc, desc\nfrom pyspark.errors import PySparkTypeError\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 43161, \"A\"),\n        (1, 51200, \"A\"),\n        (2, None, \"B\"),\n        (3, 78367, \"B\"),\n        (4, None, \"C\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"sqft\", \"division\"])\n\nstoresDF.printSchema()\n# root\n#  |-- storeID: long (nullable = true)\n#  |-- sqft: long (nullable = true)\n#  |-- division: string (nullable = true)"
      },
      {
        "date": "2023-05-01T07:00:00.000Z",
        "voteCount": 3,
        "content": "E. The printSchema member of DataFrame is an operation and needs to be followed by parentheses.\n\nThe correct code block should be storesDF.printSchema() with parentheses to indicate that it's a method call."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/databricks/view/108119-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should create and register a SQL UDF named \"ASSESS_PERFORMANCE\" using the Python function assessPerformance() and apply it to column customerSatisfaction in table stores. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br>Code block:<br>spark._1_._2_(_3_, _4_)<br>spark.sql(\"SELECT customerSatisfaction, _5_(customerSatisfaction) AS result FROM stores\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. udf<br>2. register<br>3. \"ASSESS_PERFORMANCE\"<br>4. assessPerformance<br>5. ASSESS_PERFORMANCE\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. udf<br>2. register<br>3. assessPerformance<br>4. \"ASSESS_PERFORMANCE\"<br>5. \"ASSESS_PERFORMANCE\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. udf<br>2. register<br>3.\"ASSESS_PERFORMANCE\"<br>4. assessPerformance<br>5. \"ASSESS_PERFORMANCE\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. register<br>2. udf<br>3. \"ASSESS_PERFORMANCE\"<br>4. assessPerformance<br>5. \"ASSESS_PERFORMANCE\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. udf<br>2. register<br>3. ASSESS_PERFORMANCE<br>4. assessPerformance<br>5. ASSESS_PERFORMANCE"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-24T04:57:00.000Z",
        "voteCount": 1,
        "content": "Answer: A\nSee code example below with Spark 3.5.1:\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 43161, \"A\"),\n        (1, 51200, \"A\"),\n        (3, 78367, \"B\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"sqft\", \"division\"])\n\ndef assess_performance(x):\n    return \"Large\" if x &gt; 50000 else \"Small\"\n\nspark.udf.register(\"ASSESS_PERFORMANCE\", assess_performance, \"STRING\")\n\nstoresDF.createOrReplaceTempView(\"stores\")\n\ndf = spark.sql(\"SELECT StoreID, ASSESS_PERFORMANCE(sqft) AS performance FROM stores\")\ndf.show()"
      },
      {
        "date": "2024-03-07T12:48:00.000Z",
        "voteCount": 2,
        "content": "def assessperformance():\n    return 'Good'\n\nspark.udf.register(\"assessperformance\",assessperformance)\ndf = spark.sql(\"SELECT assessperformance()\")\ndf.show()\n\nA"
      },
      {
        "date": "2023-05-01T07:01:00.000Z",
        "voteCount": 2,
        "content": "Answer: A\n\nExplanation:\n\nudf: create a user-defined function (UDF) in PySpark\nregister: register the UDF with Spark so it can be used in SQL queries\n\"ASSESS_PERFORMANCE\": name the UDF \"ASSESS_PERFORMANCE\"\nassessPerformance: specify the Python function to use for the UDF\nASSESS_PERFORMANCE: use the registered UDF in the SQL query to apply the assessPerformance() function to the customerSatisfaction column."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/databricks/view/108120-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to create a Python UDF assessPerformanceUDF() using the integer-returning Python function assessPerformance() and apply it to column customerSatisfaction in DataFrame storesDF. Identify the error.<br>Code block:<br>assessPerformanceUDF \u2013 udf(assessPerformance)<br>storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe assessPerformance() operation is not properly registered as a UDF.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe withColumn() operation is not appropriate here \u2013 UDFs should be applied by iterating over rows instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUDFs can only be applied vie SQL and not through the DataFrame API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe return type of the assessPerformanceUDF() is not specified in the udf() operation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe assessPerformance() operation should be used on column customerSatisfaction rather than the assessPerformanceUDF() operation."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-06T15:16:00.000Z",
        "voteCount": 9,
        "content": "The right answer is D.\npyspark.sql.functions.udf(f=None, returnType=StringType)\nThe default return type is string, but this question requires integer returning.\nso it should be D. \"The return type of the assessPerformanceUDF() is not specified in the udf() operation.\""
      },
      {
        "date": "2024-07-24T05:01:00.000Z",
        "voteCount": 2,
        "content": "Good explanation for Answer being D. Thank you!"
      },
      {
        "date": "2024-07-24T05:04:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer as otherwise the return type is the default StringType().\nTest code below:\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 3, \"A\"),\n        (1, 1, \"A\"),\n        (2, 2, \"A\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"customerSatisfaction\", \"division\"])\n\ndef assessPerformance(x):\n    return 1 if x &gt; 3 else 0\n\nprint(\"IntegerType()\")\nassessPerformanceUDF = udf(assessPerformance, IntegerType())\ndf = storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))\ndf.printSchema()\n\nprint(\"Default\")\nassessPerformanceUDF = udf(assessPerformance)\ndf = storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))\ndf.printSchema()"
      },
      {
        "date": "2024-06-26T08:08:00.000Z",
        "voteCount": 1,
        "content": "correct answer is D"
      },
      {
        "date": "2023-11-02T07:42:00.000Z",
        "voteCount": 1,
        "content": "It is necessary to inform the return type as IntegerType().\n\nfrom pyspark.sql.functions import udf, col\nfrom pyspark.sql.types import IntegerType\n\nstoresDF = spark.createDataFrame([('1', '123'), ('2', '234')], ['id', 'customerSatisfaction'])\nassessPerformance = lambda x: int(x)\n\nassessPerformanceUDF = udf(assessPerformance, IntegerType())\nstoresDF.withColumn('result', assessPerformanceUDF(col('customerSatisfaction'))).printSchema()"
      },
      {
        "date": "2023-09-29T22:37:00.000Z",
        "voteCount": 2,
        "content": "|      1. When `f` is a Python function:\n |      \n |          `returnType` defaults to string type and can be optionally specified. The produced\n |          object must match the specified type. In this case, this API works as if\n |          `register(name, f, returnType=StringType())`."
      },
      {
        "date": "2023-09-15T22:25:00.000Z",
        "voteCount": 2,
        "content": "The error in the code block is that the return type of the assessPerformanceUDF() is not specified in the udf() operation. In PySpark, when you register a Python function as a UDF, you should also specify the return type. This is important because Spark SQL needs to understand the return type to properly handle the UDF. Therefore, the correct answer is:"
      },
      {
        "date": "2023-08-03T08:39:00.000Z",
        "voteCount": 1,
        "content": "if they mean that - is =, then we need a second parameter, the output type. so, D is the answe"
      },
      {
        "date": "2023-07-07T07:22:00.000Z",
        "voteCount": 2,
        "content": "Right answer is D, return type has to be specified into udf() or it will return StringType by default, the code should be : \nfunction_UDF = udf(function, returnType=IntegerType())"
      },
      {
        "date": "2023-05-01T07:05:00.000Z",
        "voteCount": 3,
        "content": "The error in the code block is A. The function assessPerformance() needs to be passed as a parameter to the udf() operation in order to create a UDF from it. The correct code block should be:\n\nassessPerformanceUDF = udf(assessPerformance)\nstoresDF.withColumn(\"result\", assessPerformanceUDF(col("
      },
      {
        "date": "2023-06-06T15:14:00.000Z",
        "voteCount": 3,
        "content": "what is the difference between your code and question itsefl?\nassessPerformanceUDF \u2013 udf(assessPerformance)\nassessPerformanceUDF = udf(assessPerformance)\nchanging \"-\" to \"=\"?"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/databricks/view/108121-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to use SQL to return a new DataFrame containing column storeId and column managerName from a table created from DataFrame storesDF. Identify the error.<br>Code block:<br>storesDF.createOrReplaceTempView(\"stores\")<br>storesDF.sql(\"SELECT storeId, managerName FROM stores\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe createOrReplaceTempView() operation does not make a Dataframe accessible via SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe sql() operation should be accessed via the spark variable rather than DataFrame storesDF.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is the sql() operation in DataFrame storesDF. The operation query() should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThis cannot be accomplished using SQL \u2013 the DataFrame API should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe createOrReplaceTempView() operation should be accessed via the spark variable rather than DataFrame storesDF."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-24T05:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct. 'storeDF' has not attribute or method `sql`\nTest code below:\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\ndata = [\n        (0, 3, \"John\"),\n        (1, 1, \"Jane\"),\n        (2, 2, \"Jack\"),\n        ]\nstoresDF = spark.createDataFrame(data, [\"storeID\", \"customerSatisfaction\", \"managerName\"])\n\nstoresDF.createOrReplaceTempView(\"stores\")\ntry:\n    storesDF.sql(\"SELECT storeId, managerName FROM stores\")\nexcept AttributeError as e:\n    print(e)\nfinally:\n    spark.sql(\"SELECT storeId, managerName FROM stores\").show()"
      },
      {
        "date": "2023-11-02T07:52:00.000Z",
        "voteCount": 2,
        "content": "B is correct:\n\nstoresDF = spark.createDataFrame([('1', 'juan'), ('2', 'perez')], ['storeId', 'managerName'])\nstoresDF.createOrReplaceTempView(\"stores\")\nspark.sql(\"SELECT storeId, managerName FROM stores\").show()"
      },
      {
        "date": "2023-05-01T07:17:00.000Z",
        "voteCount": 2,
        "content": "Option B is correct because the sql() function is not a method of a DataFrame object. It is actually a method of the SparkSession object spark. Therefore, the correct way to execute a SQL statement using Spark SQL is to call sql() on the SparkSession object as follows:\n\nspark.sql(\"SELECT storeId, managerName FROM stores\")\n\nIn the code block provided in the question, sql() is called on a DataFrame object, which will result in a DataFrame object without executing the SQL statement. Therefore, option B correctly identifies the error in the code block."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/databricks/view/106136-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should create a single-column DataFrame from Python list years which is made up of integers. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br>Code block:<br>_1_._2_(_3_, _4_)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createDataFrame<br>3. years<br>4. IntegerType",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. DataFrame<br>2. create<br>3. [years]<br>4. IntegerType",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createDataFrame<br>3. [years]<br>4. IntegertType",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createDataFrame<br>3. [years]<br>4. IntegertType()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createDataFrame<br>3. years<br>4. IntegertType()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-13T12:41:00.000Z",
        "voteCount": 10,
        "content": "The answer should be E because Year is already a python list."
      },
      {
        "date": "2024-08-26T10:12:00.000Z",
        "voteCount": 1,
        "content": "it uses spark.createDataFrame correctly with the Python list years and the appropriate data type IntegerType(). All other options have errors either in syntax or the use of PySpark methods and types."
      },
      {
        "date": "2024-07-24T05:10:00.000Z",
        "voteCount": 3,
        "content": "E is the right answer\nSee code below:\n\n# Create a DataFrame from a list of integers\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import IntegerType\n\nspark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n\nyears = [2017, 2018, 2019]\ndf = spark.createDataFrame(years, IntegerType())\n\ndf.show()\ndf.printSchema()"
      },
      {
        "date": "2024-02-19T23:56:00.000Z",
        "voteCount": 1,
        "content": "E is the most suitable, but it also contains an error.\n\nIn PySpark, the correct class name for the integer data type is IntegerType (not \"IntegertType\")."
      },
      {
        "date": "2023-12-31T03:28:00.000Z",
        "voteCount": 1,
        "content": "e is the right"
      },
      {
        "date": "2023-11-02T08:05:00.000Z",
        "voteCount": 1,
        "content": "E is correct:\n\nfrom pyspark.sql.types import IntegerType\nyears = [2023, 2024]\nprint(type(years))\nstoresDF = spark.createDataFrame(years, IntegerType())\nstoresDF.show()\n\n&lt;class 'list'&gt;\n+-----+\n|value|\n+-----+\n| 2023|\n| 2024|\n+-----+"
      },
      {
        "date": "2023-10-19T08:41:00.000Z",
        "voteCount": 1,
        "content": "D\n\nfrom pyspark.sql.types import IntegerType\nspark.createDataFrame([1991,2023],IntegerType()).show()\n\n+-----+\n|value|\n+-----+\n| 1991|\n| 2023|\n+-----+"
      },
      {
        "date": "2024-06-22T08:30:00.000Z",
        "voteCount": 2,
        "content": "it's E. years is already a list"
      },
      {
        "date": "2023-09-15T23:13:00.000Z",
        "voteCount": 1,
        "content": "1. spark\n2. createDataFrame\n3. years\n4. IntegertType()"
      },
      {
        "date": "2023-08-03T08:57:00.000Z",
        "voteCount": 1,
        "content": "if years is variable, it works, just tested it: years = [1, 3, 4, 5 , 9]\ndf7 = spark.createDataFrame(years, IntegerType())\ndf7.show()\n\nthis works as well: df7 = spark.createDataFrame([1, 3, 4, 5 , 9], IntegerType())\ndf7.show()\n\nthis won't work:\ndf7 = spark.createDataFrame([years], IntegerType())\ndf7.show()\n\nso, the answer is E"
      },
      {
        "date": "2023-08-01T02:57:00.000Z",
        "voteCount": 1,
        "content": "E. D is giving an error ."
      },
      {
        "date": "2023-07-30T10:10:00.000Z",
        "voteCount": 1,
        "content": "D throws a big error.\n/usr/local/spark/python/pyspark/sql/types.py in verify_acceptable_types(obj)\n   1291         # subclass of them can not be fromInternal in JVM\n   1292         if type(obj) not in _acceptable_types[_type]:\n-&gt; 1293             raise TypeError(new_msg(\"%s can not accept object %r in type %s\"\n   1294                                     % (dataType, obj, type(obj))))\n   1295 \n\nTypeError: field value: IntegerType can not accept object [1, 2, 3, 4, 5] in type &lt;class 'list'&gt;\n\nE is correct answer \n\nfrom pyspark.sql.types import IntegerType\na = [1,2,3,4,5]\nspark.createDataFrame(a, IntegerType()).show()"
      },
      {
        "date": "2023-04-25T12:43:00.000Z",
        "voteCount": 2,
        "content": "Two responses\n1. D is an error. E will split the array into rows\n2. spark.createDataFrame([arraryVar_name],ArrayType(IntegerType())) will store the whole array as a row"
      },
      {
        "date": "2023-04-25T12:39:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/databricks/view/106137-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to cache DataFrame storesDF only in Spark\u2019s memory and then return the number of rows in the cached DataFrame. Identify the error.<br>Code block:<br>storesDF.cache().count()",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cache() operation caches DataFrames at the MEMORY_AND_DISK level by default \u2013 the storage level must be specified to MEMORY_ONLY as an argument to cache().",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cache() operation caches DataFrames at the MEMORY_AND_DISK level by default \u2013 the storage level must be set via storesDF.storageLevel prior to calling cache().",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe storesDF DataFrame has not been checkpointed \u2013 it must have a checkpoint in order to be cached.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrames themselves cannot be cached \u2013 DataFrame storesDF must be cached as a table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cache() operation can only cache DataFrames at the MEMORY_AND_DISK level (the default) \u2013 persist() should be used instead.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-14T12:03:00.000Z",
        "voteCount": 1,
        "content": "A because the default behavior of cache() is MEMORY_AND_DISK, and if you want MEMORY_ONLY, you must specify it explicitly"
      },
      {
        "date": "2024-03-07T11:55:00.000Z",
        "voteCount": 1,
        "content": "E is wrong. The cache() operation can only cache DataFrames at the MEMORY_AND_DISK level (the default) \nnote the use of 'only' here, cache can also store in disk if required. \n\nB is also wrong, there is no condition to set storagelevel prior to calling cache()\n\ncorrect answer is A."
      },
      {
        "date": "2023-11-02T09:02:00.000Z",
        "voteCount": 1,
        "content": "E is correct!\n\nfrom pyspark.sql.types import IntegerType\nfrom pyspark import StorageLevel\n\nstoresDF = spark.createDataFrame([2023, 2024], IntegerType())\nprint(storesDF.persist(StorageLevel.MEMORY_ONLY).storageLevel)"
      },
      {
        "date": "2023-10-19T08:59:00.000Z",
        "voteCount": 1,
        "content": "E\ncache() -&gt; 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Persists the :class:`DataFrame` with the default storage level (`MEMORY_AND_DISK`).\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Supports Spark Connect.\n    \n    Notes\n    -----\n    The default storage level has changed to `MEMORY_AND_DISK` to match Scala in 2.0."
      },
      {
        "date": "2023-08-01T03:38:00.000Z",
        "voteCount": 4,
        "content": "E is correct. You cannot set StorageLevel Memory_only with cache(), if memory available then it keeps everything into memory else it will spill to disk. To keep everything into Memory you need to use Persist() with Storage Level Memory only."
      },
      {
        "date": "2023-07-09T04:25:00.000Z",
        "voteCount": 1,
        "content": "there are two options here: B and E. Who chose B =&gt; you can't explicitly set the storage level, it's a read-only property, so the correct answer is E."
      },
      {
        "date": "2023-05-27T17:48:00.000Z",
        "voteCount": 1,
        "content": "E\n\nB. The cache() operation caches DataFrames at the MEMORY_AND_DISK level by default \u2013 the storage level must be set via storesDF.storageLevel prior to calling cache().\n\nThis option is incorrect. The storage level does not need to be set via storesDF.storageLevel prior to calling cache(). The cache() operation can be used directly on the DataFrame without explicitly setting the storage level.\n\nE. The cache() operation can only cache DataFrames at the MEMORY_AND_DISK level (the default) \u2013 persist() should be used instead.\n\nThis option is the correct answer. The error in the code block is that the cache() operation is used instead of persist(). While cache() caches DataFrames at the default MEMORY_AND_DISK level, persist() provides more flexibility by allowing different storage levels to be specified, such as MEMORY_ONLY for caching only in memory. Therefore, persist() should be used instead of cache() to achieve the desired caching behavior."
      },
      {
        "date": "2023-05-03T02:51:00.000Z",
        "voteCount": 1,
        "content": "B. The cache() operation caches DataFrames at the MEMORY_AND_DISK level by default \u2013 the storage level must be set via storesDF.storageLevel prior to calling cache().\n\nThe storage level of a DataFrame cache can be specified as an argument to the cache() operation, but if the storage level has not been specified, the default MEMORY_AND_DISK level is used. Therefore, option A is incorrect.\n\nOption C is incorrect because caching and checkpointing are different operations in Spark. Caching stores a DataFrame in memory or on disk, while checkpointing saves a DataFrame to a reliable storage system like HDFS, which is necessary for iterative computations.\n\nOption D is incorrect because DataFrames can be cached in memory or on disk using the cache() operation.\n\nOption E is incorrect because cache() is the recommended method for caching DataFrames in Spark, and it supports caching at all storage levels, including MEMORY_ONLY. The persist() operation can be used to specify a storage level, but cache() is simpler and more commonly used."
      },
      {
        "date": "2023-06-06T10:33:00.000Z",
        "voteCount": 3,
        "content": "Wrong explanation. you can call cache() or persist() without set storage level, it will use default Memoery_and_disk. \nYou clearly misunderstand the question itself. storesDF.cache().count() is a workable code, but fail the requirement. This is the issue.\nThe question asked \"only in memory\", that means, if the data size is out of the memory, i do not want to store it in disk, but rather recompute. Therefore, you need to specifically set the storage level as \"MEMORY ONLY\".\nA is correct"
      },
      {
        "date": "2023-04-13T12:53:00.000Z",
        "voteCount": 2,
        "content": "The answer should be E. See this post for reference https://stackoverflow.com/questions/26870537/what-is-the-difference-between-cache-and-persist"
      },
      {
        "date": "2023-05-03T02:51:00.000Z",
        "voteCount": 1,
        "content": "No, option E is incorrect. The cache() method is the appropriate method to cache a DataFrame in Spark's memory, and it can cache DataFrames at the MEMORY_ONLY level if that's what is desired. The persist() method is a more general-purpose method that allows the user to specify other storage levels (such as MEMORY_AND_DISK), but it is not required for this task."
      },
      {
        "date": "2023-10-19T09:00:00.000Z",
        "voteCount": 1,
        "content": "You should use storesDF.persist(StorageLevel.MEMORY_ONLY).count()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/databricks/view/108372-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations can be used to return a new DataFrame from DataFrame storesDF without inducing a shuffle?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.intersect()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.repartition(1)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.union()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.coalesce(1)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.rdd.getNumPartitions()"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-07T12:08:00.000Z",
        "voteCount": 3,
        "content": "Though Union does not cause a shuffle, you need another dataframe to do union. in this question its limited to storesDF. coalesce(1) is the correct answer, as it does not cause shuffle rather combines multiple partitions into 1, i.e. reducing partitions = no shuffle.\nexecute storedDF.coalesce(1) and check DAG"
      },
      {
        "date": "2024-02-23T08:24:00.000Z",
        "voteCount": 1,
        "content": "Answer C : union\nNarrow transformation -  all transformation logic performed within one partition\nWide transformations - transformation during which is needed shuffle/exchange, distribution of data to other partitions\nUnion is narrow transaction"
      },
      {
        "date": "2023-11-05T09:21:00.000Z",
        "voteCount": 2,
        "content": "union is the only operation from mentioned here that won't do shuffling. And as @ZSun mentioned, do not follow any of the 4be8126 answers, they are all blindly from GPT"
      },
      {
        "date": "2023-08-29T11:55:00.000Z",
        "voteCount": 1,
        "content": "C is the correct\ncoalesce may induce a partial shuffle"
      },
      {
        "date": "2023-06-07T16:22:00.000Z",
        "voteCount": 2,
        "content": "I think this question contains error, it should not be which one without shuffle, it should be which one cause shuffle.\nunion is a narrow transformation, not causing shuffle.\ncoalesce simply combine partitions together into one, not shuffle them.\nrdd.getNumPartitions just evaluate the number of partition of a dataframe, no shuffle.\neven for repartition(1), since there is only one partition in the end, it also not causing shuffle, it simply combine all partition together.\nTherefore, it should be A, this is the only one inducing a shuffle.\nor, B C D E without inducing a shuffle"
      },
      {
        "date": "2023-06-07T15:55:00.000Z",
        "voteCount": 2,
        "content": "The Answer is C. Union rather than coalesce.\nUnion is a narrow transformation. unlike wide transformationl, narrow transformation does not require shuffle.\nCoalesce is wide transformation, combine multiple partition to smaller number of partition. Don't this process require shuffling partition together?\nif you ask ChatGPT, it will tell you what 4be8126 comment."
      },
      {
        "date": "2023-06-07T16:15:00.000Z",
        "voteCount": 5,
        "content": "This is incorrect explanation, delete it"
      },
      {
        "date": "2024-02-10T06:34:00.000Z",
        "voteCount": 2,
        "content": "The problem with Union answer is that it returns an error if we run it without arg."
      },
      {
        "date": "2023-05-03T02:53:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D. coalesce() can be used to return a new DataFrame with a reduced number of partitions, without inducing a shuffle.\n\nA shuffle is an expensive operation that involves the redistribution of data across a cluster, so it's important to minimize its use whenever possible. In this case, repartition() and union() both involve shuffles, while intersect() returns only the common rows between two DataFrames, and rdd.getNumPartitions() returns the number of partitions in the RDD underlying the DataFrame."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/databricks/view/108373-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a new 12-partition DataFrame from the 8-partition DataFrame storesDF by inducing a shuffle. Identify the error.<br>Code block:<br>storesDF.coalesce(12)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe coalesce() operation cannot guarantee the number of target partitions \u2013 the repartition() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe coalesce() operation does not induce a shuffle and cannot increase the number of partitions \u2013 the repartition() operation should be used instead.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe coalesce() operation will only work if the DataFrame has been cached to memory \u2013 the repartition() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe coalesce() operation requires a column by which to partition rather than a number of partitions \u2013 the repartition() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe number of resulting partitions, 12, is not achievable for an 8-partition DataFrame."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-11T07:21:00.000Z",
        "voteCount": 1,
        "content": "with version 3.4.0, \n\ndf.repartition(12).coalesce(16).rdd.getNumPartitions() returns 12. it doesn't throw error, but only doesn't increase partition either"
      },
      {
        "date": "2023-05-03T02:56:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is B.\n\nThe coalesce() operation can decrease the number of partitions but cannot increase the number of partitions. It also does not induce a shuffle, and is therefore more efficient when decreasing the number of partitions.\n\nIf the goal is to increase the number of partitions, repartition() should be used instead."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/databricks/view/108374-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following Spark properties is used to configure whether DataFrame partitions that do not meet a minimum size threshold are automatically coalesced into larger partitions during a shuffle?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.shuffle.partitions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.autoBroadcastJoinThreshold",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.skewJoin.enabled",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.inMemoryColumnarStorage.batchSize",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.coalescePartitions.enabled\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-06T11:13:00.000Z",
        "voteCount": 1,
        "content": "https://spark.apache.org/docs/latest/sql-performance-tuning.html\n\nspark.sql.adaptive.coalescePartitions.enabled: When true and spark.sql.adaptive.enabled is true, Spark will coalesce contiguous shuffle partitions according to the target size (specified by spark.sql.adaptive.advisoryPartitionSizeInBytes), to avoid too many small tasks."
      },
      {
        "date": "2023-05-03T02:59:00.000Z",
        "voteCount": 2,
        "content": "The answer is E. spark.sql.adaptive.coalescePartitions.enabled is the Spark property used to configure whether DataFrame partitions that do not meet a minimum size threshold are automatically coalesced into larger partitions during a shuffle. When set to true, Spark automatically coalesces partitions that are smaller than the configured minimum size into larger partitions to optimize shuffles."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/databricks/view/110402-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a DataFrame containing a column openDateString, a string representation of Java\u2019s SimpleDateFormat. Identify the error.<br>Note that column openDate is of type integer and represents a date in the UNIX epoch format \u2013 the number of seconds since midnight on January 1st, 1970.<br>An example of Java\u2019s SimpleDateFormat is \"Sunday, Dec 4, 2008 1:05 PM\".<br>A sample of storesDF is displayed below:<br><img title=\"image6\" src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image6.png\"><br>Code block:<br>storesDF.withColumn(\"openDateString\", from_unixtime(col(\"openDate\"), \"EEE, MMM d, yyyy h:mm a\", TimestampType()))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe from_unixtime() operation only accepts two parameters \u2013 the TimestampTime() arguments not necessary.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe from_unixtime() operation only works if column openDate is of type long rather than integer \u2013 column openDate must first be converted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe second argument to from_unixtime() is not correct \u2013 it should be a variant of TimestampType() rather than a string.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe from_unixtime() operation automatically places the input column in java\u2019s SimpleDateFormat \u2013 there is no need for a second or third argument.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe column openDate must first be converted to a timestamp, and then the Date() function can be used to reformat to java\u2019s SimpleDateFormat."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-14T08:04:00.000Z",
        "voteCount": 2,
        "content": "A is correct:\n\nfrom pyspark.sql.functions import from_unixtime, col\n\nstoresDF = spark.createDataFrame([(0, 1100746394), (1, 1474410343)], ['storeId', 'openDate'])\nstoresDF = storesDF.withColumn(\"openDateString\", from_unixtime(col(\"openDate\"), \"EEE, MMM d, yyyy h:mm a\"))\ndisplay(storesDF)"
      },
      {
        "date": "2023-08-23T00:28:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer. \n\nFunction signature from the docs:\npyspark.sql.functions.from_unixtime(timestamp, format='uuuu-MM-dd HH:mm:ss')"
      },
      {
        "date": "2023-07-30T10:32:00.000Z",
        "voteCount": 2,
        "content": "A is also right."
      },
      {
        "date": "2023-05-27T17:56:00.000Z",
        "voteCount": 1,
        "content": "B. The from_unixtime() operation only works if column openDate is of type long rather than integer - column openDate must first be converted.\n\nThis option is correct. The code block has an error because the from_unixtime() function expects the column openDate to be of type long, not integer. The column should be cast to long before applying the function."
      },
      {
        "date": "2023-06-06T10:18:00.000Z",
        "voteCount": 3,
        "content": "This is completely nonsense about long and integer.\nlong (or bigint): It is a 64-bit signed integer data type anging from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807. \ninteger (or int): It is a 32-bit signed integer data ranging from -2,147,483,648 to 2,147,483,647"
      },
      {
        "date": "2023-11-14T08:03:00.000Z",
        "voteCount": 1,
        "content": "That not make sense, the code below works perfectly:\n\nfrom pyspark.sql.functions import from_unixtime, col\n\nstoresDF = spark.createDataFrame([(0, 1100746394), (1, 1474410343)], ['storeId', 'openDate'])\nstoresDF = storesDF.withColumn('openDate', col('openDate').cast('integer'))\nstoresDF = storesDF.withColumn(\"openDateString\", from_unixtime(col(\"openDate\"), \"EEE, MMM d, yyyy h:mm a\"))\ndisplay(storesDF)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/databricks/view/106140-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame containing a column dayOfYear, an integer representation of the day of the year from column openDate from DataFrame storesDF?<br>Note that column openDate is of type integer and represents a date in the UNIX epoch format \u2013 the number of seconds since midnight on January 1st, 1970.<br>A sample of storesDF is displayed below:<br><img title=\"image7\" src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image7.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\"))<br>. withColumn(\"dayOfYear\", dayofyear(col(\"openTimestamp\"))))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"dayOfYear\", get dayofyear(col(\"openDate\")))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"dayOfYear\", dayofyear(col(\"openDate\")))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"openDateFormat\", col(\"openDate\").cast(\"Date\"))<br>. withColumn(\"dayOfYear\", dayofyear(col(\"openDateFormat\"))))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"dayOfYear\", substr(col(\"openDate\"), 4, 6))"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-14T08:23:00.000Z",
        "voteCount": 1,
        "content": "A is correct:\n\nfrom pyspark.sql.functions import col, dayofyear\n\nstoresDF = spark.createDataFrame([(0, 1100746394), (1, 1474410343)], ['storeId', 'openDate'])\nstoresDF = (storesDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\")).withColumn(\"dayOfYear\", dayofyear(col(\"openTimestamp\"))))\ndisplay(storesDF)"
      },
      {
        "date": "2023-11-07T02:09:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-09-15T23:27:00.000Z",
        "voteCount": 2,
        "content": "storesDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\")).withColumn(\"dayOfYear\", dayofyear(col(\"openTimestamp\")))"
      },
      {
        "date": "2023-08-01T04:03:00.000Z",
        "voteCount": 1,
        "content": "A. dayofyear function in PySpark's functions module expects the column openDate to be of type timestamp rather than long."
      },
      {
        "date": "2023-05-03T03:14:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C.\n\nOption A is correct because it casts the openDate column to a timestamp using cast(\"Timestamp\") and then uses the dayofyear function to extract the day of the year from the timestamp.\n\nOption B is incorrect because it contains syntax errors, including the \"get\" keyword, which is not necessary or valid in this context.\n\nOption C is close, but it does not cast the openDate column to a timestamp, which is necessary to use the dayofyear function.\n\nOption D is incorrect because it converts column \"openDate\" to a date format, which is unnecessary for extracting the day of the year. Additionally, the dayofyear() function can be applied directly to the \"openDate\" column.\n\nOption E is incorrect because it uses the substr() function to extract a substring from the \"openDate\" column, which does not correspond to the day of the year."
      },
      {
        "date": "2023-04-13T13:30:00.000Z",
        "voteCount": 2,
        "content": "The answer should be A. Unixtime should be cast to timestamp first"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/databricks/view/110462-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block intended to return a new DataFrame that is the result of an inner join between DataFrame storesDF and DataFrame employeesDF on column storeId. Identify the error.<br>Code block:<br>StoresDF.join(employeesDF, \"inner\", \"storeID\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe key column storeID needs to be wrapped in the col() operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe key column storeID needs to be in a list like [\"storeID\"].",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe key column storeID needs to be specified in an expression of both DataFrame columns like storesDF.storeId == employeesDF.storeId.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no DataFrame.join() operation \u2013 DataFrame.merge() should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe column key is the second parameter to join() and the type of join in the third parameter to join() \u2013 the second and third arguments should be switched.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-07T02:58:00.000Z",
        "voteCount": 1,
        "content": "there are diff methods to join dataframes, one of the: \njoinedDF = StoresDF.join(employeesDF, \"storeId\", \"inner\")"
      },
      {
        "date": "2023-05-28T19:56:00.000Z",
        "voteCount": 3,
        "content": "E\n\nstoresDF.join(employeesDF, \"storeID\", \"inner\")\n\nThe column key is the second parameter to join() and the type of join is in the third parameter to join() \u2013 the second and third arguments should be switched"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/databricks/view/108377-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations can perform an outer join on two DataFrames?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.crossJoin()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStandalone join() function",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.outerJoin()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.join()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.merge()"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-25T13:24:00.000Z",
        "voteCount": 5,
        "content": "D. result_df = df1.join(df2, on=\"id\", how=\"outer\")"
      },
      {
        "date": "2023-11-14T08:30:00.000Z",
        "voteCount": 2,
        "content": "D is correct.\n\nThere is no exists outerJoin() operation in pyspark."
      },
      {
        "date": "2023-05-03T03:40:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C - DataFrame.outerJoin(). The outer join operation can be performed by specifying the join type as \"outer\" when calling the outerJoin() function on a DataFrame. The join() function in Spark only performs an inner join, while the merge() function is not a valid function in Spark SQL. The crossJoin() function performs a Cartesian product between two DataFrames, which is not an outer join."
      },
      {
        "date": "2024-04-23T16:37:00.000Z",
        "voteCount": 1,
        "content": "Wrong answer, check documentation"
      },
      {
        "date": "2023-06-06T12:37:00.000Z",
        "voteCount": 9,
        "content": "There is no outerjoin, bro!\nonly dataframe.join(how='outer')"
      },
      {
        "date": "2023-11-19T13:04:00.000Z",
        "voteCount": 1,
        "content": "this guy always post wrong answers, sometime gpts as well. ignore his commnmets"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/databricks/view/110463-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following pairs of arguments cannot be used in DataFrame.join() to perform an inner join on two DataFrames, named and aliased with \"a\" and \"b\" respectively, to specify two key columns?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ton = [a.column1 == b.column1, a.column2 == b.column2]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ton = [col(\"column1\"), col(\"column2\")]\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ton = [col(\"a.column1\") == col(\"b.column1\"), col(\"a.column2\") == col(\"b.column2\")]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll of these options can be used to perform an inner join with two key columns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ton = [\"column1\", \"column2\"]"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-08T13:48:00.000Z",
        "voteCount": 1,
        "content": "B cannot be used as this seems ambiguous"
      },
      {
        "date": "2023-12-23T07:06:00.000Z",
        "voteCount": 1,
        "content": "B throws AnalysisException: [AMBIGUOUS_REFERENCE] Reference `column1` is ambiguous, could be: [`a`.`column1`, `b`.`column1`]"
      },
      {
        "date": "2023-11-14T09:13:00.000Z",
        "voteCount": 3,
        "content": "According to the following code, only response B returns an error. The key concept here is that dataframes must be \"named\" AND \"aliased\".\n\nfrom pyspark.sql.functions import col\n\na = spark.createDataFrame([(1, 2), (3, 4)], ['column1', 'column2'])\nb = spark.createDataFrame([(1, 2), (5, 6)], ['column1', 'column2'])\n\na = a.alias('a')\nb = b.alias('b')\n\ndf = a.join(b, on = [a.column1 == b.column1, a.column2 == b.column2])\ndisplay(df)\n# df = a.join(b, on = [col(\"column1\"), col(\"column2\")])\ndf = a.join(b, on = [col(\"a.column1\") == col(\"b.column1\"), col(\"a.column2\") == col(\"b.column2\")])\ndisplay(df)\ndf = a.join(b, on = [\"column1\", \"column2\"])\ndisplay(df)"
      },
      {
        "date": "2023-11-07T04:49:00.000Z",
        "voteCount": 3,
        "content": "100% B \nBelow code to test:\n\ndataA = [Row(column1=1, column2=2), Row(column1=2, column2=4), Row(column1=3, column2=6)]\ndfA = spark.createDataFrame(dataA)"
      },
      {
        "date": "2023-11-07T04:49:00.000Z",
        "voteCount": 3,
        "content": "# Sample data for DataFrame 'b'\ndataB = [Row(column1=1, column2=2), Row(column1=2, column2=5), Row(column1=3, column2=4)]\ndfB = spark.createDataFrame(dataB)\n\n# Alias DataFrames as 'a' and 'b'\na = dfA.alias(\"a\")\nb = dfB.alias(\"b\")\n\na.show()\nb.show()\n\n\n#Option A \njoinedDF_A = a.join(b, [a.column1 == b.column1, a.column2 == b.column2])\njoinedDF_A.show()\n\n\n#Option B \n#joinedDF_B = a.join(b, [col(\"column1\"), col(\"column2\")])\n#joinedDF_B.show()\n\n\n#Option C\njoinedDF_C = a.join(b, [col(\"a.column1\") == col(\"b.column1\"), col(\"a.column2\") == col(\"b.column2\")])\njoinedDF_C.show()\n\n\n#Option E\njoinedDF_E = a.join(b, [\"column1\", \"column2\"])\njoinedDF_E.show()"
      },
      {
        "date": "2023-10-18T13:11:00.000Z",
        "voteCount": 2,
        "content": "I tried all of the options and I got 2 errors from:\n\nB\nAMBIGUOUS_REFERENCE] Reference `Category` is ambiguous, could be: [`Category`, `Category`]\n\nC:\n[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `df_1`.`Category` cannot be resolved. \n\nDid you mean one of the following? [`Category`, `Category`, `Truth`, `Truth`, `Value`].;"
      },
      {
        "date": "2023-10-28T07:08:00.000Z",
        "voteCount": 1,
        "content": "it's B, it seems you didn't do the alias \na = df1.alias(\"a\")\nb = df2.alias(\"b\")"
      },
      {
        "date": "2023-09-29T23:11:00.000Z",
        "voteCount": 1,
        "content": "from pyspark.sql.functions import col\ndf2.alias('a').join(df3.alias('b'),\n         [col(\"a.name\") == col(\"b.name\"), col(\"a.name\") == col(\"b.name\")],\n         'full_outer').select(df2['name'],'height','age').show()\n It worked. so every answer is correct."
      },
      {
        "date": "2023-08-03T10:55:00.000Z",
        "voteCount": 3,
        "content": "should be C as in col() we specify only a column name as a string, not a dataframe"
      },
      {
        "date": "2023-05-28T19:58:00.000Z",
        "voteCount": 2,
        "content": "A. on = [a.column1 == b.column1, a.column2 == b.column2]\nThis option is valid and can be used to perform an inner join on two key columns. It specifies the key columns using the syntax a.column1 == b.column1 and a.column2 == b.column2."
      },
      {
        "date": "2023-06-06T09:05:00.000Z",
        "voteCount": 2,
        "content": "I think the question \"which one cannot be used to perform inner join\", is confusing,\nBecause only A works, the rest of answer is incorrect.\nThe question should be \"which one can be used\""
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/databricks/view/108380-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The below code block contains a logical error resulting in inefficiency. The code block is intended to efficiently perform a broadcast join of DataFrame storesDF and the much larger DataFrame employeesDF using key column storeId. Identify the logical error.<br>Code block:<br>storesDF.join(broadcast(employeesDF), \"storeId\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe larger DataFrame employeesDF is being broadcasted rather than the smaller DataFrame storesDF.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is never a need to call the broadcast() operation in Apache Spark 3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe entire line of code should be wrapped in broadcast() rather than just DataFrame employeesDF.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe broadcast() operation will only perform a broadcast join if the Spark property spark.sql.autoBroadcastJoinThreshold is manually set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly one of the DataFrames is being broadcasted rather than both of the DataFrames."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-14T09:28:00.000Z",
        "voteCount": 1,
        "content": "A si correct:\n\n# https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html\n\nfrom pyspark.sql import types\nfrom pyspark.sql.functions import broadcast\n\ndf = spark.createDataFrame([1, 2, 3, 3, 4], types.IntegerType())\ndf_small = spark.range(3)\ndf.join(broadcast(df_small), df.value == df_small.id).show()"
      },
      {
        "date": "2023-05-03T03:48:00.000Z",
        "voteCount": 2,
        "content": "The answer is A.\n\nThe logical error in the code block is that the larger DataFrame, employeesDF, is being broadcasted instead of the smaller DataFrame, storesDF. This defeats the purpose of a broadcast join, which is to optimize performance by broadcasting the smaller DataFrame to all the worker nodes, avoiding the need to shuffle data over the network.\n\nTo perform a broadcast join efficiently, the smaller DataFrame should be broadcasted, which in this case is storesDF. The corrected code should be:\n\nbroadcast(storesDF).join(employeesDF, \"storeId\")"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/databricks/view/105657-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a new DataFrame that is the result of a cross join between DataFrame storesDF and DataFrame employeesDF. Identify the error.<br>Code block:<br>storesDF.join(employeesDF, \"cross\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA cross join is not implemented by the DataFrame.join() operations \u2013 the standalone CrossJoin() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no direct cross join in Spark, but it can be implemented by performing an outer join on all columns of both DataFrames.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA cross join is not implemented by the DataFrame.join()operation \u2013 the DataFrame.crossJoin()operation should be used instead.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no key column specified \u2013 the key column \"storeId\" should be the second argument.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA cross join is not implemented by the DataFrame.join() operations \u2013 the standalone join() operation should be used instead."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-19T09:19:00.000Z",
        "voteCount": 1,
        "content": "Cross Join in PySpark: A cross join (also known as a Cartesian product) returns the Cartesian product of the two DataFrames, meaning every row from the first DataFrame is paired with every row from the second DataFrame. In PySpark, the crossJoin() method is used specifically for this type of join."
      },
      {
        "date": "2024-08-13T18:55:00.000Z",
        "voteCount": 1,
        "content": "The correct identification of the error is:\n\nC. A cross join is not implemented by the DataFrame.join() operation \u2013 the DataFrame.crossJoin() operation should be used instead.\n\nExplanation:\n\nIn Spark, to perform a cross join between two DataFrames, you should use the crossJoin() method, not the join() method with the \"cross\" argument."
      },
      {
        "date": "2024-02-26T03:19:00.000Z",
        "voteCount": 1,
        "content": "Correct answer C \nfrom pyspark.sql import Row\ndf = spark.createDataFrame(\n    [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\ndf2 = spark.createDataFrame(\n    [Row(height=80, name=\"Tom\"), Row(height=85, name=\"Bob\")])\ndf.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").show()\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.crossJoin.html"
      },
      {
        "date": "2024-02-08T14:06:00.000Z",
        "voteCount": 2,
        "content": "D is the answer here as key is missing. As per syntax, key is needed."
      },
      {
        "date": "2023-11-14T09:36:00.000Z",
        "voteCount": 2,
        "content": "C is correct.\n\n# https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.crossJoin.html\n\na = spark.createDataFrame([(1, 2), (3, 4)], ['column1', 'column2'])\nb = spark.createDataFrame([(5, 6), (7, 8)], ['column3', 'column4'])\n\ndf = a.crossJoin(b)\ndisplay(df)"
      },
      {
        "date": "2023-11-07T05:30:00.000Z",
        "voteCount": 3,
        "content": "I know it looks confusing to have key column for cross join, but it ijoin method syntaxis: \nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.join.html\n\nsee example below : \ndataA = [Row(column1=1, column2=2), Row(column1=2, column2=4), Row(column1=3, column2=6)]\ndfA = spark.createDataFrame(dataA)\n\n# Sample data for DataFrame 'b'\ndataB = [Row(column1=1, column2=2), Row(column1=2, column2=5), Row(column1=3, column2=4)]\ndfB = spark.createDataFrame(dataB)\n\njoinedDF = dfA.join(dfB, on=None, how=\"cross\")\njoinedDF.show()\n\nit is possible to do Cross join this way as well DataFrame.crossJoin() but answer C states that df.join () doesn't do cross, which is wrong."
      },
      {
        "date": "2023-05-03T03:52:00.000Z",
        "voteCount": 2,
        "content": "C. A cross join is not implemented by the DataFrame.join()operation \u2013 the DataFrame.crossJoin()operation should be used instead."
      },
      {
        "date": "2023-04-13T13:43:00.000Z",
        "voteCount": 2,
        "content": "cross join doesn't need a key. Answer is C"
      },
      {
        "date": "2023-05-03T03:51:00.000Z",
        "voteCount": 1,
        "content": "No, the issue is not that the key column is missing. In a cross join, there is no key column to join on. The correct answer is C: a cross join is not implemented by the DataFrame.join() operation \u2013 the DataFrame.crossJoin() operation should be used instead."
      },
      {
        "date": "2023-04-09T06:51:00.000Z",
        "voteCount": 4,
        "content": "Key is missing. Answer is D."
      },
      {
        "date": "2023-05-03T03:51:00.000Z",
        "voteCount": 1,
        "content": "No, the issue is not that the key column is missing. In a cross join, there is no key column to join on. The correct answer is C: a cross join is not implemented by the DataFrame.join() operation \u2013 the DataFrame.crossJoin() operation should be used instead."
      },
      {
        "date": "2023-06-06T12:23:00.000Z",
        "voteCount": 2,
        "content": "completely wrong.\njoin(other, on=None, how=None)\nJoins with another DataFrame, using the given join expression.\n[source]\nParameters:\nother \u2013 Right side of the join\non \u2013 a string for the join column name, a list of column names, a join expression (Column), or a list of Columns. If on is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join.\nhow \u2013 str, default inner. Must be one of: inner, cross, outer, full, fullouter, full_outer, left, leftouter, left_outer, right, rightouter, right_outer, semi, leftsemi, left_semi, anti, leftanti and left_anti."
      },
      {
        "date": "2023-06-06T12:25:00.000Z",
        "voteCount": 2,
        "content": "you can specify cross in dataframe.join( how = 'cross')\nthe reason why this code block doesn't work, because the second parameter is on. You need to specify the key column and then use how = 'cross'.\notherwise, the function will regard 'cross' for 'on' instead of 'how'"
      },
      {
        "date": "2023-11-07T05:25:00.000Z",
        "voteCount": 1,
        "content": "ZSun is as always right. 4be8126 - it is not a problem to use gpt, but check its answers. Otherwise do not post it anywhere."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/databricks/view/108382-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a new DataFrame that is the result of a position-wise union between DataFrame storesDF and DataFrame acquiredStoresDF. Identify the error.<br>Code block:<br>storesDF.unionByName(acquiredStoresDF)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no DataFrame.unionByName() operation \u2013 the concat() operation should be used instead with both DataFrames as arguments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are no key columns specified \u2013 similar column names should be the second argument.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe DataFrame.unionByName() operation does not union DataFrames based on column position \u2013 it uses column name instead.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe unionByName() operation is a standalone operation rather than a method of DataFrame \u2013 it should have both DataFrames as arguments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are no column positions specified \u2013 the desired column positions should be the second argument."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-14T09:43:00.000Z",
        "voteCount": 1,
        "content": "C is correct according to documentation:\nhttps://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.unionByName.html\n\n\"The difference between this function and union() is that this function resolves columns by name (not by position)\""
      },
      {
        "date": "2023-11-07T05:39:00.000Z",
        "voteCount": 1,
        "content": "C is correct - https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.unionByName.html"
      },
      {
        "date": "2023-05-03T03:54:00.000Z",
        "voteCount": 3,
        "content": "The error in the code block is:\n\nC. The DataFrame.unionByName() operation does not union DataFrames based on column position \u2013 it uses column name instead.\n\nThe unionByName() operation performs a position-wise union based on column names, not based on column positions. Therefore, the error in the code block is that the intended operation should be union(), which performs a position-wise union regardless of column names.\n\nThe correct code block to perform a position-wise union between DataFrame storesDF and DataFrame acquiredStoresDF would be:\n\nstoresDF.union(acquiredStoresDF)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/databricks/view/108384-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks writes DataFrame storesDF to file path filePath as JSON?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.option(\"json\").path(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.json(filePath)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.path(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write().json(filePath)"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-14T09:51:00.000Z",
        "voteCount": 1,
        "content": "B is correct according documentation:\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.json.html"
      },
      {
        "date": "2023-05-03T03:56:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B.\n\nExplanation:\n\nThe write method is used to write a DataFrame to a file system in various formats.\nThe json method specifies that the output format should be JSON.\nThe filePath argument specifies the location to write the output file.\nOption A is incorrect because option requires a key-value pair (e.g., option(\"key\", \"value\")).\n\nOption C is incorrect because path is not a valid option for write.\n\nOption D is incorrect because write method requires a format argument to specify the output format.\n\nOption E is a valid option, but the parentheses after write are unnecessary."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/databricks/view/125577-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "In what order should the below lines of code be run in order to write DataFrame storesDF to file path filePath as parquet and partition by values in column division?<br>Lines of code:<br>1. .write() \\<br>2. .partitionBy(\"division\") \\<br>3. .parquet(filePath)<br>4. .storesDF \\<br>5. .repartition(\"division\")<br>6. .write \\<br>7. .path(filePath, \"parquet\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t4, 1, 2, 3",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t4, 1, 5, 7",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t4, 6, 2, 3\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t4, 1, 5, 3",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t4, 6, 2, 7"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-14T11:14:00.000Z",
        "voteCount": 1,
        "content": "C is correct:\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html"
      },
      {
        "date": "2023-11-07T06:39:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/databricks/view/108385-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block intended to read a parquet at the file path filePath into a DataFrame. Identify the error.<br>Code block:<br>spark.read.load(filePath, source \u2013 \"parquet\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no source parameter to the load() operation \u2013 the schema parameter should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no load() operation \u2013 it should be parquet() instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe spark.read operation should be followed by parentheses to return a DataFrameReader object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe filePath argument to the load() operation should be quoted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no source parameter to the load() operation \u2013 it can be removed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-14T11:27:00.000Z",
        "voteCount": 1,
        "content": "E is correct. The \"format\" parameter should be used instead of \"source\" (default \"parquet\"):\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.load.html\n\nformat: str, optional\n    optional string for format of the data source. Default to \u2018parquet\u2019."
      },
      {
        "date": "2023-11-07T06:46:00.000Z",
        "voteCount": 1,
        "content": "I would go for E"
      },
      {
        "date": "2023-09-29T23:30:00.000Z",
        "voteCount": 1,
        "content": "spark.read.load(PARQUET_PATH,format='parquet')\n\nLoad is valid, if provided with format."
      },
      {
        "date": "2023-08-15T06:47:00.000Z",
        "voteCount": 2,
        "content": "Intention is to read a parquet at the file path filePath into a DataFrame"
      },
      {
        "date": "2023-07-31T08:50:00.000Z",
        "voteCount": 2,
        "content": "The parameters for load() function are: path, format, schema, **options\nA. Overall it makes sense, but do we really need to use schema?\nB. There is load operation, that's FALSE\nC. read is used without parenthesis, FALSE\nD. It should indeed, but there's no source parameter, FALSE\nE. That's true, but we need to put quotes for the filePath, then it's FALSE\n\nMakes it A, but the question is really strange and not clear."
      },
      {
        "date": "2023-07-31T09:03:00.000Z",
        "voteCount": 2,
        "content": "UPD - parquet already has schema in it, it's not needed, then, I don't know what the answer is then"
      },
      {
        "date": "2023-06-22T13:34:00.000Z",
        "voteCount": 3,
        "content": "Answer should be E. Removing source and default is 'parquet' anyway. However, it is not ideal to use load, rather the respective method.\n\nhttps://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrameReader.load.html?highlight=dataframereader%20load#pyspark.sql.DataFrameReader.load"
      },
      {
        "date": "2023-06-06T11:31:00.000Z",
        "voteCount": 2,
        "content": "1. pyspark.sql.SparkSession.read Returns a DataFrameReader\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.read.html#pyspark.sql.SparkSession.read\n2. we check this DataFrameReader, it contains both \"load\" and \"parquet\" methods.\n2.1. for load, load(path, format, schema)\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.load.html#pyspark.sql.DataFrameReader.load\nTherefore, the answer is A or E.\nTypically parquet contains schema information. \nI do not like this question, because if reading a parquet file, directly use spark.read.parquet()"
      },
      {
        "date": "2023-05-03T04:05:00.000Z",
        "voteCount": 4,
        "content": "The correct code block to read a parquet file would be \n\nspark.read.parquet(filePath)."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/databricks/view/110744-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "In what order should the below lines of code be run in order to read a JSON file at the file path filePath into a DataFrame with the specified schema schema?<br>Lines of code:<br>1. .json(filePath, schema = schema)<br>2. .storesDF<br>3. .spark \\<br>4. .read() \\<br>5. .read \\<br>6. .json(filePath, format = schema)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t3, 5, 6",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t2, 4, 1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t3, 5, 1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t2, 5, 1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t3, 4, 1"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-08T14:24:00.000Z",
        "voteCount": 1,
        "content": "we use the following structure: spark.read.json(filePath, schema=schemaName)"
      },
      {
        "date": "2023-11-14T11:38:00.000Z",
        "voteCount": 2,
        "content": "C is correct:\n\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html\n\njson function does not have a \"format\" parameter."
      },
      {
        "date": "2023-06-07T06:06:00.000Z",
        "voteCount": 3,
        "content": "storesDF = spark.read.json(filePath, schema = schema)\nC"
      },
      {
        "date": "2023-05-31T23:22:00.000Z",
        "voteCount": 1,
        "content": "2. .storesDF: This line is unrelated to reading the JSON file and can be disregarded.\n.read(): This line invokes the DataFrameReader's read() method to create a DataFrameReader object.\n.json(filePath, schema=schema): This line uses the DataFrameReader object to read the JSON file at the specified filePath into a DataFrame with the provided schema."
      },
      {
        "date": "2024-01-02T11:44:00.000Z",
        "voteCount": 1,
        "content": "This is so wrong.. in order to read a table you need to use spark.read.json / parquet / Table."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/databricks/view/123987-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following storage levels should be used to store as much data as possible in memory on two cluster nodes while storing any data that does not fit in memory on disk to be read in when needed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMEMORY_ONLY_2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMEMORY_AND_DISK_SER",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMEMORY_AND_DISK",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMEMORY_AND_DISK_2\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMEMORY_ONLY"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-07T07:02:00.000Z",
        "voteCount": 1,
        "content": "Correct is D"
      },
      {
        "date": "2023-10-18T13:41:00.000Z",
        "voteCount": 1,
        "content": "B\n\nhttps://stackoverflow.com/questions/30520428/what-is-the-difference-between-memory-only-and-memory-and-disk-caching-level-in"
      },
      {
        "date": "2023-11-07T07:02:00.000Z",
        "voteCount": 1,
        "content": "not sure how you came to the conclusion of B after that article, but it states clearly in the question \"on two cluster nodes\" ! So, the answer is D"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/databricks/view/120817-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following Spark properties is used to configure the maximum size of an automatically broadcasted DataFrame when performing a join?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.broadcastTimeout",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.autoBroadcastJoinThreshold\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.shuffle.partitions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.inMemoryColumnarStorage.batchSize",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.skewedJoin.enabled"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-15T18:43:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. spark.sql.autoBroadcastJoinThreshold. This property in Apache Spark is used to configure the maximum size (in bytes) of a table that will be broadcast to all worker nodes when performing a join. If the size of the table is below this threshold, it will be broadcasted, which can significantly speed up join operations."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/databricks/view/113000-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following Spark properties is used to configure whether skewed partitions are automatically detected and subdivided into smaller partitions when joining two DataFrames together?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.skewedJoin.enabled\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.coalescePartitions.enable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.skewHints.enabled",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.shuffle.partitions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.shuffle.skewHints.enabled"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-22T13:43:00.000Z",
        "voteCount": 5,
        "content": "Answer should be A, but the config is skewJoin not skewedJoin"
      },
      {
        "date": "2024-01-24T10:07:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-09-13T16:32:00.000Z",
        "voteCount": 2,
        "content": "A\n\nThe Spark property used to configure whether skewed partitions are automatically detected and subdivided into smaller partitions when joining two DataFrames together is `spark.sql.adaptive.skewJoin.enabled`. This feature dynamically handles skew in sort-merge join by splitting (and replicating if needed) skewed tasks into roughly evenly sized tasks. It takes effect when both `spark.sql.adaptive.enabled` and `spark.sql.adaptive.skewJoin.enabled` configurations are enabled."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/databricks/view/133404-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following statements about the Spark DataFrame is true?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark DataFrames are mutable unless they've been collected to the driver.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Spark DataFrame is rarely used aside from the import and export of data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark DataFrames cannot be distributed into partitions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Spark DataFrame is a tabular data structure that is the most common Structured API in Spark.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Spark DataFrame is exactly the same as a data frame in Python or R."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-08T14:33:00.000Z",
        "voteCount": 1,
        "content": "D seems correct here"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/databricks/view/119255-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations can be used to return a new DataFrame from DataFrame storesDF without columns that are specified by name?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.select()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.drop()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.subset()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.dropColumn()"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-07T19:11:00.000Z",
        "voteCount": 1,
        "content": "Its C not B"
      },
      {
        "date": "2023-11-07T07:38:00.000Z",
        "voteCount": 1,
        "content": "you should be careful; with the question. The specified columns should not be in the new DF \ntherefore it is drop , you specify in drop what you want to remove and they won't be in the new DF"
      },
      {
        "date": "2023-08-28T10:43:00.000Z",
        "voteCount": 1,
        "content": "in select you can use * instead of the column names"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/databricks/view/130400-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 OR the value in column customerSatisfaction is greater than or equal to 30?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(\"sqft\") &lt;= 25000 and col(\"customerSatisfaction\") &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(\"sqft\") &lt;= 25000 | col(\"customerSatisfaction\") &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(sqft) &lt;= 25000 or col(customerSatisfaction) &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(sqft &lt;= 25000 | customerSatisfaction &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(\"sqft\") &lt;= 25000 or col(\"customerSatisfaction\") &gt;= 30)"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-05T02:52:00.000Z",
        "voteCount": 7,
        "content": "I dont think even B is correct the conditions should be inside parenthesis as well"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/databricks/view/137956-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame from DataFrame storesDF where column storeId is of the type string. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>storesDF.__1__(\"storeId\", __2__(\"storeId\").__3__(__4__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. col<br>3. cast<br>4. StringType()<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. cast<br>3. col<br>4. StringType()<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. newColumn<br>2. col<br>3. cast<br>4. StringType()<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. cast<br>3. col<br>4. StringType<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. col<br>3. cast<br>4. StringType"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-05T04:00:00.000Z",
        "voteCount": 1,
        "content": "should be A"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/databricks/view/137957-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame from DataFrame storesDF where column modality is the constant string \"PHYSICAL\"? Assume DataFrame storesDF is the only defined language variable.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"modality\", lit(PHYSICAL))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"modality\", col(\"PHYSICAL\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"modality\", lit(\"PHYSICAL\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"modality\", StringType(\"PHYSICAL\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"modality\", \"PHYSICAL\")"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-05T04:00:00.000Z",
        "voteCount": 1,
        "content": "should be C"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/databricks/view/116795-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a new DataFrame where column managerName from DataFrame storesDF is split at the space character into column managerFirstName and column managerLastName. Identify the error.<br><br>A sample of DataFrame storesDF is displayed below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image8.png\"><br><br>Code block:<br><br>storesDF.withColumn(\"managerFirstName\", col(\"managerName\").split(\" \").getItem(0))<br>.withColumn(\"managerLastName\", col(\"managerName\").split(\" \").getItem(1))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe index values of 0 and 1 are not correct \u2013 they should be 1 and 2, respectively.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe index values of 0 and 1 should be provided as second arguments to the split() operation rather than indexing the result.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe split() operation comes from the imported functions object. It accepts a string column name and split character as arguments. It is not a method of a Column object.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe split() operation comes from the imported functions object. It accepts a Column object and split character as arguments. It is not a method of a Column object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe withColumn operation cannot be called twice in a row."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-10T10:49:00.000Z",
        "voteCount": 1,
        "content": "D. The split() operation comes from the imported functions object. It accepts a Column object and split character as arguments. It is not a method of a Column object."
      },
      {
        "date": "2024-02-26T03:50:00.000Z",
        "voteCount": 1,
        "content": "Answer C\npyspark.sql.functions provides a function split() to split DataFrame string Column into multiple columns.\nhttps://sparkbyexamples.com/pyspark/pyspark-split-dataframe-column-into-multiple-columns/"
      },
      {
        "date": "2023-11-07T09:02:00.000Z",
        "voteCount": 2,
        "content": "I think it is  C \ndata = [\n    (\"John Smith\",),  \n    (\"Jane Doe\",),\n    (\"Mike Johnson\",)\n]\n\ndf = spark.createDataFrame(data, [\"managerName\"])\n\ndf.show()\n\ndf = df.withColumn(\"managerFirstName\", split(col(\"managerName\"), \" \").getItem(0)) \\\n    .withColumn(\"managerLastName\", split(col(\"managerName\"), \" \").getItem(1))\n\ndf.show()"
      },
      {
        "date": "2024-07-08T05:47:00.000Z",
        "voteCount": 1,
        "content": "in your example, your are using split( col(\"managerName\"), ... ) and not split(\"managerName\", ...) &lt;- means that answer is D"
      },
      {
        "date": "2023-07-30T11:35:00.000Z",
        "voteCount": 1,
        "content": "Can be C as an answer too."
      },
      {
        "date": "2023-07-31T13:53:00.000Z",
        "voteCount": 4,
        "content": "But you have to pass a column as an object, not a string. you have to use col() expression. So D is the right one."
      },
      {
        "date": "2024-05-05T18:41:00.000Z",
        "voteCount": 1,
        "content": "Yes, I agree with you, D is correct we have to pass a column as an object"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/databricks/view/116934-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame where single quotes in column storeSlogan have been replaced with double quotes. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>A sample of DataFrame storesDF is below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image9.png\"><br><br>Code block:<br><br>storesDF.__1__(__2__, __3__(__4__, __5__, __6__))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"storeSlogan\"<br>3. regexp_extract<br>4. col(\"storeSlogan\")<br>5. \"\\\"\"<br>6. \"'\"<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. newColumn<br>2. storeSlogan<br>3. regexp_extract<br>4. col(storeSlogan)<br>5. \"\\\"\"<br>6. \"'\"<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"storeSlogan\"<br>3. regexp_replace<br>4. col(\"storeSlogan\")<br>5. \"\\\"\"<br>6. \"'\"<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"storeSlogan\"<br>3. regexp_replace<br>4. col(\"storeSlogan\")<br>5. \"'\"<br>6. \"\\\"\"<br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"storeSlogan\"<br>3. regexp_extract<br>4. col(\"storeSlogan\")<br>5. \"'\"<br>6. \"\\\"\""
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T06:16:00.000Z",
        "voteCount": 1,
        "content": "To me it would like this:\n\nstoresDF.withColumn(\"storeSlogan\", regexp_replace(col(\"storeSlogan\"), \" ' \", \"\\\"\"))"
      },
      {
        "date": "2023-07-31T13:58:00.000Z",
        "voteCount": 2,
        "content": "D is correct, we need a replacement function with first argument \"'\""
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/databricks/view/116805-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame where column division from DataFrame storesDF has been replaced and renamed to column state and column managerName from DataFrame storesDF has been replaced and renamed to column managerFullName?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumnRenamed(\"division\", \"state\")<br>.withColumnRenamed(\"managerName\", \"managerFullName\")<br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"state\", \"division\")<br>.withColumn(\"managerFullName\", \"managerName\")<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"state\", col(\"division\"))<br>.withColumn(\"managerFullName\", col(\"managerName\"))<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumnRenamed(Seq(\"division\", \"state\"), Seq(\"managerName\", \"managerFullName\"))<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumnRenamed(\"state\", \"division\")<br>.withColumnRenamed(\"managerFullName\", \"managerName\")"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T06:20:00.000Z",
        "voteCount": 1,
        "content": "A would be the right one here."
      },
      {
        "date": "2023-07-30T12:01:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2023-11-07T09:09:00.000Z",
        "voteCount": 1,
        "content": "you are wrong! The answer is A"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/databricks/view/133328-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame where column sqft from DataFrame storesDF has had its missing values replaced with the value 30,000?<br><br>A sample of DataFrame storesDF is below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image10.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.na.fill(30000, Seq(\"sqft\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.nafill(30000, col(\"sqft\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.na.fill(30000, col(\"sqft\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.fillna(30000, col(\"sqft\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.na.fill(30000, \"sqft\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-08T13:14:00.000Z",
        "voteCount": 2,
        "content": "E is answer"
      },
      {
        "date": "2024-06-04T11:39:00.000Z",
        "voteCount": 2,
        "content": "E is answer. It's tested.\nA)AttributeError: module 'pyspark.sql.functions' has no attribute 'Seq'\nB)AttributeError: 'DataFrame' object has no attribute 'nafill'\nC)PySparkTypeError: [NOT_LIST_OR_TUPLE] Argument `subset` should be a list or tuple, got Column.\nD)PySparkTypeError: [NOT_LIST_OR_TUPLE] Argument `subset` should be a list or tuple, got Column."
      },
      {
        "date": "2024-03-21T02:15:00.000Z",
        "voteCount": 1,
        "content": "Check fill function at scala API docs\nhttps://spark.apache.org/docs/3.0.0/api/scala/org/apache/spark/sql/DataFrameNaFunctions.html#fill(value:Long,cols:Array%5BString%5D):org.apache.spark.sql.DataFrame"
      },
      {
        "date": "2024-02-09T06:32:00.000Z",
        "voteCount": 1,
        "content": "To me C is likely correct, because we need to use col()\n\nC. storesDF.na.fill(30000, col(\"sqft\"))"
      },
      {
        "date": "2024-02-07T18:59:00.000Z",
        "voteCount": 2,
        "content": "typo error I THINK  1st arg is col name right ?"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/databricks/view/120030-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations can be used to return a DataFrame with no duplicate rows? Please select the most complete answer.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.distinct()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.dropDuplicates() and DataFrame.distinct()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.dropDuplicates()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.drop_duplicates()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.dropDuplicates(), DataFrame.distinct() and DataFrame.drop_duplicates()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-26T04:05:00.000Z",
        "voteCount": 1,
        "content": "Answer E\ndrop_duplicates() is an alias for dropDuplicates() it also work in pyspark"
      },
      {
        "date": "2024-02-09T06:35:00.000Z",
        "voteCount": 1,
        "content": "it asks \"most complete\" one, so E would be correct as all these three options would work in pyspark"
      },
      {
        "date": "2023-09-05T20:38:00.000Z",
        "voteCount": 1,
        "content": "B\nThe most complete answer is B. DataFrame.dropDuplicates() and DataFrame.distinct(). Both DataFrame.distinct() and DataFrame.dropDuplicates() methods in PySpark can be used to return a new DataFrame with duplicate rows removed. The DataFrame.drop_duplicates() method is used in pandas, not in PySpark."
      },
      {
        "date": "2023-10-19T11:13:00.000Z",
        "voteCount": 1,
        "content": "It should be E, drop_duplicates() works in pyspark too."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/databricks/view/137737-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "QUESTION NO: 75 -<br><br>Which of the following code blocks returns a DataFrame where column divisionDistinct is the approximate number of distinct values in column division from DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"divisionDistinct\", approx_count_distinct(col(\"division\")))<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.agg(col(\"division\").approx_count_distinct(\"divisionDistinct\"))<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.agg(approx_count_distinct(col(\"division\")).alias(\"divisionDistinct\"))<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"divisionDistinct\", col(\"division\").approx_count_distinct())<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.agg(col(\"division\").approx_count_distinct().alias(\"divisionDistinct\"))"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-02T05:05:00.000Z",
        "voteCount": 1,
        "content": "I think it's C\nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.approx_count_distinct.html"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/databricks/view/116935-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame with the mean of column sqft from DataFrame storesDF in column sqftMean. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>storesDF.__1__(__2__(__3__).alias(\"sqftMean\"))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. agg<br>2. mean<br>3. col(\"sqft\")<br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. mean<br>3. col(\"sqft\")<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. agg<br>2. average<br>3. col(\"sqft\")<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. mean<br>2. col<br>3. \"sqft\"<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. agg<br>2. mean<br>3. \"sqft\""
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-31T14:20:00.000Z",
        "voteCount": 1,
        "content": "A is the right one"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/databricks/view/133450-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns the number of rows in DataFrame storesDF for each unique value in column division?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy(\"division\").agg(count())",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.agg(groupBy(\"division\").count())",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupby.count(\"division\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy().count(\"division\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy(\"division\").count()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T06:46:00.000Z",
        "voteCount": 2,
        "content": "E is the right answer here"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/databricks/view/116808-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame sorted alphabetically based on column division?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sort(\"division\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.orderBy(desc(\"division\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.orderBy(col(\"division\").desc())",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.orderBy(\"division\", ascending - true)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sort(desc(\"division\"))"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-11T02:21:00.000Z",
        "voteCount": 2,
        "content": "Option D works as well"
      },
      {
        "date": "2024-08-23T09:19:00.000Z",
        "voteCount": 2,
        "content": "no. Only A. D would work if it was \"ascending = True\". \"ascending = true\" is not a valid statement."
      },
      {
        "date": "2024-06-22T08:34:00.000Z",
        "voteCount": 1,
        "content": "i think so, both works :s"
      },
      {
        "date": "2024-02-09T06:48:00.000Z",
        "voteCount": 1,
        "content": "A is good one here"
      },
      {
        "date": "2023-09-05T23:33:00.000Z",
        "voteCount": 2,
        "content": "A\n\nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.orderBy.html#pyspark.sql.DataFrame.orderBy"
      },
      {
        "date": "2023-08-01T11:42:00.000Z",
        "voteCount": 1,
        "content": "it's A, checked in databricks"
      },
      {
        "date": "2023-07-30T12:20:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-08-01T11:43:00.000Z",
        "voteCount": 3,
        "content": "we need ascending, not descending, mate"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/databricks/view/117001-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a 10 percent sample of rows from DataFrame storesDF with replacement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample(true)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample(true, fraction = 0.1)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample(true, fraction = 0.15)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sampleBy(fraction = 0.1)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample(false, fraction = 0.1)"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-08-01T11:53:00.000Z",
        "voteCount": 1,
        "content": "correct, an easy one"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/databricks/view/113029-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns the first 3 rows of DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.top_n(3)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.n(3)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.take(3)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.head(3)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.collect(3)"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-09T06:56:00.000Z",
        "voteCount": 2,
        "content": "odd question indeed. \nhead() and take() methods perform a similar operation of retrieving rows from the beginning of the DataFrame. However, head() may be slightly slower than take() due to additional checks for boundary conditions. I can't see the difference in terms of retrieving 1st three rows."
      },
      {
        "date": "2023-09-10T11:46:00.000Z",
        "voteCount": 1,
        "content": "weird question, taking into account that head essentially calls take on the DataFrame."
      },
      {
        "date": "2023-08-01T12:24:00.000Z",
        "voteCount": 2,
        "content": "C, D and even E are correct. There's no requirement to return a dataframe in the question"
      },
      {
        "date": "2023-06-22T16:24:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is D. head(n) returns the first n rows and take(n) returns first n rows as a list"
      },
      {
        "date": "2023-07-30T12:27:00.000Z",
        "voteCount": 2,
        "content": "Both head and take gives list as an output , am confused need to do more investigation"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/databricks/view/133451-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks applies the function assessPerformance() to each row of DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.collect.foreach(assessPerformance(row))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.collect().apply(assessPerformance)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.collect.apply(row =&gt; assessPerformance(row))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.collect.map(assessPerformance(row))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.collect.foreach(row =&gt; assessPerformance(row))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T06:58:00.000Z",
        "voteCount": 2,
        "content": "Short explanation below:\n\n   - collect() retrieves all the rows of the DataFrame and returns them as an array.\n   - foreach() applies the specified function to each element of the array.\nSo, in this case, foreach(row =&gt; assessPerformance(row)) applies the function assessPerformance() to each row of the DataFrame storesDF."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/databricks/view/117004-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to print the schema of DataFrame storesDF. Identify the error.<br><br>Code block:<br><br>storesDF.printSchema.getAs[String]",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no printSchema member of DataFrame \u2013 the getSchema() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no printSchema member of DataFrame \u2013 the schema() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe entire line needs to be a string \u2013 it should be wrapped by str().",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe printSchema member of DataFrame is an operation prints the DataFrame \u2013 there is no need to call getAs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no printSchema member of DataFrame \u2013 schema and the print() function should be used instead."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T07:00:00.000Z",
        "voteCount": 1,
        "content": "storesDF.printSchema() would be enough in this case"
      },
      {
        "date": "2023-08-01T12:34:00.000Z",
        "voteCount": 1,
        "content": "question is formulated poorly, but most possibly it's D"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/databricks/view/137959-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks creates and registers a SQL UDF named \"ASSESS_PERFORMANCE\" using the Scala function assessPerformance() and applies it to column customerSatisfaction in table stores?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)<br>spark.sql(\"SELECT customerSatisfaction, ASSESS_PERFORMANCE(customerSatisfaction) AS result FROM stores\")<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)<br>spark.sql(\"SELECT customerSatisfaction, assessPerformance(customerSatisfaction) AS result FROM stores\")<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)<br>storesDF.withColumn(\"result\", assessPerformance(col(\"customerSatisfaction\")))<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)<br>storesDF.withColumn(\"result\", ASSESS_PERFORMANCE(col(\"customerSatisfaction\")))"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-25T00:33:00.000Z",
        "voteCount": 1,
        "content": "column customerSatisfaction haven't quoted in A\n so Ithink that the good answer is E"
      },
      {
        "date": "2024-06-22T08:36:00.000Z",
        "voteCount": 1,
        "content": "Option E is incorrect because it attempts to use the SQL UDF name directly in the DataFrame API, which is not supported without using expr or callUDF."
      },
      {
        "date": "2024-04-05T04:11:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/databricks/view/134412-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should use SQL to return a new DataFrame containing column storeId and column managerName from a table created from DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__(\"stores\")<br>__3__.__4__(\"SELECT storeId, managerName FROM stores\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createOrReplaceTempView<br>3. storesDF<br>4. query<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createTable<br>3. storesDF<br>4. sql<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. createOrReplaceTempView<br>3. spark<br>4. query<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createOrReplaceTempView<br>3. storesDF<br>4. sql<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. createOrReplaceTempView<br>3. spark<br>4. sql\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T09:09:00.000Z",
        "voteCount": 1,
        "content": "It's E"
      },
      {
        "date": "2024-06-04T12:14:00.000Z",
        "voteCount": 1,
        "content": "It's E"
      },
      {
        "date": "2024-04-05T04:12:00.000Z",
        "voteCount": 1,
        "content": "It's E"
      },
      {
        "date": "2024-04-02T05:11:00.000Z",
        "voteCount": 3,
        "content": "It's E.\n\ndf = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\ndf.createTempView(\"people\")\ndf2 = spark.sql(\"SELECT * FROM people\")"
      },
      {
        "date": "2024-02-22T19:23:00.000Z",
        "voteCount": 1,
        "content": "spark.createOrReplaceTempView(\"stores\")\nstoresDF.sql(\"SELECT storeId, managerName FROM stores\")"
      },
      {
        "date": "2024-06-04T12:13:00.000Z",
        "voteCount": 1,
        "content": "spark.createOrReplaceTempView(\"stores\") gives an error \nAttributeError: 'SparkSession' object has no attribute 'createOrReplaceTempView'"
      },
      {
        "date": "2024-02-24T10:07:00.000Z",
        "voteCount": 2,
        "content": "no Bro\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.createTempView.html check this E is correct only"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/databricks/view/116814-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block intended to create a single-column DataFrame from Scala List years which is made up of integers. Identify the error.<br><br>Code block:<br><br>spark.createDataset(years)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe years list should be wrapped in another list like List(years) to make clear that it is a column rather than a row.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe data type is not specified \u2013 the second argument to createDataset should be IntegerType.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no operation createDataset \u2013 the createDataFrame operation should be used instead.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe result of the above is a Dataset rather than a DataFrame \u2013 the toDF operation must be called at the end.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe column name must be specified as the second argument to createDataset."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-18T07:02:00.000Z",
        "voteCount": 1,
        "content": "It should be D.\nScala has a createDataset function which returns a dataset - where then toDF has to be called.\nDoc: https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html"
      },
      {
        "date": "2024-07-23T10:35:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D:\nspark.createDataset() creates a Dataset, not a DataFrame.\nTo convert a Dataset to a DataFrame, you use toDF()."
      },
      {
        "date": "2024-06-30T10:48:00.000Z",
        "voteCount": 1,
        "content": "Since this is a scala question, the correct syntax would be :\nspark.createDataset(years).toDF(\"year\")\nbut that isn't one of the options"
      },
      {
        "date": "2024-04-10T23:38:00.000Z",
        "voteCount": 1,
        "content": "Official Databricks tests (where answer is A)\nQuestion 44 Which of the following code blocks creates a single-column DataFrame from Scala Listyears which is made up of integers? A. spark.createDataset(years).toDF B. spark.createDataFrame(years, IntegerType) C. spark.createDataset(years) D. spark.DataFrame(years, IntegerType) E. spark.createDataFrame(years)"
      },
      {
        "date": "2024-04-10T23:38:00.000Z",
        "voteCount": 1,
        "content": "Hence I'll go for D."
      },
      {
        "date": "2024-02-22T19:32:00.000Z",
        "voteCount": 1,
        "content": "C. There is no operation createDataset \u2013 the createDataFrame operation should be used instead.\n\nThe correct method to create a DataFrame in Spark using Scala is createDataFrame, not createDataset. The correct syntax would be:\n\nscala\nCopy code\nval df = spark.createDataFrame(years.map(Tuple1.apply)).toDF(\"columnName\")\nThis assumes that years is a List of integers, and the resulting DataFrame will have a single column named \"columnName\"."
      },
      {
        "date": "2023-07-30T12:33:00.000Z",
        "voteCount": 3,
        "content": "C is the answer"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/databricks/view/133453-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks will always return a new 4-partition DataFrame from the 8-partition DataFrame storesDF without inducing a shuffle?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.repartition(4, \"sqft\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.repartition()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.coalesce(4)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.repartition(4)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.coalesce"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T07:15:00.000Z",
        "voteCount": 1,
        "content": "C is the right one here.\nUnlike repartition(), coalesce() reduces the number of partitions without shuffling the data. By specifying the number of partitions (4), it ensures that the resulting DataFrame has 4 partitions without inducing a shuffle."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/databricks/view/137960-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new 12-partition DataFrame from DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__(__3__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. coalesce<br>3. 4<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. coalesce<br>3. 4, \"storeId\"<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. repartition<br>3. \"storeId\"<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. repartition<br>3. 12<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. repartition<br>3. Nothing"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-05T04:15:00.000Z",
        "voteCount": 1,
        "content": "Should be D"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/databricks/view/118188-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to adjust the number of partitions used in wide transformations like join() to 32. Identify the error.<br><br>Code block:<br><br>spark.conf.set(\"spark.default.parallelism\", \"32\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.default.parallelism is not the right Spark configuration parameter \u2013 spark.sql.shuffle.partitions should be used instead.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no way to adjust the number of partitions used in wide transformations \u2013 it defaults to the number of total CPUs in the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark configuration parameters cannot be set in runtime.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark configuration parameters are not set with spark.conf.set().",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe second argument should not be the string version of \"32\" \u2013 it should be the integer 32."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-03T18:20:00.000Z",
        "voteCount": 1,
        "content": "A ist richtig"
      },
      {
        "date": "2023-11-09T00:56:00.000Z",
        "voteCount": 3,
        "content": "spark.conf.set(\"spark.sql.shuffle.partitions\", \"32\")"
      },
      {
        "date": "2023-08-15T09:37:00.000Z",
        "voteCount": 4,
        "content": "should be spark.sql.shuffle.partitions for joins"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/databricks/view/121318-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block intended to return a DataFrame containing a column dayOfYear, an integer representation of the day of the year from column openDate from DataFrame storesDF. Identify the error.<br><br>Note that column openDate is of type integer and represents a date in the UNIX epoch format \u2013 the number of seconds since midnight on January 1st, 1970.<br><br>A sample of storesDF is displayed below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image11.png\"><br><br>Code block:<br><br>storesDF.withColumn(\"dayOfYear\", dayofyear(col(\"openDate\")))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe dayofyear() operation cannot extract the day of year from a column of type integer \u2013 column openDate must first be converted to type Timestamp.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe dayofyear() operation takes a quoted column name rather than a Column object as its first argument \u2013 the first argument should be \"openDate\".",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe dayofyear() operation cannot extract the day of year from a column of type integer \u2013 column openDate must first be converted to type Date.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe dayofyear() operation is not applicable in a withColumn() call \u2013 the newColumn() operation must be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no dayofyear() operation \u2013 the day of year number must be extracted using substring utilities."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-22T19:46:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-11-09T01:32:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-09-24T08:06:00.000Z",
        "voteCount": 1,
        "content": "But question 50 is the same question and states that the code block with an error is the correct"
      },
      {
        "date": "2023-11-09T01:31:00.000Z",
        "voteCount": 1,
        "content": "the answer there is wrong. I posted the correct one."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/databricks/view/116815-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame that is the result of an inner join between DataFrame storeDF and DataFrame employeesDF on column storeId. Choose the response chat correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>storesDF.__1__(__2__, __3__, __4__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. join<br>2. employeesDF<br>3. \"inner\"<br>4. storesDF.storeId === employeesDF.storeId<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. join<br>2. employeesDF<br>3. \"storeId\"<br>4. \"inner\"<br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. merge<br>2. employeesDF<br>3. \"storeId\"<br>4. \"inner\"<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. join<br>2. employeesDF<br>3. \"inner\"<br>4. \"storeId\"<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. join<br>2. employeesDF<br>3. \"inner\"<br>4. \"storeId\""
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T07:29:00.000Z",
        "voteCount": 2,
        "content": "this is the right one"
      },
      {
        "date": "2023-10-12T00:12:00.000Z",
        "voteCount": 4,
        "content": "correct syatx=&gt; df.join(x, KEY, 'inner'), so option B"
      },
      {
        "date": "2023-09-12T07:03:00.000Z",
        "voteCount": 4,
        "content": "B is correct."
      },
      {
        "date": "2023-08-16T05:50:00.000Z",
        "voteCount": 4,
        "content": "B looks to be right"
      },
      {
        "date": "2023-07-30T12:43:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      },
      {
        "date": "2023-08-01T19:39:00.000Z",
        "voteCount": 2,
        "content": "agreed, mate, it's B, the type is the last argument"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/databricks/view/117034-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame that is the result of an outer join between DataFrame storesDF and DataFrame employeesDF on column storeId. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>storesDF.__1__(__2__, __3__, __4__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. join<br>2. employeesDF<br>3. \"outer\"<br>4. Seq(\"storeId\")<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. merge<br>2. employeesDF<br>3. \"outer\"<br>4. Seq(\"storeId\")<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. join<br>2. employeesDF<br>3. \"outer\"<br>4. storesDF.storeId === employeesDF.storeId<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. merge<br>2. employeesDF<br>3. Seq(\"storeId\")<br>4. \"outer\"<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. join<br>2. employeesDF<br>3. Seq(\"storeId\")<br>4. \"outer\"\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T09:13:00.000Z",
        "voteCount": 1,
        "content": "It's E but th is question is for scala since Seq is not a python data type."
      },
      {
        "date": "2023-08-01T19:41:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/databricks/view/125670-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks fails to return a new DataFrame that is the result of an inner join between DataFrame storesDF and DataFrame employeesDF on column storeId and column employeeId?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.join(employeesDF, Seq(col(\"storeId\"), col(\"employeeId\")))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.join(employeesDF, Seq(\"storeId\", \"employeeId\"))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.join(employeesDF, storesDF(\"storeId\") === employeesDF(\"storeId\") and storesDF(\"employeeId\") === employeesDF(\"employeeId\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.join(employeesDF, Seq(\"storeId\", \"employeeId\"), \"inner\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.alias(\"s\").join(employeesDF.alias(\"e\"), col(\"s.storeId\") === col(\"e.storeId\") and col(\"s.employeeId\") === col(\"e.employeeId\"))"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-29T03:18:00.000Z",
        "voteCount": 2,
        "content": "A, because col keyword is not needed while joining"
      },
      {
        "date": "2024-03-01T21:04:00.000Z",
        "voteCount": 1,
        "content": "Sorry A, no col"
      },
      {
        "date": "2024-03-01T20:45:00.000Z",
        "voteCount": 1,
        "content": "storesDF.join(employeesDF, Seq(\"storeId\", \"employeeId\"))"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/databricks/view/113099-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should efficiently perform a broadcast join of DataFrame storesDF and the much larger DataFrame employeesDF using key column storeId.<br><br>Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.join(__2__(__3__), \"storeId\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. employeesDF<br>2. broadcast<br>3. storesDF<br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. broadcast(employeesDF)<br>2. broadcast<br>3. storesDF<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. broadcast<br>2. employeesDF<br>3. storesDF<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. broadcast<br>3. employeesDF<br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. broadcast(storesDF)<br>2. broadcast<br>3. employeesDF"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-23T07:40:00.000Z",
        "voteCount": 8,
        "content": "Correct answer is A. storesDF is smaller and should be broadcasted."
      },
      {
        "date": "2023-08-01T20:07:00.000Z",
        "voteCount": 1,
        "content": "Agreed!"
      },
      {
        "date": "2024-02-09T07:40:00.000Z",
        "voteCount": 1,
        "content": "I would go with A as storesDF is smaller and right one to broadcast"
      },
      {
        "date": "2023-09-29T01:18:00.000Z",
        "voteCount": 2,
        "content": "\u0410 is the correct answer!"
      },
      {
        "date": "2023-09-11T05:24:00.000Z",
        "voteCount": 1,
        "content": "A\nThe correct answer is:\n\nA. 1. employeesDF\n2. broadcast\n3. storesDF\n\nSo the correct code would be:\n\n```scala\nemployeesDF.join(broadcast(storesDF), \"storeId\")\n```\n\nThis code will perform a broadcast join of the DataFrame `storesDF` (which is smaller) with the much larger DataFrame `employeesDF` using the key column `storeId`. The `broadcast()` function is used to mark a DataFrame to be broadcast when performing a join operation. The smaller DataFrame `storesDF` is broadcasted to all nodes, where it's joined with the larger DataFrame `employeesDF`."
      },
      {
        "date": "2023-08-16T10:50:00.000Z",
        "voteCount": 2,
        "content": "smaller dataset needs to be broadcasted"
      },
      {
        "date": "2023-08-01T20:07:00.000Z",
        "voteCount": 2,
        "content": "the larger dataset has to be the initial and the smaller one should be broadcasted"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/databricks/view/137681-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations performs a cross join on two DataFrames?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.join()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe standalone join() function",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe standalone crossJoin() function",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.crossJoin()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.merge()"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-07T04:30:00.000Z",
        "voteCount": 1,
        "content": "D - https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.DataFrame.crossJoin.html"
      },
      {
        "date": "2024-04-01T02:13:00.000Z",
        "voteCount": 1,
        "content": "D. DataFrame.crossJoin()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/databricks/view/137680-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks writes DataFrame storesDF to file path filePath as CSV?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write().csv(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.csv(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.option(\"csv\").path(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.path(filePath)"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-01T02:12:00.000Z",
        "voteCount": 1,
        "content": "Correct is C. storesDF.write.csv(filePath)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/databricks/view/137682-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks writes DataFrame storesDF to file path filePath as parquet and partitions by values in column division?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.partitionBy(col(\"division\")).path(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.option(\"parquet\").partitionBy(\"division\").path(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.option(\"parquet\").partitionBy(col(\"division\")).path(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.partitionBy(\"division\").parquet(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write().partitionBy(\"division\").parquet(filePath)"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-01T02:17:00.000Z",
        "voteCount": 1,
        "content": "Correct is D. storesDF.write.partitionBy(\"division\").parquet(filePath)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/databricks/view/137683-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Of the following, which is the coarsest level in the Spark execution hierarchy?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlot",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJob",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTask",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutor"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-01T02:19:00.000Z",
        "voteCount": 1,
        "content": "B. Job"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/databricks/view/121873-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following statements about slots is incorrect?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlots are the most granular level of execution in the Spark execution hierarchy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlots are resources for parallelization within an executor.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTasks are assigned to slots for computation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere can be more slots than tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere must be at least as many slots as there are executors."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-02T19:45:00.000Z",
        "voteCount": 2,
        "content": "Should be c , slots are not part of the exec hierarchy , remember Job, Stages , Tasks"
      },
      {
        "date": "2024-03-01T21:12:00.000Z",
        "voteCount": 1,
        "content": "A to me"
      },
      {
        "date": "2024-02-09T07:48:00.000Z",
        "voteCount": 2,
        "content": "A is the right to me"
      },
      {
        "date": "2023-12-21T01:43:00.000Z",
        "voteCount": 2,
        "content": "B - slots indicate threads available to perform parallel work for Spark"
      },
      {
        "date": "2023-11-09T06:44:00.000Z",
        "voteCount": 3,
        "content": "I think it is A, cause Task is the most granular level (but please correct if I am wrong )"
      },
      {
        "date": "2024-01-08T06:08:00.000Z",
        "voteCount": 1,
        "content": "This is correct."
      },
      {
        "date": "2023-09-30T03:48:00.000Z",
        "voteCount": 1,
        "content": "Isnt C the right answer?"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/databricks/view/137961-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new Data Frame from DataFrame storesDF with no duplicate rows?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.removeDuplicates()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.getDistinct()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.duplicates.drop()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.duplicates()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.dropDuplicates()"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-05T04:20:00.000Z",
        "voteCount": 1,
        "content": "Should be E"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/databricks/view/118281-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return the exact number of distinct values in column division in DataFrame storesDF. Identify the error.<br><br>Code block:<br><br>storesDF.agg(approx_count_distinct(col(\u201cdivision\u201d)).alias(\u201cdivisionDistinct\u201d))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe approx_count_distinct() operation needs a second argument to set the rsd parameter to ensure it returns the exact number of distinct values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no alias() operation for the approx_count_distinct() operation's output.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no way to return an exact distinct number in Spark because the data Is distributed across partitions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe approx_count_distinct()operation is not a standalone function - it should be used as a method from a Column object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe approx_count_distinct() operation cannot determine an exact number of distinct values in a column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T07:52:00.000Z",
        "voteCount": 1,
        "content": "storesDF.agg(countDistinct(\"division\").alias(\"divisionDistinct\")) can give an exact distinct value unlike E option"
      },
      {
        "date": "2023-09-14T05:25:00.000Z",
        "voteCount": 1,
        "content": "E\nThe error in the code block is that the approx_count_distinct() operation cannot determine an exact number of distinct values in a column."
      },
      {
        "date": "2023-08-16T11:32:00.000Z",
        "voteCount": 3,
        "content": "can not get exact distinct using apox function"
      },
      {
        "date": "2023-11-09T06:50:00.000Z",
        "voteCount": 1,
        "content": "agree, should be E"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/databricks/view/137684-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns the number of rows in DataFrame storesDF for each distinct combination of values in column division and column storeCategory?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy(Seq(col(\u201cdivision\u201d), col(\u201cstoreCategory\u201d))).count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy(division, storeCategory).count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy(\u201cdivision\u201d, \u201cstoreCategory\u201d).count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy(\u201cdivision\u201d).groupBy(\u201cStoreCategory\u201d).count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy(Seq(\u201cdivision\u201d, \u201cstoreCategory\u201d)).count()"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-01T02:22:00.000Z",
        "voteCount": 1,
        "content": "C. storesDF.groupBy(\u201cdivision\u201d, \u201cstoreCategory\u201d).count()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/databricks/view/137740-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a collection of summary statistics for column sqft in Data Frame storesDF. Identify the error.<br><br>Code block:<br><br>storesDF.describes(col(\u201csgft\u201d))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe column sqft should be subsetted from DataFrame storesDF prior to computing summary statistics on it alone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe describe() operation does not accept a Column object as an argument outside of a sequence \u2014 the sequence Seq(col(\u201csqft\u201d)) should be specified instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe describe()operation doesn\u2019t compute summary statistics for a single column \u2014 the summary() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe describe()operation doesn't compute summary statistics for numeric columns \u2014 the summary() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe describe()operation does not accept a Column object as an argument \u2014 the column name string \u201csqft\u201d should be specified instead.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-04T06:07:00.000Z",
        "voteCount": 1,
        "content": "E is the right choice."
      },
      {
        "date": "2024-04-02T05:22:00.000Z",
        "voteCount": 1,
        "content": "I think it's E. It should be storesDF.describe(\"sqft\")"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/databricks/view/117144-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should extract the integer value for column sqft from the first row of DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__.__3__[Int](__4__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. first()<br>3. getAs()<br>4. \u201csqft\u201d",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. first<br>3. getAs<br>4. sqft",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. first()<br>3. getAs<br>4. col(\u201csqft\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. first<br>3. getAs<br>4. \u201csqft\u201d\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-06T10:04:00.000Z",
        "voteCount": 2,
        "content": "This is part of the official databricks exam samples.\nThis is the correct answer:\n\nD.1. storesDF 2. first 3. getAs 4. \"sqft\""
      },
      {
        "date": "2023-11-09T08:25:00.000Z",
        "voteCount": 1,
        "content": "could anyone show working test case ?"
      },
      {
        "date": "2023-09-06T20:30:00.000Z",
        "voteCount": 2,
        "content": "A\n\nA is correct. D is incorrect.\nOption D is incorrect because it uses the first method without parentheses. The first method is a method of a DataFrame and must be called with parentheses to return the first row of the DataFrame. Here\u2019s the corrected code block:\n```\nstoresDF.first().getAsInt\n```"
      },
      {
        "date": "2024-06-22T08:39:00.000Z",
        "voteCount": 1,
        "content": "on scala works"
      },
      {
        "date": "2024-06-22T08:40:00.000Z",
        "voteCount": 1,
        "content": "the using of first without parentheses"
      },
      {
        "date": "2023-08-02T10:38:00.000Z",
        "voteCount": 2,
        "content": "correct. we can use first and first() interchangeably, getAs works as getAs[int](\"col_name\")"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/databricks/view/117147-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should print the schema of DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. printSchema(\u201call\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. schema",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. getAs[str]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. printSchema(true)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. printSchema"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-04T06:20:00.000Z",
        "voteCount": 1,
        "content": "B &amp; E results an output. But using df.schema gives you the schema in more detail, df.printSchema just gives the columns that are present in the dataframe. df.printSchema() gives the detailed schema."
      },
      {
        "date": "2024-04-04T06:15:00.000Z",
        "voteCount": 1,
        "content": "It is df.printSchema()."
      },
      {
        "date": "2024-04-02T05:47:00.000Z",
        "voteCount": 1,
        "content": "I think it's B."
      },
      {
        "date": "2024-01-16T19:23:00.000Z",
        "voteCount": 1,
        "content": "E is correct. This question is for scala not python"
      },
      {
        "date": "2023-08-02T11:03:00.000Z",
        "voteCount": 1,
        "content": "should be B\nfor printSchema() is should be DataFrame.printSchema() with parenthesis"
      },
      {
        "date": "2023-11-09T08:34:00.000Z",
        "voteCount": 1,
        "content": "interesting case, either the answer E is indeed not complete () is missing  or it is B, though B doesn't print schema but returns schema object"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/databricks/view/116821-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to create and register a SQL UDF named \u201cASSESS_PERFORMANCE\u201d using the Scala function assessPerformance() and apply it to column customerSatisfaction in the table stores. Identify the error.<br><br>Code block:<br><br>spark.udf.register(\u201cASSESS_PERFORMANCE\u201d, assessPerforance)<br>spark.sql(\u201cSELECT customerSatisfaction, assessPerformance(customerSatisfaction) AS result FROM stores\u201d)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customerSatisfaction column cannot be called twice inside the SQL statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegistered UDFs cannot be applied inside of a SQL statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe order of the arguments to spark.udf.register() should be reversed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe wrong SQL function is used to compute column result - it should be ASSESS_PERFORMANCE instead of assessPerformance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no sql() operation - the DataFrame API must be used to apply the UDF assessPerformance()."
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-08T09:20:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2024-04-04T06:23:00.000Z",
        "voteCount": 1,
        "content": "D is the right choice."
      },
      {
        "date": "2023-09-29T01:38:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-09-06T20:39:00.000Z",
        "voteCount": 1,
        "content": "D\nThe error in the code block is D. The wrong SQL function is used to compute column result - it should be ASSESS_PERFORMANCE instead of assessPerformance."
      },
      {
        "date": "2023-08-17T09:38:00.000Z",
        "voteCount": 2,
        "content": "yup D is correct"
      },
      {
        "date": "2023-07-30T13:20:00.000Z",
        "voteCount": 2,
        "content": "D is the answer"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/databricks/view/137746-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to create the Scala UDF assessPerformanceUDF() and apply it to the integer column customers1t1sfaction in Data Frame storesDF. Identify the error.<br><br>Code block:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image12.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe input type of customerSatisfaction is not specified in the udf() operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe return type of assessPerformanceUDF() must be specified.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe withColumn() operation is not appropriate here - UDFs should be applied by iterating over rows instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe assessPerformanceUDF() must first be defined as a Scala function and then converted to a UDF.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUDFs can only be applied via SQL and not through the Data Frame API."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-08-12T10:51:00.000Z",
        "voteCount": 1,
        "content": "these dumps is for python or scala?"
      },
      {
        "date": "2024-06-20T18:55:00.000Z",
        "voteCount": 1,
        "content": "Ans. B\nval assessPerformanceUDF = udf((customerSatisfaction: Int) =&gt; {\n  customerSatisfaction match {\n    case x if x &lt; 20 =&gt; 1\n    case x if x &gt; 80 =&gt; 3\n    case _ =&gt; 2\n  }\n}, IntegerType)\n\n// Apply the UDF to the DataFrame\nval resultDF = storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))\nresultDF.show()"
      },
      {
        "date": "2024-06-22T13:07:00.000Z",
        "voteCount": 2,
        "content": "But you also gave a type for the customerSatisfaction input parameter. I think the answer is A"
      },
      {
        "date": "2024-04-02T05:49:00.000Z",
        "voteCount": 1,
        "content": "I think it's B"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/databricks/view/137748-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should create a single-column DataFrame from Scala list years which is made up of integers. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__(__3__).__4__",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createDataFrame<br>3. years<br>4. IntegerType",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createDataset<br>3. years<br>4. IntegerType",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createDataset<br>3. List(years)<br>4. toDF",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. createDataFrame<br>3. List(years)<br>4. IntegerType"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-02T05:53:00.000Z",
        "voteCount": 2,
        "content": "I think it's C\nC. 1. spark\n2. createDataset\n3. List(years)\n4. toDF\n\nit says \"Scala list\""
      },
      {
        "date": "2024-06-22T13:21:00.000Z",
        "voteCount": 1,
        "content": "I think C is correct, but the result is a DataFrame with a single row whose value is a list of Integers, not a row for each element in the list. None of the other choices makes sense though."
      },
      {
        "date": "2024-04-10T11:27:00.000Z",
        "voteCount": 2,
        "content": "It's part of official databricks questions:\nQuestion 44 Which of the following code blocks creates a single-column DataFrame from Scala Listyears which is made up of integers? A. spark.createDataset(years).toDF B. spark.createDataFrame(years, IntegerType) C. spark.createDataset(years) D. spark.DataFrame(years, IntegerType) E. spark.createDataFrame(years) --&gt; and the answer here is A."
      },
      {
        "date": "2024-06-22T07:10:00.000Z",
        "voteCount": 1,
        "content": "spark.createDataset(years).toDF, to avoid confuse, it's answer C here, and answer A on official databruicks questions. :)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/databricks/view/127342-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should cache DataFrame storesDF only in Spark's memory. Choose the response that correctly fil ls in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__(__3__).count()",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. cache<br>3. StorageLevel.MEMORY_ONLY",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. storageLevel<br>3. cache",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. cache<br>3. Nothing",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. persist<br>3. Nothing",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. persist<br>3. StorageLevel.MEMORY_ONLY"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-11-27T09:58:00.000Z",
        "voteCount": 2,
        "content": "E: df.persist(StorageLevel.MEMORY_ONLY).count()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/databricks/view/137749-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame containing a column month, an integer representation of the day of the year from column openDate from DataFrame storesDF.<br><br>Note that column openDate is of type integer and represents a date in the UNIX epoch format \u2013 the number of seconds since midnight on January 1st, 1970.<br><br>A sample of storesDF is displayed below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image13.png\"><br><br>Code block:<br><br>stored.withColumn(\u201copenTimestamp\u201d, col(\u201copenDate\u201d).cast(__1__))<br>.withColumn(__2__, __3__(__4__))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. \u201cData\u201d<br>2. month<br>3. \u201cmonth\u201d<br>4. \u201copenTimestamp\u201d",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. \u201cTimestamp\u201d<br>2. month<br>3. \u201cmonth\u201d<br>4. col(\u201copenTimestamp\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. \u201cTimestamp\u201d<br>2. month<br>3. getMonth<br>4. col(\u201copenTimestamp\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. \u201cTimestamp\u201d<br>2. \u201cmonth\u201d<br>3. month<br>4. col(\u201copenTimestamp\u201d)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-04T06:39:00.000Z",
        "voteCount": 2,
        "content": "D is the right choice"
      },
      {
        "date": "2024-04-02T05:56:00.000Z",
        "voteCount": 2,
        "content": "It s D"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/databricks/view/117151-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block intended to return a new DataFrame that is the result of an inner join between DataFrame storesDF and DataFrame employeesDF on column storeId. Identify the error.<br><br>Code block:<br><br>StoresDF.join(employeesDF, Seq(\"storeId\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe key column storeId needs to be a string like \u201cstoreId\u201d.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe key column storeId needs to be specified in an expression of both Data Frame columns like storesDF.storeId ===employeesDF.storeId.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe default argument to the joinType parameter is \u201cinner\u201d - an additional argument of \u201cleft\u201d must be specified.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no DataFrame.join() operation - DataFrame.merge() should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe key column storeId needs to be wrapped in the col() operation."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-22T13:31:00.000Z",
        "voteCount": 1,
        "content": "I don't see any error in the code other than a typo"
      },
      {
        "date": "2024-04-04T06:41:00.000Z",
        "voteCount": 2,
        "content": "since the default join is inner so key column should be \"StoreId\""
      },
      {
        "date": "2024-04-02T05:57:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-02-02T04:03:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2023-11-09T11:06:00.000Z",
        "voteCount": 2,
        "content": "The answer (C) is just the most wrong that could have been here"
      },
      {
        "date": "2023-08-02T11:51:00.000Z",
        "voteCount": 1,
        "content": "I think the question is corrupt. The most plausible answer is A, even though the column name is already presented as a string."
      },
      {
        "date": "2023-11-09T11:06:00.000Z",
        "voteCount": 2,
        "content": "yup, 100% is smth wrong, cause storesDF.join(employeesDF, Seq(\"storeId\")) will work"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/databricks/view/137751-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following pairs of arguments cannot be used in DataFrame.join() to perform an inner join on two DataFrames, named and aliased with \"a\" and \"b\" respectively, to specify two key columns column1 and column2?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tjoinExprs = col(\u201ca.column1\u201d) === col(\u201cb.column1\u201d) and col(\u201ca.column2\u201d) === col(\u201cb.column2\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tusingColumns = Seq(col(\u201ccolumn1\u201d), col(\u201ccolumn2\u201d))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll of these options can be used to perform an inner join with two key columns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tjoinExprs = storesDF(\u201ccolumn1\u201d) === employeesDF(\u201ccolumn1\u201d) and storesDF(\u201ccolumn2\u201d) === employeesDF (\u201ccolumn2\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tusingColumns = Seq(\u201ccolumn1\u201d, \u201ccolumn2\u201d)"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-30T20:46:00.000Z",
        "voteCount": 2,
        "content": "Will option D work? I have never seen this way of accessing columns in a dataframe : \ndf(\"column\")  , it should be either df.column or col(\"a.column\") [ considering a as an alias of df]"
      },
      {
        "date": "2024-04-02T05:58:00.000Z",
        "voteCount": 1,
        "content": "I think it s B"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/databricks/view/125679-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a new DataFrame that is the result of a position-wise union between DataFrame storesDF and DataFrame acquiredStoresDF.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tconcat(storesDF, acquiredStoresDF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.unionByName(acquiredStoresDF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tunion(storesDF, acquiredStoresDF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tunionAll(storesDF, acquiredStoresDF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.union(acquiredStoresDF)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-08T03:57:00.000Z",
        "voteCount": 1,
        "content": "The correct method to perform a position-wise union (i.e., a union by name) between two DataFrames in PySpark is `unionByName`. The correct answer is the one that does not perform a position-wise union. \n\nThe code block that contains an error is:\n\n**A. concat(storesDF, acquiredStoresDF)**\n\nThe `concat` function does not exist in PySpark for DataFrame operations. Instead, you should use `union` or `unionByName`."
      },
      {
        "date": "2024-04-04T06:47:00.000Z",
        "voteCount": 1,
        "content": "E is the right choice."
      },
      {
        "date": "2023-11-09T11:12:00.000Z",
        "voteCount": 3,
        "content": "could you please fix the question? It is missing part of it. Though the answer is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/databricks/view/137753-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks writes DataFrame storesDF to file path filePath as parquet overwriting any existing files in that location?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write(filePath, mode = \u201coverwrite\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write().mode(\u201coverwrite\u201d).parquet(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.mode(\u201coverwrite\u201d).parquet(filePath)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.option(\u201cparquet\u201d, \u201coverwrite\u201d).path(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.mode(\u201coverwrite\u201d).path(filePath)"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-04T06:47:00.000Z",
        "voteCount": 1,
        "content": "it is C"
      },
      {
        "date": "2024-04-02T06:00:00.000Z",
        "voteCount": 1,
        "content": "It's c"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/databricks/view/137754-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks reads a CSV at the file path filePath into a Data Frame with the specified schema schema?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.read().csv(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.read().schema(\u201cschema\u201d).csv(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.read.schema(schema).csv(filePath)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.read.schema(\u201cschema\u201d).csv(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.read().schema(schema).csv(filePath)"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-04T06:48:00.000Z",
        "voteCount": 1,
        "content": "C is the right choice"
      },
      {
        "date": "2024-04-02T06:01:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/databricks/view/117553-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 AND the value in column customerSatisfaction is greater than or equal to 30?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(\"sqft\") &lt;= 25000 and col(\"customerSatisfaction\") &gt;= 30)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(\"sqft\") &lt;= 25000 or col(\"customerSatisfaction\") &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(sqft) &lt;= 25000 and customerSatisfaction &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(\"sqft\") &lt;= 25000 &amp; col(\"customerSatisfaction\") &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(sqft &lt;= 25000) &amp; customerSatisfaction &gt;= 30)"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-03T05:52:00.000Z",
        "voteCount": 1,
        "content": "The most similar is D, And and OR are not valid statements for filtering in pySpark"
      },
      {
        "date": "2024-08-23T10:09:00.000Z",
        "voteCount": 2,
        "content": "in pyspark, all wrong as the conditions inside the filter should be wrapped inside parentesis. should be: D. storesDF.filter((col(\"sqft\") &lt;= 25000) &amp; (col(\"customerSatisfaction\") &gt;= 30))"
      },
      {
        "date": "2024-08-15T17:10:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2024-06-23T08:14:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2024-09-03T05:53:00.000Z",
        "voteCount": 1,
        "content": "no its not, ..."
      },
      {
        "date": "2024-04-02T06:05:00.000Z",
        "voteCount": 2,
        "content": "It's D:\nhttps://sparkbyexamples.com/spark/spark-and-or-not-operators/\n\nPySpark Logical operations use the bitwise operators:\n\n&amp; for and\n| for or\n~ for not"
      },
      {
        "date": "2023-11-24T07:31:00.000Z",
        "voteCount": 1,
        "content": "The answer should be E. In case of multiple conditions spark requires () such as:\ndf.filter( (cond1) &amp; (cond2) )"
      },
      {
        "date": "2023-08-07T13:45:00.000Z",
        "voteCount": 1,
        "content": "A is the right answer."
      },
      {
        "date": "2023-11-09T11:24:00.000Z",
        "voteCount": 2,
        "content": "No, you do not use and but &amp; in Pyspark \nD is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/databricks/view/137757-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following sets of DataFrame methods will both return a new DataFrame only containing rows that meet a specified logical condition?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdrop(), where()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfilter(), select()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfilter(), where()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tselect(), where()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfilter(), drop()"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-04T06:51:00.000Z",
        "voteCount": 1,
        "content": "C is the right choice"
      },
      {
        "date": "2024-04-02T06:08:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/databricks/view/137759-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a DataFrame containing all columns from DataFrame storesDF except for column sqft and column customerSatisfaction. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__(__3__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. drop<br>2. storesDF<br>3. col(\u201csqft\u201d), col(\u201ccustomerSatisfaction\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. drop<br>3. sqft, customerSatisfaction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. drop<br>3. \u201csqft\u201d, \u201ccustomerSatisfaction\u201d\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. drop<br>3. col(sqft), col(customerSatisfaction)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. drop<br>2. storesDF<br>3. col(sqft), col(customerSatisfaction)"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-04T06:52:00.000Z",
        "voteCount": 1,
        "content": "C is the right choice"
      },
      {
        "date": "2024-04-02T06:09:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/databricks/view/117155-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following describes the difference between DataFrame.repartition(n) and DataFrame.coalesce(n)?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.repartition(n) will split a DataFrame into n number of new partitions with data distributed evenly.<br>DataFrame.coalesce(n) will more quickly combine the existing partitions of a DataFrame but might result in an uneven distribution of data across the new partitions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhile the results are similar, DataFrame.repartition(n) will be more efficient than DataFrame.coalesce(n) because it can partition a Data Frame by the column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.repartition(n) will split a Data Frame into any number of new partitions while minimizing shuffling.<br>DataFrame.coalesce(n) will split a DataFrame onto any number of new partitions utilizing a full shuffle.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhile the results are similar, DataFrame.repartition(n) will be less efficient than DataFrame.coalesce(n) because it can partition a Data Frame by the column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.repartition(n) will combine the existing partitions of a DataFrame but may result in an uneven distribution of data across the new partitions.<br>DataFrame.coalesce(n) will more slowly split a Data Frame into n number of new partitions with data distributed evenly."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-04T06:53:00.000Z",
        "voteCount": 2,
        "content": "A is the right choice"
      },
      {
        "date": "2024-03-30T17:32:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-02-02T04:09:00.000Z",
        "voteCount": 1,
        "content": "It's A"
      },
      {
        "date": "2023-09-09T06:10:00.000Z",
        "voteCount": 4,
        "content": "A\nThe correct answer is A. DataFrame.repartition(n) will split a DataFrame into n number of new partitions with data distributed evenly. DataFrame.coalesce(n) will more quickly combine the existing partitions of a DataFrame but might result in an uneven distribution of data across the new partitions. The `repartition()` method can be used to increase or decrease the number of partitions in a DataFrame, while the `coalesce()` method is used to only decrease the number of partitions in an efficient way\u00b2. The `repartition()` method does a full shuffle and creates new partitions with data that's distributed evenly. On the other hand, `coalesce()` avoids a full shuffle by allowing only the reduction of partitions."
      },
      {
        "date": "2023-08-02T12:57:00.000Z",
        "voteCount": 2,
        "content": "IMO it's A:\nB - repartition is less efficient because it involves shuffling - -&gt;false\nC - same for the B reason --&gt; false\nD - it's because of shuffling, not because of some column --&gt; false\nE - coalesce if more fast --&gt; false\nE -"
      },
      {
        "date": "2023-08-06T04:44:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/databricks/view/125680-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following cluster configurations is most likely to experience delays due to garbage collection of a large Dataframe?<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image14.png\"><br><br>Note: each configuration has roughly the same compute power using 100GB of RAM and 200 cores.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMore information is needed to determine an answer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #5",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #4",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #2"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-23T11:34:00.000Z",
        "voteCount": 1,
        "content": "I think it is D- scenario 1 because the other scenarios can take advantage of parallelism."
      },
      {
        "date": "2024-04-09T06:06:00.000Z",
        "voteCount": 1,
        "content": "I think it's D - Scenario 1\n\nScenario #1 would most likely experience delays due to garbage collection because it has the largest heap space per executor, leading to longer garbage collection times when managing large DataFrames."
      },
      {
        "date": "2024-02-29T04:57:00.000Z",
        "voteCount": 1,
        "content": "The answer is Scen 6 and than answer doesn\u00b4t appear, please align the answers"
      },
      {
        "date": "2023-11-09T11:32:00.000Z",
        "voteCount": 2,
        "content": "Please correct the question - answers alighment \nThe scenarious do not match \nthough I would say Scen 6 is the answer"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/databricks/view/137563-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following DataFrame operations is classified as a transformation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.select()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.show()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.first()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.collect()"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-04T06:57:00.000Z",
        "voteCount": 1,
        "content": "A is the right choice"
      },
      {
        "date": "2024-03-30T17:37:00.000Z",
        "voteCount": 1,
        "content": "It's A"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/databricks/view/132760-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations will fail to trigger evaluation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.collect()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.first()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.join()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.take()"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-03T19:57:00.000Z",
        "voteCount": 1,
        "content": "Join is a tranformation so it will not throw results, while others are actions, so Join() is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/databricks/view/137900-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should read a JSON at the file path filePath into a DataFrame with the specified schema schema. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__.__3__(__4__).format(\"csv\").__5__(__6__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read()<br>3. schema<br>4. schema<br>5. json<br>6. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read()<br>3. json<br>4. filePath<br>5. format<br>6. schema",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read()<br>3. schema<br>4. schema<br>5. load<br>6. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read<br>3. schema<br>4. schema<br>5. load<br>6. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read<br>3. format<br>4. \"json\"<br>5. load<br>6. filePath"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-26T03:33:00.000Z",
        "voteCount": 3,
        "content": "I think that we have an error in the question, the format is CSV so the good answer is D"
      },
      {
        "date": "2024-04-28T17:50:00.000Z",
        "voteCount": 2,
        "content": "I agree with you correct is D"
      },
      {
        "date": "2024-04-04T07:07:00.000Z",
        "voteCount": 2,
        "content": "The question itself is bogus."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/databricks/view/117165-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame with a new column customerSatisfactionAbs that is the absolute value of column customerSatisfaction in DataFrame storesDF? Note that column customerSatisfactionAbs is not in the original DataFrame storesDF.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\u201ccustomerSatisfactionAbs\u201d, abs(col(\u201ccustomerSatisfaction\u201d)))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumnRenamed(\u201ccustomerSatisfactionAbs\u201d, abs(col(\u201ccustomerSatisfaction\u201d)))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(col(\u201ccustomerSatisfactionAbs\u201d, abs(col(\u201ccustomerSatisfaction\u201d)))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\u201ccustomerSatisfactionAbs\u201d, abs(col(customerSatisfaction)))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\u201ccustomerSatisfactionAbs\u201d, abs(\u201ccustomerSatisfaction\u201d))"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-09T06:11:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-08-02T14:22:00.000Z",
        "voteCount": 3,
        "content": "it works for A and E:\npyspark.sql.functions.abs(col: ColumnOrName) \u2192 pyspark.sql.column.Column[source]\nComputes the absolute value.\n\nNew in version 1.3.0.\n\nChanged in version 3.4.0: Supports Spark Connect.\n\nParameters\ncolColumn or str\ntarget column to compute on."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/databricks/view/117166-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following statements about the Spark driver is true?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark driver is horizontally scaled to increase overall processing throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark driver is the most coarse level of the Spark execution hierarchy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark driver is fault tolerant \u2014 if it fails, it will recover the entire Spark application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark driver is responsible for scheduling the execution of data by various worker nodes in cluster mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark driver is only compatible with its included cluster manager."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-09T06:14:00.000Z",
        "voteCount": 1,
        "content": "It's D"
      },
      {
        "date": "2023-08-26T10:36:00.000Z",
        "voteCount": 2,
        "content": "I believe D is the correct one according to documentation from Databricks [1]:\n\"The driver process runs your main() function, sits on a node in the cluster, and is responsible for three things: maintaining information about the Spark Application; responding to a user\u2019s program or input; and analyzing, distributing, and scheduling work across the executors (defined momentarily).\"\n\nAddittionaly:\n\"The cluster manager controls physical machines and allocates resources to Spark Applications.\"\n\nBased on the above we could say that cluster manager is charge assign resources (CPU, Memory, etc) to the VMs used. Keep in mind that this is based on the definition from Databricks other definitions may include what was mentioned by cookiemonster42.\n\n[1] https://www.databricks.com/glossary/what-are-spark-applications"
      },
      {
        "date": "2023-08-02T14:25:00.000Z",
        "voteCount": 1,
        "content": "Should be B - \nD - Spark driver is not directly responsible for scheduling the execution of data by various worker nodes in cluster mode. It submits tasks to the cluster manager (e.g., YARN, Mesos, or Kubernetes), and the cluster manager handles the scheduling of tasks on worker nodes."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/databricks/view/137901-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should write DataFrame storesDF to file path filePath as parquet and partition by values in column division. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>storesDF.__1__.__2__(__3__).__4__(__5__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. write<br>2. partitionBy<br>3. \u201cdivision\u201d<br>4. path<br>5. filePath, node = parquet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. write<br>2. partitionBy<br>3. \u201cdivision\u201d<br>4. parquet<br>5. filePath\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. write<br>2. partitionBy<br>3. col(\u201cdivision\u201d)<br>4. parquet<br>5. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. write()<br>2. partitionBy<br>3. col(\u201cdivision\u201d)<br>4. parquet<br>5. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. write<br>2. repartition<br>3. \u201cdivision\u201d<br>4. path<br>5. filePath, mode = \u201cparquet\u201d"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-04T07:19:00.000Z",
        "voteCount": 1,
        "content": "B is the right choice"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/databricks/view/138250-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following types of processes induces a stage boundary?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShuffle",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCaching",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutor failure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJob delegation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApplication failure"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-09T06:16:00.000Z",
        "voteCount": 2,
        "content": "A. Shuffle\n\nA stage boundary in Spark is induced when data needs to be shuffled across the executors. A shuffle occurs when the current operations require data to be redistributed so that new partitions of data are formed based on which downstream operations are computed."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/databricks/view/137902-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following cluster configurations will induce the least network traffic during a shuffle operation?<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image15.png\"><br><br>Note: each configuration has roughly the same compute power using 100GB of RAM and 200 cores.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThis cannot be determined without knowing the number of partitions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario 5",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario 1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario 4",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario 6"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-09T06:18:00.000Z",
        "voteCount": 1,
        "content": "Scenario #1 would likely induce the least network traffic during a shuffle operation because all the data would stay within the single node, avoiding any cross-node traffic. Therefore, the answer is: C. Scenario 1"
      },
      {
        "date": "2024-04-04T07:20:00.000Z",
        "voteCount": 1,
        "content": "C is the right choice"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/databricks/view/137830-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following describes a partition?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA partition is the amount of data that fits in a single executor.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA partition is an automatically-sized segment of data that is used to create efficient logical plans.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA partition is the amount of data that fits on a single worker node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA partition is a portion of a Spark application that is made up of similar jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA partition is a collection of rows of data that fit on a single machine in a cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-09T06:20:00.000Z",
        "voteCount": 1,
        "content": "I think it's E."
      },
      {
        "date": "2024-04-04T07:30:00.000Z",
        "voteCount": 1,
        "content": "E. A partition is a collection of rows of data that fit on a single machine in a cluster.\n\nExplanation:\n\nIn Spark, data is divided into partitions, which are distributed across the nodes in a cluster.\nEach partition holds a subset of the data, and operations in Spark are performed in parallel on these partitions.\nPartitions are the unit of parallelism in Spark, allowing computations to be distributed across multiple executor nodes efficiently."
      },
      {
        "date": "2024-04-03T10:26:00.000Z",
        "voteCount": 2,
        "content": "It is B"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/databricks/view/138252-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following identifies multiple narrow operations that are executed in sequence?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlot",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJob",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTask",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutor"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-09T06:48:00.000Z",
        "voteCount": 1,
        "content": "C. Stage\n\nIn Spark, a stage represents a sequence of narrow transformations that can be executed without shuffling the entire data across partitions. Narrow transformations are those where each partition of the parent RDD contributes to only one partition of the child RDD. Spark groups as many narrow transformations as possible into a single stage, and these operations are pipelined together to optimize performance. A stage is completed before the next stage begins, and usually, a shuffle operation would define the boundary between stages."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/databricks/view/138253-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Spark's execution/deployment mode determines where the driver and executors are physically located when a Spark application is run. Which of the following Spark execution/deployment modes does not exist? If they all exist, please indicate so with Response E.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClient mode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster mode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStandard mode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLocal mode",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll of these execution/deployment modes exist"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-09T06:51:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/databricks/view/138316-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following will cause a Spark job to fail?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNever pulling any amount of data onto the driver node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrying to cache data larger than an executor's memory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData needing to spill from memory to disk.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA failed worker node.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA failed driver node."
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T05:12:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/databricks/view/138317-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following best describes the similarities and differences between the MEMORY_ONLY storage level and the MEMORY_AND_DISK storage level?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe MEMORY_ONLY storage level will store as much data as possible in memory and will store any data that does on fit in memory on disk and read it as it's called.<br><br>The MEMORY_AND_DISK storage level will store as much data as possible in memory and will recompute any data that does not fit in memory as it\u2019s called.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe MEMORY_ONLY storage level will store as much data as possible in memory on two cluster nodes and will recompute any data that does not fit in memory as it\u2019s called.<br><br>The MEMORY_AND_DISK storage level will store as much data as possible in memory on two cluster nodes and will store any data that does on fit in memory on disk and read it as it's called.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe MEMORY_ONLY storage level will store as much data as possible in memory on two cluster nodes and will store any data that does on fit in memory on disk and read it as it's called.<br><br>The MEMORY_AND_DISK storage level will store as much data as possible in memory on two cluster nodes and will recompute any data that does not fit in memory as it's called.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe MEMORY_ONLY storage level will store as much data as possible in memory and will recompute any data that does not fit in memory as it's called.<br><br>The MEMORY_AND_DISK storage level will store as much data as possible in memory and will store any data that does on fit in memory on disk and read it as it's called.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe MEMORY_ONLY storage level will store as much data as possible in memory and will recompute any data that does not fit in memory as it\u2019s called.<br><br>The MEMORY_AND_DISK storage level will store half of the data in memory and store half of the memory on disk. This provides quick preview and better logical plan design."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T05:14:00.000Z",
        "voteCount": 1,
        "content": "D. is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/databricks/view/141598-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following Spark properties is used to configure whether DataFrames found to be below a certain size threshold at runtime will be automatically broadcasted?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.broadcastTimeout",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.autoBroadcastJoinThreshold\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.shuffle.partitions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.inMemoryColumnarStorage.batchSize",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.localShuffleReader.enabled"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-12T01:37:00.000Z",
        "voteCount": 1,
        "content": "This property is used to configure the threshold for automatically broadcasting small tables in join operations. When the size of a DataFrame is below this threshold, it will be broadcasted to all executor nodes for efficient join operations."
      },
      {
        "date": "2024-05-30T06:09:00.000Z",
        "voteCount": 1,
        "content": "Sorry,its B, can't edit previous answer"
      },
      {
        "date": "2024-05-30T06:08:00.000Z",
        "voteCount": 1,
        "content": "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", value)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/databricks/view/130835-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a new DataFrame from DataFrame storesDF where column storeId is of the type string. Identify the error.<br><br>Code block:<br><br>storesDF.withColumn(\u201cstoreId\u201d, cast(col(\u201cstoreId\u201d), StringType()))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCalls to withColumn() cannot create a new column of the same name on which it is operating.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame columns cannot be converted to a new type inside of a call to withColumn().",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe call to StringType should not be followed by parentheses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe column name storeId inside the col() operation should not be quoted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe cast() operation is a method in the Column class rather than a standalone function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-29T04:27:00.000Z",
        "voteCount": 1,
        "content": "https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.Column.cast.html?highlight=cast"
      },
      {
        "date": "2024-02-09T13:17:00.000Z",
        "voteCount": 1,
        "content": "E is the right"
      },
      {
        "date": "2024-02-03T20:29:00.000Z",
        "voteCount": 1,
        "content": "It is E, no doubt about it"
      },
      {
        "date": "2024-01-11T02:35:00.000Z",
        "voteCount": 1,
        "content": "E should be the answer"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/databricks/view/132010-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame where column division is the first two characters of column division in DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\u201cdivision\u201d, substr(col(\u201cdivision\u201d), 0, 2))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\u201cdivision\u201d, susbtr(col(\u201cdivision\u201d), 1, 2))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF,withColumn(\u201cdivision\u201d, col(\u201cdivision\u201d).substr(0, 3))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\u201cdivision\u201d, col(\u201cdivision\u201d).substr(0, 2))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\u201cdivision\u201d, col(\u201cdivision\u201d).substr(l, 2))"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T13:02:00.000Z",
        "voteCount": 1,
        "content": "df = spark.createDataFrame([('abcd',)], ['s',])\ndf.select(substring(df.s, 1, 2).alias('s')).collect()"
      },
      {
        "date": "2024-08-10T21:17:00.000Z",
        "voteCount": 1,
        "content": "The correct code block that returns a new DataFrame where the column division is the first two characters of the division column in DataFrame storesDF is:\n\nB. storesDF.withColumn(\u201cdivision\u201d, substr(col(\u201cdivision\u201d), 1, 2))\n\nExplanation:\n\nsubstr(col(\u201cdivision\u201d), 1, 2) extracts the substring starting from the 1st character (index 1) and takes the next 2 characters.\nSpark's substr function is 1-indexed, meaning it starts counting from 1, not 0."
      },
      {
        "date": "2024-06-12T01:51:00.000Z",
        "voteCount": 2,
        "content": "D is right"
      },
      {
        "date": "2024-02-29T04:26:00.000Z",
        "voteCount": 3,
        "content": "it should be D, the first two characters should be from 0-2"
      },
      {
        "date": "2024-02-20T06:08:00.000Z",
        "voteCount": 2,
        "content": "E is right"
      },
      {
        "date": "2024-02-09T13:22:00.000Z",
        "voteCount": 2,
        "content": "D would be correct as it asks first two characters.\n   - substr(startIndex: Int, length: Int): This function takes two arguments:\n   -&gt; startIndex: The starting index of the substring to extract. Indexing starts from 0.\n   -&gt; length: The length of the substring to extract."
      },
      {
        "date": "2024-01-24T03:29:00.000Z",
        "voteCount": 2,
        "content": "it should be D, the first two characters should be from 0-2"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/databricks/view/130837-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame where column division from DataFrame storesDF has been renamed to column state and column managerName from DataFrame storesDF has been renamed to column managerFullName. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>storesDF. __1__(__2__, __2__).__4__(__5__, __6__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumnRenamed<br>2.\"state\"<br>3.\"division\"<br>4.withColumnRenamed<br>5.\"managerFullName\"<br>6.\"managerName\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1.withColumnRenamed<br>2.division<br>3.col(\"state\")<br>4. withColumnRenamed<br>5.\"managerName\"<br>6.col(\"managerFullName\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. WithColumnRenamed<br>2. \"division\"<br>3.\"state\"<br>4. withColumnRenamed<br>5. \"managerName\"<br>6.\"managerFullName\"\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"division\"<br>3. \"state\"<br>4.withcolumn<br>5.\"managerName\"<br>6.\"managerFullName",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"division\"<br>3. \"state\"<br>4. withColumn<br>5.\"managerName\"<br>6.\"managerFullName\""
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T13:24:00.000Z",
        "voteCount": 2,
        "content": "C is correct from these scenarios"
      },
      {
        "date": "2024-01-24T03:32:00.000Z",
        "voteCount": 2,
        "content": "columnRenamed(sourceColName, newColName)"
      },
      {
        "date": "2024-01-11T02:55:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/databricks/view/132264-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame where rows in DataFrame storesDF with missing values in every column have been dropped. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>storesDF.__1__.2__(3__ = __4__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. na<br>2.drop<br>3. how<br>4.\"any\"\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. na<br>2.drop<br>3. subset<br>4.\"all\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. na<br>2.drop<br>3.subset<br>4. \"any\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. na<br>2.drop<br>3. how<br>4. \"all\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. drop<br>2. na<br>3. how<br>4. \"all\""
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-07T10:35:00.000Z",
        "voteCount": 1,
        "content": "in every column ---&gt;all\nany of columns ---&gt;any"
      },
      {
        "date": "2024-06-07T10:34:00.000Z",
        "voteCount": 1,
        "content": "in every column  ---&gt;all\nany of columns  ---&gt;any"
      },
      {
        "date": "2024-05-22T21:46:00.000Z",
        "voteCount": 1,
        "content": "storesDF.na.drop(how=\"any\").show()"
      },
      {
        "date": "2024-04-10T05:19:00.000Z",
        "voteCount": 1,
        "content": "D. 1. na\n2.drop\n3. how\n4. \"all\""
      },
      {
        "date": "2024-01-27T03:44:00.000Z",
        "voteCount": 2,
        "content": "Isn't it option B ?\nResult_df = storesDF.na.drop(subset=\"all\")"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/databricks/view/132263-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a collection of summary statistics for column sqft in DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>storesDF.__1__(__2__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. summary<br>2. col(\"sqft\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. describe<br>2. col(\"sqft\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. summary<br>2. \"sqft\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. describe<br>2. \"sqft\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. summary<br>2. \"all\""
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-27T03:27:00.000Z",
        "voteCount": 1,
        "content": "Isn't it option B ?\nstoresDF.describe(col(\"sqft\"))"
      },
      {
        "date": "2024-02-02T04:45:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/databricks/view/138318-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a 15 percent sample of rows from DataFrame storesDF without replacement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample(True, fraction = 0.15)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample(fraction = 0.15)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sampleBy(fraction = 0.15)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample(fraction = 0.10)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.sample()"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T05:20:00.000Z",
        "voteCount": 1,
        "content": "B. storesDF.sample(fraction = 0.15)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/databricks/view/137566-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks extracts the value for column sqft from the first row of DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.first()[col(\"sqft\")]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF[0][\"sqft\"]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.collect(l)[0][\"sqft\"]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.first.sqft",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.first().sqft"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-03-30T18:31:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/databricks/view/138319-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks uses SQL to return a new DataFrame containing column storeId and column managerName from a table created from DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.createOrReplaceTempView()<br>spark.sql(\"SELECT storeId, managerName FROM stores\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.query(\u201dSELECT storeid, managerName from stores\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.createOrReplaceTempView(\"storesDF\")<br>storesDF.sql(\"SELECT storeId, managerName from stores\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.createOrReplaceTempView(\"stores\")<br>spark.sql(\"SELECT storeId, managerName FROM stores\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.createOrReplaceTempView(\"stores\")<br>storesDF.query(\"SELECT storeId, managerName FROM stores\")"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T05:21:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/databricks/view/138320-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should adjust the number of partitions used in wide transformations like join() to 32. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__(__2__, __3__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark.conf.get<br>2. \"spark.sql.shuffle.partitions\"<br>3. \"32\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark.conf.set<br>2. \"spark.default.parallelism\"<br>3. 32",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark.conf.text<br>2. \"spark.default.parallelism\"<br>3. \"32\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark.conf.set<br>2. \"spark.default.parallelism\"<br>3. \"32\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark.conf.set<br>2. \"spark.sql.shuffle.partitions\"<br>3. \"32\""
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-08-16T08:41:00.000Z",
        "voteCount": 2,
        "content": "in Option E there is \"32\" is in string format is it acceptable? I think it should be numeric 32"
      },
      {
        "date": "2024-07-07T05:42:00.000Z",
        "voteCount": 1,
        "content": "A and E same"
      },
      {
        "date": "2024-07-07T05:42:00.000Z",
        "voteCount": 1,
        "content": "nevermind"
      },
      {
        "date": "2024-04-10T05:22:00.000Z",
        "voteCount": 1,
        "content": "E. 1. spark.conf.set\n2. \"spark.sql.shuffle.partitions\"\n3. \"32\""
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/databricks/view/133472-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame containing a column openDateString, a string representation of Java\u2019s SimpleDateFormat?<br><br>Note that column openDate is of type integer and represents a date in the UNIX epoch format \u2014 the number of seconds since midnight on January 1st, 1970.<br><br>An example of Java's SimpleDateFormat is \"Sunday, Dec 4, 2008 1:05 pm\".<br><br>A sample of storesDF is displayed below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image16.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"openDatestring\", from unixtime(col(\"openDate\u201c), \u201cEEEE, MMM d, yyyy h:mm a\"))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"openDateString\", from_unixtime(col(\"openDate\u201c), \"EEEE, MMM d, yyyy h:mm a\", TimestampType()))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"openDateString\", date(col(\"openDate\"), \"EEEE, MMM d, yyyy h:mm a\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.newColumn(col(\"openDateString\"), from_unixtime(\"openDate\", \"EEEE, MMM d, yyyy h:mm a\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"openDateString\", date(col(\"openDate\u201c), \"EEEE, MMM d, yyyy h:mm a\", TimestampType))"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-21T05:50:00.000Z",
        "voteCount": 1,
        "content": "pyspark.sql.functions.from_unixtime(timestamp: ColumnOrName, format: str = 'yyyy-MM-dd HH:mm:ss') \u2192 pyspark.sql.column.Column[source]\nConverts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the given format."
      },
      {
        "date": "2024-08-16T04:26:00.000Z",
        "voteCount": 1,
        "content": "The correct code block that returns a DataFrame containing a column openDateString, which is a string representation of Java's SimpleDateFormat, is:\n\nA. storesDF.withColumn(\"openDatestring\", from_unixtime(col(\"openDate\"), \"EEEE, MMM d, yyyy h:mm a\"))\n\nThis option correctly converts the openDate column from UNIX epoch format to a human-readable string format using the from_unixtime function in PySpark, which is equivalent to Java's SimpleDateFormat. The format \"EEEE, MMM d, yyyy h:mm a\" aligns with the example provided: \"Sunday, Dec 4, 2008 1:05 pm\"."
      },
      {
        "date": "2024-06-23T13:25:00.000Z",
        "voteCount": 1,
        "content": "new column should be a String, existing column is Int"
      },
      {
        "date": "2024-04-10T05:24:00.000Z",
        "voteCount": 1,
        "content": "A. storesDF.withColumn(\"openDatestring\", from unixtime(col(\"openDate\u201c), \u201cEEEE, MMM d, yyyy h:mm a\"))"
      },
      {
        "date": "2024-03-20T14:19:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer \n\n# Assuming you have a SparkSession created (replace with your setup if needed)\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import from_unixtime\n\n# Sample data (replace with your actual DataFrame)\ndata = [(\"store1\", 1668518400), (\"store2\", 1668432000)]  # Sample UNIX epoch timestamps\ndf = spark.createDataFrame(data, [\"storeName\", \"openDate\"])\n\n# Convert openDate to formatted string\nformatted_df = df.withColumn(\n    \"openDateString\", from_unixtime(col(\"openDate\"), \"EEEE, MMM d, yyyy h:mm a\")\n)\n\n# Print the resulting DataFrame\nformatted_df.show(truncate=False)\n\n# Stop the SparkSession (optional)\n#spark.stop()"
      },
      {
        "date": "2024-02-09T15:05:00.000Z",
        "voteCount": 2,
        "content": "B is likely tru, because A option has some typo issue and the underscore in from keyword is missing before unixtime, and the return type is not specified."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/databricks/view/138321-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame that is the result of a cross join between DataFrame storesDF and DataFrame employeesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.crossJoin(employeesDF)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.join(employeesDF, \"storeId\", \"cross\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcrossJoin(storesDF, employeesDF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tjoin(storesDF, employeesDF, \"cross\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.join(employeesDF, \"cross\")"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-07T11:01:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      },
      {
        "date": "2024-04-10T05:24:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/databricks/view/138322-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame that is the result of a position-wise union between DataFrame storesDF and DataFrame acquiredStoresDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.unionByName(acquiredStoresDF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tunionAll(storesDF, acquiredStoresDF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tunion(storesDF, acquiredStoresDF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tconcat(storesDF, acquiredStoresDF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.union(acquiredStoresDF)"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T05:25:00.000Z",
        "voteCount": 1,
        "content": "E. storesDF.union(acquiredStoresDF)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/databricks/view/138323-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "In what order should the below lines of code be run in order to read a parquet at the file path filePath into a DataFrame?<br><br>Lines of code:<br><br>1. storesDF<br>2. .load(filePath, source = \"parquet\")<br>3. .read \\<br>4. spark \\<br>5. .read() \\<br>6. .parquet(filePath)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1, 5, 2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t4, 5, 2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t4, 3, 6",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t4, 5, 6",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t4, 3, 2"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T05:25:00.000Z",
        "voteCount": 1,
        "content": "C. 4, 3, 6"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/databricks/view/130863-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should read a CSV at the file path filePath into a DataFrame with the specified schema schema. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__.__3__(__4__).format(\"csv\").__5__(__6__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read()<br>3. schema<br>4. schema<br>5. json<br>6. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read()<br>3. schema<br>4. schema<br>5. load<br>6. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read<br>3. format<br>4. \"json\"<br>5. load<br>6. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read()<br>3. json<br>4. filePath<br>5. format<br>6. schema",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read<br>3. schema<br>4. schema<br>5. load<br>6. filePath\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-12T10:23:00.000Z",
        "voteCount": 1,
        "content": "E is correct.   read is a property of spark.  so spark.read(). is wrong.  B is wrong."
      },
      {
        "date": "2024-04-04T08:22:00.000Z",
        "voteCount": 1,
        "content": "E definetly"
      },
      {
        "date": "2024-02-09T15:10:00.000Z",
        "voteCount": 1,
        "content": "E is right"
      },
      {
        "date": "2024-02-03T20:46:00.000Z",
        "voteCount": 1,
        "content": "It is E, 100 %"
      },
      {
        "date": "2024-01-11T08:44:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/databricks/view/138324-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following describes slots?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlots are the most coarse level of execution in the Spark execution hierarchy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlots are resource threads that can be used for parallelization within a Spark application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlots are resources that are used to run multiple Spark applications at once on a single cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlots are the most granular level of execution in the Spark execution hierarchy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlots are unique segments of data from a DataFrame that are split up by row."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T05:26:00.000Z",
        "voteCount": 1,
        "content": "B. Slots are resource threads that can be used for parallelization within a Spark application."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/databricks/view/138341-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations is least likely to result in a shuffle?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.join()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.fliter()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.orderBy()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.distinct()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.intersect()"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T06:40:00.000Z",
        "voteCount": 1,
        "content": "B. DataFrame.fliter()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/databricks/view/138342-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following cluster configurations is least likely to experience delays due to garbage collection of a large DataFrame?<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image17.png\"><br><br>Note: each configuration has roughly the same compute power using 100GB of RAM and 200 cores.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #4",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #5",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMore information is needed to determine an answer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #6\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-03T19:10:00.000Z",
        "voteCount": 2,
        "content": "Large cluster will keep large DF objects live and GC will take more time to collect those"
      },
      {
        "date": "2024-04-10T06:43:00.000Z",
        "voteCount": 3,
        "content": "I think it's Scemario 6"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/databricks/view/138343-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame where column product\u0421ategories only has one word per row, resulting in a DataFrame with many more rows than DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>A sample of storesDF is displayed below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image18.png\"><br><br>Code block:<br><br>storesDF.__1__(__2__, __3__(__4__(__5__)))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. newColumn<br>2. \"productCategories\"<br>3. col<br>4. split<br>5. \"productCategories\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"productCategory\"<br>3. split<br>4. col<br>5. \"productCategories\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"productCategory\"<br>3. explode<br>4. col<br>5. \"productCategories\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. newColumn<br>2. \"productCategory\"<br>3. explode<br>4. col<br>5. \"productCategories\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. \"productCategories\"<br>3. explode<br>4. col<br>5. \"productCategories\"\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-21T06:03:00.000Z",
        "voteCount": 1,
        "content": "productCategories"
      },
      {
        "date": "2024-07-12T09:39:00.000Z",
        "voteCount": 1,
        "content": "The answer is E.  If you feel like getting in an argument with the question on the proper use of plural field names, then pick C."
      },
      {
        "date": "2024-06-23T13:53:00.000Z",
        "voteCount": 2,
        "content": "new column should be \"productCategory\" singular"
      },
      {
        "date": "2024-06-23T13:55:00.000Z",
        "voteCount": 1,
        "content": "ok, that wasn't in the spec, but it should have been. I guess E is ok then."
      },
      {
        "date": "2024-04-10T06:44:00.000Z",
        "voteCount": 1,
        "content": "E. 1. withColumn\n2. \"productCategories\"\n3. explode\n4. col\n5. \"productCategories\""
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/databricks/view/132690-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a new DataFrame with column storeReview where the pattern \"End\" has been removed from the end of column storeReview in DataFrame storesDF?<br><br>A sample DataFrame storesDF is below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image19.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeReview\", col(\"storeReview\").regexp_replace(\" End$\", \"\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeReview\", regexp_replace(col(\"storeReview\"), \" End$\", \"\"))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeReview\u201d, regexp_replace(col(\"storeReview\"), \" End$\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeReview\", regexp_replace(\"storeReview\", \" End$\", \"\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeReview\", regexp_extract(col(\"storeReview\"), \" End$\", \"\"))"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-14T05:19:00.000Z",
        "voteCount": 1,
        "content": "the official documentation says\npyspark.sql.functions.regexp_replace(str, pattern, replacement)\nReplace all substrings of the specified string value that match regexp with rep.\nso I think D is the only correct option"
      },
      {
        "date": "2024-05-23T01:48:00.000Z",
        "voteCount": 2,
        "content": "B &amp; D work"
      },
      {
        "date": "2024-04-03T14:00:00.000Z",
        "voteCount": 2,
        "content": "B is the right choice"
      },
      {
        "date": "2024-02-09T15:22:00.000Z",
        "voteCount": 1,
        "content": "B is right one it seems, col should be used"
      },
      {
        "date": "2024-02-02T05:09:00.000Z",
        "voteCount": 3,
        "content": "B also works right?"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/databricks/view/138344-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame where rows in DataFrame storesDF containing at least one missing value have been dropped. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>StoresDF.__1__.__2__(__3__ = __4__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. na<br>2. drop<br>3. subset<br>4. \"any\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. na<br>2. drop<br>3. how<br>4. \"all\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. na<br>2. drop<br>3. subset<br>4. \"all\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. na<br>2. drop<br>3. how<br>4. \"any\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. drop<br>2. na<br>3. how<br>4. \"any\""
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T06:45:00.000Z",
        "voteCount": 1,
        "content": "D. 1. na\n2. drop\n3. how\n4. \"any\""
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/databricks/view/138376-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations calculates the simple average of a group of values, like a column?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsimpleAvg()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmean()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tagg()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\taverage()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tapproxMean()"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T12:00:00.000Z",
        "voteCount": 1,
        "content": "B. mean()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/databricks/view/134140-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks fails to return the number of rows in DataFrame storesDF for each distinct combination of values in column division and column storeCategory?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy((col(\"division\"), col(\"storeCategory\")]).count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy(\"division\").groupBy(\"storeCategory\").count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy([\"division\", \"storeCategory\"]).count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy(\"division\", \"storeCategory\").count()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.groupBy(col(\"division\u201c), col(\"storeCategory\")).count()"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-03T13:59:00.000Z",
        "voteCount": 1,
        "content": "B is the right choice. I tested with my dataframe option B threw this error\nAttributeError: 'GroupedData' object has no attribute 'groupBy'"
      },
      {
        "date": "2024-02-18T04:29:00.000Z",
        "voteCount": 1,
        "content": "D is correct !"
      },
      {
        "date": "2024-02-27T09:36:00.000Z",
        "voteCount": 2,
        "content": "it is possible to run in pyspark - storesDF.groupBy(\"division\", \"storeCategory\").count()\nCorrect answer is B"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/databricks/view/134766-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a collection of summary statistics for column sqft in Data Frame storesDF. Identify the error.<br><br>Code block:<br><br>storesDF.describes(col(\"sgft \"))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe describe() operation doesn't compute summary statistics for a single column \u2014 the summary() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe column sqft should be subsetted from DataFrame storesDF prior to computing summary statistics on it alone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe describe()  operation does not accept a Column object as an argument outside of a list \u2014 the list [col(\"sqft\")] should be specified instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe describe() operation does not accept a Column object as an argument \u2014 the column name string \"sqft\" should be specified instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe describe() operation doesn't compute summary statistics for numeric columns \u2014 the sumwary() operation should be used instead."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-27T09:39:00.000Z",
        "voteCount": 1,
        "content": "Answer D correct\nhttps://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/databricks/view/138352-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a 25 percent sample of rows from DataFrame storesDF with reproducible results. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>StoresDF.__1__(__2__ = __3__, __4__ = __5__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. sample<br>2. fraction<br>3. 0.25<br>4. seed<br>5. True",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. sample<br>2. withReplacement<br>3. True<br>4. seed<br>5. True",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. sample<br>2. fraction<br>3. 0.25<br>4. seed<br>5. 1234",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. sample<br>2. fraction<br>3. 0.15<br>4. seed<br>5. 1234",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. sample<br>2. withReplacement<br>3. True<br>4. seed<br>5. 1234"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T08:07:00.000Z",
        "voteCount": 1,
        "content": "C. 1. sample\n2. fraction\n3. 0.25\n4. seed\n5. 1234"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/databricks/view/138353-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks creates a Python UDF assessPerformanceUDF() using the integer-returning Python function assessPerformance() and applies it to Column customerSatisfaction in DataFrame storesDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tassessPerformanceUDF = udf(assessPerformance, IntegerType) storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tassessPerformanceUDF = udf(assessPerformance, IntegerType()) storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tassessPerformanceUDF - udf(assessPerformance) storesDF.withColumn(\"result\", assessPerformance(col(\u201ccustomerSatisfaction\")))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tassessPerformanceUDF = udf(assessPerformance) storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tassessPerformanceUDF = udf(assessPerformance, IntegerType()) storesDF.withColumn(\"result\", assessPerformance(col(\"customerSatisfaction\")))"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T08:09:00.000Z",
        "voteCount": 1,
        "content": "B. assessPerformanceUDF = udf(assessPerformance, IntegerType()) storesDF.withColumn(\"result\", assessPerformanceUDF(col(\"customerSatisfaction\")))"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/databricks/view/132693-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to create a single-column DataFrame from Python list years which is made up of integers. Identify the error.<br><br>Code block:<br><br>spark.createDataFrame(years, IntegerType)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe column name must be specified.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe years list should be wrapped in another list like [years] to make clear that it is a column rather than a row.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no createDataFrame operation in spark.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IntegerType call must be followed by parentheses.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe IntegerType call should not be present \u2014 Spark can tell that list years is full of integers."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-12T08:42:00.000Z",
        "voteCount": 1,
        "content": "The answer is D"
      },
      {
        "date": "2024-02-09T15:32:00.000Z",
        "voteCount": 4,
        "content": "D is right"
      },
      {
        "date": "2024-02-02T05:31:00.000Z",
        "voteCount": 2,
        "content": "Answer should be D"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/databricks/view/138354-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a DataFrame containing a column openDateString, a string representation of Java\u2019s SimpleDateFormat. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Note that column openDate is of type integer and represents a date in the UNIX epoch format \u2014 the number of seconds since midnight on January 1st, 1970.<br><br>An example of Java\u2019s SimpleDateFormat is \"Sunday, Dec 4, 2008 1:05 pm\".<br><br>A sample of storesDF is displayed below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image20.png\"><br><br>Code block:<br><br>storesDF.__1__(\"openDateString\", __2__(__3__, __4__))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. from_unixtime<br>3. col(\"openDate\")<br>4. \u201cEEEE, MMM d, yyyy h:mm a\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. date_format<br>3. col(\"openDate\")<br>4. \"EEEE, mmm d, yyyy h:mm a\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. newColumn<br>2. from_unixtinie<br>3. \"openDate\"<br>4. \"EEEE, MMM d, yyyy h:mm a\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. from_unixtlme<br>3. col(\"openDate\")<br>4. SimpleDateFormat",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. withColumn<br>2. from_unixtime<br>3. col(\"openDate\")<br>4. \"dw, MMM d, yyyy h:mm a\""
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T08:10:00.000Z",
        "voteCount": 1,
        "content": "A. 1. withColumn\n2. from_unixtime\n3. col(\"openDate\")\n4. \u201cEEEE, MMM d, yyyy h:mm a\""
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/databricks/view/138355-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations can be used to perform a left join on two DataFrames?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.join()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.crossJoin()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.merge()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.leftJoin()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStandalone join() function"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T08:10:00.000Z",
        "voteCount": 1,
        "content": "A. DataFrame.join()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/databricks/view/138360-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame that is the result of an inner join between DataFrame storesDF and DataFrame employeesDF on column storeId and column employeeId. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>storesDF.join(employeesDF, [__1__ == __2__, __3__ == __4__])",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF.storeId<br>2. storesDF.employeeId<br>3. employeesDF.storeId<br>4. employeesDF.employeeId",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. col(\"storeId\")<br>2.col(\"storeId\")<br>3.col(\"employeeId\")<br>4. col(\"employeeId\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storeId<br>2. storeId<br>3. employeeId<br>4. employeeId",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. col(\"storeId\")<br>2. col(\"employeeId\")<br>3. col(\"employeeId\")<br>4. col(''storeId\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF.storeId<br>2. employeesDF.storeId<br>3. storesDF.employeeId<br>4. employeesDF.employeeId"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T09:49:00.000Z",
        "voteCount": 1,
        "content": "E. 1. storesDF.storeId\n2. employeesDF.storeId\n3. storesDF.employeeId\n4. employeesDF.employeeId"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/databricks/view/138359-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a new DataFrame that is the result of a position-wise union between DataFrame storesDF and DataFrame acquiredStoresDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__(__3__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. DataFrame<br>2. union<br>3. storesDF, acquiredStoresDF",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. DataFrame<br>2. concat<br>3. storesDF, acqulredStoresDF",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. union<br>3.acquiredStoresDF",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. unionByName<br>3. acquiredStoresDF",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. DataFrame<br>2. unionAll<br>3. storesDF, acquiredStoresDF"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T09:49:00.000Z",
        "voteCount": 1,
        "content": "C. 1. storesDF\n2. union\n3.acquiredStoresDF"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/databricks/view/138361-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks writes DataFrame storesDF to file path filePath as text files overwriting any existing files in that location?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write(filePath, mode = \"overwrite\", source = \"text\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.mode(\"overwrite\").text(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.mode(\"overwrite\").path(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write.option(\"text\", \"overwrite\").path(filePath)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.write().mode(\"overwrite\").text(filePath)"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T09:51:00.000Z",
        "voteCount": 1,
        "content": "B. storesDF.write.mode(\"overwrite\").text(filePath)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/databricks/view/138362-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to read JSON at the file path filePath into a DataFrame with the specified schema schema. Identify the error.<br><br>Code block:<br><br>spark.read.schema(\"schema\").format(\"json\").load(filePath)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe schema operation from read takes a schema object rather than a string \u2014 the argument should be schema.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no load() operation for DataFrameReader \u2014 it should be replaced with the json() operation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe spark.read operation should be followed by parentheses in order to return a DataFrameReader object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no read property of spark \u2014 spark should be replaced with DataFrame.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe schema operation from read takes a column rather than a string \u2014 the argument should be col(\"schema\")."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T09:51:00.000Z",
        "voteCount": 1,
        "content": "A. The schema operation from read takes a schema object rather than a string \u2014 the argument should be schema."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/databricks/view/138363-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following describes executors?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutors are the communication pathways from the driver node to the worker nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutors are the most granular level of execution in the Spark execution hierarchy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutors always have a one-to-one relationship with worker nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutors are synonymous with worker nodes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutors are processing engine instances for performing data computations which run on a worker node."
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T09:57:00.000Z",
        "voteCount": 2,
        "content": "E. Executors are processing engine instances for performing data computations which run on a worker node."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/databricks/view/138364-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should return a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 AND the value in column customerSatisfaction is greater than or equal to 30. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>storesDF.__1__(__2__ __3__ __4__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. filter<br>2. (col(\"sqft\") &lt;= 25000)<br>3. &amp;<br>4. (col(\"customerSatisfaction\") &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. filter<br>2. (col(\"sqft\") &lt;= 25000<br>3. &amp;<br>4. col(\"customerSatisfaction\") &gt;= 30",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. filter<br>2. (col(\"sqft\") &lt;= 25000)<br>3. and<br>4. (col(\"customerSatisfaction\") &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. drop<br>2. (col(sqft) &lt;= 25000)<br>3. &amp;<br>4. (col(customerSatisfaction) &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. filter<br>2. col(\"sqft\") &lt;= 25000<br>3. and<br>4. col(\"customerSatisfaction\") &gt;= 30"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T09:59:00.000Z",
        "voteCount": 1,
        "content": "A. 1. filter\n2. (col(\"sqft\") &lt;= 25000)\n3. &amp;\n4. (col(\"customerSatisfaction\") &gt;= 30)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/databricks/view/137494-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame with column storeSlogan where single quotes in column storeSlogan in DataFrame storesDF have been replaced with double quotes?<br><br>A sample of DataFrame storesDF is below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image21.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeSlogan\", col(\"storeSlogan\").regexp_replace(\"\u2019\" \"\\\"\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeSlogan\", regexp_replace(col(\"storeSlogan\"), \"\u2019\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeSlogan\", regexp_replace(col(\"storeSlogan\"), \"\u2019\", \"\\\"\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeSlogan\", regexp_replace(\"storeSlogan\", \"\u2019\", \"\\\"\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"storeSlogan\", regexp_extract(col(\"storeSlogan\"), \"\u2019\", \"\\\"\"))"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T10:00:00.000Z",
        "voteCount": 1,
        "content": "It's C. https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.regexp_replace.html"
      },
      {
        "date": "2024-03-29T11:56:00.000Z",
        "voteCount": 1,
        "content": "Why C and nod D ?"
      },
      {
        "date": "2024-05-23T02:56:00.000Z",
        "voteCount": 1,
        "content": "needs a col object"
      },
      {
        "date": "2024-05-23T03:02:00.000Z",
        "voteCount": 1,
        "content": "yes,D also works. disregard my previous comment."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/databricks/view/138365-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations can be used to rename and replace an existing column in a DataFrame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.renamedColumn()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.withColumnRenamed()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.wlthColumn()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcol()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataFrame.newColumn()"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T10:01:00.000Z",
        "voteCount": 1,
        "content": "B. DataFrame.withColumnRenamed()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/databricks/view/138366-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should print the schema of DataFrame storesDF. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br><br>__1__.__2__(__3__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. schema<br>3. Nothing",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. str<br>3. schema",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. printSchema<br>3. True",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. printSchema<br>3. Nothing",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. printSchema<br>3. \"all\""
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T10:02:00.000Z",
        "voteCount": 1,
        "content": "D. 1. storesDF\n2. printSchema\n3. Nothing"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/databricks/view/138367-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to create and register a SQL UDF named \"ASSESS_PERFORMANCE\" using the Python function assessPerformance() and apply it to column customerSatistfaction in table stores. Identify the error.<br><br>Code block:<br><br>spark.udf.register(\"ASSESS_PERFORMANCE\", assessPerformance)<br>spark.sql(\"SELECT customerSatisfaction, assessPerformance(customerSatisfaction) AS result FROM stores\")",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no sql() operation \u2014 the DataFrame API must be used to apply the UDF assessPerformance().",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe order of the arguments to spark.udf.register() should be reversed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customerSatisfaction column cannot be called twice inside the SQL statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegistered UDFs cannot be applied inside of a SQL statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe wrong SQL function is used to compute column result \u2014 it should be ASSESS_PERFORMANCE instead of assessPerformance."
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T10:02:00.000Z",
        "voteCount": 1,
        "content": "E. The wrong SQL function is used to compute column result \u2014 it should be ASSESS_PERFORMANCE instead of assessPerformance."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/databricks/view/138368-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks attempts to cache the partitions of DataFrame storesDF only in Spark\u2019s memory?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.cache(StorageLevel.MEMORY_ONLY).count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.persist().count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.cache().count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.persist(StorageLevel.MEMORY_ONLY).count()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.persist(\"MEMORY_ONLY\").count()"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T10:03:00.000Z",
        "voteCount": 1,
        "content": "D. storesDF.persist(StorageLevel.MEMORY_ONLY).count()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/databricks/view/138370-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following operations will always return a new DataFrame with updated partitions from DataFrame storesDF by inducing a shuffle?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.coalesce()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.rdd.getNumPartitions()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.repartition()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.union()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.intersect()"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T10:03:00.000Z",
        "voteCount": 1,
        "content": "C. storesDF.repartition()"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/databricks/view/138371-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame containing a column month, an integer representation of the month from column openDate from DataFrame storesDF?<br><br>Note that column openDate is of type integer and represents a date in the UNIX epoch format \u2014 the number of seconds since midnight on January 1 st, 1970.<br><br>A sample of storesDF is displayed below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image20.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"month\", getMonth(col(\"openDate\")))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"month\", substr(col(\"openDate\"), 4, 2))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"openDateFormat\", col(\"openDate\").cast(\"Date\"))<br>.withColumn(\"month\", month(col(\"openDateFormat\"))))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\"))<br>.withColumn(\"month\", month(col(\"openTimestamp\"))))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.withColumn(\"month\", month(col(\"openDate\")))"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-10T10:04:00.000Z",
        "voteCount": 1,
        "content": "D. (storesDF.withColumn(\"openTimestamp\", col(\"openDate\").cast(\"Timestamp\"))\n.withColumn(\"month\", month(col(\"openTimestamp\"))))"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/databricks/view/133473-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below should read a parquet at the file path filePath into a DataFrame. Choose the response that correctly fills in the numbered blanks within the code block to complete this task.<br><br>Code block:<br>__1__.__2__.__3__(__4__)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1.spark<br>2. read()<br>3. parquet<br>4. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read()<br>3. load<br>4. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read<br>3. load<br>4. filePath, source = \"parquet\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. storesDF<br>2. read()<br>3. load<br>4. filePath",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. spark<br>2. read<br>3. load<br>4. filePath\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-12T07:25:00.000Z",
        "voteCount": 1,
        "content": "It's E.  read is a property of spark, not a method, so that makes the choice C or E.  In C the parameter source is being assigned \"parquet\".  Parquet is the format.  So C is wrong.   E is correct, because the default format is parquet."
      },
      {
        "date": "2024-04-10T10:04:00.000Z",
        "voteCount": 2,
        "content": "E. 1. spark\n2. read\n3. load\n4. filePath"
      },
      {
        "date": "2024-02-09T16:01:00.000Z",
        "voteCount": 2,
        "content": "To me E is most likely correct\n\nSyntax: spark.read.load(filePath)"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/databricks/view/146221-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following statements describing a difference between transformations and actions is incorrect?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are wide and narrow transformations but there are not wide and narrow actions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransformations do not trigger execution while actions do trigger execution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransformations work on DataFrames/Datasets while actions are reserved for native language objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSome actions can be used to return data objects in a format native to the programming language being used to access the Spark API while transformations do not provide this ability.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransformations are typically logic operations while actions are typically focused on returning results."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-19T05:29:00.000Z",
        "voteCount": 1,
        "content": "We need to identify INCORRECT statement, so C is the answer"
      },
      {
        "date": "2024-09-03T18:58:00.000Z",
        "voteCount": 1,
        "content": "is telling you what is the incorrect statement, so C is incorrect"
      },
      {
        "date": "2024-08-21T01:46:00.000Z",
        "voteCount": 1,
        "content": "I think it's B"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/databricks/view/143645-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following describes why garbage collection in Spark is important?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLogical results will be incorrect if inaccurate data is not collected and removed from the Spark job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark jobs will fail or run slowly if inaccurate data is not collected and removed from the Spark job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark jobs will fail or run slowly if memory is not available for new objects to be created.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark jobs will produce inaccurate results if there are too many different transformations called before a single action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark jobs will produce inaccurate results if memory is not available for new tasks to run and complete."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-09T11:51:00.000Z",
        "voteCount": 1,
        "content": "C is correct. Garbage in JVM is about releasing memory of objects that are no longer used so that new objects can be created"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/databricks/view/140675-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame where rows in DataFrame storesDF containing missing values in every column have been dropped?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.na.drop()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.dropna()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.na.drop(\"all\", subset = \"sqft\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.na.drop(\"all\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.nadrop(\"all\")"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-14T19:36:00.000Z",
        "voteCount": 2,
        "content": "https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.dropna.html"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/databricks/view/147839-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "The code block shown below contains an error. The code block is intended to return a new DataFrame where column productCategories only has one word per row, resulting in a DataFrame with many more rows than DataFrame storesDF. Identify the error and how to fix it.<br><br>A sample of storesDF is displayed below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image22.png\"><br><br>storesDF.withColumn(\"productCategories\", split(col(\"productCategories\")))",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe split() operation does not accomplish the requested task in the way that it is used. It should be used provided an alias.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe split() operation does not accomplish the requested task. The broadcast() operation should be used instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe split() operation does not accomplish the requested task in the way that it is used. It should be used as a column object method instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe split() operation does not accomplish the requested task. The explode() operation should be used instead.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe split() operation does not accomplish the requested task. The array_distinct() operation should be used instead."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-19T05:59:00.000Z",
        "voteCount": 2,
        "content": "Based on the condition \"column productCategories only has one word per row, resulting in a DataFrame with many more rows than DataFrame storesDF\" there should be an explode() function"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/databricks/view/147840-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame where column managerName from DataFrame storesDF is split at the space character into column managerFirstName and column managerLastName?<br><br>A sample of DataFrame storesDF is displayed below:<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image23.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"managerFirstName\", split(col(\"managerName\"), \" \")[0])<br>.withColumn(\"managerLastName\", split(col(\"managerName\"), \" \")[1]))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"managerFirstName\", col(\"managerName\"). split(\" \")[1])<br>.withColumn(\"managerLastName\", col(\"managerName\").split(\" \")[2]))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"managerFirstName\", split(col(\"managerName\"), \" \")[1])<br>.withColumn(\"managerLastName\", split(col(\"managerName\"), \" \")[2]))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"managerFirstName\", col(\"managerName\").split(\" \")[0])<br>.withColumn(\"managerLastName\", col(\"managerName\").split(\" \")[1]))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(storesDF.withColumn(\"managerFirstName\", split(\"managerName\"), \" \")[0])<br>.withColumn(\"managerLastName\", split(\"managerName\"), \" \")[1]))"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-10-14T06:04:00.000Z",
        "voteCount": 1,
        "content": "Right answer is A as the array returned by the split function is 0-based not 1-based. You can try yourself with the following code:\ndf = spark.createDataFrame([('John Doe',)], ['Person',])\ndf2 = df \\\n  .withColumn('first_name', split(col('Person'), ' ')[0]) \\\n  .withColumn('last_name', split(col('Person'), ' ')[1])\ndf2.show()"
      },
      {
        "date": "2024-10-06T13:11:00.000Z",
        "voteCount": 1,
        "content": "(storesDF.withColumn(\"managerFirstName\", split(col(\"managerName\"), \" \")[0])\n .withColumn(\"managerLastName\", split(col(\"managerName\"), \" \")[1]))"
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/databricks/view/147841-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following cluster configurations will fail to ensure completion of a Spark application in light of a worker node failure?<br><br><img src=\"https://img.examtopics.com/certified-associate-developer-for-apache-spark/image24.png\"><br><br>Note: each configuration has roughly the same compute power using 100GB of RAM and 200 cores.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #5",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #4",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #6",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScenario #1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey should all ensure completion because worker nodes are fault tolerant"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-19T06:19:00.000Z",
        "voteCount": 2,
        "content": "scenario #1 uses a configuration with only 1 executor. If the worker node running that executor fails, the entire application will fail because there are no other executors available to continue processing the tasks.\nSpark needs redundancy across multiple worker nodes or executors to tolerate node failures. Since this configuration has only one executor, it lacks fault tolerance."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/databricks/view/147842-exam-certified-associate-developer-for-apache-spark-topic-1/",
    "body": "Which of the following code blocks returns a DataFrame containing only the rows from DataFrame storesDF where the value in column sqft is less than or equal to 25,000 AND the value in column customerSatisfaction is greater than or equal to 30?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(\"sqft\") &lt;= 25000 and col(\"customerSatisfaction\") &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(sqft) &lt;= 25000 &amp; col(customerSatisfaction) &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(col(\"sqft\") &lt;= 25000 &amp; col(\"customerSatisfaction\") &gt;= 30)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter((col(\"sqft\") &lt;= 25000) &amp; (col(\"customerSatisfaction\") &gt;= 30))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstoresDF.filter(sqft &lt;= 25000 and customerSatisfaction &gt;= 30)"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-19T06:22:00.000Z",
        "voteCount": 1,
        "content": "D. storesDF.filter((col(\"sqft\") &lt;= 25000) &amp; (col(\"customerSatisfaction\") &gt;= 30)): This correctly uses the filter() method along with logical AND (&amp;) to apply both conditions. Each condition is wrapped in parentheses to ensure proper evaluation."
      }
    ],
    "examNameCode": "certified-associate-developer-for-apache-spark",
    "topicNumber": "1"
  }
]