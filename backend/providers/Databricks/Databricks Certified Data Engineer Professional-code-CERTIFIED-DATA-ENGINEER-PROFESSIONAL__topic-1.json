[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/databricks/view/117083-exam-certified-data-engineer-professional-topic-1-question-1/",
    "body": "An upstream system has been configured to pass the date for a given batch of data to the Databricks Jobs API as a parameter. The notebook to be scheduled will use this parameter to load data with the following code: df = spark.read.format(\"parquet\").load(f\"/mnt/source/(date)\")<br>Which code block should be used to create the date Python variable used in the above code block?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdate = spark.conf.get(\"date\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tinput_dict = input()<br>date= input_dict[\"date\"]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\timport sys<br>date = sys.argv[1]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdate = dbutils.notebooks.getParam(\"date\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbutils.widgets.text(\"date\", \"null\")<br>date = dbutils.widgets.get(\"date\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-25T01:29:00.000Z",
        "voteCount": 7,
        "content": "dbutils.widget.\nJust passed the exam with score &gt;80%.\nexamtopics covers about 90% of questions. there were 5 questions I didn't see here in examtopics.\nBut friends, you need to look at the discussions, and do test yourself.\nmany answers provided here, even most voted answer, does NOT exists anymore in the exam - not the question, but the answer.\nWish you all good luck, friends!"
      },
      {
        "date": "2024-09-28T05:15:00.000Z",
        "voteCount": 1,
        "content": "Jobs API allows to sending parameters via jobs parameter. This parameter must have the same notebook params. Eventually, it can be read using dbutils.widgets.get"
      },
      {
        "date": "2024-08-13T01:10:00.000Z",
        "voteCount": 2,
        "content": "E is correct because:\nA - gets configuration of a spark session\nB - gets a value from a manual input - non relevant for the job run\nC - sys.argv - gets parameters, which were used to run a Pyrhon script from CMD - completely non-related\nD - haven't found this function on the web at all, assume that it doesn't exist\nTherefore E is correct, though it's a bad practice to type a date as a parameter, it's better to get it with datetime library and then use it in the code"
      },
      {
        "date": "2024-07-13T20:00:00.000Z",
        "voteCount": 1,
        "content": "Answer is E.\nEven though the value is passed from a upstream system, you can create parameters using widgets inside notebook and use the value as an input from the databricks jobs API."
      },
      {
        "date": "2024-06-08T09:04:00.000Z",
        "voteCount": 1,
        "content": "Widgets are used to create parameters in notebook that can be then utilized by e.g. jobs"
      },
      {
        "date": "2024-05-31T20:20:00.000Z",
        "voteCount": 1,
        "content": "E.\nE. dbutils.widgets.text(\"date\", \"null\")\ndate = dbutils.widgets.get(\"date\")"
      },
      {
        "date": "2024-04-19T08:34:00.000Z",
        "voteCount": 1,
        "content": "correct ans is E"
      },
      {
        "date": "2024-04-14T02:36:00.000Z",
        "voteCount": 1,
        "content": "Are you reading the question? It asks about an upstream system that has been configured to pass the date for a given batch of data to the Databricks Jobs API as a parameter. Upstream system usually don't use widgets. Widgets they are made for humans. Only C and D are correct but D is better so D."
      },
      {
        "date": "2024-02-26T04:37:00.000Z",
        "voteCount": 1,
        "content": "vote for E\ndbutils.widget"
      },
      {
        "date": "2024-01-21T11:28:00.000Z",
        "voteCount": 1,
        "content": "Correct Ans is E"
      },
      {
        "date": "2024-01-13T05:07:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2024-01-04T00:54:00.000Z",
        "voteCount": 2,
        "content": "In https://docs.databricks.com/en/notebooks/notebook-workflows.html#dbutilsnotebook-api the \"run Example\" is an equivalent use-case as E."
      },
      {
        "date": "2023-12-21T08:20:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2023-10-07T00:06:00.000Z",
        "voteCount": 1,
        "content": "I think D is correct answer, refer to https://docs.databricks.com/en/notebooks/notebook-workflows.html#dbutilsnotebook-api"
      },
      {
        "date": "2023-08-14T16:20:00.000Z",
        "voteCount": 3,
        "content": "E is correct answer"
      },
      {
        "date": "2023-09-16T02:45:00.000Z",
        "voteCount": 3,
        "content": "Did you take exam? Are these questions valid?"
      },
      {
        "date": "2023-08-10T05:41:00.000Z",
        "voteCount": 1,
        "content": "Correct. Ans: E"
      },
      {
        "date": "2023-08-02T03:03:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/databricks/view/117384-exam-certified-data-engineer-professional-topic-1-question-2/",
    "body": "The Databricks workspace administrator has configured interactive clusters for each of the data engineering groups. To control costs, clusters are set to terminate after 30 minutes of inactivity. Each user should be able to execute workloads against their assigned clusters at any time of the day.<br>Assuming users have been added to a workspace but not granted any permissions, which of the following describes the minimal permissions a user would need to start and attach to an already configured cluster.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"Can Manage\" privileges on the required cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorkspace Admin privileges, cluster creation allowed, \"Can Attach To\" privileges on the required cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster creation allowed, \"Can Attach To\" privileges on the required cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"Can Restart\" privileges on the required cluster\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster creation allowed, \"Can Restart\" privileges on the required cluster"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-28T05:36:00.000Z",
        "voteCount": 1,
        "content": "Questions is users need to start &amp; use so it will be Can restrat . Can attach cannot start compute"
      },
      {
        "date": "2024-09-09T08:43:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer. Focus on this line \"user would need to start and attach to an already configured cluster.\""
      },
      {
        "date": "2024-07-13T09:28:00.000Z",
        "voteCount": 1,
        "content": "COrrect"
      },
      {
        "date": "2024-06-23T18:28:00.000Z",
        "voteCount": 1,
        "content": "D is the Correct Answer"
      },
      {
        "date": "2024-05-27T16:09:00.000Z",
        "voteCount": 1,
        "content": "D is correct. Not A."
      },
      {
        "date": "2024-05-19T08:47:00.000Z",
        "voteCount": 3,
        "content": "\"Can restart\" permission is only required. \"Can Manage\" permission gives the ability to edit the configurations of the cluster.\nhttps://docs.databricks.com/en/security/auth-authz/access-control/cluster-acl.html"
      },
      {
        "date": "2024-05-19T08:21:00.000Z",
        "voteCount": 1,
        "content": "Answer should be D not A"
      },
      {
        "date": "2024-04-28T18:44:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-04-19T11:13:00.000Z",
        "voteCount": 1,
        "content": "Correct Ans is D"
      },
      {
        "date": "2024-01-21T11:33:00.000Z",
        "voteCount": 2,
        "content": "Correct Ans is D"
      },
      {
        "date": "2024-01-13T05:08:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-12-21T08:21:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-11-20T16:14:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/security/auth-authz/access-control/cluster-acl.html"
      },
      {
        "date": "2023-11-19T10:53:00.000Z",
        "voteCount": 1,
        "content": "D-Can restart is is minimum permission to attach and start the cluster. For more information. Read this page https://docs.databricks.com/en/security/auth-authz/access-control/cluster-acl.html"
      },
      {
        "date": "2023-11-02T10:45:00.000Z",
        "voteCount": 1,
        "content": "Option D is correct. 'Can Restart' privilege is required"
      },
      {
        "date": "2023-10-24T01:17:00.000Z",
        "voteCount": 1,
        "content": "Option D is the right answer"
      },
      {
        "date": "2023-10-11T00:04:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/cluster-acl"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/databricks/view/117385-exam-certified-data-engineer-professional-topic-1-question-3/",
    "body": "When scheduling Structured Streaming jobs for production, which configuration automatically recovers from query failures and keeps costs low?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster: New Job Cluster;<br>Retries: Unlimited;<br>Maximum Concurrent Runs: Unlimited",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster: New Job Cluster;<br>Retries: None;<br>Maximum Concurrent Runs: 1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster: Existing All-Purpose Cluster;<br>Retries: Unlimited;<br>Maximum Concurrent Runs: 1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster: New Job Cluster;<br>Retries: Unlimited;<br>Maximum Concurrent Runs: 1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster: Existing All-Purpose Cluster;<br>Retries: None;<br>Maximum Concurrent Runs: 1"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-04T23:43:00.000Z",
        "voteCount": 7,
        "content": "the answer given is correct:\nMaximum concurrent runs: Set to 1. There must be only one instance of each query concurrently active.\nRetries: Set to Unlimited.\nhttps://docs.databricks.com/en/structured-streaming/query-recovery.html"
      },
      {
        "date": "2024-09-28T05:45:00.000Z",
        "voteCount": 2,
        "content": "Use databricks jobs as it as native integration with Streaming use case.  See the example Job here https://docs.databricks.com/en/structured-streaming/query-recovery.html#configure-structured-streaming-jobs-to-restart-streaming-queries-on-failure"
      },
      {
        "date": "2024-06-04T18:29:00.000Z",
        "voteCount": 1,
        "content": "D. Cluster: New Job Cluster;\nRetries: Unlimited;\nMaximum Concurrent Runs: 1"
      },
      {
        "date": "2024-05-31T20:26:00.000Z",
        "voteCount": 1,
        "content": "D. Cluster: New Job Cluster;\nRetries: Unlimited;\nMaximum Concurrent Runs: 1"
      },
      {
        "date": "2024-04-08T08:35:00.000Z",
        "voteCount": 1,
        "content": "D is correct\nhttps://docs.databricks.com/en/structured-streaming/query-recovery.html"
      },
      {
        "date": "2024-01-21T11:35:00.000Z",
        "voteCount": 1,
        "content": "Correct Ans is D"
      },
      {
        "date": "2024-01-13T05:08:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-12-21T08:22:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-10-15T22:49:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/databricks/view/121876-exam-certified-data-engineer-professional-topic-1-question-4/",
    "body": "The data engineering team has configured a Databricks SQL query and alert to monitor the values in a Delta Lake table. The recent_sensor_recordings table contains an identifying sensor_id alongside the timestamp and temperature for the most recent 5 minutes of recordings.<br>The below query is used to create the alert:<br><img title=\"image1\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image1.png\"><br>The query is set to refresh each minute and always completes in less than 10 seconds. The alert is set to trigger when mean (temperature) &gt; 120. Notifications are triggered to be sent at most every 1 minute.<br>If this alert raises notifications for 3 consecutive minutes and then stops, which statement must be true?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total average temperature across all sensors exceeded 120 on three consecutive executions of the query",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe recent_sensor_recordings table was unresponsive for three consecutive runs of the query",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe source query failed to update properly for three consecutive minutes and then restarted",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe maximum temperature recording for at least one sensor exceeded 120 on three consecutive executions of the query",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe average temperature recordings for at least one sensor exceeded 120 on three consecutive executions of the query\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-19T00:48:00.000Z",
        "voteCount": 1,
        "content": "A excluded because there is a group by clause\nB &amp; C excluded table needs to be updated to mean value to change \nD excluded, because alert is set on average not max temperature\nCorrect answer is E by elimination"
      },
      {
        "date": "2024-06-23T18:37:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-06-04T18:29:00.000Z",
        "voteCount": 1,
        "content": "E. The average temperature recordings for at least one sensor exceeded 120 on three consecutive executions of the query"
      },
      {
        "date": "2024-01-13T05:10:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-10-11T00:08:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-09-30T05:43:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is E\nhttps://www.myexamcollection.com/databricks-certified-professional-data-engineer-databricks-certified-professional-data-engineer-exam-question-answers.htm"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/databricks/view/123733-exam-certified-data-engineer-professional-topic-1-question-5/",
    "body": "A junior developer complains that the code in their notebook isn't producing the correct results in the development environment. A shared screenshot reveals that while they're using a notebook versioned with Databricks Repos, they're using a personal branch that contains old logic. The desired branch named dev-2.3.9 is not available from the branch selection dropdown.<br>Which approach will allow this developer to review the current logic for this notebook?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to make a pull request use the Databricks REST API to update the current branch to dev-2.3.9",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to checkout the dev-2.3.9 branch and auto-resolve conflicts with the current branch",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge all changes back to the main branch in the remote Git repository and clone the repo again",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to merge the current branch and the dev-2.3.9 branch, then make a pull request to sync with the remote repository"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-07T23:18:00.000Z",
        "voteCount": 1,
        "content": "I would also say B but could anyone explain how to pick that branch if it is not available from dropdown?"
      },
      {
        "date": "2024-09-19T23:37:00.000Z",
        "voteCount": 1,
        "content": "I would say B"
      },
      {
        "date": "2024-06-04T18:29:00.000Z",
        "voteCount": 1,
        "content": "B. Use Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch."
      },
      {
        "date": "2024-01-21T11:46:00.000Z",
        "voteCount": 1,
        "content": "correct ans is B"
      },
      {
        "date": "2024-01-13T05:13:00.000Z",
        "voteCount": 2,
        "content": "vote for B also"
      },
      {
        "date": "2023-10-15T22:51:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/databricks/view/117453-exam-certified-data-engineer-professional-topic-1-question-6/",
    "body": "The security team is exploring whether or not the Databricks secrets module can be leveraged for connecting to an external database.<br>After testing the code with all Python variables being defined with strings, they upload the password to the secrets module and configure the correct permissions for the currently active user. They then modify their code to the following (leaving all other variables unchanged).<br><img title=\"image2\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image2.png\"><br>Which statement describes what will happen when the above code is executed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe connection to the external table will fail; the string \"REDACTED\" will be printed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the encoded password will be saved to DBFS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the password will be printed in plain text.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe connection to the external table will succeed; the string value of password will be printed in plain text.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe connection to the external table will succeed; the string \"REDACTED\" will be printed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-07T23:21:00.000Z",
        "voteCount": 1,
        "content": "Shave like a bomber"
      },
      {
        "date": "2024-09-28T06:08:00.000Z",
        "voteCount": 2,
        "content": "Whatever we read using dbutls.secret module is always printed as '[REDACTED]', but when consumed in code, underlying vales are passed."
      },
      {
        "date": "2024-06-04T18:30:00.000Z",
        "voteCount": 1,
        "content": "E. The connection to the external table will succeed; the string \"REDACTED\" will be printed."
      },
      {
        "date": "2024-02-08T04:50:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-01-21T11:48:00.000Z",
        "voteCount": 1,
        "content": "correct ans is E"
      },
      {
        "date": "2024-01-13T05:14:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2024-01-05T22:01:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2023-12-21T08:34:00.000Z",
        "voteCount": 2,
        "content": "Correct answer E"
      },
      {
        "date": "2023-10-11T00:14:00.000Z",
        "voteCount": 4,
        "content": "Correct: https://docs.databricks.com/en/external-data/jdbc.html"
      },
      {
        "date": "2023-08-14T02:15:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/azure/databricks/security/secrets/redaction\n\nOption E - which is selected answer seems correct."
      },
      {
        "date": "2023-08-06T00:28:00.000Z",
        "voteCount": 1,
        "content": "This option is correct, although the password won't be printed out, the connection will still succeed."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/databricks/view/119221-exam-certified-data-engineer-professional-topic-1-question-7/",
    "body": "The data science team has created and logged a production model using MLflow. The following code correctly imports and applies the production model to output the predictions as a new DataFrame named preds with the schema \"customer_id LONG, predictions DOUBLE, date DATE\".<br><img title=\"image3\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image3.png\"><br>The data science team would like predictions saved to a Delta Lake table with the ability to compare all predictions across time. Churn predictions will be made at most once per day.<br>Which code block accomplishes this task while minimizing potential compute costs?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpreds.write.mode(\"append\").saveAsTable(\"churn_preds\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpreds.write.format(\"delta\").save(\"/preds/churn_preds\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image4\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image4.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image5\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image5.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image6\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image6.png\">"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-07T04:00:00.000Z",
        "voteCount": 12,
        "content": "You need:\n- Batch operation since it is at most once a day\n- Append, since you need to keep track of past predictions\n\nA is the correct answer. You don't need to specify \"format\" when you use saveAsTable."
      },
      {
        "date": "2024-10-07T23:25:00.000Z",
        "voteCount": 1,
        "content": "Batch, Append"
      },
      {
        "date": "2024-05-19T08:55:00.000Z",
        "voteCount": 1,
        "content": "default table format is delta so no need to specify the format.\nAs per the requirement, \"append\" mode is required to maintain the history. Default mode is \"ErrorIfExists\""
      },
      {
        "date": "2024-01-13T05:15:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-12-21T08:36:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-10-11T00:18:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-10-11T00:17:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-09-21T08:37:00.000Z",
        "voteCount": 2,
        "content": "answer is B"
      },
      {
        "date": "2023-10-09T13:24:00.000Z",
        "voteCount": 3,
        "content": "First: the default node Databricks saves tables IS Delta Format. So no reason why you say it wouldn't benefit from Lakehouse features.\nSecond: the default write mode is Error, means that if you try to write to a location and that already exists there, it will prone a Error. And the question specify that you gonna write once a day.\nYou better revisit basic topics before continue to the professional level certification, or buy the dump entirely."
      },
      {
        "date": "2023-09-21T08:39:00.000Z",
        "voteCount": 3,
        "content": "Here's why:\nA. saves the data as a managed table, which may not be efficient for large-scale data or frequent updates. It doesn't utilize Delta Lake capabilities.\nC.is used for streaming operations, not batch processing. Also, using \"overwrite\" as output mode will replace the existing data each time, which is not suitable for keeping historical predictions.\nD.is similar to option A but with \"overwrite\" mode. It will replace the entire table each time, which is not suitable for maintaining a historical record of predictions.\n\nE. is also for streaming operations and not for batch processing. Additionally, it uses the \"table\" method, which is not typically used for writing batch data into Delta Lake tables.\nOption B is suitable for batch processing, writes data in Delta Lake format, and allows you to efficiently maintain a historical record of predictions while minimizing compute costs."
      },
      {
        "date": "2023-10-23T04:51:00.000Z",
        "voteCount": 4,
        "content": "Its also said they want to compare past values as well, so mode needs to be append. By default is error mode."
      },
      {
        "date": "2023-08-28T03:48:00.000Z",
        "voteCount": 1,
        "content": "Selected answer is wrong, not write Format is specified in A."
      },
      {
        "date": "2023-08-28T03:47:00.000Z",
        "voteCount": 1,
        "content": "Selected answer is wrong, not writeMode is specified in A."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/databricks/view/120171-exam-certified-data-engineer-professional-topic-1-question-8/",
    "body": "An upstream source writes Parquet data as hourly batches to directories named with the current date. A nightly batch job runs the following code to ingest all data from the previous day as indicated by the date variable:<br><img title=\"image7\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image7.png\"><br>Assume that the fields customer_id and order_id serve as a composite key to uniquely identify each order.<br>If the upstream system is known to occasionally produce duplicate entries for a single order hours apart, which statement is correct?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach write to the orders table will only contain unique records, and only those records without duplicates in the target table will be written.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach write to the orders table will only contain unique records; if existing records with the same key are present in the target table, these records will be overwritten.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach write to the orders table will only contain unique records; if existing records with the same key are present in the target table, the operation will fail.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach write to the orders table will run deduplication over the union of new and existing records, ensuring no duplicate records are present."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-21T08:44:00.000Z",
        "voteCount": 11,
        "content": "B. Each write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table.\n\nExplanation:\n\nIn the provided code, the .dropDuplicates([\"customer_id\",\"order_id\"]) operation is performed on the data loaded from the Parquet files. This operation ensures that only unique records, based on the composite key of \"customer_id\" and \"order_id,\" are retained in the DataFrame before writing to the \"orders\" table.\n\nHowever, this operation does not consider duplicates that may already exist in the \"orders\" table. It only filters duplicates from the current batch of data. If there are duplicates in the \"orders\" table from previous batches, they will remain in the table.\n\nSo, newly written records will not have duplicates within the batch being written, but duplicates from previous batches may still exist in the target table."
      },
      {
        "date": "2024-09-19T23:49:00.000Z",
        "voteCount": 1,
        "content": "Append method does not take in consideration any key in the target table, it simply add all rows of the input table to the target table."
      },
      {
        "date": "2024-06-24T06:00:00.000Z",
        "voteCount": 1,
        "content": "Yes it should be B"
      },
      {
        "date": "2024-06-04T18:31:00.000Z",
        "voteCount": 1,
        "content": "B. Each write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table.\n\nUsing merge this problem would not happen"
      },
      {
        "date": "2024-03-09T02:28:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer. The above code only remove duplicates from the batch that is processed, no logic is applied to already saved records."
      },
      {
        "date": "2024-01-13T05:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-01T02:59:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2023-12-21T08:43:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-30T10:57:00.000Z",
        "voteCount": 1,
        "content": "correct B"
      },
      {
        "date": "2023-10-11T00:23:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-10-09T13:26:00.000Z",
        "voteCount": 1,
        "content": "Correct. B"
      },
      {
        "date": "2023-09-07T04:03:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/databricks/view/119186-exam-certified-data-engineer-professional-topic-1-question-9/",
    "body": "A junior member of the data engineering team is exploring the language interoperability of Databricks notebooks. The intended outcome of the below code is to register a view of all sales that occurred in countries on the continent of Africa that appear in the geo_lookup table.<br>Before executing the code, running SHOW TABLES on the current database indicates the database contains only two tables: geo_lookup and sales.<br><img title=\"image8\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image8.png\"><br>Which statement correctly describes the outcome of executing these command cells in order in an interactive notebook?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBoth commands will succeed. Executing show tables will show that countries_af and sales_af have been registered as views.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 1 will succeed. Cmd 2 will search all accessible databases for a table or view named countries_af: if this entity exists, Cmd 2 will succeed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable representing a PySpark DataFrame.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBoth commands will fail. No new variables, tables, or views will be created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-20T16:21:00.000Z",
        "voteCount": 7,
        "content": "Cmd 1 is a PySpark command that collects the list of countries from the 'geo_lookup' table where the continent is Africa ('AF'). This command will execute successfully, resulting in countries_af being a list of country names (strings) in Python's local memory.\n\nCmd 2 is an SQL command intended to create a view named 'sales_af' from the 'sales' table, filtered by the cities in the countries_af list. However, this will fail because the countries_af variable exists in the Python environment and is not recognized in the SQL context. SQL does not have access to Python variables directly; they are two separate execution contexts within a Databricks notebook. There is no table or view named countries_af that SQL can reference; it is merely a Python list variable.\n\nThe other options are incorrect because they either assume cross-contextual operation between Python and SQL within a Databricks notebook (which is not possible in the way described in the commands), or they do not correctly interpret the outcome of running the commands."
      },
      {
        "date": "2024-10-03T04:32:00.000Z",
        "voteCount": 1,
        "content": "E , the collect method outputs strings so the python variable bill be a list of string which should not be called as a spark table as in cmd 2"
      },
      {
        "date": "2024-06-04T18:32:00.000Z",
        "voteCount": 1,
        "content": "E. Cmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings."
      },
      {
        "date": "2024-04-08T12:00:00.000Z",
        "voteCount": 3,
        "content": "E is correct.\n\n%sql\ncreate table geo_lookup (continent varchar(2), country varchar(15));\ninsert into geo_lookup (continent, country) values\n('AF','Nigeria'),\n('AF','Kenya');\ncreate table sales (city varchar(15), continent varchar(2));\ninsert into sales (city, continent) values\n('Nigeria','AF'),\n('Kenya','AF');\n\n%python\ncountries_af = [x[0] for x in spark.table('geo_lookup').filter(\"continent='AF'\").select('country').collect()]\n\n%sql\ncreate view sales_af as\nselect *\nfrom sales\nwhere city in countries_af\nand continent = \"AF\";\n\nParseException: [PARSE_SYNTAX_ERROR] Syntax error at or near 'in'.(line 4, pos 11)\n\ni.e. countries_af is a python list of strings and can't be used inside a sql statement"
      },
      {
        "date": "2024-08-19T07:23:00.000Z",
        "voteCount": 1,
        "content": "%python\nprint(countries_af)\ntype(countries_af)"
      },
      {
        "date": "2024-02-14T05:56:00.000Z",
        "voteCount": 1,
        "content": "By simulating this code in databricks we can see an error being thrown in the SQL statement\n\nParseException: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'IN'.(line 1, pos 38)\n\n== SQL ==\nSELECT * FROM backup.sales WHERE CITY IN countries_af AND CONTINENT = \"AF\""
      },
      {
        "date": "2024-02-09T23:03:00.000Z",
        "voteCount": 1,
        "content": "B shows the actual flow of spark sql, where E shows the question context, i mean from databricks point of view E never looked, it's true that question state that database has no other tables, so ?? that mean databricks will not check for that particular table ? it will right ? i also confused by \"database has no other database statement\" and E and B both are right, but again B state \"if countries table exists then command 2 will run\" here \"if\" used, but question want to describe the language interoperability, so most of us selected E"
      },
      {
        "date": "2024-09-20T00:01:00.000Z",
        "voteCount": 1,
        "content": "how could it succed if all people tested sql parse syntax error?"
      },
      {
        "date": "2024-02-08T05:06:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2024-01-13T05:42:00.000Z",
        "voteCount": 2,
        "content": "vote for E"
      },
      {
        "date": "2024-01-10T05:54:00.000Z",
        "voteCount": 1,
        "content": "E is correct answer"
      },
      {
        "date": "2023-11-04T04:55:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/notebooks/notebooks-code.html#mix-languages\nVariables defined in one language (and hence in the REPL for that language) are not available in the REPL of another language"
      },
      {
        "date": "2023-11-26T23:28:00.000Z",
        "voteCount": 1,
        "content": "It is mentioned there exists only 2 objects in database. so B is not an option"
      },
      {
        "date": "2023-11-04T16:00:00.000Z",
        "voteCount": 2,
        "content": "even if it exists, a table or a view won't work in cmd 2"
      },
      {
        "date": "2023-10-11T00:27:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-09-06T08:10:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-08-27T09:35:00.000Z",
        "voteCount": 2,
        "content": "E is right nswer"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/databricks/view/117162-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Delta table of weather records is partitioned by date and has the below schema: date DATE, device_id INT, temp FLOAT, latitude FLOAT, longitude FLOAT<br>To find all the records from within the Arctic Circle, you execute a query with the below filter: latitude &gt; 66.3<br>Which statement describes how the Delta engine identifies which files to load?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll records are cached to an operational database and then the filter is applied",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Parquet file footers are scanned for min and max statistics for the latitude column",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll records are cached to attached storage and then the filter is applied",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Delta log is scanned for min and max statistics for the latitude column\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Hive metastore is scanned for min and max statistics for the latitude column"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-23T04:50:00.000Z",
        "voteCount": 21,
        "content": "Answer D:\n\nIn the Transaction log, Delta Lake captures statistics for each data file of the table. These statistics indicate per file:\n - Total number of records\n - Minimum value in each column of the first 32 columns of the table\n - Maximum value in each column of the first 32 columns of the table\n - Null value counts for in each column of the first 32 columns of the table\n\nWhen a query with a selective filter is executed against the table, the query optimizer uses these statistics to generate the query result. it leverages them to identify data files that may contain records matching the conditional filter.\nFor the SELECT query in the question, The transaction log is scanned for min and max statistics for the price column"
      },
      {
        "date": "2024-09-28T08:32:00.000Z",
        "voteCount": 1,
        "content": "Above mentioned points are correct. If the table was just parquet table then parquet file footer have been used. But since this is Delta table, then delta log is used to scan &amp; skip files. It uses stats written in in transaction log."
      },
      {
        "date": "2024-08-19T07:56:00.000Z",
        "voteCount": 1,
        "content": "Answer D : \n\nDelta data skipping automatically collects the stats (min, max, etc.) for the first 32 columns for each underlying Parquet file when you write data into a Delta table. Databricks takes advantage of this information (minimum and maximum values) at query time to skip unnecessary files in order to speed up the queries.\n\nhttps://www.databricks.com/discover/pages/optimize-data-workloads-guide#delta-data"
      },
      {
        "date": "2024-08-16T23:01:00.000Z",
        "voteCount": 1,
        "content": "Delta table stores file statistics in transaction log"
      },
      {
        "date": "2024-06-26T11:53:00.000Z",
        "voteCount": 2,
        "content": "No explanation needed, this is where the information is stored."
      },
      {
        "date": "2024-06-04T18:32:00.000Z",
        "voteCount": 1,
        "content": "D. The Delta log is scanned for min and max statistics for the latitude column"
      },
      {
        "date": "2024-05-19T09:21:00.000Z",
        "voteCount": 1,
        "content": "Delta log collects statistics like min value, max value, no of records, no of files for each transaction that happens on the table for the first 32 columns (default value)"
      },
      {
        "date": "2024-04-30T14:32:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2024-03-26T09:03:00.000Z",
        "voteCount": 1,
        "content": "Based on Docu is D I don't know why here is showing B"
      },
      {
        "date": "2024-03-12T23:09:00.000Z",
        "voteCount": 1,
        "content": "Delta log first"
      },
      {
        "date": "2024-03-10T07:20:00.000Z",
        "voteCount": 1,
        "content": "Statistics on first 32 columns of a table are computed and written in the Delta Log by default."
      },
      {
        "date": "2024-02-29T04:12:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      },
      {
        "date": "2024-02-27T14:14:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2024-02-16T14:03:00.000Z",
        "voteCount": 1,
        "content": "D is correct one"
      },
      {
        "date": "2024-02-10T01:03:00.000Z",
        "voteCount": 2,
        "content": "I checked the delta log, and it dose store stat, stats\":\"{\\\"numRecords\\\":1,\\\"minValues\\\":{\\\"id\\\":1,\\\"name\\\":\\\"one\\\",\\\"age\\\":11},\\\"maxValues\\\":{\\\"id\\\":1,\\\"name\\\":\\\"one\\\",\\\"age\\\":11},\\\"nullCount\\\":{\\\"id\\\":0,\\\"name\\\":0,\\\"age\\\":0}}\""
      },
      {
        "date": "2024-01-21T12:15:00.000Z",
        "voteCount": 1,
        "content": "correct ans is D"
      },
      {
        "date": "2024-01-13T05:43:00.000Z",
        "voteCount": 1,
        "content": "D for sure"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/databricks/view/117060-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineering team has configured a job to process customer requests to be forgotten (have their data deleted). All user data that needs to be deleted is stored in Delta Lake tables using default table settings.<br>The team has decided to process all deletions from the previous week as a batch job at 1am each Sunday. The total duration of this job is less than one hour. Every Monday at 3am, a batch job executes a series of VACUUM commands on all Delta Lake tables throughout the organization.<br>The compliance officer has recently learned about Delta Lake's time travel functionality. They are concerned that this might allow continued access to deleted data.<br>Assuming all delete logic is correctly implemented, which statement correctly addresses this concern?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the VACUUM command permanently deletes all files containing deleted records, deleted records may be accessible with time travel for around 24 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the default data retention threshold is 24 hours, data files containing deleted records will be retained until the VACUUM job is run the following day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause Delta Lake time travel provides full access to the entire history of a table, deleted records can always be recreated by users with full admin privileges.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause Delta Lake's delete statements have ACID guarantees, deleted records will be permanently purged from all storage systems as soon as a delete job completes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-02T00:46:00.000Z",
        "voteCount": 14,
        "content": "Answer is E, default retention period is 7 days https://learn.microsoft.com/en-us/azure/databricks/delta/vacuum"
      },
      {
        "date": "2024-02-26T19:34:00.000Z",
        "voteCount": 8,
        "content": "The answer has to be A.\nThe deletion is done on Sunday 1am and then the next day Monday 3am, VACUUM was initiated, so one can only time travel for about 24 hours."
      },
      {
        "date": "2024-09-05T18:46:00.000Z",
        "voteCount": 4,
        "content": "The default retention threshold for time travel is 7 days. VACUUM which is executed on Monday 3 am will remove history for changes where time travel has expired for previous 7 days."
      },
      {
        "date": "2024-10-14T23:55:00.000Z",
        "voteCount": 1,
        "content": "Exactly!"
      },
      {
        "date": "2024-10-14T23:56:00.000Z",
        "voteCount": 1,
        "content": "Default retention period is 7 days so the vacuum command won't delete the files corresponding to deleted rows at Sunday 1 am  but the ones of the previous week instead."
      },
      {
        "date": "2024-10-01T22:22:00.000Z",
        "voteCount": 2,
        "content": "Delta Lake's default retention threshold for old data files (which allows time travel) is 7 days. This means that even after records are deleted, the files that previously contained those records are kept for 7 days before they are eligible for permanent deletion by the VACUUM command.\nThe VACUUM command is responsible for permanently deleting the old data files after the retention period. Since the job runs every Monday, this means that data deleted during the previous week will not be fully purged until after the retention period has passed (which would be 8 days after the deletion, considering the weekly processing)."
      },
      {
        "date": "2024-09-28T09:08:00.000Z",
        "voteCount": 1,
        "content": "Delete job is running as batch job for all requests made current week on Sunday &amp; Vacuum is ran next day . Since there is no mention of change is retention period then it is 7 days. Vacuum will delete data older than 7 days, i.e. it will delete data of previous week &amp; not current week. Current weeks data will be removed in next week\u2019s vacuum job."
      },
      {
        "date": "2024-08-14T10:34:00.000Z",
        "voteCount": 3,
        "content": "From the documentation.\n\"The default retention threshold for data files after running VACUUM is 7 days.\" \nIt doesn't matter if VACUUM is ran the following day, the retention period on a default setup is still 7 days after they do the VACUUM on Monday."
      },
      {
        "date": "2024-06-26T20:55:00.000Z",
        "voteCount": 3,
        "content": "They expect the deleted records for the previous week to be deleted Sunday from 1am to 2am. Then the next day(Monday) at 3am approx 24hrs later, the vacuum command is ran. This means the records from the previous week are only around for 24ish hours before they are removed with the vacuum command. They aren't waiting 8 days to run the command, there fore E is wrong."
      },
      {
        "date": "2024-09-28T09:09:00.000Z",
        "voteCount": 1,
        "content": "This week's vacuum will remove data of the previous week's delete command since default retention has not changed."
      },
      {
        "date": "2024-06-04T18:32:00.000Z",
        "voteCount": 1,
        "content": "E. Because the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later."
      },
      {
        "date": "2024-05-19T09:29:00.000Z",
        "voteCount": 1,
        "content": "Default retention period is 7 days so newly deleted data on Sunday will be available for next 7 days (even if vacuum was run on Monday as it will delete 7 days old data and not the data that was loaded yesterday \"Sunday\" )"
      },
      {
        "date": "2024-04-30T14:37:00.000Z",
        "voteCount": 1,
        "content": "The default retention threshold for data files after running VACUUM is 7 days."
      },
      {
        "date": "2024-04-09T04:56:00.000Z",
        "voteCount": 1,
        "content": "Answer is E"
      },
      {
        "date": "2024-04-08T13:23:00.000Z",
        "voteCount": 2,
        "content": "Si bien la data es borrada (DELETE) el domingo, a\u00fan se puede recuperar ella mediante time traveling, s\u00f3lo el d\u00eda siguiente (lunes) se eliminar\u00e1 esta posibilidad debido a que se ejecuta el VACUUM, en consecuencia la data se podr\u00e1 recuperar en ese lapso de 24 horas aprox"
      },
      {
        "date": "2024-02-10T01:29:00.000Z",
        "voteCount": 1,
        "content": "if i v0: create table, v1: insert 2 reocrds, v2: insert 2 record, v3: delete 2 records, and then run the vacuum command (with default 7 day retention), the delete records will be there and you can access using SELECT * FROM delta_table VERSION AS OF 2;"
      },
      {
        "date": "2024-01-24T10:01:00.000Z",
        "voteCount": 1,
        "content": "Answer is E"
      },
      {
        "date": "2024-01-10T06:00:00.000Z",
        "voteCount": 1,
        "content": "Answer is E"
      },
      {
        "date": "2024-01-10T05:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is E as the default retention period is 7 days"
      },
      {
        "date": "2024-01-09T00:35:00.000Z",
        "voteCount": 1,
        "content": "Correct according to the documentation: https://docs.databricks.com/en/sql/language-manual/delta-vacuum.html"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/databricks/view/120855-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer has configured a workload that posts the following JSON to the Databricks REST API endpoint 2.0/jobs/create.<br><img title=\"image9\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image9.png\"><br>Assuming that all configurations and referenced resources are available, which statement describes the result of executing this workload three times?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThree new jobs named \"Ingest new data\" will be defined in the workspace, and they will each run once daily.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe logic defined in the referenced notebook will be executed three times on new clusters with the configurations of the provided cluster ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThree new jobs named \"Ingest new data\" will be defined in the workspace, but no jobs will be executed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOne new job named \"Ingest new data\" will be defined in the workspace, but it will not be executed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe logic defined in the referenced notebook will be executed three times on the referenced existing all purpose cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-28T09:22:00.000Z",
        "voteCount": 1,
        "content": "Databricks will keep on creating new jobs if you keep running create rept api. Each will have the same name but a different ID.  Also no trigger/schedule is mentioned so they wont run."
      },
      {
        "date": "2024-06-04T18:33:00.000Z",
        "voteCount": 1,
        "content": "C. Three new jobs named \"Ingest new data\" will be defined in the workspace, but no jobs will be executed."
      },
      {
        "date": "2024-05-19T09:34:00.000Z",
        "voteCount": 2,
        "content": "Learnt new thing : DBX can have duplicate job names (Job ID will be different). So three jobs will be created with three job ids but it will not run as no schedule is mentioned."
      },
      {
        "date": "2024-03-06T09:25:00.000Z",
        "voteCount": 1,
        "content": "C for sure"
      },
      {
        "date": "2024-01-24T10:04:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C"
      },
      {
        "date": "2024-01-13T05:49:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-01-10T06:05:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C"
      },
      {
        "date": "2023-10-11T00:53:00.000Z",
        "voteCount": 3,
        "content": "databricks jobs create will create a new job with the same name each time it is run.\nIn order to overwrite the extsting job you need to run databricks jobs reset"
      },
      {
        "date": "2023-09-25T05:37:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct. The 3 API calls create 3 jobs with the same name but different job ids. There is no schedule defined so will not execute."
      },
      {
        "date": "2023-09-21T08:59:00.000Z",
        "voteCount": 2,
        "content": "correct answer is A, because an api can create can create same job with same name if executed thrice"
      },
      {
        "date": "2023-10-09T13:31:00.000Z",
        "voteCount": 1,
        "content": "Ok. So Tell me the schedule these Jobs will run?\nDont know? Why? Maybe because it is not specified, or even configured. So the answer is correct. Create 3 Jobs but none will be executed."
      },
      {
        "date": "2023-09-21T09:04:00.000Z",
        "voteCount": 1,
        "content": "to add more: when you execute this workload three times, it will define three new jobs in the workspace, each with the name \"Ingest new data.\" These jobs can be scheduled to run daily or at a specified frequency, depending on how they are configured."
      },
      {
        "date": "2023-09-17T00:45:00.000Z",
        "voteCount": 2,
        "content": "therefore answer: D"
      },
      {
        "date": "2023-09-17T00:45:00.000Z",
        "voteCount": 1,
        "content": "only one job, why 3? - because there 3 lines of JSON??\nanswer should be: \nOne new job named \"Ingest new data\" will be defined in the workspace, but it will not be executed."
      },
      {
        "date": "2023-09-17T10:45:00.000Z",
        "voteCount": 2,
        "content": "Because the create command was run 3 times. Databricks can have several jobs with the same name"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/databricks/view/119187-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "An upstream system is emitting change data capture (CDC) logs that are being written to a cloud object storage directory. Each record in the log indicates the change type (insert, update, or delete) and the values for each field after the change. The source table has a primary key identified by the field pk_id.<br>For auditing purposes, the data governance team wishes to maintain a full record of all values that have ever been valid in the source system. For analytical purposes, only the most recent value for each record needs to be recorded. The Databricks job to ingest these records occurs once per hour, but each individual record may have changed multiple times over the course of an hour.<br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate history table for each pk_id resolve the current state of the table by running a union all filtering the history tables for the most recent state.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse MERGE INTO to insert, update, or delete the most recent entry for each pk_id into a bronze table, then propagate all changes throughout the system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIterate through an ordered set of changes to the table, applying each in turn; rely on Delta Lake's versioning ability to create an audit log.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Delta Lake's change data feed to automatically process CDC data from an external system, propagating all changes to all dependent tables in the Lakehouse.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest all log information into a bronze table; use MERGE INTO to insert, update, or delete the most recent entry for each pk_id into a silver table to recreate the current table state.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-27T10:06:00.000Z",
        "voteCount": 9,
        "content": "The answer given is correct"
      },
      {
        "date": "2023-09-21T09:38:00.000Z",
        "voteCount": 2,
        "content": "I want to correct my response.It seems the right answer  Option D, it leverages Delta Lake's built-in capabilities for handling CDC data. It is designed to efficiently capture, process, and propagate changes, making it a more robust and scalable solution, particularly for large-scale data scenarios with frequent updates and auditing requirements."
      },
      {
        "date": "2023-10-09T13:37:00.000Z",
        "voteCount": 5,
        "content": "Databricks is NOT able to process CDC alone. It needs a intermediare Tool to make it on an object storage and then ingest it.\nSo how can be D?"
      },
      {
        "date": "2023-10-11T01:15:00.000Z",
        "voteCount": 1,
        "content": "the D states: process CDC data from an external system. so this delta CDF."
      },
      {
        "date": "2024-10-14T04:29:00.000Z",
        "voteCount": 1,
        "content": "E . databricks cdc is not set to process external cdc. if u have external cdc u could send to bronze for auditing purposes and use bronze to get silver where u have only valid records"
      },
      {
        "date": "2024-09-17T15:52:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-05-31T20:58:00.000Z",
        "voteCount": 3,
        "content": "E.\nThis is the correct answer because it meets the requirements of maintaining a full record of all values that have ever been valid in the source system and recreating the current table state with only the most recent value for each record. The code ingests all log information into a bronze table, which preserves the raw CDC data as it is. Then, it uses merge into to perform an upsert operation on a silver table, which means it will insert new records or update or delete existing records based on the change type and the pk_id columns. This way, the silver table will always reflect the current state of the source table, while the bronze table will keep the history of all changes."
      },
      {
        "date": "2024-01-24T10:08:00.000Z",
        "voteCount": 2,
        "content": "The answer is E"
      },
      {
        "date": "2024-01-11T00:55:00.000Z",
        "voteCount": 1,
        "content": "Complimenting kz_data's response, be aware that the data that is being consumed is not a Databrick's CDC data feed object, but rather, CDC coming from somewhere else, that is, just regular data. So, indeed, it can't be processed without another tool."
      },
      {
        "date": "2024-01-10T06:21:00.000Z",
        "voteCount": 2,
        "content": "Answer E is correct, as the CDC captured from the external database may contain duplicates for the same pk_id (key) due to multiple updates within the processed hour, we need to take the most recent update for the pk_id, and then MERGE into a silver table."
      },
      {
        "date": "2024-01-09T08:51:00.000Z",
        "voteCount": 2,
        "content": "CDF captures changes only from a Delta table and is only forward-looking once enabled. The CDC logs are writing to object storage. So you would need to ingestion those and merge into downstream tables, hence the answer is E"
      },
      {
        "date": "2023-12-02T03:49:00.000Z",
        "voteCount": 3,
        "content": "For me the answer is D, the question states that CDC logs are emitted on an external storage meaning it can be ingested into the bronze layer on a table with CDF enabled. In this case we let databricks handle the complexity of following changes and only worry about data quality. meaning with CDF enabled databricks will already work the audit data for us with the table_changes of the pre-image and post-image and also give us the last updated value for our use case.\nhere is a similar example: https://www.databricks.com/blog/2021/06/09/how-to-simplify-cdc-with-delta-lakes-change-data-feed.html"
      },
      {
        "date": "2024-01-24T10:11:00.000Z",
        "voteCount": 3,
        "content": "This article shows exactly why D is not right. Since \"CDF captures changes only from a Delta table and is only forward-looking once enabled.\""
      },
      {
        "date": "2023-10-11T01:15:00.000Z",
        "voteCount": 3,
        "content": "E is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/databricks/view/120174-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "An hourly batch job is configured to ingest data files from a cloud object storage container where each batch represent all records produced by the source system in a given hour. The batch job to process these records into the Lakehouse is sufficiently delayed to ensure no late-arriving data is missed. The user_id field represents a unique key for the data, which has the following schema: user_id BIGINT, username STRING, user_utc STRING, user_region STRING, last_login BIGINT, auto_pay BOOLEAN, last_updated BIGINT<br>New records are all ingested into a table named account_history which maintains a full record of all data in the same schema as the source. The next table in the system is named account_current and is implemented as a Type 1 table representing the most recent value for each unique user_id.<br>Assuming there are millions of user accounts and tens of thousands of records processed hourly, which implementation can be used to efficiently update the described account_current table as part of each hourly batch job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Loader to subscribe to new files in the account_history directory; configure a Structured Streaming trigger once job to batch update newly detected files into the account_current table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOverwrite the account_current table with each batch using the results of a query against the account_history table grouping by user_id and filtering for the max value of last_updated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter records in account_history using the last_updated field and the most recent hour processed, as well as the max last_iogin by user_id write a merge statement to update or insert the most recent value for each user_id.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Delta Lake version history to get the difference between the latest version of account_history and one version prior, then write these records to account_current.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter records in account_history using the last_updated field and the most recent hour processed, making sure to deduplicate on username; write a merge statement to update or insert the most recent value for each username."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T00:59:00.000Z",
        "voteCount": 6,
        "content": "My reasoning is thus:\nThe application is based on batch processes, so A is wrong.\nOverwriting the table would destroy the Type 1 SCD behavior, so B is wrong.\nComparing versions of account_history would not be efficient, as the whole data would be scanned, so D is wrong. 'username' is not a key column, so we have no guarantee that it's unique, thus de-duplicating by it can yield wrongly grouped sets of rows, so E is not a safe bet, with the information we know.\nC is the best option."
      },
      {
        "date": "2024-03-15T13:50:00.000Z",
        "voteCount": 5,
        "content": "Type 1 table means the behavior is overwriting."
      },
      {
        "date": "2024-10-17T00:17:00.000Z",
        "voteCount": 1,
        "content": "A. NO. Batch job required so AutoLoader and StructuredStreaming unecessarily complex solutions.\nB. NO. A full overwrite of the table is not efficient.\nC. YES. Seems it is filterning and merging on the id by using as less data as reasonable in the merge statement, why  not? \nD. NO. Difference operation is very ineffecient for this purpose \nE. NO. Username is not key"
      },
      {
        "date": "2024-08-05T13:16:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-08-01T05:27:00.000Z",
        "voteCount": 1,
        "content": "C is correct because:\nA record might have multiple changes and we need to select the most recent change that happened on that record. For that we will use max Log in date and rank it using window function, then we filter on rank=1 and use it for UPSERT operation."
      },
      {
        "date": "2024-06-05T04:14:00.000Z",
        "voteCount": 1,
        "content": "I think B ,Type 1 table must overwrite the data"
      },
      {
        "date": "2024-05-27T16:26:00.000Z",
        "voteCount": 3,
        "content": "C is correct.\nA Type 1 table means that it performs an \"upsert\" operation without maintaining history, based on the merge condition. This means that new records are inserted, and existing records are updated. As a result, the merge process does not retain historical records.\n\nTherefore, the correct answer is C."
      },
      {
        "date": "2024-02-08T21:30:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-01-31T05:56:00.000Z",
        "voteCount": 2,
        "content": "answer is C"
      },
      {
        "date": "2024-01-24T10:19:00.000Z",
        "voteCount": 1,
        "content": "answer is C"
      },
      {
        "date": "2024-01-10T06:26:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C"
      },
      {
        "date": "2024-01-05T10:24:00.000Z",
        "voteCount": 1,
        "content": "D is the most optimal way to identify the changes in the last data refresh"
      },
      {
        "date": "2024-08-20T02:56:00.000Z",
        "voteCount": 1,
        "content": "doesn't work because it doesn't take into account requirement on user_id"
      },
      {
        "date": "2023-10-11T01:24:00.000Z",
        "voteCount": 2,
        "content": "We need to filter on last hours and deduplicate records, then merge.\nDo is not correct, filtering on max loggin_date makes no sense."
      },
      {
        "date": "2023-10-16T03:26:00.000Z",
        "voteCount": 1,
        "content": "If the \"las log in\" is the column that shows the lates version of the record then answer c is correct"
      },
      {
        "date": "2023-11-30T01:55:00.000Z",
        "voteCount": 3,
        "content": "deduplication on username does not make sense, username is not PK."
      },
      {
        "date": "2023-09-21T10:05:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C"
      },
      {
        "date": "2023-09-07T04:35:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/databricks/view/119189-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team. The table contains information about customers derived from a number of upstream sources. Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.<br>The churn prediction model used by the ML team is fairly stable in production. The team is only interested in making predictions on records that have changed in the past 24 hours.<br>Which approach would simplify the identification of these changed records?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply the churn model to all rows in the customer_churn_params table, but implement logic to perform an upsert into the predictions table that ignores rows where predictions have not changed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the batch job to a Structured Streaming job using the complete output mode; configure a Structured Streaming job to read from the customer_churn_params table and incrementally predict against the churn model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCalculate the difference between the previous model predictions and the current customer_churn_params on a key identifying unique customers before making new predictions; only make predictions on those customers not in the previous predictions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the overwrite logic to include a field populated by calling spark.sql.functions.current_timestamp() as data are being written; use this field to identify records written on a particular date.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the current overwrite logic with a merge statement to modify only those records that have changed; write logic to make predictions on the changed records identified by the change data feed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-27T10:26:00.000Z",
        "voteCount": 6,
        "content": "E is right answer"
      },
      {
        "date": "2024-08-05T21:41:00.000Z",
        "voteCount": 1,
        "content": "I don't understand why E is correct. With E we are updating only data needed but we are then doing prediction on the whole table which means that we are doing again predictions on not changing records which is not efficient"
      },
      {
        "date": "2024-02-18T11:00:00.000Z",
        "voteCount": 2,
        "content": "E is the correct one. By removing overwrite with merge, this will lead to an UPSERT causing updating only the data needed ( When Matched Upate + When not mached insert clauses). Then, with the CDC the capability of identifying is also satisfied."
      },
      {
        "date": "2024-01-21T13:37:00.000Z",
        "voteCount": 1,
        "content": "correct ans is E"
      },
      {
        "date": "2024-01-10T06:31:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-10-16T03:28:00.000Z",
        "voteCount": 3,
        "content": "E is Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/databricks/view/117078-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A table is registered with the following code:<br><img title=\"image10\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image10.png\"><br>Both users and orders are Delta Lake tables. Which statement describes the results of querying recent_orders?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query finishes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic will execute when the table is defined and store the result of joining tables to the DBFS; this stored data will be returned when the table is queried.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResults will be computed and cached when the table is defined; these cached results will incrementally update as new records are inserted into source tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query began.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe versions of each source table will be stored in the table transaction log; query results will be saved to DBFS with each query."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-02T02:47:00.000Z",
        "voteCount": 14,
        "content": "Correct answer is B. table is created and data of join will be stored on DBFS and it will be returned on query time"
      },
      {
        "date": "2024-08-17T21:34:00.000Z",
        "voteCount": 7,
        "content": "The question says \"Which statement describes the results of querying recent_orders?\"\n\nThe question doesn't ask about the code snipped itself. This question is about the logic of \"select * from recent_orders\" after the creation of recent_orders.\n\nanswer is D\n\nD is the right answer"
      },
      {
        "date": "2024-08-20T11:54:00.000Z",
        "voteCount": 2,
        "content": "Not sure how so many people misunderstood the actual question. It already says at the top that the table is registered as the code given, they're not executing the code again."
      },
      {
        "date": "2024-09-02T14:28:00.000Z",
        "voteCount": 1,
        "content": "Why are people so fixed on how the table was created, question says what happens when a query is run against the table."
      },
      {
        "date": "2024-10-16T05:49:00.000Z",
        "voteCount": 1,
        "content": "i think B"
      },
      {
        "date": "2024-10-14T04:58:00.000Z",
        "voteCount": 1,
        "content": "i picked b"
      },
      {
        "date": "2024-06-08T01:07:00.000Z",
        "voteCount": 2,
        "content": "CTAS statements persist it results, so B"
      },
      {
        "date": "2024-05-31T21:05:00.000Z",
        "voteCount": 2,
        "content": "B. All logic will execute when the table is defined and store the result of joining tables to the DBFS; this stored data will be returned when the table is queried."
      },
      {
        "date": "2024-05-19T10:14:00.000Z",
        "voteCount": 2,
        "content": "\"Create Table\" is an action so \"B\""
      },
      {
        "date": "2024-02-08T21:44:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-10T06:34:00.000Z",
        "voteCount": 1,
        "content": "I think B is the correct answer"
      },
      {
        "date": "2024-01-08T21:26:00.000Z",
        "voteCount": 1,
        "content": "B is correct. Views compute when query is executed, not when defined. And vice versa for tables."
      },
      {
        "date": "2024-01-05T01:35:00.000Z",
        "voteCount": 1,
        "content": "Key here is that option D says \"returned\". The CTAS statement does not return results, thus option B is correct."
      },
      {
        "date": "2023-11-20T17:02:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is:\n\nB. All logic will execute when the table is defined and store the result of joining tables to the DBFS; this stored data will be returned when the table is queried.\n\nWhen the CREATE TABLE AS statement is executed, it runs the enclosed SELECT statement immediately to pull the current data from the users and orders tables where the order_date is within the last 7 days. This result is then stored as a new table called recent_orders in the Delta Lake on the DBFS (Databricks File System). Subsequent queries against recent_orders will return this stored data, and not recompute the join unless the table is updated or refreshed."
      },
      {
        "date": "2023-11-02T11:50:00.000Z",
        "voteCount": 2,
        "content": "Correct is B . CTAS command"
      },
      {
        "date": "2023-10-11T01:31:00.000Z",
        "voteCount": 1,
        "content": "Creating a table will not display results.\nYou need to make a select alter it is created."
      },
      {
        "date": "2023-09-22T07:34:00.000Z",
        "voteCount": 1,
        "content": "Based on typical Delta Lake behavior, option D is the most accurate description. Delta Lake queries generally execute at query time and retur\nn results based on the state of the source tables at the time the query began. Delta Lake provides features for managing data versions and transactions, but it doesn't precompute and store results like option B or cache results like option C."
      },
      {
        "date": "2023-10-09T13:51:00.000Z",
        "voteCount": 2,
        "content": "No. Simple no.\nWhen you execute a create table even with a Join you DONT see the results imediatly unless you query the table. \nSo correct answer is B. The create table statement by default creates a Managed table, which is stored in DBFS."
      },
      {
        "date": "2023-11-27T01:19:00.000Z",
        "voteCount": 1,
        "content": "Its create separate delta lake transaction logs for this new table. So B is the correct the answer."
      },
      {
        "date": "2023-09-10T07:45:00.000Z",
        "voteCount": 2,
        "content": "correct is B"
      },
      {
        "date": "2023-08-23T06:25:00.000Z",
        "voteCount": 2,
        "content": "Aa ok, I missed \"logic will execute at query time\" ignore my previous comment"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/databricks/view/119377-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A production workload incrementally applies updates from an external Change Data Capture feed to a Delta Lake table as an always-on Structured Stream job. When data was initially migrated for this table, OPTIMIZE was executed and most data files were resized to 1 GB. Auto Optimize and Auto Compaction were both turned on for the streaming production job. Recent review of data files shows that most data files are under 64 MB, although each partition in the table contains at least 1 GB of data and the total table size is over 10 TB.<br>Which of the following likely explains these smaller file sizes?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks has autotuned to a smaller target file size to reduce duration of MERGE operations\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tZ-order indices calculated on the table are preventing file compaction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBloom filter indices calculated on the table are preventing file compaction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks has autotuned to a smaller target file size based on the overall size of data in the table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks has autotuned to a smaller target file size based on the amount of data in each partition"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-04T23:30:00.000Z",
        "voteCount": 9,
        "content": "https://docs.databricks.com/en/delta/tune-file-size.html#autotune-table 'Autotune file size based on workload'"
      },
      {
        "date": "2024-08-05T22:17:00.000Z",
        "voteCount": 3,
        "content": "It is important here to understand the difference between the partition size and the data files. the partition size is 1GB which is caused by OPTIMIZE and also expected. In each partition are data files. Databricks did an attuning to these datafile and resized them to a small size to be able to do MERGE statements efficiently that's why A is the correct answer"
      },
      {
        "date": "2024-05-31T21:09:00.000Z",
        "voteCount": 1,
        "content": "One of the purposes of a optimize execution is the gain in merge oprations, so:\nA. Databricks has autotuned to a smaller target file size to reduce duration of MERGE operations"
      },
      {
        "date": "2024-02-10T02:30:00.000Z",
        "voteCount": 2,
        "content": "how A is correct ? While Databricks does have autotuning capabilities, it primarily considers the table size. In this case, the table is over 10 TB, which would typically lead to a target file size of 1 GB, not under 64 MB."
      },
      {
        "date": "2024-02-08T21:53:00.000Z",
        "voteCount": 2,
        "content": "The target file size is based on the current size of the Delta table. For tables smaller than 2.56 TB, the autotuned target file size is 256 MB. For tables with a size between 2.56 TB and 10 TB, the target size will grow linearly from 256 MB to 1 GB. For tables larger than 10 TB, the target file size is 1 GB.\nCorrect answer is A"
      },
      {
        "date": "2024-01-21T13:42:00.000Z",
        "voteCount": 1,
        "content": "correct ans is A"
      },
      {
        "date": "2024-01-13T11:19:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-01-10T06:46:00.000Z",
        "voteCount": 1,
        "content": "correct answer is A"
      },
      {
        "date": "2023-11-02T11:56:00.000Z",
        "voteCount": 1,
        "content": "Auto Optimize reduces file size less than 128MB to facilitate quick merge"
      },
      {
        "date": "2023-10-21T12:30:00.000Z",
        "voteCount": 1,
        "content": "E is the right answer, because the question is why there are small files"
      },
      {
        "date": "2023-10-11T01:36:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-09-09T12:07:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2023-08-30T03:40:00.000Z",
        "voteCount": 3,
        "content": "E is right answer"
      },
      {
        "date": "2023-09-21T10:36:00.000Z",
        "voteCount": 1,
        "content": "option A is correct answer as , option E is the likely explanation for the smaller file sizes"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/databricks/view/118335-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement regarding stream-static joins and static Delta tables is correct?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach microbatch of a stream-static join will use the most recent version of the static Delta table as of each microbatch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach microbatch of a stream-static join will use the most recent version of the static Delta table as of the job's initialization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe checkpoint directory will be used to track state information for the unique keys present in the join.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream-static joins cannot use static Delta tables because of consistency issues.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe checkpoint directory will be used to track updates to the static Delta table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-21T10:41:00.000Z",
        "voteCount": 9,
        "content": "B is the right answer as  Option B is more typical for stream-static joins, as it provides a consistent static DataFrame snapshot for the entire job's duration. Option A might be suitable in specialized cases where you need real-time updates of the static DataFrame for each microbatch."
      },
      {
        "date": "2023-12-02T04:43:00.000Z",
        "voteCount": 9,
        "content": "Answer is A, When Azure Databricks processes a micro-batch of data in a stream-static join, the latest valid version of data from the static Delta table joins with the records present in the current micro-batch\nfrom https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/delta-lake"
      },
      {
        "date": "2023-08-17T06:05:00.000Z",
        "voteCount": 5,
        "content": "correct answer is A"
      },
      {
        "date": "2024-09-29T22:52:00.000Z",
        "voteCount": 1,
        "content": "This is straight from docs, \"A stream-static join joins the latest valid version of a Delta table (the static data) to a data stream using a stateless join.\n\nWhen Azure Databricks processes a micro-batch of data in a stream-static join, the latest valid version of data from the static Delta table joins with the records present in the current micro-batch. Because the join is stateless, you do not need to configure watermarking and can process results with low latency. The data in the static Delta table used in the join should be slowly-changing.\"\nhttps://learn.microsoft.com/en-us/azure/databricks/transform/join#stream-static"
      },
      {
        "date": "2024-01-10T07:00:00.000Z",
        "voteCount": 1,
        "content": "correct answer is A"
      },
      {
        "date": "2023-12-02T04:44:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer A"
      },
      {
        "date": "2023-10-11T01:45:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\nWhen Databricks processes a micro-batch of data in a stream-static join, the latest valid version of data from the static"
      },
      {
        "date": "2023-10-01T01:30:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A. https://docs.databricks.com/en/structured-streaming/delta-lake.html"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/databricks/view/120176-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer has been asked to develop a streaming data pipeline with a grouped aggregation using DataFrame df. The pipeline needs to calculate the average humidity and average temperature for each non-overlapping five-minute interval. Events are recorded once per minute per device.<br>Streaming DataFrame df has the following schema:<br>\"device_id INT, event_time TIMESTAMP, temp FLOAT, humidity FLOAT\"<br>Code block:<br><img title=\"image11\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image11.png\"><br>Choose the response that correctly fills in the blank within the code block to complete this task.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tto_interval(\"event_time\", \"5 minutes\").alias(\"time\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\twindow(\"event_time\", \"5 minutes\").alias(\"time\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"event_time\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\twindow(\"event_time\", \"10 minutes\").alias(\"time\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlag(\"event_time\", \"10 minutes\").alias(\"time\")"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-31T21:13:00.000Z",
        "voteCount": 1,
        "content": "B. window(\"event_time\", \"5 minutes\").alias(\"time\")\nIn Structured Streaming, expressing such windows on event-time is simply performing a special grouping using the window() function. For example, counts over 5 minute tumbling (non-overlapping) windows on the eventTime column in the event is as following."
      },
      {
        "date": "2024-01-13T11:23:00.000Z",
        "voteCount": 1,
        "content": "correct B"
      },
      {
        "date": "2024-01-10T07:02:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-02T11:58:00.000Z",
        "voteCount": 2,
        "content": "Window of 5 mins"
      },
      {
        "date": "2023-10-11T01:48:00.000Z",
        "voteCount": 2,
        "content": "B is correct:\nhttps://www.databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html"
      },
      {
        "date": "2023-09-21T12:41:00.000Z",
        "voteCount": 2,
        "content": "answer is B"
      },
      {
        "date": "2023-09-07T05:36:00.000Z",
        "voteCount": 4,
        "content": "Correct, B."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/databricks/view/119397-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data architect has designed a system in which two Structured Streaming jobs will concurrently write to a single bronze Delta table. Each job is subscribing to a different topic from an Apache Kafka source, but they will write data with the same schema. To keep the directory structure simple, a data engineer has decided to nest a checkpoint directory to be shared by both streams.<br>The proposed directory structure is displayed below:<br><img title=\"image12\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image12.png\"><br>Which statement describes whether this checkpoint directory structure is valid for the given scenario and why?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; Delta Lake manages streaming checkpoints in the transaction log.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes; both of the streams can share a single checkpoint directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; only one stream can write to a Delta Lake table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes; Delta Lake supports infinite concurrent writers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; each of the streams needs to have its own checkpoint directory.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-07T05:45:00.000Z",
        "voteCount": 8,
        "content": "Correct, E.\n\nSource:\nhttps://docs.databricks.com/en/optimizations/isolation-level.html#:~:text=If%20a%20streaming%20query%20using%20the%20same%20checkpoint%20location%20is%20started%20multiple%20times%20concurrently%20and%20tries%20to%20write%20to%20the%20Delta%20table%20at%20the%20same%20time.%20You%20should%20never%20have%20two%20streaming%20queries%20use%20the%20same%20checkpoint%20location%20and%20run%20at%20the%20same%20time."
      },
      {
        "date": "2024-10-15T00:40:00.000Z",
        "voteCount": 1,
        "content": "E is the correct"
      },
      {
        "date": "2024-05-31T21:18:00.000Z",
        "voteCount": 1,
        "content": "E. No; each of the streams needs to have its own checkpoint directory.\nThe checkpoint directory is 1 to 1"
      },
      {
        "date": "2024-05-10T13:39:00.000Z",
        "voteCount": 1,
        "content": "It is not clear from the question that year_week=2020_01 and year_week=2020_02 are used by stream 1 and stream 2 respectively. If they use the common parent checkpoint directory with individual sub folders for checkpointing, that should work fine. In that case the answer should be B"
      },
      {
        "date": "2024-06-21T04:57:00.000Z",
        "voteCount": 1,
        "content": "That are table partitions. They are not used to build checkpoint adress. The adress finish at /bronze"
      },
      {
        "date": "2024-01-13T11:23:00.000Z",
        "voteCount": 1,
        "content": "correct E"
      },
      {
        "date": "2024-01-10T07:03:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-10-11T01:50:00.000Z",
        "voteCount": 2,
        "content": "E is correct.\nIf user wants 1 checkpoint directory then he needs to unions streams before writing."
      },
      {
        "date": "2023-08-30T05:04:00.000Z",
        "voteCount": 3,
        "content": "answer is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/databricks/view/117090-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Structured Streaming job deployed to production has been experiencing delays during peak hours of the day. At present, during normal execution, each microbatch of data is processed in less than 3 seconds. During peak hours of the day, execution time for each microbatch becomes very inconsistent, sometimes exceeding 30 seconds. The streaming write is currently configured with a trigger interval of 10 seconds.<br>Holding all other variables constant and assuming records need to be processed in less than 10 seconds, which adjustment will meet the requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the trigger interval to 5 seconds; triggering batches more frequently allows idle executors to begin processing the next batch while longer running tasks from previous batches finish.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the trigger interval to 30 seconds; setting the trigger interval near the maximum execution time observed for each batch is always best practice to ensure no records are dropped.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe trigger interval cannot be modified without modifying the checkpoint directory; to maintain the current stream state, increase the number of shuffle partitions to maximize parallelism.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the trigger once option and configure a Databricks job to execute the query every 10 seconds; this ensures all backlogged records are processed with each batch.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the trigger interval to 5 seconds; triggering batches more frequently may prevent records from backing up and large batches from causing spill.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-05T08:51:00.000Z",
        "voteCount": 2,
        "content": "In Databricks Runtime 11.3 LTS and above, the Trigger.Once setting is deprecated. Databricks recommends you use Trigger.AvailableNow for all incremental batch processing workloads.\n\nhttps://docs.databricks.com/en/structured-streaming/triggers.html\n\nDoesn't seem like E is a valid and recommended option given that it is deprecated."
      },
      {
        "date": "2024-08-05T08:52:00.000Z",
        "voteCount": 2,
        "content": "Ooops, I mean, D."
      },
      {
        "date": "2024-05-31T21:19:00.000Z",
        "voteCount": 2,
        "content": "Considering the best option for performance gain is:\nE. Decrease the trigger interval to 5 seconds; triggering batches more frequently may prevent records from backing up and large batches from causing spill."
      },
      {
        "date": "2024-02-13T21:51:00.000Z",
        "voteCount": 2,
        "content": "E is the answer. \nEnable the settings uses the 128 MB as the target file size\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size"
      },
      {
        "date": "2024-01-31T07:15:00.000Z",
        "voteCount": 1,
        "content": "E is correct as A is wrong because in Streaming you very rarely have any executors idle"
      },
      {
        "date": "2024-01-10T07:07:00.000Z",
        "voteCount": 1,
        "content": "I think is E is correct"
      },
      {
        "date": "2024-01-05T01:07:00.000Z",
        "voteCount": 2,
        "content": "I believe this is a case of the least bad option, not exactly the best option possible.\n\n- A is wrong because in Streaming you very rarely have any executors idle, as all cores are engaged in processing the window of data;\n- B is wrong because triggering every 30s will not meet the 10s target processing interval;\n- C is wrong in two manners: increasing shuffle partitions to any number above the number of available cores in the cluster will worsen performance in streaming; also, the checkpoint folder has no connection with trigger time.\n- D is wrong because, keeping all other things the same as described by the problem, keeping the trigger time as 10s will not change the underlying conditions of the delay (i.e.: too much data to be processed in a timely manner).\n\nE is the only option that might improve processing time."
      },
      {
        "date": "2023-12-25T02:13:00.000Z",
        "voteCount": 1,
        "content": "correct answer is E"
      },
      {
        "date": "2023-11-07T10:32:00.000Z",
        "voteCount": 1,
        "content": "Only C. Even if you trigger more frequently you decrease both load and time for this load. E doesn't change anything."
      },
      {
        "date": "2023-10-11T01:54:00.000Z",
        "voteCount": 4,
        "content": "Changing trigger interval to \"one\" will cause this to be a \"batch\" and will not execute in microbranches. This will not help at all"
      },
      {
        "date": "2023-09-21T12:53:00.000Z",
        "voteCount": 1,
        "content": "correct answer is E"
      },
      {
        "date": "2023-09-09T01:15:00.000Z",
        "voteCount": 1,
        "content": "sorry, the caveat is holding all other variables constant.. that means we are not allowed to change trigger intervals. is C the answer then"
      },
      {
        "date": "2023-09-09T00:12:00.000Z",
        "voteCount": 1,
        "content": "what if in between those 5 seconds trigger interval if there are more records, that would still increase the time it takes to process.. i doubt E is correct. I will go with answer D. it is not to execute all queries within 10 secs. it is to execute trigger now batch every 10 seconds."
      },
      {
        "date": "2023-09-08T22:31:00.000Z",
        "voteCount": 1,
        "content": "A option also is about setting trigger interval to 5 seconds, just to understand.. why its not the answer"
      },
      {
        "date": "2023-09-05T00:42:00.000Z",
        "voteCount": 2,
        "content": "for sure E"
      },
      {
        "date": "2023-08-30T05:09:00.000Z",
        "voteCount": 2,
        "content": "correct anwer is E"
      },
      {
        "date": "2023-08-02T03:54:00.000Z",
        "voteCount": 4,
        "content": "correct answer is E. D means a job will need to acquire resources in 10s which is impossible without serverless"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/databricks/view/117466-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement describes Delta Lake Auto Compaction?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBefore a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOptimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData is queued in a messaging bus instead of committing data directly to memory; all data is committed from the messaging bus in one batch once the job is complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 128 MB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-01T20:41:00.000Z",
        "voteCount": 1,
        "content": "If you go through this docs - then one thing is clear that it is not async job, so we have to eliminate A &amp; C. D is wrong. It has no special job wrt the partition. Also file size 0f 128 MB is legacy config, latest one is dynamic. So we are left with B"
      },
      {
        "date": "2024-09-27T06:15:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/delta/tune-file-size.html"
      },
      {
        "date": "2024-08-16T06:15:00.000Z",
        "voteCount": 1,
        "content": "Auto compaction is synchronous job."
      },
      {
        "date": "2024-07-20T19:38:00.000Z",
        "voteCount": 3,
        "content": "A and E are wrong because auto compaction is synchronous operation!\n\nI vote for B\n\nAs per documentation - \"Auto compaction occurs after a write to a table has succeeded and runs synchronously on the cluster that has performed the write. Auto compaction only compacts files that haven\u2019t been compacted previously.\"\n\nhttps://docs.delta.io/latest/optimizations-oss.html"
      },
      {
        "date": "2024-05-31T21:24:00.000Z",
        "voteCount": 1,
        "content": "E. An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 128 MB.\nhttps://community.databricks.com/t5/data-engineering/what-is-the-difference-between-optimize-and-auto-optimize/td-p/21189"
      },
      {
        "date": "2024-02-13T21:52:00.000Z",
        "voteCount": 2,
        "content": "E is the answer.\nEnable the settings uses the 128 MB as the target file size\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size"
      },
      {
        "date": "2024-01-31T07:16:00.000Z",
        "voteCount": 1,
        "content": "default file size is 128MB in auto compaction"
      },
      {
        "date": "2024-01-10T07:13:00.000Z",
        "voteCount": 1,
        "content": "E is correct as the default file size is 128MB in auto compaction, not 1GB as normal OPTIMIZE statement."
      },
      {
        "date": "2024-01-09T01:03:00.000Z",
        "voteCount": 1,
        "content": "128MB is the default."
      },
      {
        "date": "2023-12-25T16:51:00.000Z",
        "voteCount": 1,
        "content": "Question is more on auto compaction hence the answer is E, as default size or auto compaction is 128 mb"
      },
      {
        "date": "2023-12-02T05:20:00.000Z",
        "voteCount": 1,
        "content": "Optimize default target file size is 1Gb, however in this question we are dealing with auto compaction. Which when enabled runs optimize with 128MB file size by default."
      },
      {
        "date": "2023-11-21T06:43:00.000Z",
        "voteCount": 4,
        "content": "Delta Lake's Auto Compaction feature is designed to improve the efficiency of data storage by reducing the number of small files in a Delta table. After data is written to a Delta table, an asynchronous job can be triggered to evaluate the file sizes. If it determines that there are a significant number of small files, it will automatically run the OPTIMIZE command, which coalesces these small files into larger ones, typically aiming for files around 1 GB in size for optimal performance.\n\nE is incorrect because the statement is similar to A but with an incorrect default file size target."
      },
      {
        "date": "2024-06-21T11:14:00.000Z",
        "voteCount": 1,
        "content": "Table property delta.autoOptimize.autoCompact target 128 mb. For table property delta.tuneFileSizesForRewrites, tables larger than 10 TB, the target file size is 1 GB.\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size"
      },
      {
        "date": "2023-11-02T12:04:00.000Z",
        "voteCount": 1,
        "content": "E is correct. Auto compact tries to optimize to a file size of 128MB"
      },
      {
        "date": "2023-10-11T02:03:00.000Z",
        "voteCount": 3,
        "content": "E is the best feet, although databricks says that auto compaction runs runs synchronously"
      },
      {
        "date": "2023-09-21T12:59:00.000Z",
        "voteCount": 1,
        "content": "correct answer is e"
      },
      {
        "date": "2023-09-05T00:48:00.000Z",
        "voteCount": 4,
        "content": "E fits best, but according to docs it is synchronous opeartion\n \"Auto compaction occurs after a write to a table has succeeded and runs synchronously on the cluster that has performed the write. Auto compaction only compacts files that haven\u2019t been compacted previously.\""
      },
      {
        "date": "2023-08-23T05:29:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is E:\nAuto optimize consists of 2 complementary operations:\n- Optimized writes: with this feature enabled, Databricks attempts to write out 128 MB files for each table partition.\n- Auto compaction: this will check after an individual write, if files can further be compacted. If yes, it runs an OPTIMIZE job with 128 MB file sizes (instead of the 1 GB file size used in the standard OPTIMIZE)"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/databricks/view/117468-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement characterizes the general programming model used by Spark Structured Streaming?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming leverages the parallel processing of GPUs to achieve highly parallel data throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming is implemented as a messaging bus and is derived from Apache Kafka.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming uses specialized hardware and I/O streams to achieve sub-second latency for data transfer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming relies on a distributed network of nodes that hold incremental state values for cached stages."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-31T21:24:00.000Z",
        "voteCount": 2,
        "content": "D. Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table."
      },
      {
        "date": "2024-02-27T20:18:00.000Z",
        "voteCount": 2,
        "content": "Yes. answer is D"
      },
      {
        "date": "2024-01-13T11:28:00.000Z",
        "voteCount": 1,
        "content": "vote for D"
      },
      {
        "date": "2023-10-11T02:06:00.000Z",
        "voteCount": 2,
        "content": "Correct.\nStructured streaming needs to be considered as a table with append"
      },
      {
        "date": "2023-08-06T02:49:00.000Z",
        "voteCount": 1,
        "content": "correct; The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended. This leads to a new stream processing model that is very similar to a batch processing model. You will express your streaming computation as standard batch-like query as on a static table, and Spark runs it as an incremental query on the unbounded input table. Let\u2019s understand this model in more detail.\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/databricks/view/117469-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which configuration parameter directly affects the size of a spark-partition upon ingestion of data into Spark?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.files.maxPartitionBytes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.autoBroadcastJoinThreshold",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.files.openCostInBytes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.coalescePartitions.minPartitionNum",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.advisoryPartitionSizeInBytes"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T11:29:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2023-10-11T02:30:00.000Z",
        "voteCount": 1,
        "content": "from the provided list, this fits best.\nIn reality partition size/number can be influenced my many settings"
      },
      {
        "date": "2023-08-06T02:50:00.000Z",
        "voteCount": 2,
        "content": "correct; The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.\nhttps://spark.apache.org/docs/latest/sql-performance-tuning.html"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/databricks/view/121108-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Spark job is taking longer than expected. Using the Spark UI, a data engineer notes that the Min, Median, and Max Durations for tasks in a particular stage show the minimum and median time to complete a task as roughly the same, but the max duration for a task to be roughly 100 times as long as the minimum.<br>Which situation is causing increased duration of the overall job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTask queueing resulting from improper thread pool assignment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpill resulting from attached volume storage being too small.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNetwork latency due to some cluster nodes being in different regions from the source data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSkew caused by more data being assigned to a subset of spark-partitions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCredential validation errors while pulling data from an external system."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T14:34:00.000Z",
        "voteCount": 1,
        "content": "A excluded because task queueing does not increase the duration of a task\nB excluded, spill is writing to storage when a memory is insufficient (not storage insufficient)\nC excluded, region cannot have a 100 times impact on duration\nE excluded, no errors mentioned in question"
      },
      {
        "date": "2024-05-31T21:27:00.000Z",
        "voteCount": 1,
        "content": "D. Skew caused by more data being assigned to a subset of spark-partitions."
      },
      {
        "date": "2024-03-07T23:33:00.000Z",
        "voteCount": 2,
        "content": "because a particular executors are executing majority of data while rest are processing very less. The total execution time depends upon the slowest executors.\nAnswer is D."
      },
      {
        "date": "2024-01-13T11:31:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-01-10T07:24:00.000Z",
        "voteCount": 1,
        "content": "I think D is correct"
      },
      {
        "date": "2023-10-11T02:31:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-09-21T13:15:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/databricks/view/117097-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Each configuration below is identical to the extent that each cluster has 400 GB total of RAM, 160 total cores and only one Executor per VM.<br>Given a job with at least one wide transformation, which of the following cluster configurations will result in maximum performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Total VMs; 1<br>\u2022 400 GB per Executor<br>\u2022 160 Cores / Executor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Total VMs: 8<br>\u2022 50 GB per Executor<br>\u2022 20 Cores / Executor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Total VMs: 16<br>\u2022 25 GB per Executor<br>\u2022 10 Cores/Executor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Total VMs: 4<br>\u2022 100 GB per Executor<br>\u2022 40 Cores/Executor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Total VMs:2<br>\u2022 200 GB per Executor<br>\u2022 80 Cores / Executor"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-23T08:27:00.000Z",
        "voteCount": 41,
        "content": "Option A, question is about maximum performance. Wide transformation will result in often expensive shuffle. With one executor this problem will be resolved. https://docs.databricks.com/en/clusters/cluster-config-best-practices.html#complex-batch-etl"
      },
      {
        "date": "2023-11-04T02:18:00.000Z",
        "voteCount": 3,
        "content": "source : https://docs.databricks.com/en/clusters/cluster-config-best-practices.html"
      },
      {
        "date": "2024-03-27T09:07:00.000Z",
        "voteCount": 1,
        "content": "Wide transformation falls under complex etl which means Option A is correct in the documentation didn't mention to do otherwise in this scenario."
      },
      {
        "date": "2024-02-08T23:27:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-01-21T03:48:00.000Z",
        "voteCount": 2,
        "content": "Option A:\nhttps://docs.databricks.com/en/clusters/cluster-config-best-practices.html#complex-batch-etl"
      },
      {
        "date": "2024-01-08T01:42:00.000Z",
        "voteCount": 1,
        "content": "robson90's response explains it perfectly and has documentation to support it."
      },
      {
        "date": "2023-11-07T10:42:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2023-11-04T06:18:00.000Z",
        "voteCount": 2,
        "content": "Our goal is top performance.\nVertical scaling is more performant rather that horizontal. Especially we know that we need cross VM exchange. Option A."
      },
      {
        "date": "2023-11-04T02:17:00.000Z",
        "voteCount": 1,
        "content": "response A. \nas of \nComplex batch ETL\n\n\" More complex ETL jobs, such as processing that requires unions and joins across multiple tables, will probably work best when you can minimize the amount of data shuffled. Since reducing the number of workers in a cluster will help minimize shuffles, you should consider a smaller cluster like cluster A in the following diagram over a larger cluster like cluster D. \""
      },
      {
        "date": "2023-11-04T02:18:00.000Z",
        "voteCount": 1,
        "content": "source = source : https://docs.databricks.com/en/clusters/cluster-config-best-practices.html"
      },
      {
        "date": "2023-09-21T20:00:00.000Z",
        "voteCount": 4,
        "content": "Considering the need for both memory and parallelism, option D seems to offer the best balance between resources and parallel processing. It provides a reasonable amount of memory and cores per Executor while maintaining a sufficient level of parallelism with 4 Executors. This configuration is likely to result in maximum performance for a job with at least one wide transformation."
      },
      {
        "date": "2023-09-17T01:23:00.000Z",
        "voteCount": 2,
        "content": "Sorry Response C = 16VM for maximing Wide Transformation"
      },
      {
        "date": "2023-09-17T01:22:00.000Z",
        "voteCount": 1,
        "content": "Key message is : Given a job with at least one wide transformation\nPerformance, should max the number of concurrent VM, Selecting response B. 160/10 = 16 VM"
      },
      {
        "date": "2023-08-23T07:19:00.000Z",
        "voteCount": 1,
        "content": "Considering the need for both memory and parallelism, option D seems to offer the best balance between resources and parallel processing. It provides a reasonable amount of memory and cores per Executor while maintaining a sufficient level of parallelism with 4 Executors. This configuration is likely to result in maximum performance for a job with at least one wide transformation."
      },
      {
        "date": "2023-08-20T05:17:00.000Z",
        "voteCount": 1,
        "content": "correct answer is E: Option E provides a substantial amount of memory and cores per executor, allowing the job to handle wide transformations efficiently. However, performance can also be influenced by factors like the nature of your specific workload, data distribution, and overall cluster utilization. It's a good practice to conduct benchmarking and performance testing with various configurations to determine the optimal setup for your specific use case."
      },
      {
        "date": "2023-08-08T11:26:00.000Z",
        "voteCount": 2,
        "content": "C. More VMs helps to distribute the workload across the cluster, which results in better fault tolerance and increase the chances of job completion."
      },
      {
        "date": "2023-08-02T04:34:00.000Z",
        "voteCount": 1,
        "content": "answer should be E. if at least one transformation is wide, so 1 executor of 200GB can do the job, rest of tasks can be carried out on the other node"
      },
      {
        "date": "2023-08-06T02:55:00.000Z",
        "voteCount": 1,
        "content": "would it be fault-tolerant?"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/databricks/view/121337-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer on your team has implemented the following code block.<br><img title=\"image13\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image13.png\"><br>The view new_events contains a batch of records with the same schema as the events Delta table. The event_id field serves as a unique key for this table.<br>When this query is executed, what will happen with new records that have the same event_id as an existing record?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey are merged.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey are ignored.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey are updated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey are inserted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey are deleted."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-31T21:32:00.000Z",
        "voteCount": 2,
        "content": "B. They are ignored.\nBecause there is not mention so there is no WHEN statement for this condition"
      },
      {
        "date": "2024-02-08T23:29:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-01-10T07:41:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-10-15T02:10:00.000Z",
        "voteCount": 1,
        "content": "Ignored"
      },
      {
        "date": "2023-09-24T17:36:00.000Z",
        "voteCount": 3,
        "content": "The answer is correct. \"If none of the WHEN MATCHED conditions evaluate to true for a source and target row pair that matches the merge_condition, then the target row is left unchanged.\" \nhttps://docs.databricks.com/en/sql/language-manual/delta-merge-into.html#:~:text=If%20none%20of%20the%20WHEN%20MATCHED%20conditions%20evaluate%20to%20true%20for%20a%20source%20and%20target%20row%20pair%20that%20matches%20the%20merge_condition%2C%20then%20the%20target%20row%20is%20left%20unchanged."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/databricks/view/117098-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer seeks to leverage Delta Lake's Change Data Feed functionality to create a Type 1 table representing all of the values that have ever been valid for all rows in a bronze table created with the property delta.enableChangeDataFeed = true. They plan to execute the following code as a daily job:<br><img title=\"image14\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image14.png\"><br>Which statement describes the execution and results of running the above query multiple times?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach time the job is executed, newly updated records will be merged into the target table, overwriting previous values with the same primary keys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach time the job is executed, the entire available history of inserted or updated records will be appended to the target table, resulting in many duplicate entries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach time the job is executed, the target table will be overwritten using the entire history of inserted or updated records, giving the desired result.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach time the job is executed, the differences between the original and current versions are calculated; this may result in duplicate entries for some records.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach time the job is executed, only those records that have been inserted or updated since the last execution will be appended to the target table, giving the desired result."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-03T06:22:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: B (not E)\nAlthough it was pretty obvious to me, I still wrote the code to check and yes, it will append the entire change during every write since starting version is mentioned as 0. \nIf in doubt, code it yourselves"
      },
      {
        "date": "2024-06-03T17:19:00.000Z",
        "voteCount": 3,
        "content": "(\"startingVersion\", 0) that means the entiry history of table will be read so B."
      },
      {
        "date": "2024-02-08T23:31:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-01-10T07:43:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-01-01T03:28:00.000Z",
        "voteCount": 1,
        "content": "Correct B"
      },
      {
        "date": "2023-12-09T01:19:00.000Z",
        "voteCount": 1,
        "content": "correct answer is B."
      },
      {
        "date": "2023-12-04T12:38:00.000Z",
        "voteCount": 1,
        "content": "Considering that we are talking about Change Data Feed and the code is filtering by[ \"update_postimage\", \"insert\" ] the column \"_change_type\", I would go with the option E.\n\nReference:\nhttps://docs.delta.io/latest/delta-change-data-feed.html#:~:text=_change_type,update_preimage%20%2C%20update_postimage"
      },
      {
        "date": "2024-01-01T03:28:00.000Z",
        "voteCount": 5,
        "content": "Notice option (\"startingVersion\", 0), which will bring all changes from begining. Hence Answer is B."
      },
      {
        "date": "2023-10-18T10:23:00.000Z",
        "voteCount": 1,
        "content": "why is it Not E. It gets newly inserted or updated records"
      },
      {
        "date": "2023-12-04T12:42:00.000Z",
        "voteCount": 1,
        "content": "I'm with you, follow the reference:\n\nhttps://docs.delta.io/latest/delta-change-data-feed.html#:~:text=_change_type,update_preimage%20%2C%20update_postimage"
      },
      {
        "date": "2024-01-01T03:28:00.000Z",
        "voteCount": 1,
        "content": "Notice .option (\"startingVersion\", 0), which will bring all changes from begining. Hence Answer is B."
      },
      {
        "date": "2023-10-11T03:41:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-09-09T21:35:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer, sorry."
      },
      {
        "date": "2023-09-08T06:39:00.000Z",
        "voteCount": 1,
        "content": "answer is A, because there is a filter as asmayassineg said. Filter filters only existing records from change feed"
      },
      {
        "date": "2023-08-02T04:41:00.000Z",
        "voteCount": 2,
        "content": "sorry, answer is correct B."
      },
      {
        "date": "2023-08-02T04:40:00.000Z",
        "voteCount": 2,
        "content": "Answer is A, since the df is filtering on updated records using update_postimage filter"
      },
      {
        "date": "2024-01-25T11:20:00.000Z",
        "voteCount": 1,
        "content": "there is also insert in the filter."
      },
      {
        "date": "2023-08-23T05:40:00.000Z",
        "voteCount": 6,
        "content": "it's B:\nReading table\u2019s changes, captured by CDF, using spark.read means that you are reading them as a static source. So, each time you run the query, all table\u2019s changes (starting from the specified startingVersion) will be read."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/databricks/view/125387-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A new data engineer notices that a critical field was omitted from an application that writes its Kafka source to Delta Lake. This happened even though the critical field was in the Kafka source. That field was further missing from data written to dependent, long-term storage. The retention threshold on the Kafka service is seven days. The pipeline has been in production for three months.<br>Which describes how Delta Lake can help to avoid data loss of this nature in the future?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Delta log and Structured Streaming checkpoints record the full history of the Kafka producer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake schema evolution can retroactively calculate the correct value for newly added fields, as long as the data was in the original source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake automatically checks that all fields present in the source data are included in the ingestion layer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData can never be permanently dropped or deleted from Delta Lake, so data loss is not possible under any circumstance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngesting all raw data and metadata from Kafka to a bronze Delta table creates a permanent, replayable history of the data state.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-03T18:40:00.000Z",
        "voteCount": 3,
        "content": "Medallion Architecture is named in E. (Ingesting all raw data and metadata from Kafka to a bronze Delta table creates a permanent, replayable history of the data state.)"
      },
      {
        "date": "2024-02-13T23:13:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2024-02-01T06:27:00.000Z",
        "voteCount": 1,
        "content": "I think E is correct"
      },
      {
        "date": "2024-01-10T07:59:00.000Z",
        "voteCount": 1,
        "content": "I think E is correct"
      },
      {
        "date": "2023-11-05T00:55:00.000Z",
        "voteCount": 2,
        "content": "Looks good - E"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/databricks/view/120700-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A nightly job ingests data into a Delta Lake table using the following code:<br><img title=\"image15\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image15.png\"><br>The next step in the pipeline requires a function that returns an object that can be used to manipulate new records that have not yet been processed to the next table in the pipeline.<br>Which code snippet completes this function definition?<br>def new_records():",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\treturn spark.readStream.table(\"bronze\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\treturn spark.readStream.load(\"bronze\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image16\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image16.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\treturn spark.read.option(\"readChangeFeed\", \"true\").table (\"bronze\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image17\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image17.png\">"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-12T14:47:00.000Z",
        "voteCount": 9,
        "content": "# not providing a starting version/timestamp will result in the latest snapshot being fetched first\nspark.readStream.format(\"delta\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .table(\"myDeltaTable\")\nPlease refer:\nhttps://docs.databricks.com/en/delta/delta-change-data-feed.html"
      },
      {
        "date": "2024-10-10T11:37:00.000Z",
        "voteCount": 1,
        "content": "readChangeFeed is disabled by default."
      },
      {
        "date": "2024-08-18T13:31:00.000Z",
        "voteCount": 1,
        "content": "There is no stream in option D"
      },
      {
        "date": "2024-08-25T22:55:00.000Z",
        "voteCount": 2,
        "content": "You can read Delta Lake Change Data Feed without using a stream. You can use batch queries to read the change data feed by setting the readChangeFeed option to true."
      },
      {
        "date": "2023-11-16T06:02:00.000Z",
        "voteCount": 7,
        "content": "In my opinion E is not correct because we do not see parameters pass within to the function (year, month and day)... the function is  def new_records():"
      },
      {
        "date": "2024-10-10T11:56:00.000Z",
        "voteCount": 1,
        "content": "since \"bronze\" table is a delta table, readStream() only returns new data."
      },
      {
        "date": "2024-10-09T07:04:00.000Z",
        "voteCount": 1,
        "content": "If the job runs only once per day, then option E could indeed be a valid and effective solution. Here's why:\n\nDaily Execution: Since the job runs once per day, all records ingested on that day would be new and unprocessed.\nSource File Filtering: The filter condition col(\"source_file\").like(f\"/mnt/daily_batch/{year}/{month}/{day}\") would select only the records that were ingested from the current day's batch file.\nSimplicity: This approach is straightforward and doesn't require maintaining additional state (like last processed version or timestamp).\nReliability: As long as the daily batch files are consistently named and placed in the correct directory structure, this method will reliably capture all new records for that day."
      },
      {
        "date": "2024-09-25T08:33:00.000Z",
        "voteCount": 3,
        "content": "A is correct by Elimination. As stated by Alaverdi in another comment. Reads delta table as a stream and processes only newly arrived records. \n\nB excluded because of incorrect syntax\n\nC excluded, will be an empty result, as ingestion time (which comes as a param in the other method) is compared with current timestamp\n\nD excluded because of syntax error, should be : spark.read.option(\"readChangeFeed\", \"true\").option(\"startingVersion\", 1).table(\"bronze\")\n\nE excluded, will be an empty result, because \u201csource_file\u201d give a filename, while f\"/mnt \n/daily_batch/{year}/{month}/{day}\" gives a folder name"
      },
      {
        "date": "2024-08-18T13:47:00.000Z",
        "voteCount": 1,
        "content": "Actually it's hard to choose between C and E, as both are a bit incorrect:\nOption E - seems like it will be an empty result, as file name is compared with folder name\nOption C - seems like it will be an empty result, as ingestion time (which comes as a param in the other method) is compared with current timestamp.\n\nOn the other hand, if new_records method had an ingestion time param, then the task would be obvious. Also considering the very first line which imports current_timestamp, let me say it's C :))"
      },
      {
        "date": "2024-08-16T06:19:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-08-03T06:29:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer : E\nSince, it selects only those records which have been loaded on the specified date and these records are not processed yet. This is what we want\n\nNot A : It reads all records even the ones previously processed since bronze table keeps historic data.\n\nNot D : It is no where mentioned that change data feed is enabled, nor is it present in the code snippet. This is where we have to be careful with self- assumption"
      },
      {
        "date": "2024-07-30T14:12:00.000Z",
        "voteCount": 3,
        "content": "Option D. return spark.read.option(\"readChangeFeed\", \"true\").table (\"bronze\")\n\nThe following code snippet is from https://delta.io/blog/2023-07-14-delta-lake-change-data-feed-cdf/ where the writer explained what will happen if we give  \"readChangeFeed\", and \"true\". It will include all the details from the respective mentioned version.\n\nIn our in option D starting version is not described it will pick the latest record. Please refer to the doc https://docs.databricks.com/en/delta/delta-change-data-feed.html and find  \"By default, the stream returns the latest snapshot of the table when the stream first starts as an INSERT and future changes as change data.\"\n(\n    spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"people\")\n    .show(truncate=False)\n)"
      },
      {
        "date": "2024-06-27T00:18:00.000Z",
        "voteCount": 2,
        "content": "Both E and A can be correct but in the definition of the function there are no input parameters. This means we can't use them correctly in returned statement only with the given information in the question. This is why I vote for A"
      },
      {
        "date": "2024-06-03T18:45:00.000Z",
        "voteCount": 1,
        "content": "The E option makes more sense because all the partition would be filtered.\nCan't be the options that use CDF because theres no readChangeFeed option in dataframe read"
      },
      {
        "date": "2024-03-27T09:48:00.000Z",
        "voteCount": 2,
        "content": "Since the ingest_daily_batch function writes to the \"bronze\" table in batch mode using spark.read and write operations, we should not use readStream to read from it in the subsequent function."
      },
      {
        "date": "2024-03-13T04:32:00.000Z",
        "voteCount": 1,
        "content": "Probable E, but still filename not specified only folder path"
      },
      {
        "date": "2024-03-08T02:39:00.000Z",
        "voteCount": 2,
        "content": "Please read the question again .\nit is asking to get the data from bronze table to the some downstream table.\nNow as its a append only daily nightly job\nthe filter on file name will give the new data available in bronze table which is still not flown down the pipeline."
      },
      {
        "date": "2024-02-21T08:21:00.000Z",
        "voteCount": 1,
        "content": "D is correct.\nhttps://delta.io/blog/2023-07-14-delta-lake-change-data-feed-cdf/\nCDF can be enabled on non-streaming Delta table.. \"delta\" is default table format."
      },
      {
        "date": "2024-02-13T23:26:00.000Z",
        "voteCount": 1,
        "content": "the question here is how to manipulate new records that have not yet been processed to the next table, since the data has been ingested into the bronze table you need to check whether or not the data ingested daily is already there in the silver table, so I think answer is D. Enabling change data feed allows to track row-level changes between delta table versions \n\nhttps://docs.databricks.com/en/delta/delta-change-data-feed.html"
      },
      {
        "date": "2024-02-05T04:45:00.000Z",
        "voteCount": 1,
        "content": "the problem here is that both A and E are correct. E just follows the previous filtering logic while A uses the readStream method which will have to maintain a checkpoint. But both can work"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/databricks/view/120702-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer is working to implement logic for a Lakehouse table named silver_device_recordings. The source data contains 100 unique fields in a highly nested JSON structure.<br>The silver_device_recordings table will be used downstream to power several production monitoring dashboards and a production model. At present, 45 of the 100 fields are being used in at least one of these applications.<br>The data engineer is trying to determine the best approach for dealing with schema declaration given the highly-nested structure of the data and the numerous fields.<br>Which of the following accurately presents information about Delta Lake and Databricks that may impact their decision-making process?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Tungsten encoding used by Databricks is optimized for storing string data; newly-added native support for querying JSON strings means that string types are always most efficient.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause Delta Lake uses Parquet for data storage, data types can be easily evolved by just modifying file footer information in place.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHuman labor in writing code is the largest cost associated with data engineering workloads; as such, automating table declaration logic should be a priority in all migration workloads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause Databricks will infer schema using types that allow all observed data to be processed, setting types manually provides greater assurance of data quality enforcement.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchema inference and evolution on Databricks ensure that inferred types will always accurately match the data types used by downstream systems."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T02:05:00.000Z",
        "voteCount": 10,
        "content": "A is wrong, because Tungsten is a project around improving Spark's efficiency on memory and CPU usage;\nB is wrong because Parquet does not support file editing, it only supports overwrite and create operations by itself;\nC is wrong because completely automating schema declaration for tables will incur in reduced previsibility for data types and data quality;\nE is false because unlucky sampling can yield bad inferences by Spark;"
      },
      {
        "date": "2024-03-13T23:42:00.000Z",
        "voteCount": 2,
        "content": "from my exam today, both C &amp; D are no longer available, so they can't be correct.\nE &amp; A are available. E states \"always accurate\" so I hesitate to choose it.\nThere is a new option stating like \"delta lake indexes first 32column in delta log for Z order and optimization\"(not sure I remember exactly, it looks statementfully correct). and I chosed this \"new\" option. Because, this should impact the schema decision by putting high-usage field in the first 32 columns."
      },
      {
        "date": "2024-02-05T04:49:00.000Z",
        "voteCount": 1,
        "content": "Only answer that makes sense"
      },
      {
        "date": "2024-01-28T06:26:00.000Z",
        "voteCount": 1,
        "content": "Correct Ans is D"
      },
      {
        "date": "2023-10-16T03:56:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-09-13T17:07:00.000Z",
        "voteCount": 2,
        "content": "D is correct. \nwe can use `schema hint`  to enforce the schema information that we know and expect on an inferred schema."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/databricks/view/123741-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineering team maintains the following code:<br><img title=\"image18\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image18.png\"><br>Assuming that this code produces logically correct results and the data in the source tables has been de-duplicated and validated, which statement describes what will occur when this code is executed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA batch job will update the enriched_itemized_orders_by_account table, replacing only those rows that have different values than the current version of the table, using accountID as the primary key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe enriched_itemized_orders_by_account table will be overwritten using the current valid version of data in each of the three tables referenced in the join logic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn incremental job will leverage information in the state store to identify unjoined rows in the source tables and write these rows to the enriched_iteinized_orders_by_account table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn incremental job will detect if new rows have been written to any of the source tables; if new rows are detected, all results will be recalculated and used to overwrite the enriched_itemized_orders_by_account table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo computation will occur until enriched_itemized_orders_by_account is queried; upon query materialization, results will be calculated using the current valid version of data in each of the three tables referenced in the join logic."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-21T04:40:00.000Z",
        "voteCount": 1,
        "content": "B because code has : .mode(\u201cOverwrite\u201d)"
      },
      {
        "date": "2024-06-03T18:50:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-28T06:30:00.000Z",
        "voteCount": 2,
        "content": "Correct Ans is B"
      },
      {
        "date": "2024-01-13T11:43:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-10-16T03:57:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/databricks/view/124155-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineering team is migrating an enterprise system with thousands of tables and views into the Lakehouse. They plan to implement the target architecture using a series of bronze, silver, and gold tables. Bronze tables will almost exclusively be used by production data engineering workloads, while silver tables will be used to support both data engineering and machine learning workloads. Gold tables will largely serve business intelligence and reporting purposes. While personal identifying information (PII) exists in all tiers of data, pseudonymization and anonymization rules are in place for all data at the silver and gold levels.<br>The organization is interested in reducing security concerns while maximizing the ability to collaborate across diverse teams.<br>Which statement exemplifies best practices for implementing this system?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIsolating tables in separate databases based on data quality tiers allows for easy permissions management through database ACLs and allows physical separation of default storage locations for managed tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause databases on Databricks are merely a logical construct, choices around database organization do not impact security or discoverability in the Lakehouse.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStoring all production tables in a single database provides a unified view of all data assets available throughout the Lakehouse, simplifying discoverability by granting all users view privileges on this database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorking in the default Databricks database provides the greatest security when working with managed tables, as these will be created in the DBFS root.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause all tables must live in the same storage containers used for the database they're created in, organizations should be prepared to create between dozens and thousands of databases depending on their data isolation requirements."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-17T05:31:00.000Z",
        "voteCount": 1,
        "content": "The most logical answer is A"
      },
      {
        "date": "2024-06-03T18:51:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-02-13T23:45:00.000Z",
        "voteCount": 1,
        "content": "answer is A"
      },
      {
        "date": "2024-01-28T07:19:00.000Z",
        "voteCount": 2,
        "content": "Correct Ans is A"
      },
      {
        "date": "2023-11-25T08:42:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/databricks/view/124156-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data architect has mandated that all tables in the Lakehouse should be configured as external Delta Lake tables.<br>Which approach will ensure that this requirement is met?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhenever a database is being created, make sure that the LOCATION keyword is used",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen configuring an external data warehouse for all table storage, leverage Databricks for all ELT.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhenever a table is being created, make sure that the LOCATION keyword is used.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen tables are created, make sure that the EXTERNAL keyword is used in the CREATE TABLE statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the workspace is being configured, make sure that external cloud object storage has been mounted."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-20T04:33:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer. According to the documentation only the LOCATION is needed to make a table external. Moreover, we can also assume the keyword EXTERNAL is optional in the SQL statement.\n\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-external-tables.html"
      },
      {
        "date": "2024-02-10T14:39:00.000Z",
        "voteCount": 3,
        "content": "'A' seems more appropriate.\nAll the tables in Delta lake house should be marked as external.. which can be achieved using location keyword at database level instead of each table level."
      },
      {
        "date": "2023-12-25T18:04:00.000Z",
        "voteCount": 2,
        "content": "Why not D? i know both C and D are same, but D is more precise"
      },
      {
        "date": "2023-12-27T04:46:00.000Z",
        "voteCount": 2,
        "content": "my bad. D is having EXTERNAL keyword, got confused. C is correct answer"
      },
      {
        "date": "2023-11-16T06:48:00.000Z",
        "voteCount": 1,
        "content": "If you set a location in a database level, all tables under this database are automatically external table, in my opinion is A is correct."
      },
      {
        "date": "2024-06-08T03:04:00.000Z",
        "voteCount": 3,
        "content": "According to what I've found in Databricks forums: \"Database location and Table location are independent\". So it looks like specifying location at DB level is not sufficient as tables will be still created as managed ones."
      },
      {
        "date": "2023-10-30T00:04:00.000Z",
        "voteCount": 4,
        "content": "C is correct. Location keyword should be in create script of the table"
      },
      {
        "date": "2023-10-24T06:42:00.000Z",
        "voteCount": 3,
        "content": "C is correct, the key word to be used is Location, the keyword external is optional"
      },
      {
        "date": "2023-10-21T00:17:00.000Z",
        "voteCount": 2,
        "content": "The correct is D"
      },
      {
        "date": "2024-01-25T11:38:00.000Z",
        "voteCount": 1,
        "content": "there is no EXTERNAL key word in databricks, however it is there for other systems like Oracle, Hive, Cassandra etc."
      },
      {
        "date": "2024-05-31T03:08:00.000Z",
        "voteCount": 1,
        "content": "and microsoft synapse"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/databricks/view/122481-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "To reduce storage and compute costs, the data engineering team has been tasked with curating a series of aggregate tables leveraged by business intelligence dashboards, customer-facing applications, production machine learning models, and ad hoc analytical queries.<br>The data engineering team has been made aware of new requirements from a customer-facing application, which is the only downstream workload they manage entirely. As a result, an aggregate table used by numerous teams across the organization will need to have a number of fields renamed, and additional fields will also be added.<br>Which of the solutions addresses the situation while minimally interrupting other teams in the organization without increasing the number of tables that need to be managed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend all users notice that the schema for the table will be changing; include in the communication the logic necessary to revert the new table schema to match historic queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a new table with all the requisite fields and new names and use this as the source for the customer-facing application; create a view that maintains the original data schema and table name by aliasing select fields from the new table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new table with the required schema and new fields and use Delta Lake's deep clone functionality to sync up changes committed to one table to the corresponding table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the current table definition with a logical view defined with the query logic currently writing the aggregate table; create a new table to power the customer-facing application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a table comment warning all users that the table schema and field names will be changing on a given date; overwrite the table in place to the specifications of the customer-facing application."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-13T05:52:00.000Z",
        "voteCount": 6,
        "content": "Create view. Can't be B as -&gt; without increasing the number of tables that need to be managed"
      },
      {
        "date": "2024-02-05T05:22:00.000Z",
        "voteCount": 5,
        "content": "B makes way more sense, the number of tables managed do not increase since the old table won't be used anymore, then the view on top of this table is not another table to manage, just maintains the \"original API\" of the table to avoid breaking changes in downstream applications"
      },
      {
        "date": "2024-10-10T12:54:00.000Z",
        "voteCount": 1,
        "content": "D will not increate the number of table. It will create a new table and replace the aggregation table with a view.  B will create a new table, a new view match old table name and schema,  aggregation table still there."
      },
      {
        "date": "2024-09-13T23:26:00.000Z",
        "voteCount": 2,
        "content": "option D is correct \ndocs : https://docs.databricks.com/en/delta/update-schema.html\nalso they specifically says that they dont want to increase managed tables!"
      },
      {
        "date": "2024-09-13T23:32:00.000Z",
        "voteCount": 2,
        "content": "Reasons :\nNo Increase in Managed Tables: By replacing the current table with a view, you maintain the same number of managed tables.\nBackward Compatibility: The view can mimic the original table\u2019s schema, ensuring that existing queries and applications continue to function without modification.\nDedicated Table for New Requirements: The new table can be tailored to meet the specific needs of the customer-facing application without affecting other users."
      },
      {
        "date": "2024-08-21T05:54:00.000Z",
        "voteCount": 1,
        "content": "B is correct, no new tables, and minimally interrupting other teams in the organization\nA &amp; E excluded, because they interrupt other teams in the organisation, usually answer that require user communication are wrong answers.\nC excluded, because it\u2019s used for table creation, not after creation\nD excluded because it increases the number of tables"
      },
      {
        "date": "2024-08-21T04:48:00.000Z",
        "voteCount": 2,
        "content": "B,C and D all state creating a new table, therefore increasing the number of tables to manage. This is exactly what the question says to avoid.\n\n\"minimally interrupting other teams in the organization without increasing the number of tables that need to be managed\"\n\nAnswer A is the only one that makes sense and is pretty standard operation procedure for databases. E is wrong because you would never update a column comment to inform users of anything."
      },
      {
        "date": "2024-08-03T06:38:00.000Z",
        "voteCount": 2,
        "content": "B is correct.\nWhy not D: Because it will create interruption when you replace the current table with a view and question says minimal interruption"
      },
      {
        "date": "2024-07-16T01:29:00.000Z",
        "voteCount": 2,
        "content": "I would go for B.\n\nWith option B you will run the aggregations once and store in in a table, then present these aggregations in the old schema in a view.\n\nWith D the aggregations will be done twice, for the old schema view and for the new table."
      },
      {
        "date": "2024-04-26T00:26:00.000Z",
        "voteCount": 1,
        "content": "to me it's b because by creating a new table + the view that will substitute the previous table we still have 1 table. It seems to be the most efficient way to solve this. Not 100% sure though"
      },
      {
        "date": "2024-03-13T23:45:00.000Z",
        "voteCount": 2,
        "content": "in my exam today I chose D."
      },
      {
        "date": "2024-01-10T00:18:00.000Z",
        "voteCount": 2,
        "content": "I think it's B. D replaces original table definition with a view, which will run up compute costs for queries using the table."
      },
      {
        "date": "2023-12-18T16:58:00.000Z",
        "voteCount": 4,
        "content": "D. B has new table and view created."
      },
      {
        "date": "2023-10-30T00:36:00.000Z",
        "voteCount": 1,
        "content": "B is definitely the best option"
      },
      {
        "date": "2023-10-21T17:57:00.000Z",
        "voteCount": 2,
        "content": "B is suitable for fact , don't interrupt the end-user , just managed by technical term. The technical team will create view refer field mapping ."
      },
      {
        "date": "2023-10-16T04:02:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-10-04T21:18:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/databricks/view/123743-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Delta Lake table representing metadata about content posts from users has the following schema: user_id LONG, post_text STRING, post_id STRING, longitude FLOAT, latitude FLOAT, post_time TIMESTAMP, date DATE<br>This table is partitioned by the date column. A query is run with the following filter: longitude &lt; 20 &amp; longitude &gt; -20<br>Which statement describes how data will be filtered?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStatistics in the Delta Log will be used to identify partitions that might Include files in the filtered range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo file skipping will occur because the optimizer does not know the relationship between the partition column and the longitude.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Delta Engine will use row-level statistics in the transaction log to identify the flies that meet the filter criteria.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStatistics in the Delta Log will be used to identify data files that might include records in the filtered range.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Delta Engine will scan the parquet file footers to identify each row that meets the filter criteria."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-25T08:48:00.000Z",
        "voteCount": 7,
        "content": "D is correct. A partition can include multiple files. And the statistics are collected for each file."
      },
      {
        "date": "2024-08-21T06:04:00.000Z",
        "voteCount": 1,
        "content": "Min and max values of each parquet file are stored in Delta Logs\nDelta data skipping automatically collects the stats (min, max, etc.) for the first 32 columns for each underlying Parquet file when you write data into a Delta table. Databricks takes advantage of this information (minimum and maximum values) at query time to skip unnecessary files in order to speed up the queries.\nhttps://www.databricks.com/discover/pages/optimize-data-workloads-guide#delta-data"
      },
      {
        "date": "2024-01-28T07:25:00.000Z",
        "voteCount": 2,
        "content": "Correct Ans is D"
      },
      {
        "date": "2023-10-30T00:39:00.000Z",
        "voteCount": 1,
        "content": "I guess C option is right since transaction log contains information about max/min values of first 32 columns, it can be used in order to filter files."
      },
      {
        "date": "2023-10-31T01:05:00.000Z",
        "voteCount": 3,
        "content": "I reread the question and thing that I made a mistake, in option C there is information about row-level statistics, but, I guess, statistics in Delta Log it is more less about columns. So, now D looks fine for me."
      },
      {
        "date": "2023-10-16T04:04:00.000Z",
        "voteCount": 3,
        "content": "D is Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/databricks/view/124285-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A small company based in the United States has recently contracted a consulting firm in India to implement several new data engineering pipelines to power artificial intelligence applications. All the company's data is stored in regional cloud storage in the United States.<br>The workspace administrator at the company is uncertain about where the Databricks workspace used by the contractors should be deployed.<br>Assuming that all data governance considerations are accounted for, which statement accurately informs this decision?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks runs HDFS on cloud volume storage; as such, cloud virtual machines must be deployed in the region where the data is stored.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks workspaces do not rely on any regional infrastructure; as such, the decision should be made based upon what is most convenient for the workspace administrator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCross-region reads and writes can incur significant costs and latency; whenever possible, compute should be deployed in the same region the data is stored.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks leverages user workstations as the driver during interactive development; as such, users should always use a workspace deployed in a region they are physically near.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks notebooks send all executable code from the user\u2019s browser to virtual machines over the open internet; whenever possible, choosing a workspace region near the end users is the most secure."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-03T19:16:00.000Z",
        "voteCount": 1,
        "content": "(C)\nThe decision is about where the Databricks workspace used by the contractors should be deployed. The contractors are based in India, while all the company's data is stored in regional cloud storage in the United States. When choosing a region for deploying a Databricks workspace, one of the important factors to consider is the proximity to the data sources and sinks. Cross-region reads and writes can incur significant costs and latency due to network bandwidth and data transfer fees. Therefore, whenever possible, compute should be deployed in the same region the data is stored to optimize performance and reduce costs"
      },
      {
        "date": "2024-01-24T12:37:00.000Z",
        "voteCount": 3,
        "content": "C is the answer."
      },
      {
        "date": "2024-01-05T03:24:00.000Z",
        "voteCount": 2,
        "content": "An important part of data governance is usage cost, and, as a general data engineering practice, egress costs related to moving data between regions is always an important consideration. Having the workspaces located in a different region than the contractors will incur to them in very little nuisance, while greatly saving in this sense."
      },
      {
        "date": "2023-12-29T05:54:00.000Z",
        "voteCount": 1,
        "content": "From where data engineering team developes pipelines is independent of where the data objects reside in the cloud storage."
      },
      {
        "date": "2024-05-19T17:00:00.000Z",
        "voteCount": 2,
        "content": "These pipelines will create clusters (machines) which will reside in a different region than the data and that will cause latency issues. So C should be the correct option."
      },
      {
        "date": "2023-10-21T18:47:00.000Z",
        "voteCount": 2,
        "content": "C is correct."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/databricks/view/117481-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The downstream consumers of a Delta Lake table have been complaining about data quality issues impacting performance in their applications. Specifically, they have complained that invalid latitude and longitude values in the activity_details table have been breaking their ability to use other geolocation processes.<br>A junior engineer has written the following code to add CHECK constraints to the Delta Lake table:<br><img title=\"image19\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image19.png\"><br>A senior engineer has confirmed the above logic is correct and the valid ranges for latitude and longitude are provided, but the code fails when executed.<br>Which statement explains the cause of this failure?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause another team uses this table to support a frequently running application, two-phase locking is preventing the operation from committing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe activity_details table already exists; CHECK constraints can only be added during initial table creation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe activity_details table already contains records that violate the constraints; all existing data must pass CHECK constraints in order to add them to an existing table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe activity_details table already contains records; CHECK constraints can only be added prior to inserting values into a table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe current table schema does not contain the field valid_coordinates; schema evolution will need to be enabled before altering the table to add a constraint."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-06T07:56:00.000Z",
        "voteCount": 12,
        "content": "incorrect the correct option is C, with constraints, if added to an existing table the existing data in the table must be consistent with the constraint otherwise it fails\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-alter-table.html#add-constraint"
      },
      {
        "date": "2024-08-21T06:42:00.000Z",
        "voteCount": 1,
        "content": "-- CREATE TABLE \ncreate table test_constraint (t1 varchar(2), n1 int);\n\n-- ADD VALUE \ninsert into test_constraint values ('v3', 3);\n\n-- ADD CONSTAINT VIOLATED BY CURRENT DATA \n-- should throw error : 1 row in spark_catalog.default.test_constaint violate the new CHECK constraint (n1 &lt; 3)\n\nalter table test_constraint add constraint valid_n1 check (n1 &lt; 3);\n\n-- ADD CONSTAINT NOT VIOLATED BY CURRENT DATA (no error)\nalter table test_constraint add constraint valid_n1 check (n1 &lt; 100);"
      },
      {
        "date": "2024-08-03T06:44:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2024-02-09T00:51:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-02-01T21:27:00.000Z",
        "voteCount": 1,
        "content": "correct ans is C"
      },
      {
        "date": "2024-01-28T07:41:00.000Z",
        "voteCount": 1,
        "content": "correct ans is C"
      },
      {
        "date": "2024-01-13T11:54:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-01-10T08:58:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-12-29T05:57:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-12-03T03:49:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-11-25T08:51:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-11-21T07:49:00.000Z",
        "voteCount": 2,
        "content": "When adding a CHECK constraint to an existing table, the operation will fail if there are any rows in the table that do not meet the constraint. Before a CHECK constraint can be added, the data already in the table must be validated to ensure that it complies with the constraint conditions. If any existing records violate the new constraints, they must be corrected or removed before the ALTER TABLE command can be successfully executed."
      },
      {
        "date": "2023-11-02T12:29:00.000Z",
        "voteCount": 1,
        "content": "Correct option C : existing data violated check constraint condition"
      },
      {
        "date": "2023-10-29T23:50:00.000Z",
        "voteCount": 1,
        "content": "Right answer is C"
      },
      {
        "date": "2023-10-16T04:07:00.000Z",
        "voteCount": 1,
        "content": "C - table already has data"
      },
      {
        "date": "2023-09-23T16:20:00.000Z",
        "voteCount": 1,
        "content": "Yes the correct is option C"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/databricks/view/124424-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which of the following is true of Delta Lake and the Lakehouse?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause Parquet compresses data row by row. strings will only be compressed when a character is repeated multiple times.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake automatically collects statistics on the first 32 columns of each table which are leveraged in data skipping based on query filters.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tViews in the Lakehouse maintain a valid cache of the most recent versions of source tables at all times.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPrimary and foreign key constraints can be leveraged to ensure duplicate values are never entered into a dimension table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tZ-order can only be applied to numeric values stored in Delta Lake tables."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T00:53:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-02-05T05:37:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-01-24T12:41:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-04T07:52:00.000Z",
        "voteCount": 1,
        "content": "Can anyone explain why D is not correct?"
      },
      {
        "date": "2024-01-05T02:59:00.000Z",
        "voteCount": 2,
        "content": "Because Primary &amp; Foreign Key information is not enforced. \n\"Primary and foreign keys are informational only and are not enforced\" from:\nhttps://docs.databricks.com/en/tables/constraints.html#declare-primary-key-and-foreign-key-relationships"
      },
      {
        "date": "2023-12-29T05:59:00.000Z",
        "voteCount": 3,
        "content": "B is correct since statistics are collected for the first 32 columns and stored in the transaction log."
      },
      {
        "date": "2024-08-21T06:53:00.000Z",
        "voteCount": 1,
        "content": "https://www.databricks.com/discover/pages/optimize-data-workloads-guide#delta-data\n\nDelta data skipping automatically collects the stats (min, max, etc.) for the first 32 columns for each underlying Parquet file when you write data into a Delta table. Databricks takes advantage of this information (minimum and maximum values) at query time to skip unnecessary files in order to speed up the queries."
      },
      {
        "date": "2023-12-26T19:50:00.000Z",
        "voteCount": 1,
        "content": "B is correct\uff0c \nC is error, con't have new cache in view"
      },
      {
        "date": "2023-12-20T00:15:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-10-22T22:41:00.000Z",
        "voteCount": 1,
        "content": "B is correct.\nhttps://docs.delta.io/2.0.0/table-properties.html"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/databricks/view/124425-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The view updates represents an incremental batch of all newly ingested data to be inserted or updated in the customers table.<br>The following logic is used to process these records.<br><img title=\"image20\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image20.png\"><br>Which statement describes this implementation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customers table is implemented as a Type 3 table; old values are maintained as a new column alongside the current value.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customers table is implemented as a Type 2 table; old values are maintained but marked as no longer current and new values are inserted.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customers table is implemented as a Type 0 table; all writes are append only with no changes to existing values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customers table is implemented as a Type 1 table; old values are overwritten by new values and no history is maintained.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customers table is implemented as a Type 2 table; old values are overwritten and new customers are appended."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-03T19:28:00.000Z",
        "voteCount": 2,
        "content": "B. The customers table is implemented as a Type 2 table; old values are maintained but marked as no longer current and new values are inserted.\n\nA Type 1 table does not track changes in dimensional attributes - the new value overwrites the existing value. Here, we do not preserve historical changes in data.\n\nA Type 2 Table tracks change over time by creating new rows for each change. A new dimension record is inserted with a high-end date or one with NULL. The previous record is \"closed\" with an end date. This approach maintains a complete history of changes and allows for as-was reporting use cases.\n\nA data warehousing method called Slowly Changing Dimension (SCD) Type 3 is used to track both the old and new values while managing historical changes in data over time. To reflect the historical and present values of an attribute, SCD Type 3 keeps two extra columns in the dimension table."
      },
      {
        "date": "2024-01-24T12:43:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2024-01-10T09:03:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-10-22T23:09:00.000Z",
        "voteCount": 2,
        "content": "B is correct."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/databricks/view/124922-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The DevOps team has configured a production workload as a collection of notebooks scheduled to run daily using the Jobs UI. A new data engineering hire is onboarding to the team and has requested access to one of these notebooks to review the production logic.<br>What are the maximum notebook permissions that can be granted to the user without allowing accidental changes to production code or data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCan Manage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCan Edit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo permissions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCan Read\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCan Run"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-08T05:19:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C. https://learn.microsoft.com/en-us/azure/databricks/security/auth/access-control/#--notebook-acls"
      },
      {
        "date": "2024-08-26T06:49:00.000Z",
        "voteCount": 2,
        "content": "can run is the correct answer here because the question asked for the maximum possible permission without editing."
      },
      {
        "date": "2024-09-05T12:06:00.000Z",
        "voteCount": 2,
        "content": "disagree, it says \"changes to production code or data\". Can Run permission allows to run a workflow which will cause changes in data"
      },
      {
        "date": "2024-08-21T07:33:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/security/auth/access-control/index.html#notebook-acls"
      },
      {
        "date": "2023-11-12T02:42:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2023-10-29T23:29:00.000Z",
        "voteCount": 1,
        "content": "Correct, D"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/databricks/view/124427-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A table named user_ltv is being used to create a view that will be used by data analysts on various teams. Users in the workspace are configured into groups, which are used for setting up data access using ACLs.<br>The user_ltv table has the following schema:<br>email STRING, age INT, ltv INT<br>The following view definition is executed:<br><img title=\"image21\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image21.png\"><br>An analyst who is not a member of the marketing group executes the following query:<br><br>SELECT * FROM email_ltv -<br>Which statement describes the results returned by this query?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThree columns will be returned, but one column will be named \"REDACTED\" and contain only null values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly the email and ltv columns will be returned; the email column will contain all null values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe email and ltv columns will be returned with the values in user_ltv.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe email.age, and ltv columns will be returned with the values in user_ltv.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly the email and ltv columns will be returned; the email column will contain the string \"REDACTED\" in each row.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-21T07:43:00.000Z",
        "voteCount": 1,
        "content": "A, D incorrect because 2 columns email &amp; ltv are returned. \nB incorrect because email will not always contain null values (unless email is null)\nThe user is not a member of \u201cmarketing\u201d, so 3 is the correct answer. If  the user were a member of \"marketing\u201d group, correct answer would have been C"
      },
      {
        "date": "2024-06-08T05:32:00.000Z",
        "voteCount": 1,
        "content": "E, only email column is selected and is not allowed to be viewed by the user"
      },
      {
        "date": "2023-11-12T02:51:00.000Z",
        "voteCount": 2,
        "content": "sure E"
      },
      {
        "date": "2023-11-04T23:33:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2023-10-23T00:22:00.000Z",
        "voteCount": 1,
        "content": "E is correct."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/databricks/view/128158-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data governance team has instituted a requirement that all tables containing Personal Identifiable Information (PH) must be clearly annotated. This includes adding column comments, table comments, and setting the custom table property \"contains_pii\" = true.<br>The following SQL DDL statement is executed to create a new table:<br><img title=\"image22\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image22.png\"><br>Which command allows manual confirmation that these three requirements have been met?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDESCRIBE EXTENDED dev.pii_test\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDESCRIBE DETAIL dev.pii_test",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSHOW TBLPROPERTIES dev.pii_test",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDESCRIBE HISTORY dev.pii_test",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSHOW TABLES dev"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-02T03:24:00.000Z",
        "voteCount": 3,
        "content": "looks like A &amp; C are correct.. https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-show-tblproperties.html#show-tblproperties"
      },
      {
        "date": "2024-01-02T03:25:00.000Z",
        "voteCount": 3,
        "content": "if we want to see also columns comments then A"
      },
      {
        "date": "2023-12-09T10:29:00.000Z",
        "voteCount": 4,
        "content": "correct A !"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/databricks/view/124428-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data governance team is reviewing code used for deleting records for compliance with GDPR. They note the following logic is used to delete records from the Delta Lake table named users.<br><img title=\"image23\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image23.png\"><br>Assuming that user_id is a unique identifying key and that delete_requests contains all users that have requested deletion, which statement describes whether successfully executing the above logic guarantees that the records to be deleted are no longer accessible and why?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes; Delta Lake ACID guarantees provide assurance that the DELETE command succeeded fully and permanently purged these records.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; the Delta cache may return records from previous versions of the table until the cluster is restarted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes; the Delta cache immediately updates to reflect the latest data files recorded to disk.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; the Delta Lake DELETE command only provides ACID guarantees when combined with the MERGE INTO command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T07:47:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2023-12-25T01:14:00.000Z",
        "voteCount": 2,
        "content": "E is correct."
      },
      {
        "date": "2023-11-12T03:00:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-10-23T00:34:00.000Z",
        "voteCount": 1,
        "content": "E is correct."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/databricks/view/118940-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "An external object storage container has been mounted to the location /mnt/finance_eda_bucket.<br>The following logic was executed to create a database for the finance team:<br><img title=\"image24\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image24.png\"><br>After the database was successfully created and permissions configured, a member of the finance team runs the following code:<br><img title=\"image25\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image25.png\"><br>If all users on the finance team are members of the finance group, which statement describes how the tx_sales table will be created?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA logical table will persist the query plan to the Hive Metastore in the Databricks control plane.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn external table will be created in the storage container mounted to /mnt/finance_eda_bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA logical table will persist the physical plan to the Hive Metastore in the Databricks control plane.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn managed table will be created in the storage container mounted to /mnt/finance_eda_bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA managed table will be created in the DBFS root storage container."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-23T19:57:00.000Z",
        "voteCount": 11,
        "content": "Correct Answer D\n\nhttps://docs.databricks.com/en/data-governance/unity-catalog/create-schemas.html#language-SQL"
      },
      {
        "date": "2023-09-05T03:35:00.000Z",
        "voteCount": 2,
        "content": "you are right, it is managed table"
      },
      {
        "date": "2023-09-06T05:07:00.000Z",
        "voteCount": 2,
        "content": "Nope, you are talking about MANAGED LOCATION (from Unity). In the question states LOCATION (not Unity based), which is not managed\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-schema.html"
      },
      {
        "date": "2023-09-06T05:19:00.000Z",
        "voteCount": 8,
        "content": "Effectively doing a test on one of my clusters the table is MANAGED"
      },
      {
        "date": "2023-09-23T17:18:00.000Z",
        "voteCount": 7,
        "content": "Every unmanaged(external) table creation needs to put keyword LOCATION despite if database, that table resides, is put with LOCATION sententece. So B is incorrect. D is correct because the sentence to creates the table is a managed table.\n\nhttps://docs.databricks.com/en/lakehouse/data-objects.html"
      },
      {
        "date": "2024-10-17T04:46:00.000Z",
        "voteCount": 1,
        "content": "No USE DATABASE statement otherwise it would have been external"
      },
      {
        "date": "2024-10-17T04:46:00.000Z",
        "voteCount": 1,
        "content": "No USE DATABASE statement otherwise it would have been external"
      },
      {
        "date": "2024-05-19T17:16:00.000Z",
        "voteCount": 2,
        "content": "D as the word LOCATION is not specified. Although the data will be stored in an external location but the table will still be a managed table."
      },
      {
        "date": "2024-02-26T00:51:00.000Z",
        "voteCount": 2,
        "content": "E is correct coz this table is managed on top of the external source file. amanaged tables are stored on DBFS."
      },
      {
        "date": "2024-02-25T06:29:00.000Z",
        "voteCount": 2,
        "content": "Correct answer D.\njust did a test. As with DBR12.2, UC databases are not supported with location on dbfs, but s3/abfss. However, Hive_metastore databases are supported with location on dbfs. Then, a table created in this database IS a managed table, as verified with describe extend command."
      },
      {
        "date": "2024-01-27T04:37:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer E"
      },
      {
        "date": "2024-01-11T07:51:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer, the table created is a managed table and not external, and it will be located under the location defined in the database's creation DDL."
      },
      {
        "date": "2023-12-09T04:36:00.000Z",
        "voteCount": 2,
        "content": "It will be a managed table created under specified database. Location keyword used for database will make sure all the managed tables are stored in database location."
      },
      {
        "date": "2023-11-25T08:59:00.000Z",
        "voteCount": 4,
        "content": "D is correct. The table will be created as managed, because no LOCATION is specified on table creation. The table will be created in the location specified with database creation"
      },
      {
        "date": "2023-11-08T16:16:00.000Z",
        "voteCount": 1,
        "content": "I think the answer id D"
      },
      {
        "date": "2023-11-05T12:29:00.000Z",
        "voteCount": 2,
        "content": "I followed the steps to create schema and table, the answer is D"
      },
      {
        "date": "2023-10-30T09:15:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D. \"data for a managed table resides in the location of the database it is registered to"
      },
      {
        "date": "2023-10-16T04:29:00.000Z",
        "voteCount": 2,
        "content": "A managed table will be created on DBFS."
      },
      {
        "date": "2024-04-20T04:51:00.000Z",
        "voteCount": 2,
        "content": "The LOCATION of a database will determine the default location for data of all tables registered to that database.\nfrom the documentation"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/databricks/view/122555-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Although the Databricks Utilities Secrets module provides tools to store sensitive credentials and avoid accidentally displaying them in plain text users should still be careful with which credentials are stored here and which users have access to using these secrets.<br>Which statement describes a limitation of Databricks Secrets?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the SHA256 hash is used to obfuscate stored secrets, reversing this hash will display the value in plain text.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccount administrators can see all secrets in plain text by logging on to the Databricks Accounts console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSecrets are stored in an administrators-only table within the Hive Metastore; database administrators have permission to query this table by default.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIterating through a stored secret and printing each character will display secret contents in plain text.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Databricks REST API can be used to list secrets in plain text if the personal access token has proper credentials."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-21T05:57:00.000Z",
        "voteCount": 2,
        "content": "value = dbutils.secrets.get(scope=\"myScope\", key=\"myKey\")\n\nfor char in value:\n    print(char, end=\" \")\n\nOut:\ny o u r _ v a l u e"
      },
      {
        "date": "2024-05-19T17:35:00.000Z",
        "voteCount": 1,
        "content": "Only through REST API or CLI you can fetch the secret if you have valid token"
      },
      {
        "date": "2024-04-11T17:54:00.000Z",
        "voteCount": 2,
        "content": "E: https://docs.databricks.com/api/azure/workspace/secrets/listsecrets \nGET  /api/2.0/secrets/list won\u2019t list secrets in plain text.\nD: if print it without iterating it in a for loop the output is kind of encrypted where it is showing [REDACTED]. But, if I do it as shown in the screenshot, I'm able to see the value of the secret key.\nhttps://community.databricks.com/t5/data-engineering/how-to-avoid-databricks-secret-scope-from-exposing-the-value-of/td-p/12254\nhttps://docs.databricks.com/en/security/secrets/redaction.html\nSecret redaction for notebook cell output applies only to literals. The secret redaction functionality does not prevent deliberate and arbitrary transformations of a secret literal."
      },
      {
        "date": "2024-02-24T11:19:00.000Z",
        "voteCount": 2,
        "content": "Both D and E seems correct.\nThey are poorly written thought because for D just printing the characters (not separated by spaces, newlines or something) would not work, while E if launched inside databricks workspace would not work neither."
      },
      {
        "date": "2024-02-09T01:25:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2024-02-07T02:20:00.000Z",
        "voteCount": 2,
        "content": "D is for sure correct (tried it several times on a Databricks environment)."
      },
      {
        "date": "2024-02-07T02:21:00.000Z",
        "voteCount": 1,
        "content": "Regarding E, it can list secrets (with scopes) but I am not sure it can list secret contents."
      },
      {
        "date": "2024-02-02T01:09:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2024-01-26T12:19:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2024-01-24T01:17:00.000Z",
        "voteCount": 1,
        "content": "At least E is a correct answer.\n\nB: You can't see secrets in Admin console. Only via REST API, CLI etc.\nC: Secrets are. not stored in Hive Metastore.\nD: I am not sure if iterating through secret character by character would work?\nE: This is at least correct. Using this."
      },
      {
        "date": "2024-01-14T05:55:00.000Z",
        "voteCount": 1,
        "content": "B and E both seems to be correct: \n\nhttps://community.databricks.com/t5/data-engineering/how-to-avoid-databricks-secret-scope-from-exposing-the-value-of/td-p/12254/page/2"
      },
      {
        "date": "2024-01-13T12:18:00.000Z",
        "voteCount": 2,
        "content": "For sure it's D"
      },
      {
        "date": "2023-12-29T10:47:00.000Z",
        "voteCount": 3,
        "content": "Answer is E: \n/api/2.0/secrets/get\n{\n  \"key\": \"string\",\n  \"value\": \"string\"\n}\nThe REST API can potentially expose secrets in plain text if a user with appropriate permissions (including access to both secrets/list and secrets/get) uses a personal access token."
      },
      {
        "date": "2023-12-29T06:11:00.000Z",
        "voteCount": 2,
        "content": "Iterating through the secrets provides a way to see the secret's password."
      },
      {
        "date": "2023-11-25T09:02:00.000Z",
        "voteCount": 1,
        "content": "D is correct, see https://community.databricks.com/t5/data-engineering/how-to-avoid-databricks-secret-scope-from-exposing-the-value-of/td-p/12254/page/2"
      },
      {
        "date": "2023-12-29T10:49:00.000Z",
        "voteCount": 1,
        "content": "you didn't read the entire document, they are also using the get api to print the secret."
      },
      {
        "date": "2023-11-21T08:25:00.000Z",
        "voteCount": 3,
        "content": "While Databricks Secrets are designed to secure sensitive information such as passwords and tokens, one limitation is that if a user's personal access token is compromised, and that token has the necessary permissions, the REST API could potentially be used to retrieve secrets. This means that the security of secrets is also dependent on the security of personal access tokens and the permissions assigned to them."
      },
      {
        "date": "2023-11-04T18:50:00.000Z",
        "voteCount": 2,
        "content": "E is the correct answer because it describes a limitation of Databricks Secrets. Databricks Secrets is a module that provides tools to store sensitive credentials and avoid accidentally displaying them in plain text. Databricks Secrets allows creating secret scopes, which are collections of secrets that can be accessed by users or groups. Databricks Secrets also allows creating and managing secrets using the Databricks CLI or the Databricks REST API. However, a limitation of Databricks Secrets is that the Databricks REST API can be used to list secrets in plain text if the personal access token has proper credentials. Therefore, users should still be careful with which credentials are stored in Databricks Secrets and which users have access to using these secrets."
      },
      {
        "date": "2023-11-02T08:06:00.000Z",
        "voteCount": 2,
        "content": "Answer is D based on Udemy practice test"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/databricks/view/117482-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "What statement is true regarding the retention of job run history?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt is retained until you export or delete job run logs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt is retained for 30 days, during which time you can deliver job run logs to DBFS or S3",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt is retained for 60 days, during which you can export notebook run results to HTML\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt is retained for 60 days, after which logs are archived",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt is retained for 90 days or until the run-id is re-used through custom run configuration"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-08T11:16:00.000Z",
        "voteCount": 9,
        "content": "B is wrong, Should be C."
      },
      {
        "date": "2023-12-25T20:09:00.000Z",
        "voteCount": 6,
        "content": "C is correct answer. https://docs.databricks.com/en/workflows/jobs/monitor-job-runs.html"
      },
      {
        "date": "2024-02-25T07:05:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/monitor-job-runs\nAzure Databricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends exporting results before they expire. For more information, see Export job run results."
      },
      {
        "date": "2024-01-05T21:54:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-12-29T06:13:00.000Z",
        "voteCount": 2,
        "content": "c is correct"
      },
      {
        "date": "2023-12-20T12:08:00.000Z",
        "voteCount": 2,
        "content": "Option C is correct"
      },
      {
        "date": "2023-12-20T00:32:00.000Z",
        "voteCount": 1,
        "content": "A secret CAN be printer character-by-character, so it's not really that secure."
      },
      {
        "date": "2023-12-20T00:34:00.000Z",
        "voteCount": 1,
        "content": "Whoops, answer meant for previous question in the bank. Admin, please delete or move."
      },
      {
        "date": "2023-12-09T10:49:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-12-09T04:40:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      },
      {
        "date": "2023-10-16T04:33:00.000Z",
        "voteCount": 1,
        "content": "C is correct: retention is 60 days and export to html"
      },
      {
        "date": "2023-08-06T08:04:00.000Z",
        "voteCount": 3,
        "content": "this is incorrect databricks maintains a history of job runs for 60 dayshttps://docs.databricks.com/en/workflows/jobs/monitor-job-runs.html#:~:text=Databricks%20maintains%20a%20history%20of,see%20Export%20job%20run%20results."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/databricks/view/128161-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data engineer, User A, has promoted a new pipeline to production by using the REST API to programmatically create several jobs. A DevOps engineer, User B, has configured an external orchestration tool to trigger job runs through the REST API. Both users authorized the REST API calls using their personal access tokens.<br>Which statement describes the contents of the workspace audit logs concerning these events?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the REST API was used for job creation and triggering runs, a Service Principal will be automatically used to identify these events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause User B last configured the jobs, their identity will be associated with both the job creation events and the job run events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause these events are managed separately, User A will have their identity associated with the job creation events and User B will have their identity associated with the job run events.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the REST API was used for job creation and triggering runs, user identity will not be captured in the audit logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause User A created the jobs, their identity will be associated with both the job creation events and the job run events.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-12T08:40:00.000Z",
        "voteCount": 2,
        "content": "By default, jobs run as the identity of the job owner. This means that the job assumes the permissions of the job owner. You can change the identity that the job is running as to a service principal. Then, the job assumes the permissions of that service principal instead of the owner.\nhttps://docs.databricks.com/en/jobs/create-run-jobs.html#run-a-job-as-a-service-principal"
      },
      {
        "date": "2024-02-25T07:38:00.000Z",
        "voteCount": 4,
        "content": "https://docs.databricks.com/api/azure/workspace/jobs/create\nAPI/jobs/create:run_as\nobject\nWrite-only setting, available only in Create/Update/Reset and Submit calls. Specifies the user or service principal that the job runs as. If not specified, the job runs as the user who created the job.\nIn the question, it's not stated that user A creates a service principal. So runas can only be himself."
      },
      {
        "date": "2024-02-10T07:56:00.000Z",
        "voteCount": 1,
        "content": "When you create a job your role is IS OWNER and RUN AS. So when you trigger a job, it will run as the RUN AS entity. And it should be user A if someone dosen't have changed it"
      },
      {
        "date": "2024-01-24T13:21:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-12-09T10:53:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/databricks/view/118941-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A user new to Databricks is trying to troubleshoot long execution times for some pipeline logic they are working on. Presently, the user is executing code cell-by-cell, using display() calls to confirm code is producing the logically correct results as new transformations are added to an operation. To get a measure of average time to execute, the user is running each cell multiple times interactively.<br>Which of the following adjustments will get a more accurate measure of how code is likely to perform in production?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScala is the only language that can be accurately tested using interactive notebooks; because the best performance is achieved by using Scala code compiled to JARs, all PySpark and Spark SQL logic should be refactored.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe only way to meaningfully troubleshoot code execution times in development notebooks Is to use production-sized data and production-sized clusters with Run All execution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProduction code development should only be done using an IDE; executing code against a local build of open source Spark and Delta Lake will provide the most accurate benchmarks for how code will perform in production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCalling display() forces a job to trigger, while many transformations will only add to the logical query plan; because of caching, repeated execution of the same logic does not provide meaningful results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Jobs UI should be leveraged to occasionally run the notebook as a job and track execution time during incremental code development because Photon can only be enabled on clusters launched for scheduled jobs."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-16T21:20:00.000Z",
        "voteCount": 2,
        "content": "B and D are correct. The question says \"which statements\" which suggests us that this is a question with multiple choices"
      },
      {
        "date": "2024-07-26T01:06:00.000Z",
        "voteCount": 3,
        "content": "Both D and B are correct. But in real life some times clients dose not accept to gave you there production data to test easily. Also it says in B it is \u201cthe only way\u201d ans this is not true for me\n\nSo i will go with D"
      },
      {
        "date": "2024-09-29T17:07:00.000Z",
        "voteCount": 1,
        "content": "I would add to this and say that this *could* be a multi-choice question (possibly) as practicioner mentions above. But if it isn't, I would go with D as well."
      },
      {
        "date": "2024-03-13T11:31:00.000Z",
        "voteCount": 4,
        "content": "These people voting D have no reading comprehension."
      },
      {
        "date": "2024-03-13T06:35:00.000Z",
        "voteCount": 2,
        "content": "Close env size volumes as possible so results make sense"
      },
      {
        "date": "2024-03-06T14:53:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2024-02-27T03:21:00.000Z",
        "voteCount": 1,
        "content": "I will go with D"
      },
      {
        "date": "2024-02-21T10:50:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer\n\nA. Scala is the only language accurately tested using notebooks: Not true. Spark SQL and PySpark can be accurately tested in notebooks, and production performance doesn't solely depend on language choice.\nB. Production-sized data and clusters are necessary: While ideal, it's not always feasible for development. Smaller datasets and clusters can provide indicative insights.\nC. IDE and local Spark/Delta Lake: Local environments won't replicate production's scale and configuration fully.\nE. Jobs UI and Photon: True that Photon benefits scheduled jobs, but Jobs UI can track execution times regardless of Photon usage. However, Jobs UI runs might involve additional overhead compared to notebook cells.\nOption D addresses the specific limitations of using display() for performance measurement"
      },
      {
        "date": "2024-02-07T02:36:00.000Z",
        "voteCount": 3,
        "content": "Both B and D are correct statements. However, D is not an adjustment (see the question), it is just an afirmation which happens to be correct. B, however, is an adjustment, and it will definitely help with profiling."
      },
      {
        "date": "2024-02-02T18:21:00.000Z",
        "voteCount": 3,
        "content": "As B not talking about how to deal with display() function. We know that way to testing performance for the whole notebook need to avoid using display as it is way to test the code and display the data"
      },
      {
        "date": "2024-01-30T20:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-26T12:22:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-12-31T14:52:00.000Z",
        "voteCount": 2,
        "content": "Calling display() forces a job to trigger - doesnt make sense\ndisplay is used to display a df/table in tabular format, has nothing to do with a job trigger"
      },
      {
        "date": "2024-02-07T02:37:00.000Z",
        "voteCount": 2,
        "content": "Actually they mean a spark job. This is true, whenever you call display, spark needs to execute the transformations up to this point to be able to collect the results."
      },
      {
        "date": "2023-12-20T00:15:00.000Z",
        "voteCount": 1,
        "content": "D  is correct"
      },
      {
        "date": "2023-12-09T10:57:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-10-16T04:42:00.000Z",
        "voteCount": 1,
        "content": "Yes D is a True statement. But it does not answer the question.\nThe ask is for \"which adjustments will get a more accurate measure of how code is likely to perform in production\". Answer D just describes why the chosen approach is not correct. It does not provide a solution."
      },
      {
        "date": "2023-10-16T04:45:00.000Z",
        "voteCount": 2,
        "content": "D would be the answer if it was preceded by: We should avoid calling display() too often or clear the cache before running each cell."
      },
      {
        "date": "2023-08-23T20:09:00.000Z",
        "voteCount": 1,
        "content": "Is it not B?"
      },
      {
        "date": "2023-08-27T08:36:00.000Z",
        "voteCount": 3,
        "content": "Option B one of possibility happening. Option D fully meaning"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/databricks/view/119184-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A production cluster has 3 executor nodes and uses the same virtual machine type for the driver and executor.<br>When evaluating the Ganglia Metrics for this cluster, which indicator would signal a bottleneck caused by code executing on the driver?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe five Minute Load Average remains consistent/flat",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBytes Received never exceeds 80 million bytes per second",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTotal Disk Space remains constant",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNetwork I/O never spikes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOverall cluster CPU utilization is around 25%"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-27T08:39:00.000Z",
        "voteCount": 16,
        "content": "Option E: In a Spark cluster, the driver node is responsible for managing the execution of the Spark application, including scheduling tasks, managing the execution plan, and interacting with the cluster manager. If the overall cluster CPU utilization is low (e.g., around 25%), it may indicate that the driver node is not utilizing the available resources effectively and might be a bottleneck."
      },
      {
        "date": "2024-08-21T06:59:00.000Z",
        "voteCount": 3,
        "content": "A bottleneck occurs when resources are over utilized not underutilized, so that explanation doesn't make too much sense. CPU utilization would be at 100% and you wouldn't see spike in I/O if the driver was the issue. Conversely if the I/O was spiked and CPU utilization was at 25% , then network could be the issue. D is the only logical answer in this case."
      },
      {
        "date": "2024-02-07T02:47:00.000Z",
        "voteCount": 2,
        "content": "Overall CPU utilization can be misleading. The 25% utilization could be caused by the workload not requiring more than that rather than everything being executed in the driver node."
      },
      {
        "date": "2024-10-18T07:47:00.000Z",
        "voteCount": 1,
        "content": "D also means that Driver never send big data chunks to the Worker nodes but as it is not mentioned to be 0 then it has a constant flow of data going in &amp; out between the Driver node and the Worker nodes. Therefore it is not a measure of Driver bottleneck. However Answer E means one of the 4 cluster nodes is always working at 100% which can not be other than the Driver node as it is always working and coordinating work across Executors."
      },
      {
        "date": "2024-08-21T07:02:00.000Z",
        "voteCount": 2,
        "content": "Executors talk between each other and between nodes, if the code/driver is working as intended you would see a spike in I/O while transferring data. If the code/driver was the issue you would see a spike in CPU usage and little network traffic between nodes. The correct answer is D."
      },
      {
        "date": "2024-06-10T00:13:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-02-07T02:46:00.000Z",
        "voteCount": 1,
        "content": "If there's no IO between driver and executor nodes then the executor nodes are not working"
      },
      {
        "date": "2023-12-29T06:16:00.000Z",
        "voteCount": 2,
        "content": "D seems to be right"
      },
      {
        "date": "2023-12-09T11:02:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-12-09T04:52:00.000Z",
        "voteCount": 2,
        "content": "25% indicates Cluster CPU under-utilized"
      },
      {
        "date": "2024-01-24T01:21:00.000Z",
        "voteCount": 1,
        "content": "Not correct. 25% could (in theory) mean driver is using 100% CPU"
      },
      {
        "date": "2023-10-24T07:16:00.000Z",
        "voteCount": 3,
        "content": "If the overall cluster CPU utilization is around 25%, it means that only one out of the four nodes (driver + 3 executors) is using its full CPU capacity, while the other three nodes are idle or underutilized"
      },
      {
        "date": "2023-10-16T04:49:00.000Z",
        "voteCount": 4,
        "content": "If the overall cluster CPU utilization is around 25%, it means that only one out of the four nodes (driver + 3 executors) is using its full CPU capacity, while the other three nodes are idle or underutilized"
      },
      {
        "date": "2023-10-24T07:16:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is E."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/databricks/view/133247-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Where in the Spark UI can one diagnose a performance problem induced by not leveraging predicate push-down?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Executor\u2019s log file, by grepping for \"predicate push-down\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Stage\u2019s Detail screen, in the Completed Stages table, by noting the size of data read from the Input column",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Storage Detail screen, by noting which RDDs are not stored on disk",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Delta Lake transaction log. by noting the column statistics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Query Detail screen, by interpreting the Physical Plan\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-06T07:05:00.000Z",
        "voteCount": 1,
        "content": "E is correct : https://docs.datastax.com/en/dse/6.9/spark/predicate-push-down.html"
      },
      {
        "date": "2024-02-07T02:16:00.000Z",
        "voteCount": 1,
        "content": "Query plan. Correct is E"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/databricks/view/120580-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Review the following error traceback:<br><img title=\"image26\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image26.png\"><br>Which statement describes the error being raised?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe code executed was PySpark but was executed in a Scala notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no column in the table named heartrateheartrateheartrate\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is a type error because a column object cannot be multiplied.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is a type error because a DataFrame object cannot be multiplied.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is a syntax error because the heartrate column is not correctly identified as a column."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-12T00:47:00.000Z",
        "voteCount": 7,
        "content": "It's B, there is no column with that name"
      },
      {
        "date": "2023-12-09T11:09:00.000Z",
        "voteCount": 5,
        "content": "E is correct"
      },
      {
        "date": "2024-02-07T03:01:00.000Z",
        "voteCount": 2,
        "content": "It's B. Regarding E, a syntax error would mean that the query is not valid due to a wrongfully written SQL statement. However, this is not the case. The column just does not exist."
      },
      {
        "date": "2024-01-13T12:25:00.000Z",
        "voteCount": 1,
        "content": "https://sparkbyexamples.com/spark/spark-cannot-resolve-given-input-columns/"
      },
      {
        "date": "2023-12-05T11:32:00.000Z",
        "voteCount": 2,
        "content": "the answer is E, because \ndf.select(3*df['heartrate']).show() perfectly returns"
      },
      {
        "date": "2024-01-21T00:42:00.000Z",
        "voteCount": 1,
        "content": "3*\"heartrate\"  is triple of string \"heartrate\" ,isn't value of heartrate multiplied by 3."
      },
      {
        "date": "2023-12-05T08:36:00.000Z",
        "voteCount": 2,
        "content": "Answer is E\ndf.select(3*df['heartrate']) returns perfect result without error"
      },
      {
        "date": "2023-11-09T03:03:00.000Z",
        "voteCount": 2,
        "content": "Answer B"
      },
      {
        "date": "2023-11-08T16:55:00.000Z",
        "voteCount": 2,
        "content": "Answer is B"
      },
      {
        "date": "2023-10-16T04:54:00.000Z",
        "voteCount": 2,
        "content": "No such column found"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/databricks/view/128879-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which distribution does Databricks support for installing custom Python code packages?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsbt",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCRANC. npm",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWheels\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tjars"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-18T00:06:00.000Z",
        "voteCount": 1,
        "content": "I think D is correct"
      },
      {
        "date": "2024-02-26T04:59:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/how-to/use-python-wheels-in-workflows"
      },
      {
        "date": "2023-12-18T11:19:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/how-to/use-python-wheels-in-workflows"
      },
      {
        "date": "2023-12-18T00:29:00.000Z",
        "voteCount": 2,
        "content": "Wheels should be ok"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/databricks/view/128880-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which Python variable contains a list of directories to be searched when trying to locate required modules?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\timportlib.resource_path",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.path\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tos.path",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpypi.path",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpylib.source"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-18T00:43:00.000Z",
        "voteCount": 5,
        "content": "sys. path is a built-in variable within the sys module. It contains a list of directories that the interpreter will search in for the required module"
      },
      {
        "date": "2024-10-18T00:09:00.000Z",
        "voteCount": 1,
        "content": "sys.path is a built-in variable within the sys module. It contains a list of directories that the interpreter will search in for the required module. "
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/databricks/view/128881-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Incorporating unit tests into a PySpark application requires upfront attention to the design of your jobs, or a potentially significant refactoring of existing code.<br>Which statement describes a main benefit that offset this additional effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImproves the quality of your data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tValidates a complete use case of your application",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTroubleshooting is easier since all steps are isolated and tested individually\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYields faster deployment and execution times",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsures that all steps interact correctly to achieve the desired end result"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-15T09:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is C."
      },
      {
        "date": "2023-12-18T00:47:00.000Z",
        "voteCount": 4,
        "content": "Unit tests are small, isolated tests that are used to check specific parts of the code, such as functions or classes"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/databricks/view/128882-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement describes integration testing?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tValidates interactions between subsystems of your application\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequires an automated testing framework",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequires manual intervention",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tValidates an application use case",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tValidates behavior of individual elements of your application"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T04:32:00.000Z",
        "voteCount": 2,
        "content": "Answer is A"
      },
      {
        "date": "2023-12-18T00:50:00.000Z",
        "voteCount": 3,
        "content": "Integration testing is a type of software testing where components of the software are gradually integrated and then tested as a unified group"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/databricks/view/127546-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which REST API call can be used to review the notebooks configured to run as tasks in a multi-task job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/jobs/runs/list",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/jobs/runs/get-output",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/jobs/runs/get",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/jobs/get\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/jobs/list"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-14T16:04:00.000Z",
        "voteCount": 2,
        "content": "multi-task: /jobs/get\nsingle-task: /jobs/runs/get"
      },
      {
        "date": "2024-02-26T05:16:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/api/workspace/jobs/get\nresponses/settings/tasks/notebook_task/notebook_path"
      },
      {
        "date": "2023-12-31T16:42:00.000Z",
        "voteCount": 3,
        "content": "The question asks for notebooks configured for a job, not a instance of a job run. D is correct."
      },
      {
        "date": "2023-12-18T00:57:00.000Z",
        "voteCount": 1,
        "content": "Get\nMulti-task format jobs return an array of task data structures containing task settings."
      },
      {
        "date": "2023-12-04T09:42:00.000Z",
        "voteCount": 1,
        "content": "/jobs/get response under task array shows all the desired notebooks"
      },
      {
        "date": "2023-11-30T08:51:00.000Z",
        "voteCount": 1,
        "content": "should be B"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/databricks/view/118942-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Databricks job has been configured with 3 tasks, each of which is a Databricks notebook. Task A does not depend on other tasks. Tasks B and C run in parallel, with each having a serial dependency on task A.<br>If tasks A and B complete successfully but task C fails during a scheduled run, which statement describes the resulting state?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic expressed in the notebook associated with tasks A and B will have been successfully completed; some operations in task C may have completed successfully.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic expressed in the notebook associated with tasks A and B will have been successfully completed; any changes made in task C will be rolled back due to task failure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic expressed in the notebook associated with task A will have been successfully completed; tasks B and C will not commit any changes because of stage failure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause all tasks are managed as a dependency graph, no changes will be committed to the Lakehouse until ail tasks have successfully been completed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnless all tasks complete successfully, no changes will be committed to the Lakehouse; because task C failed, all commits will be rolled back automatically."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-23T01:36:00.000Z",
        "voteCount": 8,
        "content": "Should be 'A' only, as ACID compliance is applicable at operation level. For example if task C is having 3 target delta table writes (in independent Notebook cells) then it could have after 1 write the task fails during 2nd write. In that case 1st write will still be persisted. The ACID compliance will be applicable for only the 2nd write."
      },
      {
        "date": "2024-10-06T07:28:00.000Z",
        "voteCount": 1,
        "content": "https://community.databricks.com/t5/data-engineering/does-cancelling-a-job-run-rollback-any-actions-performed-by/td-p/8135"
      },
      {
        "date": "2023-12-18T01:01:00.000Z",
        "voteCount": 3,
        "content": "A - for sure this is NOT ACID operations"
      },
      {
        "date": "2023-08-23T20:24:00.000Z",
        "voteCount": 2,
        "content": "Correct answer should be B  as Databricks is ACID compliant"
      },
      {
        "date": "2023-09-05T07:52:00.000Z",
        "voteCount": 2,
        "content": "What if an operation of C is to delete a file, will the file be created after a roll back?"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/databricks/view/132441-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Delta Lake table was created with the below query:<br><img title=\"image27\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image27.png\"><br>Realizing that the original query had a typographical error, the below code was executed:<br>ALTER TABLE prod.sales_by_stor RENAME TO prod.sales_by_store<br>Which result will occur after running the second command?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table reference in the metastore is updated and no data is changed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table name change is recorded in the Delta transaction log.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll related files and metadata are dropped and recreated in a single ACID transaction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table reference in the metastore is updated and all data files are moved.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA new Delta transaction log Is created for the renamed table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-06T06:59:00.000Z",
        "voteCount": 6,
        "content": "did a test. No data is changed. dir &amp; filename not changed. the rename is not recorded in transition log neither."
      },
      {
        "date": "2024-03-05T16:38:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer. When you alter a table name in Delta Lake, the change is logged in the transaction log that Delta Lake uses to maintain a versioned history of all changes to the table. This is how Delta Lake maintains ACID properties and ensures a consistent view of the data. The transaction log is key to supporting features like time travel, auditing, and rollbacks in Delta Lake. The metadata and the actual data remain intact, and the reference to the table in the metastore is updated to reflect the new name."
      },
      {
        "date": "2024-01-30T04:26:00.000Z",
        "voteCount": 3,
        "content": "A is Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/databricks/view/120443-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineering team maintains a table of aggregate statistics through batch nightly updates. This includes total sales for the previous day alongside totals and averages for a variety of time periods including the 7 previous days, year-to-date, and quarter-to-date. This table is named store_saies_summary and the schema is as follows:<br><img title=\"image28\" src=\"https://img.examtopics.com/certified-data-engineer-professional/image28.png\"><br>The table daily_store_sales contains all the information needed to update store_sales_summary. The schema for this table is: store_id INT, sales_date DATE, total_sales FLOAT<br>If daily_store_sales is implemented as a Type 1 table and the total_sales column might be adjusted after manual data auditing, which approach is the safest to generate accurate reports in the store_sales_summary table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the appropriate aggregate logic as a batch read against the daily_store_sales table and overwrite the store_sales_summary table with each Update.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the appropriate aggregate logic as a batch read against the daily_store_sales table and append new rows nightly to the store_sales_summary table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the appropriate aggregate logic as a batch read against the daily_store_sales table and use upsert logic to update results in the store_sales_summary table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement the appropriate aggregate logic as a Structured Streaming read against the daily_store_sales table and use upsert logic to update results in the store_sales_summary table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Structured Streaming to subscribe to the change data feed for daily_store_sales and apply changes to the aggregates in the store_sales_summary table with each update."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "E",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-10T13:44:00.000Z",
        "voteCount": 11,
        "content": "The answer should be A. it is the safest to generate accurate report"
      },
      {
        "date": "2023-12-18T01:13:00.000Z",
        "voteCount": 1,
        "content": "Incorrect BATCH processing and OVERWRITE will give partial results"
      },
      {
        "date": "2024-01-23T23:06:00.000Z",
        "voteCount": 3,
        "content": "This is confusing: \"overwrite the store_sales_summary table with each Update.\" sounds like it is only doing updates, not inserting new possible stories."
      },
      {
        "date": "2024-10-11T12:36:00.000Z",
        "voteCount": 1,
        "content": "daily_store_sales is type1 table, no history is maintained. You have to treat every record as new record and do aggregation for every store. Overwrite is much efficient than upsert."
      },
      {
        "date": "2024-06-25T09:33:00.000Z",
        "voteCount": 3,
        "content": "I will go with c. upsert"
      },
      {
        "date": "2024-05-29T05:51:00.000Z",
        "voteCount": 4,
        "content": "A is not correct because the table is daily. If you overwrite you delete all history. You need to insert/update to keep history."
      },
      {
        "date": "2024-08-15T09:43:00.000Z",
        "voteCount": 2,
        "content": "Incorrect. The daily store sales table contains all of the history needed to update the table. The summary table holds no historical records. Seeing as this is a nightly job, any manual changes made to daily store sales will be captured. A is the correct answer."
      },
      {
        "date": "2024-04-15T02:17:00.000Z",
        "voteCount": 3,
        "content": "Not sure if that's right but I would go for A. What do you think?\n\nType1: Data is overwritten\nType 2: History is maintained, new data is inserted as new rows\nType 3: Stores two versions per record: a previous and a current value\n\nA. batch + overwrite -&gt; Match Type 1 requirements. YES\nB:  batch + append new rows -&gt; Would be for type 2. NO\nC. Batch + Upsert -&gt; Data is not being overwritten (which is required for Type 1). NO\nD. ReadStream + Upsert -&gt; Data is not being overwritten (which is required for Type 1). NO\nE. Change Data Feed to update -&gt; Problem is manual edits + not overwriting (required for type 1). No\n\nI have doubts around \"which approach is the safest\". Maybe because due to some manual changes it is hard to track changes or do upsert, so to make sure that the stats are right \n overwriting is safer."
      },
      {
        "date": "2024-03-09T03:46:00.000Z",
        "voteCount": 2,
        "content": "Not A because overwriting will only provide a daily based data not the history of it.\nNot B because it will not fix the issue of incorrect sales amount\nAs these data are fit for natch processing so neither D or E.\nC will only upsert the changes while making sure we are updating the records based on sales_date &amp; store_id"
      },
      {
        "date": "2024-01-30T15:09:00.000Z",
        "voteCount": 1,
        "content": "E definitely because it say that the total_sales column may be change by manual auditing so not via a job, so streaming with CDF is the only option here !"
      },
      {
        "date": "2024-01-30T10:49:00.000Z",
        "voteCount": 3,
        "content": "I would go with Option A. \nBecause it has manual auditing hence values can change. Uses type 1 hence replace original data"
      },
      {
        "date": "2024-01-26T12:39:00.000Z",
        "voteCount": 1,
        "content": "It should be E, as structure streaming has built-in fault-tolerance feature."
      },
      {
        "date": "2024-01-25T14:05:00.000Z",
        "voteCount": 1,
        "content": "It said type 1 so A is the correct answer !"
      },
      {
        "date": "2023-12-31T17:07:00.000Z",
        "voteCount": 2,
        "content": "The question is unclear whether the aggregated table needs to support a rolling history. Note the aggregated table does not have a date column to distinguish which date the summary is generated for so one could assume the table is maintained only for the current snapshot.\n\nAssuming the above - A would be the safest option as all stores and aggregates would need to be refreshed nightly"
      },
      {
        "date": "2023-12-28T11:17:00.000Z",
        "voteCount": 3,
        "content": "A is correct because it's a static table that is written nightly through a batch job.  The summary table does not maintain history and so an upsert results in having extra, unecessary records.  Overwrite it nightly with updated aggregates for the required time period."
      },
      {
        "date": "2024-01-23T23:03:00.000Z",
        "voteCount": 1,
        "content": "\"Safest\" probably includes having Delta table. And history is maintained anyway."
      },
      {
        "date": "2023-12-27T14:24:00.000Z",
        "voteCount": 3,
        "content": "The answer is A. Note that the target table has columns which stores quarter to date,previous day sates etc, which will result in daily updates, i.e. large volume of records will be updated, hence better to overwirte than to update large volume of records."
      },
      {
        "date": "2023-12-18T01:14:00.000Z",
        "voteCount": 3,
        "content": "Batch processing so you need to update and insert - C"
      },
      {
        "date": "2023-11-26T09:42:00.000Z",
        "voteCount": 3,
        "content": "Answer C is correct. \nAnswer E would do the job too, but the table schema and the question indicates, that there will be only one update daily needed. Therefore a structured streaming job is way too expensive to archive the outcome."
      },
      {
        "date": "2023-12-06T01:44:00.000Z",
        "voteCount": 1,
        "content": "You are absolutely right!"
      },
      {
        "date": "2023-11-04T12:36:00.000Z",
        "voteCount": 1,
        "content": "Correct answer A\nType 1 data is overwritten \nhttps://streamsets.com/blog/slowly-changing-dimensions-vs-change-data-capture/#:~:text=In%20Type%201%2C%20any%20new,change%20to%20maintain%20a%20history."
      },
      {
        "date": "2023-10-16T05:19:00.000Z",
        "voteCount": 2,
        "content": "I would say that it is E.  If daily_store_sales table is implemented as a Type 1 table, this means that values are overwritten, and we do not keep the history. So we would need to create a streaming from CDF and apply those changes into the aggregated table."
      },
      {
        "date": "2023-11-05T03:52:00.000Z",
        "voteCount": 1,
        "content": "manual data auditing, implies we do not know when a change is made, hence we do not know when to schedule the \"batch update\" for the aggregated table"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/databricks/view/125249-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A member of the data engineering team has submitted a short notebook that they wish to schedule as part of a larger data pipeline. Assume that the commands provided below produce the logically correct results when run as presented.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image29.png\"><br><br>Which command should be removed from the notebook before scheduling it as a job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 3",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 4",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 5",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 6\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-30T11:57:00.000Z",
        "voteCount": 7,
        "content": "When scheduling a Databricks notebook as a job, it's generally recommended to remove or modify commands that involve displaying output, such as using the display() function. Displaying data using display() is an interactive feature designed for exploration and visualization within the notebook interface and may not work well in a production job context.\n\nThe finalDF.explain() command, which provides the execution plan of the DataFrame transformations and actions, is often useful for debugging and optimizing queries. While it doesn't display interactive visualizations like display(), it can still be informative for understanding how Spark is executing the operations on your DataFrame."
      },
      {
        "date": "2024-10-18T04:43:00.000Z",
        "voteCount": 1,
        "content": "if i was multiple solutions than i would have gone for .explain method and print schema as well as they do not contribute in any sort of ETL operation but as a rule of thumb display should always be omitted first so -&gt; E"
      },
      {
        "date": "2024-08-14T07:49:00.000Z",
        "voteCount": 1,
        "content": "I agree with petrv and KhoaLe, but I will add that not displaying the finalDF would be wise as it could display and log PII data and that to me is why I choose E.  Like hal2401 said, commands 2, 5 &amp; 6 can be removed as they don't manipulate the data."
      },
      {
        "date": "2024-02-26T06:31:00.000Z",
        "voteCount": 1,
        "content": "perhaps it's a multi-choice question in exam. I'll select E and D. if single choice then E."
      },
      {
        "date": "2024-02-07T18:15:00.000Z",
        "voteCount": 2,
        "content": "Looking through at all steps, Cmd 2,5,6 can be eliminated without impacting to the whole process.\nHowever, in terms of duration cost, Cmd 2 and 5 does not impact much as they only show the current results of logical query plan. In contrast, display() in Cmd6 is actually a transformation, which will take much time to run."
      },
      {
        "date": "2023-12-18T01:19:00.000Z",
        "voteCount": 3,
        "content": "No display()"
      },
      {
        "date": "2023-11-14T13:42:00.000Z",
        "voteCount": 1,
        "content": "No actions on production scripts. D is best"
      },
      {
        "date": "2023-11-16T00:30:00.000Z",
        "voteCount": 1,
        "content": "in order to display a dataframe you also need to calculate it. So display also acts as an action."
      },
      {
        "date": "2023-11-03T01:07:00.000Z",
        "voteCount": 2,
        "content": "Why not D?"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/databricks/view/126360-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The business reporting team requires that data for their dashboards be updated every hour. The total processing time for the pipeline that extracts transforms, and loads the data for their pipeline runs in 10 minutes.<br><br>Assuming normal operating conditions, which configuration will meet their service-level agreement requirements with the lowest cost?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually trigger a job anytime the business reporting team refreshes their dashboards",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a job to execute the pipeline once an hour on a new job cluster\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a Structured Streaming job with a trigger interval of 60 minutes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a job to execute the pipeline once an hour on a dedicated interactive cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a job that executes every time new data lands in a given directory"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-31T18:21:00.000Z",
        "voteCount": 8,
        "content": "B is correct I think.\nWith option C, the cluster remains on 24/7 with trigger = 60 mins which is more costly\n\nIf there is an option with structure streaming with trigger = availablenow, and job scheduled per hour, that would be even more efficient.\nhttps://www.databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html"
      },
      {
        "date": "2024-08-20T14:12:00.000Z",
        "voteCount": 1,
        "content": "C. The lowest cost is obtained by using job cluster"
      },
      {
        "date": "2024-08-20T14:12:00.000Z",
        "voteCount": 2,
        "content": "B answer i mean"
      },
      {
        "date": "2024-02-26T15:03:00.000Z",
        "voteCount": 2,
        "content": "Databricks recommends using Structured Streaming with trigger AvailableNow for incremental workloads that do not have low latency requirements."
      },
      {
        "date": "2024-01-24T14:45:00.000Z",
        "voteCount": 4,
        "content": "B is correct"
      },
      {
        "date": "2023-12-18T01:22:00.000Z",
        "voteCount": 4,
        "content": "B :  Job cluster is cheap ,  hourly = 60 minutes"
      },
      {
        "date": "2023-11-21T10:27:00.000Z",
        "voteCount": 2,
        "content": "Scheduling a job to execute the pipeline on an hourly basis aligns with the requirement for data to be updated every hour. Using a job cluster (which is brought up for the job and torn down upon completion) rather than a dedicated interactive cluster will usually be more cost-effective. This is because you are only paying for the compute resources when the job is running, which is 10 minutes out of every hour, rather than paying for an interactive cluster that would be up and running (and incurring costs) continuously."
      },
      {
        "date": "2023-11-16T05:38:00.000Z",
        "voteCount": 1,
        "content": "It's either B or D. I think B, because we want the lowest cost."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/databricks/view/124566-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Databricks SQL dashboard has been configured to monitor the total number of records present in a collection of Delta Lake tables using the following query pattern:<br><br><br>SELECT COUNT (*) FROM table -<br><br>Which of the following describes how results are generated each time the dashboard is updated?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total count of rows is calculated by scanning all data files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total count of rows will be returned from cached results unless REFRESH is run",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total count of records is calculated from the Delta transaction logs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total count of records is calculated from the parquet file metadata",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total count of records is calculated from the Hive metastore"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-25T09:35:00.000Z",
        "voteCount": 2,
        "content": "Delta transaction log"
      },
      {
        "date": "2023-12-18T13:04:00.000Z",
        "voteCount": 4,
        "content": "Transaction log provides statistics about the delta table."
      },
      {
        "date": "2023-12-18T01:25:00.000Z",
        "voteCount": 3,
        "content": "C - transaction logs contains info about files rows count"
      },
      {
        "date": "2023-11-21T10:29:00.000Z",
        "voteCount": 4,
        "content": "Delta Lake maintains a transaction log that records details about every change made to a table. When you execute a count operation on a Delta table, Delta Lake can use the information in the transaction log to calculate the total number of records without having to scan all the data files. This is because the transaction log includes information about the number of records in each file, allowing for an efficient aggregation of these counts to get the total number of records in the table."
      },
      {
        "date": "2023-11-08T17:22:00.000Z",
        "voteCount": 2,
        "content": "The answer is C"
      },
      {
        "date": "2023-11-05T13:13:00.000Z",
        "voteCount": 2,
        "content": "The answer should be C"
      },
      {
        "date": "2023-11-04T14:03:00.000Z",
        "voteCount": 3,
        "content": "Answer C \nhttps://delta.io/blog/2023-04-19-faster-aggregations-metadata/#:~:text=You%20can%20get%20the%20number,a%20given%20Delta%20table%20version."
      },
      {
        "date": "2023-10-24T07:42:00.000Z",
        "voteCount": 3,
        "content": "total rows will be calculated from delta logs"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/databricks/view/124117-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Delta Lake table was created with the below query:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image30.png\"><br><br>Consider the following query:<br><br><br>DROP TABLE prod.sales_by_store -<br><br>If this statement is executed by a workspace admin, which result will occur?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNothing will occur until a COMMIT command is executed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table will be removed from the catalog but the data will remain in storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table will be removed from the catalog and the data will be deleted.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn error will occur because Delta Lake prevents the deletion of production data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData will be marked as deleted but still recoverable with Time Travel."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-06T19:29:00.000Z",
        "voteCount": 6,
        "content": "According to the exam courses answer is C, for a managed table dropped.\nBut, as after Nov'23, UNDROP is introduced and I have test it working with UC managed tables.\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-undrop-table.html\nHowever, I don't see any official doc says UNDROP related to 'time travel'. \nSo, be aware of the above info; in exam, watch the question carefully if it is updated."
      },
      {
        "date": "2024-09-17T07:45:00.000Z",
        "voteCount": 1,
        "content": "Agree that the answer is C - the question is misleading in saying it is Delta Lake table.  However, it is managed table b/c there is no USING delta clause."
      },
      {
        "date": "2024-04-07T22:41:00.000Z",
        "voteCount": 4,
        "content": "C. is only correct statement. Though the table can be UNDROP in 7 days\nhttps://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/sql-ref-syntax-ddl-undrop-table\nE. Time Travel can retrieve versioned records but not tables.\nhttps://www.databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html"
      },
      {
        "date": "2024-02-26T15:43:00.000Z",
        "voteCount": 2,
        "content": "I think E is better answer"
      },
      {
        "date": "2023-12-27T14:45:00.000Z",
        "voteCount": 1,
        "content": "E. Since the table is still recoverable from transaction logs."
      },
      {
        "date": "2023-12-18T01:29:00.000Z",
        "voteCount": 2,
        "content": "C : AS SELECT  - Managed table\nWill remove table and data"
      },
      {
        "date": "2023-11-21T10:32:00.000Z",
        "voteCount": 3,
        "content": "In Delta Lake, when a DROP TABLE command is executed, it removes both the metadata entry for the table from the catalog and the data in storage associated with that table. Workspace administrators typically have the necessary permissions to drop tables, and unless there are additional protections or retention policies in place, the data is not recoverable through normal operations after the table is dropped."
      },
      {
        "date": "2023-11-14T14:21:00.000Z",
        "voteCount": 3,
        "content": "I meant C is correct, not D"
      },
      {
        "date": "2023-11-14T14:19:00.000Z",
        "voteCount": 1,
        "content": "D is most correct"
      },
      {
        "date": "2023-11-08T17:25:00.000Z",
        "voteCount": 1,
        "content": "Answer is C as it is a managed table"
      },
      {
        "date": "2023-10-31T02:18:00.000Z",
        "voteCount": 1,
        "content": "it is a managed table"
      },
      {
        "date": "2023-10-31T02:17:00.000Z",
        "voteCount": 1,
        "content": "its a as it is managed table"
      },
      {
        "date": "2023-10-24T07:44:00.000Z",
        "voteCount": 1,
        "content": "it is a managed table. So both table def and data will be deleted"
      },
      {
        "date": "2023-10-20T06:04:00.000Z",
        "voteCount": 1,
        "content": "Drop will usually delete the table structure and data if its managed, hence c"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/databricks/view/124569-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Two of the most common data locations on Databricks are the DBFS root storage and external object storage mounted with dbutils.fs.mount().<br><br>Which of the following statements is correct?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDBFS is a file system protocol that allows users to interact with files stored in object storage using syntax and guarantees similar to Unix file systems.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBy default, both the DBFS root and mounted data sources are only accessible to workspace administrators.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe DBFS root is the most secure location to store data, because mounted storage volumes must have full public read and write permissions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNeither the DBFS root nor mounted storage can be accessed when using %sh in a Databricks notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe DBFS root stores files in ephemeral block volumes attached to the driver, while mounted directories will always persist saved data to external storage between sessions."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-26T15:46:00.000Z",
        "voteCount": 3,
        "content": "A is correct . For E, This statement is partially incorrect. The DBFS root does use ephemeral storage, but not block volumes. Data saved there is lost when the cluster terminates unless explicitly persisted elsewhere. Mounted storage, however, can persist data between sessions depending on the underlying storage service and configuration."
      },
      {
        "date": "2023-12-18T13:11:00.000Z",
        "voteCount": 2,
        "content": "DBFS is a layer on top of cloud storage providers."
      },
      {
        "date": "2023-11-21T10:34:00.000Z",
        "voteCount": 1,
        "content": "Databricks File System (DBFS) is a layer over a cloud object storage (like AWS S3, Azure Blob Storage, or GCP Cloud Storage) that allows users to interact with data as if they were using a traditional file system. It provides familiar file system semantics and is designed to be consistent with POSIX-like file system behavior, which includes commands and actions similar to those used in Unix and Linux file systems."
      },
      {
        "date": "2023-11-08T17:27:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2023-10-24T08:31:00.000Z",
        "voteCount": 2,
        "content": "it is not E.\nThe only on that would be plausible is A"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/databricks/view/124571-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The following code has been migrated to a Databricks notebook from a legacy workload:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image31.png\"><br><br>The code executes successfully and provides the logically correct results, however, it takes over 20 minutes to extract and load around 1 GB of data.<br><br>Which statement is a possible explanation for this behavior?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%sh triggers a cluster restart to collect and install Git. Most of the latency is related to cluster startup time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstead of cloning, the code should use %sh pip install so that the Python code can get executed in parallel across all nodes in a cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%sh does not distribute file moving operations; the final line of code should be updated to use %fs instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPython will always execute slower than Scala on Databricks. The run.py script should be refactored to Scala.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%sh executes shell code on the driver node. The code does not take advantage of the worker nodes or Databricks optimized Spark.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-21T10:37:00.000Z",
        "voteCount": 8,
        "content": "When using %sh in a Databricks notebook, the commands are executed in a shell environment on the driver node. This means that only the resources of the driver node are used, and the execution does not leverage the distributed computing capabilities of the worker nodes in the Spark cluster. This can result in slower performance, especially for data-intensive tasks, compared to an approach that distributes the workload across all nodes in the cluster using Spark."
      },
      {
        "date": "2024-08-20T14:16:00.000Z",
        "voteCount": 1,
        "content": "Option E correct"
      },
      {
        "date": "2024-05-28T07:49:00.000Z",
        "voteCount": 1,
        "content": "Option E: Correct. The %sh magic command in Databricks runs shell commands on the driver node only. This means the operations within %sh do not leverage the distributed nature of the Databricks cluster. Consequently, the Git clone, Python script execution, and file move operations are all performed on a single node (the driver), which explains why it takes a long time to process and move 1 GB of data. This approach does not utilize the parallel processing capabilities of the worker nodes or the optimization features of Databricks Spark.\n\nOption C: Incorrect. %sh does not inherently distribute any operations, but the issue here is broader than just file moving operations. Using %fs for file operations is a best practice, but it does not resolve the inefficiency of running all commands on the driver node."
      },
      {
        "date": "2023-11-08T17:31:00.000Z",
        "voteCount": 2,
        "content": "E is the answer as the command is ran in the driver node and other nodes in the cluster are not used"
      },
      {
        "date": "2023-10-29T23:47:00.000Z",
        "voteCount": 3,
        "content": "%sh run Bash commands on the driver node of the cluster.\nhttps://www.databricks.com/blog/2020/08/31/introducing-the-databricks-web-terminal.html"
      },
      {
        "date": "2023-10-24T09:25:00.000Z",
        "voteCount": 1,
        "content": "you can use mv with %sh, but the syntax is not correct , it is missing the destination operand"
      },
      {
        "date": "2023-10-29T23:42:00.000Z",
        "voteCount": 1,
        "content": "I just noticed there is a space between the paths, so syntax is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/databricks/view/124572-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data science team has requested assistance in accelerating queries on free form text from user reviews. The data is currently stored in Parquet with the below schema:<br><br>item_id INT, user_id INT, review_id INT, rating FLOAT, review STRING<br><br>The review column contains the full text of the review left by the user. Specifically, the data science team is looking to identify if any of 30 key words exist in this field.<br><br>A junior data engineer suggests converting this data to Delta Lake will improve query performance.<br><br>Which response to the junior data engineer s suggestion is correct?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake statistics are not optimized for free text fields with high cardinality.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText data cannot be stored with Delta Lake.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tZORDER ON review will need to be run to see performance gains.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Delta log creates a term matrix for free text fields to support selective filtering.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake statistics are only collected on the first 4 columns in a table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-21T10:39:00.000Z",
        "voteCount": 5,
        "content": "Delta Lake uses statistics and data skipping to improve query performance, but these optimizations are most effective for columns with low to medium cardinality (i.e., columns with a limited set of distinct values). Free-form text fields like the review column typically have high cardinality, meaning each value in the column (each review text) is unique or nearly unique. Consequently, statistics on such columns do not significantly improve the performance of queries searching for specific keywords within the text."
      },
      {
        "date": "2023-11-08T17:36:00.000Z",
        "voteCount": 2,
        "content": "answer is A"
      },
      {
        "date": "2023-10-26T07:38:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-10-24T09:39:00.000Z",
        "voteCount": 2,
        "content": "Collecting statistics on long strings is an expensive operation"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/databricks/view/124574-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Assuming that the Databricks CLI has been installed and configured correctly, which Databricks CLI command can be used to upload a custom Python Wheel to object storage mounted with the DBFS for use with a production job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tconfigure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tjobs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlibraries",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tworkspace"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-24T23:32:00.000Z",
        "voteCount": 3,
        "content": "databricks fs cp dist/&lt;\u2026&gt;.whl dbfs:/some/place/appropriate"
      },
      {
        "date": "2024-02-26T17:41:00.000Z",
        "voteCount": 1,
        "content": "Here's how you can use the libraries command to upload your wheel:\nBash\n\ndatabricks libraries upload --file &lt;path_to_wheel_file&gt; --name &lt;library_name&gt;"
      },
      {
        "date": "2024-02-15T02:03:00.000Z",
        "voteCount": 1,
        "content": "this is a bit tricky, question is asked to upload custom Python Wheel, you can use fs command, but since it'll be used in production job, job command might be needed to perform databricks jobs operations?\nhttps://docs.databricks.com/en/dev-tools/cli/commands.html"
      },
      {
        "date": "2024-01-30T11:01:00.000Z",
        "voteCount": 2,
        "content": "Its asking to upload to DBFS and not install on cluster"
      },
      {
        "date": "2023-11-30T13:05:00.000Z",
        "voteCount": 2,
        "content": "the question is about copying the file not about installing."
      },
      {
        "date": "2023-11-26T09:59:00.000Z",
        "voteCount": 4,
        "content": "Answer B is corrent:\n\"... which Databricks CLI command can be used to upload a custom Python Wheel to object storage mounted with the DBFS ...\"\nThe question asks, how to upload the wheel. Not install it or configure it in a job. \nhttps://docs.databricks.com/en/archive/dev-tools/cli/dbfs-cli.html"
      },
      {
        "date": "2023-11-21T10:41:00.000Z",
        "voteCount": 2,
        "content": "The Databricks CLI fs command is used for interacting with the Databricks File System (DBFS). You can use it to put files into DBFS, which includes uploading custom Python Wheels to a directory in DBFS. The fs command has subcommands like cp that can be used to copy files from your local file system to DBFS, which is backed by an object storage mounted with dbutils.fs.mount().\n\ndatabricks fs cp my_package.whl dbfs:/mnt/my-mount-point/my_package.whl"
      },
      {
        "date": "2023-10-26T07:46:00.000Z",
        "voteCount": 1,
        "content": "It is done using the command: databricks libraries install"
      },
      {
        "date": "2023-10-24T09:51:00.000Z",
        "voteCount": 1,
        "content": "you can add a library section to the jobs command, but you can install a wheel with the library command"
      },
      {
        "date": "2023-10-24T09:52:00.000Z",
        "voteCount": 1,
        "content": "/api/2.0/libraries/install"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/databricks/view/129697-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The business intelligence team has a dashboard configured to track various summary metrics for retail stores. This includes total sales for the previous day alongside totals and averages for a variety of time periods. The fields required to populate this dashboard have the following schema:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image32.png\"><br><br>For demand forecasting, the Lakehouse contains a validated table of all itemized sales updated incrementally in near real-time. This table, named products_per_order, includes the following fields:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image33.png\"><br><br>Because reporting on long-term sales trends is less volatile, analysts using the new dashboard only require data to be refreshed once daily. Because the dashboard will be queried interactively by many users throughout a normal business day, it should return results quickly and reduce total compute associated with each materialization.<br><br>Which solution meets the expectations of the end users while controlling and limiting possible costs?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPopulate the dashboard by configuring a nightly batch job to save the required values as a table overwritten with each update.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Structured Streaming to configure a live dashboard against the products_per_order table within a Databricks notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a webhook to execute an incremental read against products_per_order each time the dashboard is refreshed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Delta Cache to persist the products_per_order table in memory to quickly update the dashboard with each query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a view against the products_per_order table and define the dashboard against this view."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-29T07:36:00.000Z",
        "voteCount": 10,
        "content": "looks like A to me, as long as they only need the data for the aggregates based on the previous day only"
      },
      {
        "date": "2024-01-24T00:01:00.000Z",
        "voteCount": 1,
        "content": "E - a view, could be an option but it would require computation every time used."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/databricks/view/124575-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data ingestion task requires a one-TB JSON dataset to be written out to Parquet with a target part-file size of 512 MB. Because Parquet is being used instead of Delta Lake, built-in file-sizing features such as Auto-Optimize &amp; Auto-Compaction cannot be used.<br><br>Which strategy will yield the best performance without shuffling data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet spark.sql.files.maxPartitionBytes to 512 MB, ingest the data, execute the narrow transformations, and then write to parquet.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet spark.sql.shuffle.partitions to 2,048 partitions (1TB*1024*1024/512), ingest the data, execute the narrow transformations, optimize the data by sorting it (which automatically repartitions the data), and then write to parquet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet spark.sql.adaptive.advisoryPartitionSizeInBytes to 512 MB bytes, ingest the data, execute the narrow transformations, coalesce to 2,048 partitions (1TB*1024*1024/512), and then write to parquet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data, execute the narrow transformations, repartition to 2,048 partitions (1TB* 1024*1024/512), and then write to parquet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet spark.sql.shuffle.partitions to 512, ingest the data, execute the narrow transformations, and then write to parquet."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-21T11:04:00.000Z",
        "voteCount": 8,
        "content": "This strategy aims to control the size of the output Parquet files without shuffling the data. The spark.sql.files.maxPartitionBytes parameter sets the maximum size of a partition that Spark will read. By setting it to 512 MB, you are aligning the read partition size with the desired output file size. Since the transformations are narrow (meaning they do not require shuffling), the number of partitions should roughly correspond to the number of output files when writing out to Parquet, assuming the data is evenly distributed and there is no data expansion during processing."
      },
      {
        "date": "2024-01-24T00:10:00.000Z",
        "voteCount": 6,
        "content": "D is the only one that does the trick.\n\nNote, we can not do shuffling.\n\nWrong answers:\n\nA: spark.sql.files.maxPartitionBytes is about reading, not writing.(The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC. )\n\nB: spark.sql.adaptive.advisoryPartitionSizeInBytes takes effect while shuffling and sorting does not make sense (The advisory size in bytes of the shuffle partition during adaptive optimization (when spark.sql.adaptive.enabled is true). It takes effect when Spark coalesces small shuffle partitions or splits skewed shuffle partition.)\n\nC: Would work but spark.sql.adaptive.advisoryPartitionSizeInBytes would need shuffling.\n\nE. spark.sql.shuffle.partitions (Configures the number of partitions to use when shuffling data for joins or aggregations.) is not about writing."
      },
      {
        "date": "2024-09-15T09:20:00.000Z",
        "voteCount": 2,
        "content": "Option A\nspark.sql.files.maxPartitionBytes controls the maximum size of partitions during reading on the Spark cluster, and that reducing this value could lead to more partitions and thus potentially more output files. The key point is that it works best when no shuffles occur, which aligns with the scenario of having narrow transformations only."
      },
      {
        "date": "2024-09-15T09:21:00.000Z",
        "voteCount": 1,
        "content": "Given that no shuffle occurs and you're aiming to control the file sizes during output, adjusting spark.sql.files.maxPartitionBytes could help indirectly by determining the partition size for reading. Since the number of input partitions can influence the size of the output files when no shuffle occurs, the partition size may closely match the size of the files being written out."
      },
      {
        "date": "2024-09-15T09:21:00.000Z",
        "voteCount": 1,
        "content": "If the transformations remain narrow, then Spark won't repartition the data unless explicitly instructed to do so (e.g., through a repartition or coalesce operation). In this case, using spark.sql.files.maxPartitionBytes to adjust the read partition size to 512 MB could indirectly control the number of output files and ensure they align with the target file size."
      },
      {
        "date": "2024-09-15T09:21:00.000Z",
        "voteCount": 1,
        "content": "Thus, Option A is also a valid strategy:\n\nSet spark.sql.files.maxPartitionBytes to 512 MB, process the data with narrow transformations, and write to Parquet.\nBy reducing the value of spark.sql.files.maxPartitionBytes, you ensure more partitions are created during the read phase, leading to output files closer to the desired size, assuming the transformations are narrow and no shuffling occurs."
      },
      {
        "date": "2024-03-09T04:42:00.000Z",
        "voteCount": 1,
        "content": "D is not correct as it will create 2048 target files of 0.5 MB each\nOnly A will do the job as it will read this file in 2  partition ( 1 TB = 512*2 MB) and as we are not doing any shuffling(not mentioned in option) it will create those many partition file i.e 2 part files"
      },
      {
        "date": "2024-03-12T04:01:00.000Z",
        "voteCount": 4,
        "content": "hey, 1TB=1000GB=1^6MB."
      },
      {
        "date": "2024-03-06T22:32:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says D: This strategy directly addresses the desired part-file size by repartitioning the data. It avoids shuffling during narrow transformations.\nRecommended for achieving the desired part-file size without unnecessary shuffling."
      },
      {
        "date": "2024-02-28T16:04:00.000Z",
        "voteCount": 1,
        "content": "D is mot suitable."
      },
      {
        "date": "2024-02-07T12:41:00.000Z",
        "voteCount": 3,
        "content": "This approach ensures that each partition will be approximately the target part-file size, which can improve the efficiency of the data write. It also avoids the need for a shuffle operation, which can be expensive in terms of performance."
      },
      {
        "date": "2024-01-31T06:29:00.000Z",
        "voteCount": 1,
        "content": "\u0421 is correct"
      },
      {
        "date": "2024-01-24T15:06:00.000Z",
        "voteCount": 2,
        "content": "Rest of the answers trigger shuffles"
      },
      {
        "date": "2024-01-01T13:41:00.000Z",
        "voteCount": 2,
        "content": "A is correct. \nThe question states Which strategy will yield the best performance without shuffling data.\nThe other options involve shuffling either manually or through AQE"
      },
      {
        "date": "2023-12-20T01:02:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      },
      {
        "date": "2023-12-18T02:51:00.000Z",
        "voteCount": 1,
        "content": "- spark.sql.files.maxPartitionBytes: 128MB (The maximum number of bytes to pack into a single partition when reading files. This configuration is effective only when using file-based sources such as Parquet, JSON and ORC.)"
      },
      {
        "date": "2023-11-30T13:32:00.000Z",
        "voteCount": 3,
        "content": "Here's a breakdown of the reasons:\n\nspark.sql.adaptive.advisoryPartitionSizeInBytes: This configuration parameter is designed to provide advisory partition sizes for the adaptive query execution framework. It can help in controlling the partition sizes without triggering unnecessary shuffling.\n\ncoalesce(2048): Coalescing to a specific number of partitions after the narrow transformations allows you to control the number of output files without triggering a shuffle. This helps achieve the target part-file size without incurring the overhead of a full shuffle.\n\nSetting a specific target: The strategy outlines the goal of achieving a target part-file size of 512 MB, which aligns with the requirement."
      },
      {
        "date": "2023-11-14T11:03:00.000Z",
        "voteCount": 1,
        "content": "obviously D. It allows you to control both the number of partitions and the final part-file size, which aligns with the requirements. Option B shuffles partitions, which is not allowed."
      },
      {
        "date": "2023-10-24T10:02:00.000Z",
        "voteCount": 2,
        "content": "The number of output files saved to the disk is equal to the number of partitions in the Spark executors when the write operation is performed."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/databricks/view/124595-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer has been asked to develop a streaming data pipeline with a grouped aggregation using DataFrame df. The pipeline needs to calculate the average humidity and average temperature for each non-overlapping five-minute interval. Incremental state information should be maintained for 10 minutes for late-arriving data.<br><br>Streaming DataFrame df has the following schema:<br><br>\"device_id INT, event_time TIMESTAMP, temp FLOAT, humidity FLOAT\"<br><br>Code block:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image34.png\"><br><br>Choose the response that correctly fills in the blank within the code block to complete this task.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\twithWatermark(\"event_time\", \"10 minutes\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tawaitArrival(\"event_time\", \"10 minutes\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tawait(\"event_time + \u201810 minutes'\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tslidingWindow(\"event_time\", \"10 minutes\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdelayWrite(\"event_time\", \"10 minutes\")"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-24T23:10:00.000Z",
        "voteCount": 7,
        "content": "withWatermark.\nThere sliding window is doe through the window function"
      },
      {
        "date": "2023-11-21T11:07:00.000Z",
        "voteCount": 5,
        "content": "To handle late-arriving data in a streaming aggregation, you need to specify a watermark, which tells the streaming query how long to wait for late data. The withWatermark method is used for this purpose in Spark Structured Streaming. It defines the threshold for how late the data can be relative to the latest data that has been seen in the same window."
      },
      {
        "date": "2024-08-14T14:01:00.000Z",
        "voteCount": 1,
        "content": "The withWatermark method is used in streaming DataFrames when processing real-time data streams. This method helps in managing stateful operations, such as aggregations, by specifying a time column to use for watermarking. Watermarking is a mechanism to handle late data (data that arrives later than expected) by defining a threshold time window beyond which late data is considered too late to be included in aggregations.\n\nThe slidingWindow function mentioned in D is not a standard function in Databricks or Apache Spark."
      },
      {
        "date": "2023-11-09T01:45:00.000Z",
        "voteCount": 3,
        "content": "Answer is A"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/databricks/view/129174-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data team's Structured Streaming job is configured to calculate running aggregates for item sales to update a downstream marketing dashboard. The marketing team has introduced a new promotion, and they would like to add a new field to track the number of times this promotion code is used for each item. A junior data engineer suggests updating the existing query as follows. Note that proposed changes are in bold.<br><br>Original query:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image35.png\"><br><br>Proposed query:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image36.png\"><br><br>Proposed query:<br><br>.start(\u201c/item_agg\u201d)<br><br>Which step must also be completed to put the proposed query into production?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify a new checkpointLocation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the shuffle partitions to account for additional aggregates",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun REFRESH TABLE delta.'/item_agg'",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the data in the \"/item_agg\" directory to the Hive metastore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove .option(\u2018mergeSchema\u2019, \u2018true\u2019) from the streaming write"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-21T06:08:00.000Z",
        "voteCount": 20,
        "content": "This question is broken. Proposed query cannot be identified."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/databricks/view/126756-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Structured Streaming job deployed to production has been resulting in higher than expected cloud storage costs. At present, during normal execution, each microbatch of data is processed in less than 3s; at least 12 times per minute, a microbatch is processed that contains 0 records. The streaming write was configured using the default trigger settings. The production job is currently scheduled alongside many other Databricks jobs in a workspace with instance pools provisioned to reduce start-up time for jobs with batch execution.<br><br>Holding all other variables constant and assuming records need to be processed in less than 10 minutes, which adjustment will meet the requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the trigger interval to 3 seconds; the default trigger interval is consuming too many records per batch, resulting in spill to disk that can increase volume costs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of shuffle partitions to maximize parallelism, since the trigger interval cannot be modified without modifying the checkpoint directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the trigger interval to 10 minutes; each batch calls APIs in the source storage account, so decreasing trigger frequency to maximum allowable threshold should minimize this cost.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the trigger interval to 500 milliseconds; setting a small but non-zero trigger interval ensures that the source is not queried too frequently.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the trigger once option and configure a Databricks job to execute the query every 10 minutes; this approach minimizes costs for both compute and storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-28T07:42:00.000Z",
        "voteCount": 1,
        "content": "E WRONG. Using trigger once would stop the stream after one execution, not meeting the requirement of continuous processing."
      },
      {
        "date": "2024-08-16T22:59:00.000Z",
        "voteCount": 1,
        "content": "E is correct for two reasons:\n1) we have been using the connection pool that allows us to start our job instantly\n2) the questions are about reducing costs. Triggering one per 10 minutes allows not to use running VM (as in option C)  and to keep the same SLA (due to 1) ) with lower cost for compute as well as for storage (fewer API calls which are not free )"
      },
      {
        "date": "2024-04-07T20:06:00.000Z",
        "voteCount": 3,
        "content": "required \"to be processed in less than 10 minutes\".\nC. \"set the trigger interval to 10 minutes\" means Process time + interval &gt; 10 minutes\nE. \"trigger once\", \"execute the query every 10 minutes\""
      },
      {
        "date": "2024-03-09T05:09:00.000Z",
        "voteCount": 2,
        "content": "default trigger time is 0.5 seconds \nHence in a minute there are 120 triggers happens\nEach trigger consume 3 seconds to complete\nnow 120*3 = 360 seconds = 6 minutes\nHence the job is completing in 6 minutes\nNow there is buffer of 4 minutes which can be utilized in compute spin up \nbut as we are using the spot instances which will further decrease the start up time \nI think E is correct option to decrease the cost."
      },
      {
        "date": "2024-03-07T00:58:00.000Z",
        "voteCount": 2,
        "content": "The question indicates that they are using instance pools for fast startup time. option C would block a VM permanently which is not intended. E will grab a VM, run the job, and return it to the pool to be available for other jobs mentioned in the question."
      },
      {
        "date": "2024-08-18T02:09:00.000Z",
        "voteCount": 1,
        "content": "you are right. But we need to guarantee SLA and for this reason to block VM (with autoscaling) is a good practice"
      },
      {
        "date": "2024-01-26T13:02:00.000Z",
        "voteCount": 3,
        "content": "C is more effective than E as E will incur startup time for spinning new job cluster"
      },
      {
        "date": "2024-01-23T11:48:00.000Z",
        "voteCount": 1,
        "content": "The default trigger interval is 500ms, but the question says it processes batches with 0 records and the avg time to process is 3s. If the requirement is to process under 10 minutes the best option here is to trigger every 3s."
      },
      {
        "date": "2024-01-01T14:48:00.000Z",
        "voteCount": 2,
        "content": "Both C and E meet the requirement to reduce cloud storage cost. E further reduces compute cost however reducing compute cost is not a requirement in the question."
      },
      {
        "date": "2023-12-18T03:18:00.000Z",
        "voteCount": 3,
        "content": "For production  -&gt; records need to be processed in less than 10 minutes. So we need to schedule each 10 minutes"
      },
      {
        "date": "2023-11-21T12:49:00.000Z",
        "voteCount": 3,
        "content": "Given that there are frequent microbatches with 0 records being processed, it indicates that the job is polling the source too often. Using the \"trigger once\" option would allow each microbatch to process all available data and then stop. By scheduling the job to run every 10 minutes, you ensure that the system is not constantly checking for new data when there is none, thus reducing the number of read operations from the source storage and potentially reducing costs associated with those reads."
      },
      {
        "date": "2023-12-06T10:05:00.000Z",
        "voteCount": 1,
        "content": "in this case why not C? Processing trigger in 10 min ensures the same I guess.."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/databricks/view/124596-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement describes the correct use of pyspark.sql.functions.broadcast?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt marks a column as having low enough cardinality to properly map distinct values to available partitions, allowing a broadcast join.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt marks a column as small enough to store in memory on all executors, allowing a broadcast join.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt caches a copy of the indicated table on attached storage volumes for all active clusters within a Databricks workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt marks a DataFrame as small enough to store in memory on all executors, allowing a broadcast join.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt caches a copy of the indicated table on all nodes in the cluster for use in all future queries during the cluster lifetime."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-28T18:37:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: D. It marks a DataFrame as small enough to store in memory on all executors, allowing a broadcast join.\n\nReference: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html"
      },
      {
        "date": "2023-11-21T12:51:00.000Z",
        "voteCount": 3,
        "content": "The broadcast function in PySpark is used in the context of joins. When you mark a DataFrame with broadcast, Spark tries to send this DataFrame to all worker nodes so that it can be joined with another DataFrame without shuffling the larger DataFrame across the nodes. This is particularly beneficial when the DataFrame is small enough to fit into the memory of each node. It helps to optimize the join process by reducing the amount of data that needs to be shuffled across the cluster, which can be a very expensive operation in terms of computation and time."
      },
      {
        "date": "2023-11-09T02:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2023-11-05T17:33:00.000Z",
        "voteCount": 1,
        "content": "The answer is D"
      },
      {
        "date": "2023-10-29T13:24:00.000Z",
        "voteCount": 2,
        "content": "https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.broadcast.html"
      },
      {
        "date": "2023-10-24T23:21:00.000Z",
        "voteCount": 3,
        "content": "Marks a DataFrame as small enough for use in broadcast joins."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/databricks/view/124369-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data engineer is configuring a pipeline that will potentially see late-arriving, duplicate records.<br><br>In addition to de-duplicating records within the batch, which of the following approaches allows the data engineer to deduplicate data against previously processed records as it is inserted into a Delta table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the configuration delta.deduplicate = true.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVACUUM the Delta table after each batch completes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform an insert-only merge with a matching condition on a unique key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a full outer join on a unique key and overwrite existing data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRely on Delta Lake schema enforcement to prevent duplicate records."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-21T12:53:00.000Z",
        "voteCount": 4,
        "content": "To handle deduplication against previously processed records in a Delta table, the MERGE INTO command can be used to perform an upsert operation. This means that if the incoming data has a record that matches an existing record based on a unique key, the MERGE INTO operation can update the existing record (if needed) or simply ignore the duplicate. If there is no match (i.e., the record is new), then the record will be inserted"
      },
      {
        "date": "2023-11-15T13:22:00.000Z",
        "voteCount": 2,
        "content": "answer is C"
      },
      {
        "date": "2023-11-09T02:06:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2023-10-29T13:25:00.000Z",
        "voteCount": 2,
        "content": "merge will be more efficient"
      },
      {
        "date": "2023-10-24T23:26:00.000Z",
        "voteCount": 4,
        "content": "Merge, when not match insert"
      },
      {
        "date": "2023-10-22T11:26:00.000Z",
        "voteCount": 1,
        "content": "C\nReference: file:///C:/Users/yuen1/Downloads/databricks-certified-data-engineer-professional-exam-guide.pdf"
      },
      {
        "date": "2023-10-25T04:17:00.000Z",
        "voteCount": 8,
        "content": "you are referencing a local pdf in your computer !!!"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/databricks/view/129709-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data pipeline uses Structured Streaming to ingest data from Apache Kafka to Delta Lake. Data is being stored in a bronze table, and includes the Kafka-generated timestamp, key, and value. Three months after the pipeline is deployed, the data engineering team has noticed some latency issues during certain times of the day.<br><br>A senior data engineer updates the Delta Table's schema and ingestion logic to include the current timestamp (as recorded by Apache Spark) as well as the Kafka topic and partition. The team plans to use these additional metadata fields to diagnose the transient processing delays.<br><br>Which limitation will the team face while diagnosing this problem?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew fields will not be computed for historic records.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark cannot capture the topic and partition fields from a Kafka source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew fields cannot be added to a production Delta table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdating the table schema will invalidate the Delta transaction log metadata.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdating the table schema requires a default value provided for each field added."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-29T08:55:00.000Z",
        "voteCount": 7,
        "content": "Looks like A to me.  Does anyone think otherwise?"
      },
      {
        "date": "2024-02-05T18:53:00.000Z",
        "voteCount": 4,
        "content": "When the schema of a Delta table is updated to include new fields, these fields will only be populated for new records ingested after the schema update. The new fields will not be retroactively computed for historic records already stored in the Delta table. Therefore, the additional metadata fields (current timestamp, Kafka topic, and partition) will not exist in the historic data, limiting the scope of the diagnosis to new data ingested after the schema update."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/databricks/view/124597-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "In order to facilitate near real-time workloads, a data engineer is creating a helper function to leverage the schema detection and evolution functionality of Databricks Auto Loader. The desired function will automatically detect the schema of the source directly, incrementally process JSON files as they arrive in a source directory, and automatically evolve the schema of the table when new fields are detected.<br><br>The function is displayed below with a blank:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image37.png\"><br><br>Which response correctly fills in the blank to meet the specified requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-professional/image38.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-professional/image39.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-professional/image40.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-professional/image41.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-professional/image42.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-11T07:51:00.000Z",
        "voteCount": 2,
        "content": "write is not for spark streaming"
      },
      {
        "date": "2024-05-28T19:04:00.000Z",
        "voteCount": 2,
        "content": "Reference: https://docs.databricks.com/en/ingestion/auto-loader/schema.html\n\nwriteStream: Ensures real-time streaming write capabilities, which is essential f\nor near real-time workloads.\ncheckpointLocation: Necessary for fault tolerance and tracking progress.\nmergeSchema: Ensures automatic schema evolution, allowing new columns to be detected and added to the target table.\n\nWhy Option 'C ' is incorrect?\nUses write instead of writeStream, which is for batch processing, making it inappropriate for real-time streaming.\n\nWhy Option 'B ' is incorrect?\nAlthough it includes checkpointLocation and mergeSchema, the addition of trigger(once=True) is not necessary in this context, and it is better suited for batch-like processing.\n\nReference: https://docs.databricks.com/en/ingestion/auto-loader/schema.html"
      },
      {
        "date": "2024-03-09T05:29:00.000Z",
        "voteCount": 2,
        "content": "streamRead &amp; StreamWrite shares the schema using checkpoint location\nso cloudFiles.schemaLocation needs to be same for checkpointLocation so that we dont need to specify it manually \nalso mergeSchema True make sure if any new column detected , it will be added in the target table \n\nhttps://docs.databricks.com/en/ingestion/auto-loader/schema.html"
      },
      {
        "date": "2024-03-04T18:03:00.000Z",
        "voteCount": 2,
        "content": "https://notebooks.databricks.com/demos/auto-loader/01-Auto-loader-schema-evolution-Ingestion.html"
      },
      {
        "date": "2023-11-21T13:15:00.000Z",
        "voteCount": 1,
        "content": "This response correctly fills in the blank to meet the specified requirements of using Databricks Auto Loader for automatic schema detection and evolution in a near real-time streaming context."
      },
      {
        "date": "2023-11-10T00:18:00.000Z",
        "voteCount": 3,
        "content": "Please refer: https://docs.databricks.com/en/ingestion/auto-loader/schema.html"
      },
      {
        "date": "2023-11-09T02:20:00.000Z",
        "voteCount": 1,
        "content": "It does not mention to write as stream, it mentions to write incrementally, so option C looks correct for me"
      },
      {
        "date": "2023-10-26T08:24:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is E, it is a streaming write, and the default outputMode is Append (so if it's optional in this case)"
      },
      {
        "date": "2023-10-24T23:34:00.000Z",
        "voteCount": 1,
        "content": "there is a type in the statement. Is it schema or checkpoint ?\nProvided answer is not correct. It has to be a writestream, with mode append"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/databricks/view/124598-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineering team maintains the following code:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image43.png\"><br><br>Assuming that this code produces logically correct results and the data in the source table has been de-duplicated and validated, which statement describes what will occur when this code is executed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe silver_customer_sales table will be overwritten by aggregated values calculated from all records in the gold_customer_lifetime_sales_summary table as a batch job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA batch job will update the gold_customer_lifetime_sales_summary table, replacing only those rows that have different values than the current version of the table, using customer_id as the primary key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe gold_customer_lifetime_sales_summary table will be overwritten by aggregated values calculated from all records in the silver_customer_sales table as a batch job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn incremental job will leverage running information in the state store to update aggregate values in the gold_customer_lifetime_sales_summary table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn incremental job will detect if new rows have been written to the silver_customer_sales table; if new rows are detected, all aggregates will be recalculated and used to overwrite the gold_customer_lifetime_sales_summary table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-21T13:20:00.000Z",
        "voteCount": 5,
        "content": "The code is performing a batch aggregation operation on the \"silver_customer_sales\" table grouped by \"customer_id\". It calculates the first and last transaction dates, the average sales, the total number of distinct orders, and the lifetime value of sales for each customer. The .mode(\"overwrite\") operation specifies that the output table \"gold_customer_lifetime_sales_summary\" should be overwritten with the result of this aggregation. This means that every time this code runs, it will replace the existing \"gold_customer_lifetime_sales_summary\" table with a new version that reflects the current state of the \"silver_customer_sales\" table."
      },
      {
        "date": "2024-03-04T18:08:00.000Z",
        "voteCount": 1,
        "content": "C. there's nowhere implicating streaming."
      },
      {
        "date": "2023-11-09T02:22:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2023-10-26T08:26:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer is C, it is an overwrite mode"
      },
      {
        "date": "2023-10-24T23:36:00.000Z",
        "voteCount": 4,
        "content": "it does overwrite, so no incremental load"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/databricks/view/124599-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data architect has mandated that all tables in the Lakehouse should be configured as external (also known as \"unmanaged\") Delta Lake tables.<br><br>Which approach will ensure that this requirement is met?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen a database is being created, make sure that the LOCATION keyword is used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen configuring an external data warehouse for all table storage, leverage Databricks for all ELT.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen data is saved to a table, make sure that a full file path is specified alongside the Delta format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen tables are created, make sure that the EXTERNAL keyword is used in the CREATE TABLE statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the workspace is being configured, make sure that external cloud object storage has been mounted."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-24T23:38:00.000Z",
        "voteCount": 5,
        "content": "Non of the provided.\nIt should be: When a table is created, make sure LOCATION is provided"
      },
      {
        "date": "2024-09-11T17:35:00.000Z",
        "voteCount": 1,
        "content": "'create external table' statement is using in HIVE, so C is correct."
      },
      {
        "date": "2024-02-05T18:48:00.000Z",
        "voteCount": 4,
        "content": "In Delta Lake, an external (or unmanaged) table is a table created outside of the data lake but is still accessible from the data lake. The data for external tables is stored in a location specified by the user, not in the default directory of the data lake. When you save data to an external table, you need to specify the full file path where the data will be stored. This makes the table \u201cexternal\u201d because the data itself is not managed by Delta Lake, only the metadata is. This is why specifying a full file path alongside the Delta format when saving data to a table will ensure that the table is configured as an external Delta Lake table."
      },
      {
        "date": "2024-01-22T02:19:00.000Z",
        "voteCount": 2,
        "content": "C is correct."
      },
      {
        "date": "2023-12-09T13:20:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-11-21T13:31:00.000Z",
        "voteCount": 3,
        "content": "Here's why the other options may not ensure the requirement is met:\nD. Delta Lake does not use the EXTERNAL keyword in the same way as some other SQL-based systems. In Delta Lake, whether a table is external is determined by where the data files are stored, not by a keyword in the CREATE TABLE statement.\n\n%sql\nCREATE TABLE f1_demo.results_external\nUSING DELTA\nLOCATION '/mnt/formula1dl/demo/results_external'"
      },
      {
        "date": "2023-11-09T02:24:00.000Z",
        "voteCount": 1,
        "content": "possible answer is C"
      },
      {
        "date": "2023-11-06T08:52:00.000Z",
        "voteCount": 1,
        "content": "I think it should be A because when a database is created using a location all tables within this database are automatically assign as unmanaged tables."
      },
      {
        "date": "2023-11-14T16:14:00.000Z",
        "voteCount": 4,
        "content": "Not quite. Test &amp; see. The tables are 'managed' though database creation has 'LOCATION' keyword. C is best."
      },
      {
        "date": "2023-10-30T00:06:00.000Z",
        "voteCount": 1,
        "content": "provide path (LOCATION)"
      },
      {
        "date": "2023-10-26T08:28:00.000Z",
        "voteCount": 1,
        "content": "C is plausible answer, as in this case we are writing the data to an external location"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/databricks/view/129728-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The marketing team is looking to share data in an aggregate table with the sales organization, but the field names used by the teams do not match, and a number of marketing-specific fields have not been approved for the sales org.<br><br>Which of the following solutions addresses the situation while emphasizing simplicity?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a view on the marketing table selecting only those fields approved for the sales team; alias the names of any fields that should be standardized to the sales naming conventions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new table with the required schema and use Delta Lake's DEEP CLONE functionality to sync up changes committed to one table to the corresponding table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a CTAS statement to create a derivative table from the marketing table; configure a production job to propagate changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a parallel table write to the current production pipeline, updating a new sales table that varies as required from the marketing table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstruct the marketing team to download results as a CSV and email them to the sales organization."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-26T00:25:00.000Z",
        "voteCount": 1,
        "content": "A is the simplest one"
      },
      {
        "date": "2024-02-05T18:44:00.000Z",
        "voteCount": 3,
        "content": "Creating a view is a simple and efficient way to provide access to a subset of data from a table. In this case, the view can be configured to include only the fields that have been approved for the sales team. Additionally, any fields that need to be renamed to match the sales team\u2019s naming conventions can be aliased in the view. This approach does not require the creation of additional tables or the configuration of jobs to sync data, making it a relatively straightforward solution. However, it\u2019s important to note that views do not physically store data, so any changes to the underlying marketing table will be reflected in the view. This means that the sales team will always have access to the most up-to-date approved data."
      },
      {
        "date": "2024-01-25T07:27:00.000Z",
        "voteCount": 1,
        "content": "A is the simplest"
      },
      {
        "date": "2023-12-29T10:12:00.000Z",
        "voteCount": 2,
        "content": "Looks like A to me"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/databricks/view/124255-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A CHECK constraint has been successfully added to the Delta table named activity_details using the following logic:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image44.png\"><br><br>A batch job is attempting to insert new records to the table, including a record where latitude = 45.50 and longitude = 212.67.<br><br>Which statement describes the outcome of this batch insert?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe write will fail when the violating record is reached; any records previously processed will be recorded to the target table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe write will fail completely because of the constraint violation and no records will be inserted into the target table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe write will insert all records except those that violate the table constraints; the violating records will be recorded to a quarantine table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe write will include all records in the target table; any violations will be indicated in the boolean column named valid_coordinates.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe write will insert all records except those that violate the table constraints; the violating records will be reported in a warning log."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T18:41:00.000Z",
        "voteCount": 5,
        "content": "In Delta Lake, when a batch job attempts to insert records into a table that has a CHECK constraint, if any record violates the constraint, the entire write operation fails. This is because Delta Lake enforces strong transactional guarantees, which means that either all changes in a transaction are saved, or none are."
      },
      {
        "date": "2024-01-25T07:30:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-21T13:58:00.000Z",
        "voteCount": 4,
        "content": "In systems that support atomic transactions, such as Delta Lake, when a batch operation encounters a record that violates a CHECK constraint, the entire operation fails, and no records are inserted, including those that do not violate the constraint. This is to ensure the atomicity of the transaction, meaning that either all the changes are committed, or none are, maintaining data integrity. The record with a longitude of 212.67 violates the constraint because longitude values must be between -180 and 180 degrees."
      },
      {
        "date": "2023-11-09T02:27:00.000Z",
        "voteCount": 4,
        "content": "B is the answer"
      },
      {
        "date": "2023-10-24T23:50:00.000Z",
        "voteCount": 4,
        "content": "B is the answer"
      },
      {
        "date": "2023-10-21T12:07:00.000Z",
        "voteCount": 3,
        "content": "B is the ans"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/databricks/view/124600-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer has manually configured a series of jobs using the Databricks Jobs UI. Upon reviewing their work, the engineer realizes that they are listed as the \"Owner\" for each job. They attempt to transfer \"Owner\" privileges to the \"DevOps\" group, but cannot successfully accomplish this task.<br><br>Which statement explains what is preventing this privilege transfer?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks jobs must have exactly one owner; \"Owner\" privileges cannot be assigned to a group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe creator of a Databricks job will always have \"Owner\" privileges; this configuration cannot be changed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOther than the default \"admins\" group, only individual users can be granted privileges on jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA user can only transfer job ownership to a group if they are also a member of that group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly workspace administrators can grant \"Owner\" privileges to a group."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-04T19:36:00.000Z",
        "voteCount": 2,
        "content": "did a test. \"group cannot be owner\" is displayed."
      },
      {
        "date": "2024-02-05T18:38:00.000Z",
        "voteCount": 1,
        "content": "In Databricks, each job must have exactly one owner, which is typically the user who created the job. This \u201cOwner\u201d privilege allows the user to perform any action on the job, including modifying its settings or deleting it. However, this privilege cannot be assigned to a group. If you want to allow multiple users or a group of users to manage a job, you can use ACLs (Access Control Lists) to grant them the necessary permissions. But the \u201cOwner\u201d privilege will still remain with the individual user who created the job."
      },
      {
        "date": "2023-10-24T23:55:00.000Z",
        "voteCount": 3,
        "content": "Correct\nA job cannot have more than one owner. A job cannot have a group as an owner"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/databricks/view/124601-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "All records from an Apache Kafka producer are being ingested into a single Delta Lake table with the following schema:<br><br>key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG<br><br>There are 5 unique topics being ingested. Only the \"registration\" topic contains Personal Identifiable Information (PII). The company wishes to restrict access to PII. The company also wishes to only retain records containing PII in this table for 14 days after initial ingestion. However, for non-PII information, it would like to retain these records indefinitely.<br><br>Which of the following solutions meets the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll data should be deleted biweekly; Delta Lake's time travel functionality should be leveraged to maintain a history of non-PII information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData should be partitioned by the registration field, allowing ACLs and delete statements to be set for the PII directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the value field is stored as binary data, this information is not considered PII and no special precautions should be taken.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate object storage containers should be specified based on the partition field, allowing isolation at the storage level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData should be partitioned by the topic field, allowing ACLs and delete statements to leverage partition boundaries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-28T03:25:00.000Z",
        "voteCount": 12,
        "content": "I think answer E is correct, as by default partitionning by a column will create a separate folder for each subset data linked to the partition"
      },
      {
        "date": "2024-02-15T05:50:00.000Z",
        "voteCount": 1,
        "content": "i think it's best to isolate the storage to avoid mistakenly deleting tables in the same storage so I go with D"
      },
      {
        "date": "2024-01-25T07:36:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-12-24T23:21:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2023-11-21T14:02:00.000Z",
        "voteCount": 2,
        "content": "Partitioning data by the topic field would allow the data engineering team to apply access control lists (ACLs) to restrict access to the partition containing the \"registration\" topic, which holds PII. Furthermore, the team can set up automated deletion policies that specifically target the partition with PII data to delete records after 14 days, without affecting the data in other partitions. This approach meets both the privacy requirements for PII and the data retention goals for non-PII information."
      },
      {
        "date": "2023-11-09T02:36:00.000Z",
        "voteCount": 3,
        "content": "I think answer is E"
      },
      {
        "date": "2023-10-28T23:31:00.000Z",
        "voteCount": 1,
        "content": "The solution that meets the requirements is: B. Data should be partitioned by the registration field, allowing ACLs and delete statements to be set for the PII directory.\n\nPartitioning the data by the registration field allows the directory containing PII records to be isolated and access restricted via ACLs. Additionally, the data retention requirements can be met by setting up a separate job or process to remove PII records that are 14 days old. For non-PII records, they can be retained indefinitely utilizing Delta Lake's time travel functionality."
      },
      {
        "date": "2023-10-29T09:36:00.000Z",
        "voteCount": 1,
        "content": "There is no such thing as Registration field, it's a distinct topic"
      },
      {
        "date": "2023-10-30T00:18:00.000Z",
        "voteCount": 1,
        "content": "you cannot restricts privileges. with ACLs on a partition. Documentations states that Securable objects in the Hive metastore are: DB, Tables, Views, Functions: https://docs.databricks.com/en/data-governance/table-acls/object-privileges.html#securable-objects"
      },
      {
        "date": "2023-10-24T23:58:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-10-30T00:18:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/data-governance/table-acls/object-privileges.html#securable-objects"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/databricks/view/124602-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data architect has decided that once data has been ingested from external sources into the<br>Databricks Lakehouse, table access controls will be leveraged to manage permissions for all production tables and views.<br><br>The following logic was executed to grant privileges for interactive queries on a production database to the core engineering group.<br><br>GRANT USAGE ON DATABASE prod TO eng;<br>GRANT SELECT ON DATABASE prod TO eng;<br><br>Assuming these are the only privileges that have been granted to the eng group and that these users are not workspace administrators, which statement describes their privileges?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup members have full permissions on the prod database and can also assign permissions to other users or groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup members are able to list all tables in the prod database but are not able to see the results of any queries on those tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup members are able to query and modify all tables and views in the prod database, but cannot create new tables or views.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup members are able to query all tables and views in the prod database, but cannot create or edit anything in the database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup members are able to create, query, and modify all tables and views in the prod database, but cannot define custom functions."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-25T00:03:00.000Z",
        "voteCount": 6,
        "content": "Usage and Select ....sa abasically they can only select"
      },
      {
        "date": "2024-02-26T19:52:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-02-05T18:25:00.000Z",
        "voteCount": 1,
        "content": "The GRANT statements provided in the logic grant the USAGE privilege, allowing the group members to see the existence of the database, and the SELECT privilege, allowing them to query tables and views. However, they do not have permissions to create or edit anything in the database. Therefore, the correct description is that group members can query all tables and views in the prod database but cannot create or edit any objects in the database."
      },
      {
        "date": "2024-01-01T15:42:00.000Z",
        "voteCount": 1,
        "content": "D is correct assuming unity catalog is not enabled"
      },
      {
        "date": "2023-11-21T14:06:00.000Z",
        "voteCount": 3,
        "content": "The GRANT USAGE ON DATABASE statement gives the eng group the ability to access the prod database. This means they can enter the database context and list the tables. The GRANT SELECT ON DATABASE statement additionally grants them permission to perform SELECT queries on all existing tables and views within the prod database. However, these privileges do not include creating new tables or views, modifying existing tables, or assigning permissions to other users or groups."
      },
      {
        "date": "2023-11-09T02:46:00.000Z",
        "voteCount": 4,
        "content": "D is answer"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/databricks/view/124604-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A distributed team of data analysts share computing resources on an interactive cluster with autoscaling configured. In order to better manage costs and query throughput, the workspace administrator is hoping to evaluate whether cluster upscaling is caused by many concurrent users or resource-intensive queries.<br><br>In which location can one review the timeline for cluster resizing events?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorkspace audit logs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDriver's log file",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGanglia",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster Event Log\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutor's log file"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-26T19:56:00.000Z",
        "voteCount": 3,
        "content": "The Cluster Event Log provides detailed information about various events affecting the cluster throughout its lifecycle, including cluster creation, restarts, termination, and resizing events. It displays the timestamp, event type (e.g., \"CLUSTER_RESIZED\"), and relevant details for each event, allowing the administrator to review the timeline for cluster scaling behavior and identify potential patterns related to user activity or resource-intensive queries."
      },
      {
        "date": "2024-02-05T18:22:00.000Z",
        "voteCount": 1,
        "content": "The timeline for cluster resizing events can be reviewed in the Cluster Event Log. This log provides information about cluster scaling events, including when the cluster is scaled up or down. You can access this information to understand the reasons behind autoscaling events and whether they are triggered by many concurrent users or resource-intensive queries."
      },
      {
        "date": "2023-12-18T23:26:00.000Z",
        "voteCount": 2,
        "content": "Cluster event log"
      },
      {
        "date": "2023-11-21T14:09:00.000Z",
        "voteCount": 2,
        "content": "The Cluster Event Log in Databricks will show the timeline for cluster resizing events, including details about when and why a cluster was resized (scaled up or down). This log would help the workspace administrator determine the causes of cluster scaling, whether due to many concurrent users submitting jobs or a few users running resource-intensive queries.\n\nless suitable:\nC. Ganglia provides metrics on system-level performance, such as CPU and memory usage, but does not log specific cluster scaling events."
      },
      {
        "date": "2023-11-05T18:45:00.000Z",
        "voteCount": 2,
        "content": "cluster event log. D"
      },
      {
        "date": "2023-10-25T00:17:00.000Z",
        "voteCount": 3,
        "content": "Cluster Event Log"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/databricks/view/125408-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "When evaluating the Ganglia Metrics for a given cluster with 3 executor nodes, which indicator would signal proper utilization of the VM's resources?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe five Minute Load Average remains consistent/flat",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBytes Received never exceeds 80 million bytes per second",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNetwork I/O never spikes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTotal Disk Space remains constant",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCPU Utilization is around 75%\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-05T04:29:00.000Z",
        "voteCount": 7,
        "content": "I would look at max CPU utilization and max Memory usage.\nHaving 75% CPU usage would signify we have a proper utilization of cpu resources"
      },
      {
        "date": "2024-02-05T18:18:00.000Z",
        "voteCount": 2,
        "content": "Proper utilization of VM resources, especially in a distributed computing environment like Spark, often involves efficient usage of CPU resources. A CPU utilization around 75% indicates that the CPU is being utilized without being fully saturated, allowing room for additional processing without causing excessive contention."
      },
      {
        "date": "2023-12-18T23:30:00.000Z",
        "voteCount": 1,
        "content": "75%  good"
      },
      {
        "date": "2023-11-21T14:11:00.000Z",
        "voteCount": 3,
        "content": "An average CPU utilization around 75% is a good indicator of proper utilization of the VM's resources in a distributed computing environment. It suggests that the CPUs are being actively used for computation without being maxed out, which could indicate a bottleneck. It leaves some headroom to handle additional load without causing excessive queuing or delays."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/databricks/view/124605-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which of the following technologies can be used to identify key areas of text when parsing Spark Driver log4j output?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegex\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJulia",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpyspsark.ml.feature",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScala Datasets",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tC++"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T18:12:00.000Z",
        "voteCount": 3,
        "content": "It allows us to define patterns that match the structure of the log entries and capture relevant data."
      },
      {
        "date": "2023-11-21T14:13:00.000Z",
        "voteCount": 4,
        "content": "Regular expressions (regex) can be used to identify and extract patterns from text data, which makes them very useful for parsing log files like the Spark Driver's log4j output. By defining specific regex patterns, you can search for error messages, timestamps, specific log levels, or any other text that follows a particular format within the log files."
      },
      {
        "date": "2023-10-30T00:24:00.000Z",
        "voteCount": 3,
        "content": "Regex to extract text"
      },
      {
        "date": "2023-10-30T00:24:00.000Z",
        "voteCount": 2,
        "content": "Regex to extract text. C++ makes no sense in this context"
      },
      {
        "date": "2023-10-30T00:24:00.000Z",
        "voteCount": 2,
        "content": "I meant A"
      },
      {
        "date": "2023-10-29T13:46:00.000Z",
        "voteCount": 2,
        "content": "regex is for string identification"
      },
      {
        "date": "2023-10-28T03:16:00.000Z",
        "voteCount": 4,
        "content": "Using regex, we can identify key ans values areas"
      },
      {
        "date": "2023-10-25T00:33:00.000Z",
        "voteCount": 1,
        "content": "Why C++, why not python or Java? Plus there are tools om parsing the log4j output like Chainsaw and xmlstarlet."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/databricks/view/130110-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "You are testing a collection of mathematical functions, one of which calculates the area under a curve as described by another function.<br><br>assert(myIntegrate(lambda x: x*x, 0, 3) [0] == 9)<br><br>Which kind of test would the above line exemplify?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnit\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManual",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFunctional",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntegration",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnd-to-end"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-06T07:35:00.000Z",
        "voteCount": 3,
        "content": "Answer is A, unit test"
      },
      {
        "date": "2024-03-22T15:12:00.000Z",
        "voteCount": 3,
        "content": "I think it should be Functional Test"
      },
      {
        "date": "2024-07-26T05:43:00.000Z",
        "voteCount": 2,
        "content": "There are 3 testing types:\nUnit testing\nIntegration testing\nAnd end to end testing"
      },
      {
        "date": "2024-02-05T18:11:00.000Z",
        "voteCount": 3,
        "content": "A. Unit"
      },
      {
        "date": "2024-01-01T15:48:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/databricks/view/124606-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Databricks job has been configured with 3 tasks, each of which is a Databricks notebook. Task A does not depend on other tasks. Tasks B and C run in parallel, with each having a serial dependency on Task A.<br><br>If task A fails during a scheduled run, which statement describes the results of this run?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause all tasks are managed as a dependency graph, no changes will be committed to the Lakehouse until all tasks have successfully been completed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTasks B and C will attempt to run as configured; any changes made in task A will be rolled back due to task failure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnless all tasks complete successfully, no changes will be committed to the Lakehouse; because task A failed, all commits will be rolled back automatically.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTasks B and C will be skipped; some logic expressed in task A may have been committed before task failure.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTasks B and C will be skipped; task A will not commit any changes because of stage failure."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-28T03:18:00.000Z",
        "voteCount": 6,
        "content": "D is correct, taks B and C will definitely be skipped, since Task A is notebook, the ACID logic is at cell level, some logic might be executed before failing cell"
      },
      {
        "date": "2023-11-21T14:17:00.000Z",
        "voteCount": 4,
        "content": "In Databricks job execution, if a task that other tasks depend on fails, the dependent tasks will not be executed. Since Tasks B and C depend on the successful completion of Task A, they will be skipped if Task A fails. However, if Task A performs any operations that commit changes before the failure occurs (such as writing to a Delta table), those changes remain and are not automatically rolled back unless the logic within Task A specifically includes rollback mechanisms for partial failures."
      },
      {
        "date": "2023-11-09T02:54:00.000Z",
        "voteCount": 3,
        "content": "D is the answer"
      },
      {
        "date": "2023-10-25T00:35:00.000Z",
        "voteCount": 3,
        "content": "Some ops in task A may have fished before fail"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/databricks/view/127597-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement regarding Spark configuration on the Databricks platform is true?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Databricks REST API can be used to modify the Spark configuration properties for an interactive cluster without interrupting jobs currently running on the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark configurations set within a notebook will affect all SparkSessions attached to the same interactive cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark configuration properties can only be set for an interactive cluster by creating a global init script.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark configuration properties set for an interactive cluster with the Clusters UI will impact all notebooks attached to that cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the same Spark configuration property is set for an interactive cluster and a notebook attached to that cluster, the notebook setting will always be ignored."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-04T09:28:00.000Z",
        "voteCount": 7,
        "content": "I tried it myself, setting a spark conf on the cluster ui, will impact all notebooks attached to that cluster, for example i set the number of shuffle partitions to 4, and in every notebook when i inspect the number of partitions i find 4."
      },
      {
        "date": "2024-02-26T20:11:00.000Z",
        "voteCount": 1,
        "content": "A. Incorrect: Modifying configurations through the Databricks REST API while jobs are running can lead to unexpected behavior or disruption. It's generally not recommended.\n    C. Incorrect: While global init scripts can be used, it's not the only way. Configurations can also be set within notebooks.\n    D. Incorrect: Configurations set through the Clusters UI apply to the entire cluster, but they might not necessarily override configurations set within notebooks attached to the cluster.\n    E. Incorrect: Notebook configurations can take precedence over cluster-level configurations for the same property, offering finer-grained control at the notebook level."
      },
      {
        "date": "2024-02-05T18:06:00.000Z",
        "voteCount": 3,
        "content": "These settings are applied at the cluster level and affect all SparkSessions on the cluster."
      },
      {
        "date": "2023-12-18T23:50:00.000Z",
        "voteCount": 4,
        "content": "A wrong, cluster will restart  -&gt; D"
      },
      {
        "date": "2023-12-01T14:11:00.000Z",
        "voteCount": 1,
        "content": "In Databricks, you can use the Databricks REST API to modify Spark configuration properties for an interactive cluster without interrupting currently running jobs. This allows you to dynamically adjust Spark configurations to optimize performance or meet specific requirements without the need to restart the cluster."
      },
      {
        "date": "2023-12-18T23:50:00.000Z",
        "voteCount": 1,
        "content": "wrong, cluster will restart"
      },
      {
        "date": "2023-12-01T14:24:00.000Z",
        "voteCount": 2,
        "content": "If you update the configuration of a cluster using the Databricks REST API or the Clusters UI while the cluster is in a RUNNING state, the cluster will be restarted to apply the new configuration. However, Databricks typically handles this situation in a way that minimizes disruption to running jobs."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/databricks/view/130091-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A developer has successfully configured their credentials for Databricks Repos and cloned a remote Git repository. They do not have privileges to make changes to the main branch, which is the only branch currently visible in their workspace.<br><br>Which approach allows this user to share their code updates without the risk of overwriting the work of their teammates?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to checkout all changes and send the git diff log to the team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to create a fork of the remote repository, commit all changes, and make a pull request on the source repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to pull changes from the remote Git repository; commit and push changes to a branch that appeared as changes were pulled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to merge all differences and make a pull request back to the remote repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to create a new branch, commit all changes, and push changes to the remote Git repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-18T06:15:00.000Z",
        "voteCount": 1,
        "content": "Explanation:\n\nThe developer does not have privileges to make changes to the main branch of the remote repository, and it's the only branch visible in their workspace. To share their code updates without risking overwriting their teammates' work, the best approach is to:\n\nCreate a personal copy (fork) of the remote repository. This forked repository will be under the developer's own account or workspace, allowing full control over it.\nMake Changes in the Fork:\n\nCommit all code updates to the forked repository. Since the developer has full privileges on their fork, they can create branches, commit changes, and manage the repository as needed.\n\nOption E: Creating a new branch and pushing changes to the remote repository requires write access to the repository, which the developer does not have.\n\nBy forking the repository, the developer avoids any permission issues and ensures that their work does not interfere with the main codebase until it is reviewed and approved by the team."
      },
      {
        "date": "2024-03-05T00:04:00.000Z",
        "voteCount": 4,
        "content": "E is the regular collaboration approach.\nB makes a fork so breaks away from the collaborating teamates. There's no way they can make a pull request on the source repository after making change to a fork."
      },
      {
        "date": "2024-03-07T00:30:00.000Z",
        "voteCount": 2,
        "content": "Sorry, my mistake, just tested B is a do-able way. Fork from github can create pull request against the original repository and contribute back. B seems to be a better answer."
      },
      {
        "date": "2024-02-27T04:20:00.000Z",
        "voteCount": 2,
        "content": "Databricks Repos itself does not currently support creating forks directly within the platform. However, you can achieve a similar workflow using the following steps:\n\n    Use the git clone command in a terminal or IDE to create a local copy of the remote repository. This effectively creates a local fork.\n    Make your changes in the local copy.\n    Use git push to push your changes to a new remote repository you create on a Git hosting service like GitHub.\n    Create a pull request from your new remote repository to the original repository on Databricks Repos.\n\nThis approach allows you to make changes to your own copy of the code, collaborate with others through code reviews, and propose your changes for integration into the main branch without directly modifying it"
      },
      {
        "date": "2024-07-27T17:24:00.000Z",
        "voteCount": 1,
        "content": "The argument provided suggests using a traditional Git workflow that involves forking a repository on a Git hosting service like GitHub, making changes, and then creating a pull request. This is a valid and widely used approach in standard Git workflows. However, it is essential to distinguish between the capabilities and features provided directly by Databricks Repos and the general Git practices that can be applied outside of Databricks."
      },
      {
        "date": "2024-02-05T18:03:00.000Z",
        "voteCount": 3,
        "content": "This is a common workflow in collaborative development environments. The developer can create a new branch in their local repository, make changes, and then push the branch to the remote repository. This way, they can share their updates without modifying the main branch directly. After pushing the changes, they can create a pull request on the remote repository, allowing their teammates to review the changes before merging them into the main branch. This process ensures that the main branch remains stable and that all changes are reviewed and approved before they\u2019re incorporated. It also prevents any accidental overwrites of teammates\u2019 work."
      },
      {
        "date": "2024-02-02T02:36:00.000Z",
        "voteCount": 2,
        "content": "Databricks Repos cannot be used to fork a repository"
      },
      {
        "date": "2024-02-01T00:41:00.000Z",
        "voteCount": 3,
        "content": "E is correct"
      },
      {
        "date": "2024-01-30T16:16:00.000Z",
        "voteCount": 2,
        "content": "E is correct\nif you create a fork you create another repository so not B"
      },
      {
        "date": "2024-01-28T20:04:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-01-25T07:52:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-01-01T07:49:00.000Z",
        "voteCount": 1,
        "content": "Isn't this B?"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/databricks/view/126210-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "In order to prevent accidental commits to production data, a senior data engineer has instituted a policy that all development work will reference clones of Delta Lake tables. After testing both DEEP and SHALLOW CLONE, development tables are created using SHALLOW CLONE.<br><br>A few weeks after initial table creation, the cloned versions of several tables implemented as Type 1 Slowly Changing Dimension (SCD) stop working. The transaction logs for the source tables show that VACUUM was run the day before.<br><br>Which statement describes why the cloned tables are no longer working?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause Type 1 changes overwrite existing records, Delta Lake cannot guarantee data consistency for cloned tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRunning VACUUM automatically invalidates any shallow clones of a table; DEEP CLONE should always be used when a cloned table will be repeatedly queried.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTables created with SHALLOW CLONE are automatically deleted after their default retention threshold of 7 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe metadata created by the CLONE operation is referencing data files that were purged as invalid by the VACUUM command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe data files compacted by VACUUM are not tracked by the cloned metadata; running REFRESH on the cloned table will pull in recent changes."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-19T00:03:00.000Z",
        "voteCount": 6,
        "content": "Shallow clone: only duplicates the metadata of the table being cloned; the data files of the table itself are not copied. These clones are cheaper to create but are  not self-contained and depend on the source from which they were cloned as the source of data. If the files in the source that the clone depends on are removed, for example with VACUUM, a shallow clone may become unusable. Therefore, shallow clones are typically used for short-lived use cases such as testing and experimentation."
      },
      {
        "date": "2024-10-16T05:12:00.000Z",
        "voteCount": 1,
        "content": "I was not sure whether B or D but somehow I think that running VACUUM comand does not invalidate SHALLOW CLONEs . I mean its just that the data referenced by the clone is no longer present. It can still happen that a SHALLOW CLONE is working even after a VACUUM command run on the cloned table (origin) . So B is not completely correct."
      },
      {
        "date": "2024-02-05T18:00:00.000Z",
        "voteCount": 1,
        "content": "In Delta Lake, the VACUUM command deletes data files that are no longer referenced by a Delta table and are older than the retention threshold. When a table is cloned using SHALLOW CLONE, the clone references the same data files as the original table but creates a new transaction log. If VACUUM is run on the original table, it can delete data files that are still being referenced by the cloned table\u2019s metadata, causing the cloned table to stop working. This is because the VACUUM command doesn\u2019t know about the cloned table\u2019s references to the data files. Therefore, it\u2019s important to be cautious when running VACUUM on tables that have clones."
      },
      {
        "date": "2024-01-25T07:55:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-11-19T21:44:00.000Z",
        "voteCount": 2,
        "content": "Please refer: \nhttps://docs.databricks.com/en/delta/clone.html#what-are-the-semantics-of-delta-clone-operations"
      },
      {
        "date": "2023-11-15T01:05:00.000Z",
        "voteCount": 1,
        "content": "B is best"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/databricks/view/127268-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "You are performing a join operation to combine values from a static userLookup table with a streaming DataFrame streamingDF.<br><br>Which code block attempts to perform an invalid stream-static join?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tuserLookup.join(streamingDF, [\"userid\"], how=\"inner\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstreamingDF.join(userLookup, [\"user_id\"], how=\"outer\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstreamingDF.join(userLookup, [\"user_id\u201d], how=\"left\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstreamingDF.join(userLookup, [\"userid\"], how=\"inner\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tuserLookup.join(streamingDF, [\"user_id\"], how=\"right\")"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-26T10:50:00.000Z",
        "voteCount": 10,
        "content": "Answer B is correct:\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#support-matrix-for-joins-in-streaming-queries\n\nWhen we take a look in the  supported join matrix between static and stream inputs, we can identify, that Stream-Static + outer is not supported.\nAnswer E is wrong, because the Static-Stream + right join is supported."
      },
      {
        "date": "2024-05-31T20:19:00.000Z",
        "voteCount": 2,
        "content": "B.\nWe match all the records from a static DataFrame on the left with a stream DataFrame on the right. If records do not match from the static DF (Left) to stream DF (Right), then the system cannot return null since the data changes on stream DF (Right), and we cannot guarantee if we will get matching records. That is why full_outer join is not supported."
      },
      {
        "date": "2024-03-14T00:16:00.000Z",
        "voteCount": 4,
        "content": "in my exam today, BCD are removed. i chose E, because I recall that stream-static right join are less supported."
      },
      {
        "date": "2024-02-26T21:37:00.000Z",
        "voteCount": 1,
        "content": "b is correct"
      },
      {
        "date": "2024-02-05T17:55:00.000Z",
        "voteCount": 1,
        "content": "Specifically, outer joins are not supported with a static DataFrame on the right and a streaming DataFrame on the left. This is because it\u2019s not possible to guarantee all necessary rows will be available in the streaming DataFrame for every micro-batch."
      },
      {
        "date": "2024-01-12T05:44:00.000Z",
        "voteCount": 1,
        "content": "I think the correct answer is D."
      },
      {
        "date": "2024-01-12T05:47:00.000Z",
        "voteCount": 1,
        "content": "Sorry I missread the question."
      },
      {
        "date": "2024-01-05T03:05:00.000Z",
        "voteCount": 2,
        "content": "believe B is correct as provided below"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/databricks/view/126236-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Spill occurs as a result of executing various wide transformations. However, diagnosing spill requires one to proactively look for key indicators.<br><br>Where in the Spark UI are two of the primary indicators that a partition is spilling to disk?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery\u2019s detail screen and Job\u2019s detail screen",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStage\u2019s detail screen and Executor\u2019s log files\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDriver\u2019s and Executor\u2019s log files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecutor\u2019s detail screen and Executor\u2019s log files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStage\u2019s detail screen and Query\u2019s detail screen"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-15T04:45:00.000Z",
        "voteCount": 7,
        "content": "B is correct"
      },
      {
        "date": "2024-10-07T00:17:00.000Z",
        "voteCount": 1,
        "content": "b"
      },
      {
        "date": "2024-02-05T17:51:00.000Z",
        "voteCount": 4,
        "content": "In the Spark UI, the Stage\u2019s detail screen provides key metrics about each stage of a job, including the amount of data that has been spilled to disk. If you see a high number in the \u201cSpill (Memory)\u201d or \u201cSpill (Disk)\u201d columns, it\u2019s an indication that a partition is spilling to disk.\n\nThe Executor\u2019s log files can also provide valuable information about spill. If a task is spilling a lot of data, you\u2019ll see messages in the logs like \u201cSpilling UnsafeExternalSorter to disk\u201d or \u201cTask memory spill\u201d. These messages indicate that the task ran out of memory and had to spill data to disk."
      },
      {
        "date": "2023-12-19T21:02:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-12-19T21:03:00.000Z",
        "voteCount": 2,
        "content": "My bad, looking again, B is correct."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/databricks/view/128995-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A task orchestrator has been configured to run two hourly tasks. First, an outside system writes Parquet data to a directory mounted at /mnt/raw_orders/. After this data is written, a Databricks job containing the following code is executed:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image45.png\"><br><br>Assume that the fields customer_id and order_id serve as a composite key to uniquely identify each order, and that the time field indicates when the record was queued in the source system.<br><br>If the upstream system is known to occasionally enqueue duplicate entries for a single order hours apart, which statement is correct?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDuplicate records enqueued more than 2 hours apart may be retained and the orders table may contain duplicate records with the same customer_id and order_id.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll records will be held in the state store for 2 hours before being deduplicated and committed to the orders table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe orders table will contain only the most recent 2 hours of records and no duplicates will be present.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDuplicate records arriving more than 2 hours apart will be dropped, but duplicates that arrive in the same batch may both be written to the orders table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe orders table will not contain duplicates, but records arriving more than 2 hours late will be ignored and missing from the table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-19T00:28:00.000Z",
        "voteCount": 7,
        "content": "Only A seems logical"
      },
      {
        "date": "2024-10-19T01:22:00.000Z",
        "voteCount": 1,
        "content": "Every Stream micro-batch is executed on all of the new data that arrived after the last run 2 hours ago by the .trigger(once=True) option. Deduplication is done for it based on the combined key fields values but all records older than 2 hours based on the 'time' field will be ignored thanks to the .withWatermark() option/function. So target table will have deduplicated data withOUT the late records arriving more than 2 hours later based on the 2 hours watermark buffer set for the readStream."
      },
      {
        "date": "2024-10-12T13:16:00.000Z",
        "voteCount": 2,
        "content": "Data arrive outside of watermark will be dropped."
      },
      {
        "date": "2024-08-13T01:42:00.000Z",
        "voteCount": 2,
        "content": "Watermark(\"time\", \"2 hours\") --&gt;  does'nt let records arriving more than 2 hours late to be written\ndropDuplicates --&gt; removes duplicate records from the records that are read"
      },
      {
        "date": "2024-06-08T05:53:00.000Z",
        "voteCount": 3,
        "content": "It's A, rows are deduplicated only in 2hrs window, therefore final table may eventually contain duplicates"
      },
      {
        "date": "2024-06-05T20:37:00.000Z",
        "voteCount": 1,
        "content": "Should be E.\n\nWatermarking (withWatermark(\"time\", \"2 hours\")): This sets a 2-hour watermark on the time column. The watermark specifies the event time threshold for data completeness, meaning that data older than 2 hours will be considered late and may be dropped.\nDeduplication (dropDuplicates([\"customer_id\", \"order_id\"])): This operation removes duplicates based on the composite key (customer_id and order_id). However, it only works within the window defined by the watermark."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/databricks/view/126259-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer is migrating a workload from a relational database system to the Databricks Lakehouse. The source system uses a star schema, leveraging foreign key constraints and multi-table inserts to validate records on write.<br><br>Which consideration will impact the decisions made by the engineer while migrating this workload?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks only allows foreign key constraints on hashed identifiers, which avoid collisions in highly-parallel writes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks supports Spark SQL and JDBC; all logic can be directly migrated from the source system without refactoring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCommitting to multiple tables simultaneously requires taking out multiple table locks and can lead to a state of deadlock.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll Delta Lake transactions are ACID compliant against a single table, and Databricks does not enforce foreign key constraints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeign keys must reference a primary key field; multi-table inserts must leverage Delta Lake\u2019s upsert functionality."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T17:47:00.000Z",
        "voteCount": 4,
        "content": "In Databricks Delta Lake, transactions are ACID compliant at the table level, meaning that transactions apply to a single table. However, Delta Lake does not enforce foreign key constraints across tables. Therefore, the data engineer needs to be aware that Databricks does not automatically enforce referential integrity between tables through foreign key constraints, and it becomes the responsibility of the data engineer to manage these relationships appropriately."
      },
      {
        "date": "2023-12-19T00:32:00.000Z",
        "voteCount": 2,
        "content": "Primary and foreign keys are informational only and are not enforced."
      },
      {
        "date": "2023-11-15T05:41:00.000Z",
        "voteCount": 1,
        "content": "D makes more sense. \nSince there are no database-level transactions, locks, or guarantees, and since primary key &amp; foreign key constraints are informational only, there is no guarantee of enforced relations (the start schema) in place will remain in place after migration. This means B cannot be right."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/databricks/view/126260-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data architect has heard about Delta Lake\u2019s built-in versioning and time travel capabilities. For auditing purposes, they have a requirement to maintain a full record of all valid street addresses as they appear in the customers table.<br><br>The architect is interested in implementing a Type 1 table, overwriting existing records with new values and relying on Delta Lake time travel to support long-term auditing. A data engineer on the project feels that a Type 2 table will provide better performance and scalability.<br><br>Which piece of information is critical to this decision?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData corruption can occur if a query fails in a partially completed state because Type 2 tables require setting multiple fields in a single update.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShallow clones can be combined with Type 1 tables to accelerate historic queries for long-term versioning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake time travel cannot be used to query previous versions of these tables because Type 1 changes modify data files in place.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake time travel does not scale well in cost or latency to provide a long-term versioning solution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake only supports Type 0 tables; once records are inserted to a Delta Lake table, they cannot be modified."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-09T10:01:00.000Z",
        "voteCount": 1,
        "content": "its C time travel cant be performed on type 1"
      },
      {
        "date": "2024-02-05T17:46:00.000Z",
        "voteCount": 3,
        "content": "Delta Lake\u2019s time travel feature allows you to access previous versions of the data, which can be useful for auditing purposes. However, if you\u2019re planning to use time travel as a long-term versioning solution, it\u2019s important to know that it may not scale well in terms of cost or latency. This is because every time you perform a write operation, a new version of the data is created, which can consume significant storage over time. Additionally, querying older versions of the data may require scanning through many files, which can increase query latency."
      },
      {
        "date": "2023-11-15T05:44:00.000Z",
        "voteCount": 4,
        "content": "D makes more sense"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/databricks/view/128986-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A table named user_ltv is being used to create a view that will be used by data analysts on various teams. Users in the workspace are configured into groups, which are used for setting up data access using ACLs.<br><br>The user_ltv table has the following schema:<br><br>email STRING, age INT, ltv INT<br><br>The following view definition is executed:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image46.png\"><br><br>An analyst who is not a member of the auditing group executes the following query:<br><br>SELECT * FROM user_ltv_no_minors<br><br>Which statement describes the results returned by this query?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll columns will be displayed normally for those records that have an age greater than 17; records not meeting this condition will be omitted.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll age values less than 18 will be returned as null values, all other columns will be returned with the values in user_ltv.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll values for the age column will be returned as null values, all other columns will be returned with the values in user_ltv.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll records from all columns will be displayed with the values in user_ltv.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll columns will be displayed normally for those records that have an age greater than 18; records not meeting this condition will be omitted."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-03T12:21:00.000Z",
        "voteCount": 7,
        "content": "Definitely A.  It's greater than or equal to"
      },
      {
        "date": "2024-07-10T13:25:00.000Z",
        "voteCount": 1,
        "content": "Because greater than 18 doesnt include 18\n\n All columns will be displayed normally for those records that have an age greater than 18; records not meeting this condition will be omitted."
      },
      {
        "date": "2024-03-06T16:34:00.000Z",
        "voteCount": 1,
        "content": "Option E"
      },
      {
        "date": "2024-05-28T21:57:00.000Z",
        "voteCount": 2,
        "content": "Incorrect because the condition specified is age &gt;= 18, not age &gt; 18. So, the answer is A."
      },
      {
        "date": "2024-01-01T16:54:00.000Z",
        "voteCount": 4,
        "content": "A is correct. \"greater than 17\" is the equivalent to \"equal or greater than 18\""
      },
      {
        "date": "2023-12-18T23:22:00.000Z",
        "voteCount": 1,
        "content": "18 not 17."
      },
      {
        "date": "2023-12-18T23:23:00.000Z",
        "voteCount": 3,
        "content": "A is right."
      },
      {
        "date": "2024-02-05T17:44:00.000Z",
        "voteCount": 3,
        "content": "&gt;= 18 (greather than 17)"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/databricks/view/126268-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data governance team is reviewing code used for deleting records for compliance with GDPR. The following logic has been implemented to propagate delete requests from the user_lookup table to the user_aggregates table.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image47.png\"><br><br>Assuming that user_id is a unique identifying key and that all users that have requested deletion have been removed from the user_lookup table, which statement describes whether successfully executing the above logic guarantees that the records to be deleted from the user_aggregates table are no longer accessible and why?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; the Delta Lake DELETE command only provides ACID guarantees when combined with the MERGE INTO command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes; the change data feed uses foreign keys to ensure delete consistency throughout the Lakehouse.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes; Delta Lake ACID guarantees provide assurance that the DELETE command succeeded fully and permanently purged these records.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; the change data feed only tracks inserts and updates, not deleted records."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-26T02:25:00.000Z",
        "voteCount": 2,
        "content": "B records will be available in time travel until VACUUM will be executed"
      },
      {
        "date": "2023-12-19T00:45:00.000Z",
        "voteCount": 2,
        "content": "Delta travel"
      },
      {
        "date": "2023-11-15T06:24:00.000Z",
        "voteCount": 2,
        "content": "B is best. \nVACUUM command is needed to completely remove logs of the deleted files."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/databricks/view/127269-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineering team has been tasked with configuring connections to an external database that does not have a supported native connector with Databricks. The external database already has data security configured by group membership. These groups map directly to user groups already created in Databricks that represent various teams within the company.<br><br>A new login credential has been created for each group in the external database. The Databricks Utilities Secrets module will be used to make these credentials available to Databricks users.<br><br>Assuming that all the credentials are configured correctly on the external database and group membership is properly configured on Databricks, which statement describes how teams can be granted the minimum necessary access to using these credentials?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"Manage\" permissions should be set on a secret key mapped to those credentials that will be used by a given team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"Read\" permissions should be set on a secret key mapped to those credentials that will be used by a given team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"Read\" permissions should be set on a secret scope containing only those credentials that will be used by a given team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"Manage\" permissions should be set on a secret scope containing only those credentials that will be used by a given team.<br>No additional configuration is necessary as long as all users are configured as administrators in the workspace where secrets have been added."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T17:38:00.000Z",
        "voteCount": 3,
        "content": "In Databricks, secret scopes are used to manage and organize secrets. By setting \"Read\" permissions on a secret scope containing the credentials, you allow the team to access the necessary credentials without granting unnecessary privileges. This approach ensures that the teams have the minimum necessary access to the credentials required for connecting to the external database. \"Manage\" permissions would provide more access than needed for just using the credentials.\n\nOption A and D suggest setting permissions on individual secret keys, which might work, but using a secret scope for organizational purposes is a cleaner and more scalable solution."
      },
      {
        "date": "2024-01-31T05:06:00.000Z",
        "voteCount": 2,
        "content": "Access is at scope level and not key level"
      },
      {
        "date": "2023-12-02T02:39:00.000Z",
        "voteCount": 1,
        "content": "In summary, while technically feasible, setting \"Read\" permissions on a secret key might not be the most efficient or scalable solution when dealing with multiple teams and their corresponding credentials. Using secret scopes provides a more organized and maintainable approach for managing secrets in Databricks."
      },
      {
        "date": "2023-11-26T11:03:00.000Z",
        "voteCount": 3,
        "content": "Answer C is correct:\nhttps://docs.databricks.com/en/security/auth-authz/access-control/secret-acl.html#secret-access-control\n\"Access control for secrets is managed at the secret scope level\""
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/databricks/view/132982-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which indicators would you look for in the Spark UI\u2019s Storage tab to signal that a cached table is not performing optimally? Assume you are using Spark\u2019s MEMORY_ONLY storage level.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSize on Disk is &lt; Size in Memory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe RDD Block Name includes the \u201c*\u201d annotation signaling a failure to cache",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSize on Disk is &gt; 0\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe number of Cached Partitions &gt; the number of Spark Partitions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn Heap Memory Usage is within 75% of Off Heap Memory Usage"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T17:34:00.000Z",
        "voteCount": 5,
        "content": "C. Size on Disk is &gt; 0\n\nWhen using Spark's MEMORY_ONLY storage level, the ideal scenario is that the data is fully cached in memory, and the Size on Disk should be 0 (indicating that the data is not spilled to disk). If the Size on Disk is greater than 0, it suggests that some data has been spilled to disk, which can lead to degraded performance as reading from disk is slower than reading from memory."
      },
      {
        "date": "2024-06-08T09:28:00.000Z",
        "voteCount": 2,
        "content": "In this case any data on disk means that cache is not performing optimally"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/databricks/view/126297-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "What is the first line of a Databricks Python notebook when viewed in a text editor?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%python",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t// Databricks notebook source",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t# Databricks notebook source\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t-- Databricks notebook source",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t# MAGIC %python"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T01:20:00.000Z",
        "voteCount": 7,
        "content": "Python: # Databricks notebook source\nSQL: -- Databricks notebook source\nScala: // Databricks notebook source\nR: # Databricks notebook source"
      },
      {
        "date": "2024-06-25T11:41:00.000Z",
        "voteCount": 2,
        "content": "c is the answer"
      },
      {
        "date": "2024-01-01T19:56:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/notebooks/notebook-export-import.html#import-a-file-and-convert-it-to-a-notebook"
      },
      {
        "date": "2023-11-21T18:18:00.000Z",
        "voteCount": 2,
        "content": "This is the correct line that you would find at the top of a Databricks notebook when viewed in a text editor, especially for Python notebooks. The # symbol is used for comments in Python, and the comment # Databricks notebook source is used by Databricks to indicate the start of the notebook's source code in the plain text file.\n\nThese lines are comments in the respective languages (Scala uses // and SQL uses -- for single-line comments) and indicate the beginning of the Databricks notebook content in the text file."
      },
      {
        "date": "2023-11-19T20:12:00.000Z",
        "voteCount": 1,
        "content": "The Answer is C, Just downloaded a notebook from Databricks and viewed it in a text editor."
      },
      {
        "date": "2023-11-15T09:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-11-15T09:19:00.000Z",
        "voteCount": 1,
        "content": "// Databricks notebook source  - Scala\n # Databricks notebook source - Python\n -- Databricks notebook source - SQL\nAnswer is C"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/databricks/view/131948-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement describes a key benefit of an end-to-end test?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMakes it easier to automate your test suite",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPinpoints errors in the building blocks of your application",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvides testing coverage for all code paths and branches",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClosely simulates real world usage of your application\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsures code is optimized for a real-life workflow"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T17:31:00.000Z",
        "voteCount": 2,
        "content": "End-to-end testing is a methodology used to test whether the flow of an application is performing as designed from start to finish. The purpose of carrying out end-to-end tests is to identify system dependencies and to ensure that the right information is passed between various system components and systems. The entire application is tested in a real-world scenario such as communicating with the database, network, hardware, and other applications. Therefore, it closely simulates real-world usage of the application. Other options are benefits of different types of testing, not specifically end-to-end testing."
      },
      {
        "date": "2024-01-25T08:35:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2024-01-23T22:31:00.000Z",
        "voteCount": 1,
        "content": "End-to-end tests use an example scenario, do not necessarily follow complex real world. Unit tests are component wise, end-to-end tests go over components."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/databricks/view/131955-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The Databricks CLI is used to trigger a run of an existing job by passing the job_id parameter. The response that the job run request has been submitted successfully includes a field run_id.<br><br>Which statement describes what the number alongside this field represents?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe job_id and number of times the job has been run are concatenated and returned.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total number of jobs that have been run in the workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe number of times the job definition has been run in this workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe job_id is returned in this field.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe globally unique ID of the newly triggered run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T17:28:00.000Z",
        "voteCount": 3,
        "content": "The number alongside the \"run_id\" field represents the globally unique identifier assigned to the newly triggered run of the job. Each run of a job in Databricks is assigned a unique run_id, allowing you to track and reference that specific execution of the job."
      },
      {
        "date": "2024-01-23T22:36:00.000Z",
        "voteCount": 1,
        "content": "Verified from Databricks UI"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/databricks/view/126298-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data science team has created and logged a production model using MLflow. The model accepts a list of column names and returns a new column of type DOUBLE.<br><br>The following code correctly imports the production model, loads the customers table containing the customer_id key column into a DataFrame, and defines the feature columns needed for the model.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image48.png\"><br><br>Which code block will output a DataFrame with the schema \"customer_id LONG, predictions DOUBLE\"?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.map(lambda x:model(x[columns])).select(\"customer_id, predictions\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.select(\"customer_id\", model(*columns).alias(\"predictions\"))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmodel.predict(df, columns)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.select(\"customer_id\", pandas_udf(model, columns).alias(\"predictions\"))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.apply(model, columns).select(\"customer_id, predictions\")"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-01T20:02:00.000Z",
        "voteCount": 6,
        "content": "B is correct. It's a spark udf not pandas"
      },
      {
        "date": "2023-11-21T18:23:00.000Z",
        "voteCount": 5,
        "content": "This code block applies the Spark UDF created from the MLflow model to the DataFrame df by selecting the existing customer_id column and the new column produced by the model, which is aliased to predictions. The model(*columns) part is where the UDF is applied to the columns specified in the columns list, and alias(\"predictions\") is used to name the output column of the model's predictions. This will result in a DataFrame with the desired schema: \"customer_id LONG, predictions DOUBLE\"."
      },
      {
        "date": "2023-11-15T09:30:00.000Z",
        "voteCount": 2,
        "content": "I think it is B"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/databricks/view/130120-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A nightly batch job is configured to ingest all data files from a cloud object storage container where records are stored in a nested directory structure YYYY/MM/DD. The data for each date represents all records that were processed by the source system on that date, noting that some records may be delayed as they await moderator approval. Each entry represents a user review of a product and has the following schema:<br><br>user_id STRING, review_id BIGINT, product_id BIGINT, review_timestamp TIMESTAMP, review_text STRING<br><br>The ingestion job is configured to append all data for the previous date to a target table reviews_raw with an identical schema to the source system. The next step in the pipeline is a batch write to propagate all new records inserted into reviews_raw to a table where data is fully deduplicated, validated, and enriched.<br><br>Which solution minimizes the compute costs to propagate this batch of data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a batch read on the reviews_raw table and perform an insert-only merge using the natural composite key user_id, review_id, product_id, review_timestamp.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Structured Streaming read against the reviews_raw table using the trigger once execution mode to process new records as a batch job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Delta Lake version history to get the difference between the latest version of reviews_raw and one version prior, then write these records to the next table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter all records in the reviews_raw table based on the review_timestamp; batch append those records produced in the last 48 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReprocess all records in reviews_raw and overwrite the next table in the pipeline."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-14T00:22:00.000Z",
        "voteCount": 5,
        "content": "Deduplication , so insert-only merge"
      },
      {
        "date": "2024-01-11T01:36:00.000Z",
        "voteCount": 5,
        "content": "Should we consider deduplicate? For Time travel, I don't think it can be used to duplicate the target table."
      },
      {
        "date": "2024-10-13T05:42:00.000Z",
        "voteCount": 1,
        "content": "\"The next step in the pipeline is a batch write to propagate all new records inserted into reviews_raw to a table where data is fully deduplicated, validated, and enriched.\" The deduplication will be performed in the following step. Answer B should fit better with cost minimization"
      },
      {
        "date": "2024-10-12T14:08:00.000Z",
        "voteCount": 1,
        "content": "Batch read load full table, but guarantee no duplication with merge. Trigger Once only load new data, you have to run merge to guarantee no duplication in the whole target file. But B does not indicate that."
      },
      {
        "date": "2024-09-23T17:11:00.000Z",
        "voteCount": 1,
        "content": "A is Correct"
      },
      {
        "date": "2024-09-20T08:22:00.000Z",
        "voteCount": 1,
        "content": "B is correct, trigger once is the option in structured streaming for batch style job, but much more efficient."
      },
      {
        "date": "2024-09-20T08:21:00.000Z",
        "voteCount": 1,
        "content": "B is correct, trigger once is the option in structured streaming for batch style job, but much more efficient."
      },
      {
        "date": "2024-01-26T17:02:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-26T07:58:00.000Z",
        "voteCount": 2,
        "content": "B should be correct when looking at cost minimalization, a batch read would scan the whole reviews_raw table, this is unnecessary as historical data is not changed. If a review is delyaed to be approved by the moderator still it is inserted as a new record. Capturing the new data is sufficient."
      },
      {
        "date": "2024-01-01T20:09:00.000Z",
        "voteCount": 4,
        "content": "B should be correct.\nhttps://www.databricks.com/blog/2017/05/22/running-streaming-jobs-day-10x-cost-savings.html"
      },
      {
        "date": "2024-01-02T23:42:00.000Z",
        "voteCount": 2,
        "content": "It is a batch process."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/databricks/view/128996-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement describes Delta Lake optimized writes?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBefore a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData is queued in a messaging bus instead of committing data directly to memory; all data is committed from the messaging bus in one batch once the job is complete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOptimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA shuffle occurs prior to writing to try to group similar data together resulting in fewer files instead of each executor writing multiple files based on directory partitions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T17:20:00.000Z",
        "voteCount": 1,
        "content": "Optimized writes improve file size as data is written and benefit subsequent reads on the table.\n\nOptimized writes are most effective for partitioned tables, as they reduce the number of small files written to each partition. Writing fewer large files is more efficient than writing many small files, but you might still see an increase in write latency because data is shuffled before being written.\n\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/tune-file-size#--optimized-writes-for-delta-lake-on-azure-databricks"
      },
      {
        "date": "2024-01-07T08:06:00.000Z",
        "voteCount": 3,
        "content": "https://docs.databricks.com/en/delta/tune-file-size.html#optimized-writes"
      },
      {
        "date": "2023-12-19T01:10:00.000Z",
        "voteCount": 3,
        "content": "Optimized writes are most effective for partitioned tables, as they reduce the number of small files written to each partition. Writing fewer large files is more efficient than writing many small files, but you might still see an increase in write latency because data is shuffled before being writte"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/databricks/view/132138-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement describes the default execution mode for Databricks Auto Loader?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud vendor-specific queue storage and notification services are configured to track newly arriving files; the target table is materialized by directly querying all valid files in the source directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew files are identified by listing the input directory; the target table is materialized by directly querying all valid files in the source directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWebhooks trigger a Databricks job to run anytime new data arrives in a source directory; new data are automatically merged into target tables using rules inferred from the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew files are identified by listing the input directory; new files are incrementally and idempotently loaded into the target Delta Lake table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud vendor-specific queue storage and notification services are configured to track newly arriving files; new files are incrementally and idempotently loaded into the target Delta Lake table."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T17:10:00.000Z",
        "voteCount": 6,
        "content": "\"Auto Loader uses directory listing mode by default. In directory listing mode, Auto Loader identifies new files by listing the input directory.\"\n\nhttps://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/directory-listing-mode"
      },
      {
        "date": "2024-01-28T15:48:00.000Z",
        "voteCount": 2,
        "content": "D definitely !\nAuto Loader is an optimized file source that overcomes all the above limitations and provides a seamless way for data teams to load the raw data at low cost and latency with minimal DevOps effort. You just need to provide a source directory path and start a streaming job. The new structured streaming source, called \"cloudFiles\", will automatically set up file notification services that subscribe file events from the input directory and process new files as they arrive, with the option of also processing existing files in that directory."
      },
      {
        "date": "2024-09-03T19:13:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D. However,  listing the input directory is the default way of identifying new files for auto loader. Cloud Native Notification services can be used but this is not default setting for auto loader."
      },
      {
        "date": "2024-01-27T03:50:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/ingestion/auto-loader/options.html#:~:text=By%20default%2C%20Auto%20Loader%20makes,as%20true%20or%20false%20respectively.\n\nSelected answer: D"
      },
      {
        "date": "2024-01-26T14:02:00.000Z",
        "voteCount": 1,
        "content": "D is the answer. The default execution mode for Databricks Auto Loader is the Directory Listing mode"
      },
      {
        "date": "2024-01-25T09:15:00.000Z",
        "voteCount": 1,
        "content": "E is the answer"
      },
      {
        "date": "2024-01-26T17:09:00.000Z",
        "voteCount": 2,
        "content": "https://www.databricks.com/blog/2020/02/24/introducing-databricks-ingest-easy-data-ingestion-into-delta-lake.html"
      },
      {
        "date": "2024-06-08T09:25:00.000Z",
        "voteCount": 1,
        "content": "Surely it's not vendor specific solution"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/databricks/view/132980-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Delta Lake table representing metadata about content posts from users has the following schema:<br><br>user_id LONG, post_text STRING, post_id STRING, longitude FLOAT, latitude FLOAT, post_time TIMESTAMP, date DATE<br><br>Based on the above schema, which column is a good candidate for partitioning the Delta Table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpost_time",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlatitude",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpost_id",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tuser_id",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdate\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-05T17:03:00.000Z",
        "voteCount": 4,
        "content": "Partitioning a Delta Lake table on the date column is a common practice. This is because partitioning by date can significantly improve query performance when dealing with time-series data. It allows for efficient filtering of data based on time periods, which is a common requirement in many analytics workloads. Partitioning by date also helps manage the size of your partitions, as each partition will contain only the data for a specific date. This can lead to more efficient reads and writes, and can also make it easier to manage and maintain your data."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/databricks/view/126796-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A large company seeks to implement a near real-time solution involving hundreds of pipelines with parallel updates of many tables with extremely high volume and high velocity data.<br><br>Which of the following solutions would you implement to achieve this requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Databricks High Concurrency clusters, which leverage optimized cloud storage connections to maximize data throughput.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition ingestion tables by a small time duration to allow for many data files to be written in parallel.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Databricks to save all data to attached SSD volumes instead of object storage, increasing file I/O significantly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIsolate Delta Lake tables in their own storage containers to avoid API limits imposed by cloud vendors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore all tables in a single database to ensure that the Databricks Catalyst Metastore can load balance overall throughput."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T10:40:00.000Z",
        "voteCount": 2,
        "content": "\"hundreds of pipelines with parallel updates of many tables\" indicates updating many tables concurrently via many pipelines. A is the best solution for that. B is the answer for updating a few large tables with few partitions."
      },
      {
        "date": "2024-08-17T03:22:00.000Z",
        "voteCount": 1,
        "content": "\"Which of the following solutions\"\nI'm sure this is a question with multichoice.  A and B options are correct together."
      },
      {
        "date": "2024-06-05T01:59:00.000Z",
        "voteCount": 1,
        "content": "High volume and high-velocity data ingestion often becomes a bottleneck due to limited write parallelism. By partitioning ingestion tables based on small time durations (e.g., hourly or even minutes), you create many smaller partitions. This allows concurrent writes to different partitions, significantly increasing the overall throughput of your data ingestion."
      },
      {
        "date": "2024-05-23T09:38:00.000Z",
        "voteCount": 1,
        "content": "Since multiple pipelines are being used high concurrency cluster would give maximum resource utilization."
      },
      {
        "date": "2024-04-10T21:17:00.000Z",
        "voteCount": 1,
        "content": "A.\nB is only useful to improve performance of large tables ingestions."
      },
      {
        "date": "2024-02-28T16:59:00.000Z",
        "voteCount": 2,
        "content": "Why not D?"
      },
      {
        "date": "2024-02-05T17:00:00.000Z",
        "voteCount": 1,
        "content": "Both options A and B could be relevant depending on the specific details of the use case. If the emphasis is on optimizing concurrent queries and overall data throughput, option A might be more appropriate. If the primary concern is parallel updates of tables with high-volume, high-velocity data, option B is a more targeted approach."
      },
      {
        "date": "2024-02-02T02:42:00.000Z",
        "voteCount": 1,
        "content": "The best way to deal with high volume and high velocity data is to use partitioning"
      },
      {
        "date": "2024-01-11T01:57:00.000Z",
        "voteCount": 2,
        "content": "Databricks High Concurrency cluster"
      },
      {
        "date": "2023-12-02T05:09:00.000Z",
        "voteCount": 1,
        "content": "1) Partitioning by Time:\nPartitioning tables by a small time duration allows for efficient parallelism in data writes. Each time partition can be processed independently, enabling parallel updates to multiple partitions concurrently.\n2)Optimizing for Parallelism:\nBy partitioning the tables based on time, data can be ingested and processed in parallel, providing the ability to handle high volume and high velocity data effectively.\n\nRegarding option A, Databricks High Concurrency clusters are more focused on supporting a large number of concurrent users, which might not directly address the requirement for parallel updates of many tables with extremely high volume and high velocity data"
      },
      {
        "date": "2024-06-08T05:59:00.000Z",
        "voteCount": 1,
        "content": "Usage of high conc. clusters can be beneficial both for mulitple users and jobs/queries running on them"
      },
      {
        "date": "2024-06-14T12:12:00.000Z",
        "voteCount": 1,
        "content": "Sorry, after going through this question once more - I'll go with B also. It will allow utilize parallelism in an efficient way."
      },
      {
        "date": "2023-12-02T05:10:00.000Z",
        "voteCount": 1,
        "content": "sorry, the selected answer should have been B"
      },
      {
        "date": "2023-11-21T18:38:00.000Z",
        "voteCount": 4,
        "content": "High Concurrency clusters in Databricks are designed for multiple concurrent users and workloads. They provide fine-grained sharing of cluster resources and are optimized for operations such as running multiple parallel queries and updates. This would be suitable for a solution that involves many pipelines with parallel updates, especially with high volume and high velocity data."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/databricks/view/126313-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which describes a method of installing a Python package scoped at the notebook level to all nodes in the currently active cluster?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun source env/bin/activate in a notebook setup script",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse b in a notebook cell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse %pip install in a notebook cell\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse %sh pip install in a notebook cell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall libraries from PyPI using the cluster UI"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-09T13:17:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-11-21T18:39:00.000Z",
        "voteCount": 3,
        "content": "In Databricks notebooks, you can use the %pip install command in a notebook cell to install a Python package. This will install the package on all nodes in the currently active cluster at the notebook level. It is a feature provided by Databricks to facilitate the installation of Python libraries for the notebook environment specifically."
      },
      {
        "date": "2023-11-15T13:12:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/databricks/view/135995-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Each configuration below is identical to the extent that each cluster has 400 GB total of RAM 160 total cores and only one Executor per VM.<br><br>Given an extremely long-running job for which completion must be guaranteed, which cluster configuration will be able to guarantee completion of the job in light of one or more VM failures?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Total VMs: 8<br>\u2022 50 GB per Executor<br>\u2022 20 Cores / Executor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Total VMs: 16<br>\u2022 25 GB per Executor<br>\u2022 10 Cores / Executor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Total VMs: 1<br>\u2022 400 GB per Executor<br>\u2022 160 Cores/Executor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Total VMs: 4<br>\u2022 100 GB per Executor<br>\u2022 40 Cores / Executor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\u2022 Total VMs: 2<br>\u2022 200 GB per Executor<br>\u2022 80 Cores / Executor"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T11:03:00.000Z",
        "voteCount": 1,
        "content": "16 core provides more redundancy, fault tolerance and more parallelism. But if dataset is huge, 8VM maybe better. The question is missing some information."
      },
      {
        "date": "2024-07-08T08:02:00.000Z",
        "voteCount": 1,
        "content": "This setup ensures that the job can continue running and complete even if some VMs fail, as there are more VMs available to handle the workload"
      },
      {
        "date": "2024-05-24T07:24:00.000Z",
        "voteCount": 3,
        "content": "If VM is down, performance is degraded, so opting for vm's which has distributed memory per executor and optimal cores per executor."
      },
      {
        "date": "2024-03-13T23:01:00.000Z",
        "voteCount": 3,
        "content": "in my exam today, i chose B, 16VM, because the \"extremely long-run\"."
      },
      {
        "date": "2024-04-22T02:26:00.000Z",
        "voteCount": 1,
        "content": "do you have link to databricks doc?"
      },
      {
        "date": "2024-08-15T01:33:00.000Z",
        "voteCount": 1,
        "content": "I have no link for Databricks doc. It's just a logic. The more VMs we have, the more robust our pipeline is."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/databricks/view/141545-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Delta Lake table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team. The table contains information about customers derived from a number of upstream sources. Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.<br><br>Immediately after each update succeeds, the data engineering team would like to determine the difference between the new version and the previous version of the table.<br><br>Given the current implementation, which method can be used?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute a query to calculate the difference between the new version and the previous version using Delta Lake\u2019s built-in versioning and lime travel functionality.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParse the Delta Lake transaction log to identify all newly written data files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParse the Spark event logs to identify those rows that were updated, inserted, or deleted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute DESCRIBE HISTORY customer_churn_params to obtain the full operation metrics for the update, including a log of all records that have been added or modified.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Delta Lake\u2019s change data feed to identify those records that have been updated, inserted, or deleted."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-13T06:06:00.000Z",
        "voteCount": 1,
        "content": "Change data feed allows to check for changes between versions"
      },
      {
        "date": "2024-08-22T04:02:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. The easy way to get the difference between those tables is by travel time by version"
      },
      {
        "date": "2024-07-27T03:51:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. There is no clue that CDF is enabled for the table"
      },
      {
        "date": "2024-07-08T08:56:00.000Z",
        "voteCount": 3,
        "content": "Answer A"
      },
      {
        "date": "2024-06-05T06:50:00.000Z",
        "voteCount": 2,
        "content": "Answer : E"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/databricks/view/141546-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data team\u2019s Structured Streaming job is configured to calculate running aggregates for item sales to update a downstream marketing dashboard. The marketing team has introduced a new promotion, and they would like to add a new field to track the number of times this promotion code is used for each item. A junior data engineer suggests updating the existing query as follows. Note that proposed changes are in bold.<br><br>Original query:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image49.png\"><br><br>Proposed query:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image50.png\"><br><br>Which step must also be completed to put the proposed query into production?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify a new checkpointLocation\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove .option('mergeSchema', 'true') from the streaming write",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the shuffle partitions to account for additional aggregates",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun REFRESH TABLE delta.\u201b/item_agg\u201b"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-22T04:05:00.000Z",
        "voteCount": 1,
        "content": "A answer"
      },
      {
        "date": "2024-06-05T06:54:00.000Z",
        "voteCount": 3,
        "content": "Answer: A\nWhen updating the schema of a streaming job, specifying a new checkpoint location ensures that the streaming query starts fresh with the new schema. This avoids issues that might arise from schema mismatches between the previous state and the new schema. This is especially relevant when adding new fields because the existing state might not be compatible with the new schema."
      },
      {
        "date": "2024-05-29T09:55:00.000Z",
        "voteCount": 1,
        "content": "This checkpoint location preserves all of the essential information that identifies a query. Each query must have a different checkpoint location. Multiple queries should never have the same location. For more information, see the Structured Streaming Programming Guide. https://docs.databricks.com/en/structured-streaming/query-recovery.html"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/databricks/view/141547-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "When using CLI or REST API to get results from jobs with multiple tasks, which statement correctly describes the response structure?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach run of a job will have a unique job_id; all tasks within this job will have a unique job_id",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach run of a job will have a unique job_id; all tasks within this job will have a unique task_id",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach run of a job will have a unique orchestration_id; all tasks within this job will have a unique run_id",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach run of a job will have a unique run_id; all tasks within this job will have a unique task_id\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach run of a job will have a unique run_id; all tasks within this job will also have a unique run_id"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-16T00:41:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is E"
      },
      {
        "date": "2024-10-13T06:20:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is E. https://docs.databricks.com/api/workspace/jobs/getrun\nYou can visit the link and observe the response sample of the API\n{\n  \"job_id\": 11223344,\n  \"run_id\": 455644833,\n\njob_id is the unique id of the job. run_id is the unique id of the run\n\nThen each task will have its unique run id: \n\n\"tasks\": [\n    {\n      ...\n      \"run_id\": 2112892,\n      ..."
      },
      {
        "date": "2024-09-28T13:35:00.000Z",
        "voteCount": 2,
        "content": "tested answer is E : \n'tasks': [{'run_id': *****,\n   'task_key': '######', ...."
      },
      {
        "date": "2024-05-29T09:57:00.000Z",
        "voteCount": 2,
        "content": "Seems right"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/databricks/view/141548-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineering team is configuring environments for development, testing, and production before beginning migration on a new data pipeline. The team requires extensive testing on both the code and data resulting from code execution, and the team wants to develop and test against data as similar to production data as possible.<br><br>A junior data engineer suggests that production data can be mounted to the development and testing environments, allowing pre-production code to execute against production data. Because all users have admin privileges in the development environment, the junior data engineer has offered to configure permissions and mount this data for the team.<br><br>Which statement captures best practices for this situation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll development, testing, and production code and data should exist in a single, unified workspace; creating separate environments for testing and development complicates administrative overhead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn environments where interactive code will be executed, production data should only be accessible with read permissions; creating isolated databases for each environment further reduces risks.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAs long as code in the development environment declares USE dev_db at the top of each notebook, there is no possibility of inadvertently committing changes back to production data sources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause Delta Lake versions all data and supports time travel, it is not possible for user error or malicious actors to permanently delete production data; as such, it is generally safe to mount production data anywhere.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause access to production data will always be verified using passthrough credentials, it is safe to mount data to any Databricks development environment."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-29T09:58:00.000Z",
        "voteCount": 5,
        "content": "Seems right"
      },
      {
        "date": "2024-10-14T20:32:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. In environments where interactive code will be executed, production data should only be accessible with read permissions; creating isolated databases for each environment further reduces risks.\n\nExplanation:\nBest practices for managing production, development, and testing environments involve minimizing the risk of unintended data modifications or deletions, especially when dealing with production data. The ideal setup includes:\n\nLimiting permissions: Production data should only be accessible with read permissions in development or testing environments to prevent accidental changes.\nIsolating environments: Creating separate databases for development, testing, and production environments ensures that there are clear boundaries and that development code cannot unintentionally affect production data."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/databricks/view/141951-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data engineer, User A, has promoted a pipeline to production by using the REST API to programmatically create several jobs. A DevOps engineer, User B, has configured an external orchestration tool to trigger job runs through the REST API. Both users authorized the REST API calls using their personal access tokens.<br><br>A workspace admin, User C, inherits responsibility for managing this pipeline. User C uses the Databricks Jobs UI to take \"Owner\" privileges of each job. Jobs continue to be triggered using the credentials and tooling configured by User B.<br><br>An application has been configured to collect and parse run information returned by the REST API. Which statement describes the value returned in the creator_user_name field?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnce User C takes \"Owner\" privileges, their email address will appear in this field; prior to this, User A\u2019s email address will appear in this field.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUser B\u2019s email address will always appear in this field, as their credentials are always used to trigger the run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUser A\u2019s email address will always appear in this field, as they still own the underlying notebooks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnce User C takes \"Owner\" privileges, their email address will appear in this field; prior to this, User B\u2019s email address will appear in this field.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUser C will only ever appear in this field if they manually trigger the job, otherwise it will indicate User B."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-24T12:07:00.000Z",
        "voteCount": 1,
        "content": "C based on previous comment"
      },
      {
        "date": "2024-09-20T12:33:00.000Z",
        "voteCount": 1,
        "content": "Job creator can't be changed. Owner can be changed. So creator_user_name field always return who created the job: User A. But UserA no longer owns the job. So Answer C is partially correct."
      },
      {
        "date": "2024-09-20T12:32:00.000Z",
        "voteCount": 1,
        "content": "Job creator can't be changed. Owner can be changed. So creator_user_name field always return owner: User A. But UserA no longer owns the job. So Answer C is partially correct."
      },
      {
        "date": "2024-08-18T07:28:00.000Z",
        "voteCount": 3,
        "content": "When you create a job your role is IS OWNER and RUN AS. So when you trigger a job, it will run as the RUN AS entity. And it should be user A if someone dosen't have changed it"
      },
      {
        "date": "2024-08-11T06:41:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2024-07-08T21:48:00.000Z",
        "voteCount": 1,
        "content": "Should be the DevOps email address"
      },
      {
        "date": "2024-07-08T08:30:00.000Z",
        "voteCount": 1,
        "content": "the creator_user_name field reflects the user who triggered the job run"
      },
      {
        "date": "2024-06-16T10:08:00.000Z",
        "voteCount": 2,
        "content": "Answer: E"
      },
      {
        "date": "2024-06-05T11:41:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/databricks/view/146882-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A member of the data engineering team has submitted a short notebook that they wish to schedule as part of a larger data pipeline. Assume that the commands provided below produce the logically correct results when run as presented.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image51.png\"><br><br>Which command should be removed from the notebook before scheduling it as a job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 3",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 4",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 5\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-03T20:29:00.000Z",
        "voteCount": 2,
        "content": "The display function is built specifically for the notebook environment and will not work for Spark. Should you want to print contacts of DF in Spark then replace it with df.show()"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/databricks/view/144252-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement regarding Spark configuration on the Databricks platform is true?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Databricks REST API can be used to modify the Spark configuration properties for an interactive cluster without interrupting jobs currently running on the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark configurations set within a notebook will affect all SparkSessions attached to the same interactive cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the same Spark configuration property is set for an interactive cluster and a notebook attached to that cluster, the notebook setting will always be ignored.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark configuration properties set for an interactive cluster with the Clusters UI will impact all notebooks attached to that cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-09T09:21:00.000Z",
        "voteCount": 2,
        "content": "this looks correct"
      },
      {
        "date": "2024-07-20T00:58:00.000Z",
        "voteCount": 3,
        "content": "Should be D"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/databricks/view/149442-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The business reporting team requires that data for their dashboards be updated every hour. The total processing time for the pipeline that extracts, transforms, and loads the data for their pipeline runs in 10 minutes.<br><br>Assuming normal operating conditions, which configuration will meet their service-level agreement requirements with the lowest cost?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a job that executes every time new data lands in a given directory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a job to execute the pipeline once an hour on a new job cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a Structured Streaming job with a trigger interval of 60 minutes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a job to execute the pipeline once an hour on a dedicated interactive cluster"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-14T20:39:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. Schedule a job to execute the pipeline once an hour on a new job cluster.\n\nExplanation:\nIn this scenario, the business reporting team needs the data to be updated every hour, and the processing time for the pipeline takes 10 minutes. To meet this requirement with the lowest cost, the best option is to schedule the job to run once per hour using a new job cluster.\n\nA job cluster is created specifically for the duration of the job, and once the job finishes, the cluster is terminated. This is cost-efficient because resources are only consumed while the job is running, and the cluster does not stay active when it is not needed."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/databricks/view/141583-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Databricks SQL dashboard has been configured to monitor the total number of records present in a collection of Delta Lake tables using the following query pattern:<br><br><br>SELECT COUNT (*) FROM table -<br><br>Which of the following describes how results are generated each time the dashboard is updated?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total count of rows is calculated by scanning all data files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total count of rows will be returned from cached results unless REFRESH is run",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total count of records is calculated from the Delta transaction logs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe total count of records is calculated from the parquet file metadata"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-20T01:15:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-06-13T08:05:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-06-05T07:21:00.000Z",
        "voteCount": 1,
        "content": "Delta Lake optimizes COUNT(*) queries by reading the row counts stored in the Delta transaction log. This eliminates the need for a full table scan, resulting in significantly faster query performance.\n\npen_spark"
      },
      {
        "date": "2024-06-01T11:21:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C"
      },
      {
        "date": "2024-05-30T01:13:00.000Z",
        "voteCount": 2,
        "content": "I would've said C"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/databricks/view/141584-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Delta Lake table was created with the below query:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image52.png\"><br><br>Consider the following query:<br><br><br>DROP TABLE prod.sales_by_store -<br><br>If this statement is executed by a workspace admin, which result will occur?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData will be marked as deleted but still recoverable with Time Travel.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table will be removed from the catalog but the data will remain in storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table will be removed from the catalog and the data will be deleted.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn error will occur because Delta Lake prevents the deletion of production data."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-22T04:38:00.000Z",
        "voteCount": 1,
        "content": "no location keyword, so its a managed table"
      },
      {
        "date": "2024-06-14T12:08:00.000Z",
        "voteCount": 3,
        "content": "It's managed table, so data will be also removed"
      },
      {
        "date": "2024-06-13T08:05:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2024-06-11T18:03:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-06-04T18:08:00.000Z",
        "voteCount": 1,
        "content": "C because its a managed table"
      },
      {
        "date": "2024-06-01T11:24:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C\nNo location provided in the table. So, it is a managed table. This will result in deleting the table meta data as well as table data."
      },
      {
        "date": "2024-05-30T01:14:00.000Z",
        "voteCount": 1,
        "content": "Seems C, it's a managed table"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/databricks/view/142864-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A developer has successfully configured their credentials for Databricks Repos and cloned a remote Git repository. They do not have privileges to make changes to the main branch, which is the only branch currently visible in their workspace.<br><br>Which approach allows this user to share their code updates without the risk of overwriting the work of their teammates?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to create a new branch, commit all changes, and push changes to the remote Git repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to create a fork of the remote repository, commit all changes, and make a pull request on the source repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to pull changes from the remote Git repository; commit and push changes to a branch that appeared as changes were pulled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Repos to merge all differences and make a pull request back to the remote repository."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-24T16:53:00.000Z",
        "voteCount": 1,
        "content": "A seems correct"
      },
      {
        "date": "2024-06-24T05:03:00.000Z",
        "voteCount": 3,
        "content": "answer B"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/databricks/view/141900-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The security team is exploring whether or not the Databricks secrets module can be leveraged for connecting to an external database.<br><br>After testing the code with all Python variables being defined with strings, they upload the password to the secrets module and configure the correct permissions for the currently active user. They then modify their code to the following (leaving all other variables unchanged).<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image53.png\"><br><br>Which statement describes what will happen when the above code is executed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe connection to the external table will succeed; the string \"REDACTED\" will be printed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the encoded password will be saved to DBFS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn interactive input box will appear in the notebook; if the right password is provided, the connection will succeed and the password will be printed in plain text.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe connection to the external table will succeed; the string value of password will be printed in plain text."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-26T04:01:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2024-07-20T01:22:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-06-05T13:05:00.000Z",
        "voteCount": 1,
        "content": "Answer A : When using Databricks secrets, the actual value of the secret is typically protected from being displayed in plain text. Databricks automatically redacts secret values when they are printed in the notebook. So, when you use the print(password) statement, the output will not show the actual password but will instead show [REDACTED]."
      },
      {
        "date": "2024-06-04T18:09:00.000Z",
        "voteCount": 1,
        "content": "A. A. The connection to the external table will succeed; the string \"REDACTED\" will be printed."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/databricks/view/141740-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data science team has created and logged a production model using MLflow. The model accepts a list of column names and returns a new column of type DOUBLE.<br><br>The following code correctly imports the production model, loads the customers table containing the customer_id key column into a DataFrame, and defines the feature columns needed for the model.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image54.png\"><br><br>Which code block will output a DataFrame with the schema \"customer_id LONG, predictions DOUBLE\"?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.map(lambda x:model(x[columns])).select(\"customer_id, predictions\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.select(\"customer_id\",<br>model(*columns).alias(\"predictions\"))\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmodel.predict(df, columns)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdf.apply(model, columns).select(\"customer_id, predictions\")"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-26T04:02:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2024-07-20T01:23:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-06-01T11:20:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer: B\nThis option uses select to specify columns from the DataFrame and applies the model to the specified columns (columns). The output of the model is aliased as \"predictions\", which ensures the output DataFrame will have the column names \"customer_id\" and \"predictions\" with appropriate data types assuming the model returns a double type. This syntax aligns with PySpark's DataFrame transformations and is a typical way to apply a machine learning model to specific columns in Databricks."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/databricks/view/142366-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior member of the data engineering team is exploring the language interoperability of Databricks notebooks. The intended outcome of the below code is to register a view of all sales that occurred in countries on the continent of Africa that appear in the geo_lookup table.<br><br>Before executing the code, running SHOW TABLES on the current database indicates the database contains only two tables: geo_lookup and sales.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image55.png\"><br><br>What will be the outcome of executing these command cells m order m an interactive notebook?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBoth commands will succeed. Executing SHOW TABLES will show that countries_af and sales_af have been registered as views.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 1 will succeed. Cmd 2 will search all accessible databases for a table or view named countries_af: if this entity exists, Cmd 2 will succeed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable representing a PySpark DataFrame.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCmd 1 will succeed and Cmd 2 will fail. countries_af will be a Python variable containing a list of strings.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-26T04:04:00.000Z",
        "voteCount": 2,
        "content": "D is correct. It will not be DataFrame since collect() will change it to list of strings"
      },
      {
        "date": "2024-07-20T01:24:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-06-11T18:06:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/databricks/view/142967-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineering team has configured a job to process customer requests to be forgotten (have their data deleted). All user data that needs to be deleted is stored in Delta Lake tables using default table settings.<br><br>The team has decided to process all deletions from the previous week as a batch job at 1am each Sunday. The total duration of this job is less than one hour. Every Monday at 3am, a batch job executes a series of VACUUM commands on all Delta Lake tables throughout the organization.<br><br>The compliance officer has recently learned about Delta Lake's time travel functionality. They are concerned that this might allow continued access to deleted data.<br><br>Assuming all delete logic is correctly implemented, which statement correctly addresses this concern?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the VACUUM command permanently deletes all files containing deleted records, deleted records may be accessible with time travel for around 24 hours.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the default data retention threshold is 24 hours, data files containing deleted records will be retained until the VACUUM job is run the following day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the default data retention threshold is 7 days, data files containing deleted records will be retained until the VACUUM job is run 8 days later.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause Delta Lake's delete statements have ACID guarantees, deleted records will be permanently purged from all storage systems as soon as a delete job completes."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-13T08:46:00.000Z",
        "voteCount": 1,
        "content": "Is C since by default Vacuum retains files no more referenced in the current table version for 7 days. https://docs.databricks.com/en/delta/history.html#configure-data-retention-for-time-travel-queries"
      },
      {
        "date": "2024-07-26T04:06:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2024-07-21T05:12:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2024-06-26T23:23:00.000Z",
        "voteCount": 1,
        "content": "Since the team is expecting last week's data to be deleted on Sunday at 1am to 2am. The data will be available for approx 24hrs until the vacuum command is run on Monday at 3am."
      },
      {
        "date": "2024-10-13T08:44:00.000Z",
        "voteCount": 1,
        "content": "No! By default Vacuum does not remove rows deleted whithin the last 7 days. To do it you should modify the property delta.deletedFileRetentionDuration\nhttps://docs.databricks.com/en/delta/history.html#configure-data-retention-for-time-travel-queries"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/databricks/view/144256-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Assuming that the Databricks CLI has been installed and configured correctly, which Databricks CLI command can be used to upload a custom Python Wheel to object storage mounted with the DBFS for use with a production job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tconfigure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tworkspace",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlibraries"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-20T01:31:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/databricks/view/141552-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The following table consists of items found in user carts within an e-commerce website.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image56.png\"><br><br>The following MERGE statement is used to update this table using an updates view, with schema evolution enabled on this table.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image57.png\"><br><br>How would the following update be handled?<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image58.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe update throws an error because changes to existing columns in the target schema are not supported.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe new nested Field is added to the target schema, and dynamically read as NULL for existing unmatched records.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe update is moved to a separate \"rescued\" column because it is missing a column expected in the target schema.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe new nested field is added to the target schema, and files underlying existing records are updated to include NULL values for the new field."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-03T20:21:00.000Z",
        "voteCount": 1,
        "content": "because B says for unmatched records. but my new records has 1001 which is existing record. we should give ans what will happen with this new record. it will update the record in target with NULL as coupon"
      },
      {
        "date": "2024-09-03T20:19:00.000Z",
        "voteCount": 1,
        "content": "It has to be D"
      },
      {
        "date": "2024-06-01T11:37:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer: B. \nIn the question it is mentioned that the schema evolution is enabled. This option states that the new nested field is added to the target schema, and existing records not matching the new schema format are populated with NULL for the newly added columns. This behavior aligns with how schema evolution functions in Delta Lake, dynamically adapting the schema to include new fields."
      },
      {
        "date": "2024-05-29T10:55:00.000Z",
        "voteCount": 4,
        "content": "schema evolution is enabled, so B."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/databricks/view/141743-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "An upstream system is emitting change data capture (CDC) logs that are being written to a cloud object storage directory. Each record in the log indicates the change type (insert, update, or delete) and the values for each field after the change. The source table has a primary key identified by the field pk_id.<br><br>For auditing purposes, the data governance team wishes to maintain a full record of all values that have ever been valid in the source system. For analytical purposes, only the most recent value for each record needs to be recorded. The Databricks job to ingest these records occurs once per hour, but each individual record may have changed multiple times over the course of an hour.<br><br>Which solution meets these requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIterate through an ordered set of changes to the table, applying each in turn to create the current state of the table, (insert, update, delete), timestamp of change, and the values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse merge into to insert, update, or delete the most recent entry for each pk_id into a table, then propagate all changes throughout the system.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeduplicate records in each batch by pk_id and overwrite the target table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Delta Lake\u2019s change data feed to automatically process CDC data from an external system, propagating all changes to all dependent tables in the Lakehouse.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-27T05:43:00.000Z",
        "voteCount": 1,
        "content": "The MERGE INTO statement in Delta Lake is a powerful feature designed to handle Change Data Capture (CDC) data efficiently. This approach meets both the auditing and analytical requirements.\n\nCDF is not enabled by default. So these data is not generated by it to handel them."
      },
      {
        "date": "2024-08-15T04:25:00.000Z",
        "voteCount": 1,
        "content": "I'd agree, but there is  \"a full record of all values that have ever been valid in the source system\". After deleting records we can still use time-travel options... But after vacuuming audit team will be dissapinted"
      },
      {
        "date": "2024-06-24T05:28:00.000Z",
        "voteCount": 2,
        "content": "agree with D"
      },
      {
        "date": "2024-06-05T08:00:00.000Z",
        "voteCount": 2,
        "content": "Delta Lake provides built-in change data feed functionality.\nIt captures changes (inserts, updates, deletes) and propagates them to dependent tables.\nBy using Delta Lake, you can maintain historical records and propagate changes efficiently."
      },
      {
        "date": "2024-06-01T11:44:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer: D \nDelta Lake\u2019s change data feed feature is specifically designed to handle CDC scenarios. It processes data from external systems, tracking all changes (inserts, updates, deletes) and maintaining a detailed history of these changes. This feature allows for keeping a comprehensive log while also ensuring the most recent state is correctly reflected in analytical tables."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/databricks/view/141744-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "An hourly batch job is configured to ingest data files from a cloud object storage container where each batch represent all records produced by the source system in a given hour. The batch job to process these records into the Lakehouse is sufficiently delayed to ensure no late-arriving data is missed. The user_id field represents a unique key for the data, which has the following schema:<br><br>user_id BIGINT, username STRING, user_utc STRING, user_region STRING, last_login BIGINT, auto_pay BOOLEAN, last_updated BIGINT<br><br>New records are all ingested into a table named account_history which maintains a full record of all data in the same schema as the source. The next table in the system is named account_current and is implemented as a Type 1 table representing the most recent value for each unique user_id.<br><br>Which implementation can be used to efficiently update the described account_current table as part of each hourly batch job assuming there are millions of user accounts and tens of thousands of records processed hourly?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter records in account_history using the last_updated field and the most recent hour processed, making sure to deduplicate on username; write a merge statement to update or insert the most recent value for each username.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Auto Loader to subscribe to new files in the account_history directory; configure a Structured Streaming trigger available job to batch update newly detected files into the account_current table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOverwrite the account_current table with each batch using the results of a query against the account_history table grouping by user_id and filtering for the max value of last_updated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter records in account_history using the last_updated field and the most recent hour processed, as well as the max last_login by user_id write a merge statement to update or insert the most recent value for each user_id.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-24T17:09:00.000Z",
        "voteCount": 1,
        "content": "D seems like the best option"
      },
      {
        "date": "2024-09-23T09:50:00.000Z",
        "voteCount": 1,
        "content": "A, D both wrong. They only take data from the latest update. It is too narrow. Same user_id can have several updates within an hour to update different fields. So use auto loader to apply all the updates within an hour is the only correct answer."
      },
      {
        "date": "2024-08-18T11:09:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. You're meeting all the requirements with less overhead. It's only updating on the most recent record, so duplicates are handled.\n\nAnswer D is too much overhead. They're doing a full table scan for all records, which as the question stated, is millions of records."
      },
      {
        "date": "2024-09-02T11:19:00.000Z",
        "voteCount": 1,
        "content": "User Id would be a better column to merge into with, username might not be distinct"
      },
      {
        "date": "2024-06-01T13:07:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer: D\nSimilar to Option A, but specifically designed around the user_id, which is the primary key. This approach ensures that the account_current is always up-to-date with the latest information per user based on the primary key, reducing the risk of duplicate information and ensuring the integrity of the data with respect to the unique identifier."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/databricks/view/141553-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The business intelligence team has a dashboard configured to track various summary metrics for retail stores. This includes total sales for the previous day alongside totals and averages for a variety of time periods. The fields required to populate this dashboard have the following schema:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image59.png\"><br><br>For demand forecasting, the Lakehouse contains a validated table of all itemized sales updated incrementally in near real-time. This table, named products_per_order, includes the following fields:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image60.png\"><br><br>Because reporting on long-term sales trends is less volatile, analysts using the new dashboard only require data to be refreshed once daily. Because the dashboard will be queried interactively by many users throughout a normal business day, it should return results quickly and reduce total compute associated with each materialization.<br><br>Which solution meets the expectations of the end users while controlling and limiting possible costs?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPopulate the dashboard by configuring a nightly batch job to save the required values as a table overwritten with each update.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Structured Streaming to configure a live dashboard against the products_per_order table within a Databricks notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine a view against the products_per_order table and define the dashboard against this view.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Delta Cache to persist the products_per_order table in memory to quickly update the dashboard with each query."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-13T20:27:00.000Z",
        "voteCount": 1,
        "content": "E\n#69 in professional one"
      },
      {
        "date": "2024-10-13T20:28:00.000Z",
        "voteCount": 1,
        "content": "C Define a viwe"
      },
      {
        "date": "2024-05-29T11:15:00.000Z",
        "voteCount": 2,
        "content": "Seems correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/databricks/view/141585-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A view is registered with the following code:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image61.png\"><br><br>Both users and orders are Delta Lake tables.<br><br>Which statement describes the results of querying recent_orders?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic will execute when the view is defined and store the result of joining tables to the DBFS; this stored data will be returned when the view is queried.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResults will be computed and cached when the view is defined; these cached results will incrementally update as new records are inserted into source tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query finishes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic will execute at query time and return the result of joining the valid versions of the source tables at the time the query began.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-08T10:04:00.000Z",
        "voteCount": 1,
        "content": "Also voting for D, such view results are recalculated each time when called"
      },
      {
        "date": "2024-06-01T14:25:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: D\nCorrect because this option correctly describes the behavior of SQL views in Databricks. The view's query is executed against the current state of the data in the source tables at the moment the query begins. This means that any changes to the data that are committed while the query is running will not be reflected in the results of the query currently executing."
      },
      {
        "date": "2024-05-30T01:30:00.000Z",
        "voteCount": 1,
        "content": "It should be B"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/databricks/view/141745-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data ingestion task requires a one-TB JSON dataset to be written out to Parquet with a target part-file size of 512 MB. Because Parquet is being used instead of Delta Lake, built-in file-sizing features such as Auto-Optimize &amp; Auto-Compaction cannot be used.<br><br>Which strategy will yield the best performance without shuffling data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet spark.sql.files.maxPartitionBytes to 512 MB, ingest the data, execute the narrow transformations, and then write to parquet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet spark.sql.shuffle.partitions to 2,048 partitions (1TB*1024*1024/512), ingest the data, execute the narrow transformations, optimize the data by sorting it (which automatically repartitions the data), and then write to parquet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet spark.sql.adaptive.advisoryPartitionSizeInBytes to 512 MB bytes, ingest the data, execute the narrow transformations, coalesce to 2,048 partitions (1TB*1024*1024/512), and then write to parquet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest the data, execute the narrow transformations, repartition to 2,048 partitions (1TB* 1024*1024/512), and then write to parquet.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-14T21:04:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D. Ingest the data, execute the narrow transformations, repartition to 2,048 partitions (1TB * 1024 * 1024 / 512), and then write to Parquet.\n\nExplanation:\nIn this case, the goal is to write a 1 TB dataset to Parquet with a target file size of 512 MB without incurring the overhead of data shuffling. To achieve optimal performance, we must balance the number of partitions to match the file size requirements while avoiding expensive shuffle operations.\n\nNarrow transformations: These transformations (such as map, filter) don\u2019t require shuffling the data, which keeps the operation efficient.\nRepartition to 2,048 partitions: Given that the desired part-file size is 512 MB and the total dataset size is 1 TB, repartitioning the dataset into 2,048 partitions ensures that each partition will be approximately 512 MB in size, which matches the target file size. This avoids shuffle operations and allows for an efficient write."
      },
      {
        "date": "2024-09-25T06:56:00.000Z",
        "voteCount": 2,
        "content": "Not A because spark.sql.files.maxPartitionBytes primarily affects the reading of data, not the writing. It determines the maximum size of a partition when reading files, not when writing them."
      },
      {
        "date": "2024-09-23T12:05:00.000Z",
        "voteCount": 1,
        "content": "A, D will not prevent shuffling data. C using coalesce to reduce shuffling data."
      },
      {
        "date": "2024-06-26T18:49:00.000Z",
        "voteCount": 3,
        "content": "best performance without shuffling data"
      },
      {
        "date": "2024-06-12T09:46:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2024-06-01T14:32:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer D: Repartition to 2,048 partitions and write to Parquet\n\nThis option directly controls the number of output files by repartitioning the data into 2,048 partitions, assuming that 1TB/512MB per file roughly translates to 2,048 files. Repartitioning the data involves shuffling, but it's a deliberate shuffle designed to achieve a specific partitioning beneficial for writing. After repartitioning, the data is written to Parquet files, each expected to be approximately 512 MB if the data is uniformly distributed across partitions."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/databricks/view/141554-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement regarding stream-static joins and static Delta tables is correct?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe checkpoint directory will be used to track updates to the static Delta table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach microbatch of a stream-static join will use the most recent version of the static Delta table as of the job's initialization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe checkpoint directory will be used to track state information for the unique keys present in the join.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream-static joins cannot use static Delta tables because of consistency issues."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-29T11:24:00.000Z",
        "voteCount": 2,
        "content": "When Databricks processes a micro-batch of data in a stream-static join, the latest valid version of data from the static Delta table joins with the records present in the current micro-batch. Because the join is stateless, you do not need to configure watermarking and can process results with low latency. The data in the static Delta table used in the join should be slowly-changing.\nhttps://docs.databricks.com/en/transform/join.html#stream-static"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/databricks/view/142392-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Structured Streaming job deployed to production has been resulting in higher than expected cloud storage costs. At present, during normal execution, each microbatch of data is processed in less than 3s; at least 12 times per minute, a microbatch is processed that contains 0 records. The streaming write was configured using the default trigger settings. The production job is currently scheduled alongside many other Databricks jobs in a workspace with instance pools provisioned to reduce start-up time for jobs with batch execution.<br><br>Holding all other variables constant and assuming records need to be processed in less than 10 minutes, which adjustment will meet the requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the trigger interval to 3 seconds; the default trigger interval is consuming too many records per batch, resulting in spill to disk that can increase volume costs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the trigger once option and configure a Databricks job to execute the query every 10 minutes; this approach minimizes costs for both compute and storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the trigger interval to 10 minutes; each batch calls APIs in the source storage account, so decreasing trigger frequency to maximum allowable threshold should minimize this cost.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the trigger interval to 500 milliseconds; setting a small but non-zero trigger interval ensures that the source is not queried too frequently."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-14T12:01:00.000Z",
        "voteCount": 2,
        "content": "C, \nA - incorrect explanation\nB - trigger once is not correct option here\nD - 500 miliseconds is already used, it's default trigger interval"
      },
      {
        "date": "2024-06-12T09:50:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/databricks/view/144257-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement describes Delta Lake optimized writes?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBefore a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA shuffle occurs prior to writing to try to group similar data together resulting in fewer files instead of each executor writing multiple files based on directory partitions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOptimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-07T02:48:00.000Z",
        "voteCount": 1,
        "content": "B is correct. \nC is wrong OPTIMIZE is a separate process from write. \n"
      },
      {
        "date": "2024-07-20T02:45:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2024-08-04T19:01:00.000Z",
        "voteCount": 1,
        "content": "Please provide your input to Questions 144,145,146,147,149 also. Thanks in advance"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/databricks/view/144258-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which statement characterizes the general programming model used by Spark Structured Streaming?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming leverages the parallel processing of GPUs to achieve highly parallel data throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming is implemented as a messaging bus and is derived from Apache Kafka.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming relies on a distributed network of nodes that hold incremental state values for cached stages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-20T02:46:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/databricks/view/144259-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Which configuration parameter directly affects the size of a spark-partition upon ingestion of data into Spark?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.files.maxPartitionBytes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.autoBroadcastJoinThreshold",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.advisoryPartitionSizeInBytes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql.adaptive.coalescePartitions.minPartitionNum"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-20T02:46:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/databricks/view/144260-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Spark job is taking longer than expected. Using the Spark UI, a data engineer notes that the Min, Median, and Max Durations for tasks in a particular stage show the minimum and median time to complete a task as roughly the same, but the max duration for a task to be roughly 100 times as long as the minimum.<br><br>Which situation is causing increased duration of the overall job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTask queueing resulting from improper thread pool assignment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpill resulting from attached volume storage being too small.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNetwork latency due to some cluster nodes being in different regions from the source data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSkew caused by more data being assigned to a subset of spark-partitions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-20T02:47:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/databricks/view/146974-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A task orchestrator has been configured to run two hourly tasks. First, an outside system writes Parquet data to a directory mounted at /mnt/raw_orders/. After this data is written, a Databricks job containing the following code is executed:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image63.png\"><br><br>Assume that the fields customer_id and order_id serve as a composite key to uniquely identify each order, and that the time field indicates when the record was queued in the source system.<br><br>If the upstream system is known to occasionally enqueue duplicate entries for a single order hours apart, which statement is correct?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDuplicate records enqueued more than 2 hours apart may be retained and the orders table may contain duplicate records with the same customer_id and order_id.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll records will be held in the state store for 2 hours before being deduplicated and committed to the orders table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe orders table will contain only the most recent 2 hours of records and no duplicates will be present.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe orders table will not contain duplicates, but records arriving more than 2 hours late will be ignored and missing from the table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-04T19:58:00.000Z",
        "voteCount": 1,
        "content": "The default write mode is append. Duplicate will be resolved for only 2 hr window but may still exist because of previous execution."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/databricks/view/147917-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer seeks to leverage Delta Lake's Change Data Feed functionality to create a Type 1 table representing all of the values that have ever been valid for all rows in a bronze table created with the property delta.enableChangeDataFeed = true. They plan to execute the following code as a daily job:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image64.png\"><br><br>Which statement describes the execution and results of running the above query multiple times?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach time the job is executed, newly updated records will be merged into the target table, overwriting previous values with the same primary keys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach time the job is executed, the entire available history of inserted or updated records will be appended to the target table, resulting in many duplicate entries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach time the job is executed, only those records that have been inserted or updated since the last execution will be appended to the target table, giving the desired result.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach time the job is executed, the differences between the original and current versions are calculated; this may result in duplicate entries for some records."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-09-21T05:53:00.000Z",
        "voteCount": 2,
        "content": "B. This bad effect (many duplicates) happens because the code reads from the starting version 0, appending all changes since the beginning."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/databricks/view/141556-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A DLT pipeline includes the following streaming tables:<br><br>\u2022\traw_iot ingests raw device measurement data from a heart rate tracking device.<br>\u2022\tbpm_stats incrementally computes user statistics based on BPM measurements from raw_iot.<br><br>How can the data engineer configure this pipeline to be able to retain manually deleted or updated records in the raw_iot table, while recomputing the downstream table bpm_stats table when a pipeline update is run?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the pipelines.reset.allowed property to false on raw_iot\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the skipChangeCommits flag to true on raw_iot",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the pipelines.reset.allowed property to false on bpm_stats",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the skipChangeCommits flag to true on bpm_stats"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-26T06:38:00.000Z",
        "voteCount": 2,
        "content": "B. Set the skipChangeCommits flag to true on raw_iot\n\nLet's break down the requirements and explain why this is the best solution:\n\nRetain manually deleted or updated records in raw_iot: The skipChangeCommits flag, when set to true, tells Delta Live Tables (DLT) to ignore any manual changes (updates or deletes) made to the table outside of the pipeline. This means that even if records are manually deleted or updated in the raw_iot table, these changes won't be reflected in the table when the pipeline runs again.\nRecompute downstream bpm_stats table: By default, DLT will recompute downstream tables when their upstream dependencies change. Since bpm_stats is based on raw_iot, it will naturally be recomputed when the pipeline updates, without any special configuration.\nWhy the other options are not correct:\n\nA. Setting pipelines.reset.allowed to false on raw_iot would prevent the table from being reset, but it wouldn't address the requirement to retain manually deleted or updated records."
      },
      {
        "date": "2024-09-26T06:38:00.000Z",
        "voteCount": 1,
        "content": "B. Set the skipChangeCommits flag to true on raw_iot\n\nLet's break down the requirements and explain why this is the best solution:\n\nRetain manually deleted or updated records in raw_iot: The skipChangeCommits flag, when set to true, tells Delta Live Tables (DLT) to ignore any manual changes (updates or deletes) made to the table outside of the pipeline. This means that even if records are manually deleted or updated in the raw_iot table, these changes won't be reflected in the table when the pipeline runs again.\nRecompute downstream bpm_stats table: By default, DLT will recompute downstream tables when their upstream dependencies change. Since bpm_stats is based on raw_iot, it will naturally be recomputed when the pipeline updates, without any special configuration.\nWhy the other options are not correct:\n\nA. Setting pipelines.reset.allowed to false on raw_iot would prevent the table from being reset, but it wouldn't address the requirement to retain manually deleted or updated records."
      },
      {
        "date": "2024-07-09T13:23:00.000Z",
        "voteCount": 3,
        "content": "answer A\nThis property, when set to false, ensures that the table will not be reset during pipeline updates, thus preserving manually deleted or updated records. This is crucial for the raw_iot table to retain the manual modifications."
      },
      {
        "date": "2024-07-09T08:57:00.000Z",
        "voteCount": 3,
        "content": "Set the pipelines.reset.allowed property to false on raw_iot"
      },
      {
        "date": "2024-06-05T17:07:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/databricks/view/141557-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A nightly job ingests data into a Delta Lake table using the following code:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image65.png\"><br><br>The next step in the pipeline requires a function that returns an object that can be used to manipulate new records that have not yet been processed to the next table in the pipeline.<br><br>Which code snippet completes this function definition?<br><br>def new_records():",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\treturn spark.readStream.table(\"bronze\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\treturn spark.read.option(\"readChangeFeed\", \"true\").table (\"bronze\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-professional/image66.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-professional/image67.png\">"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-01T14:53:00.000Z",
        "voteCount": 6,
        "content": "Correct Answer: B\nThe Change Data Feed (CDF) feature in Delta Lake enables reading only the changes (inserts and updates) to a Delta table. This would allow the function to focus on new or modified data since the last trigger, making it ideal for processing only the new records that have not been processed yet. This directly meets the requirement for identifying and manipulating new records efficiently."
      },
      {
        "date": "2024-08-15T21:21:00.000Z",
        "voteCount": 1,
        "content": "We are ingesting data from the folder with a parquet in the bronze table. It doesn't make any sense to use the CDF feature for bronze table )"
      },
      {
        "date": "2024-08-15T21:30:00.000Z",
        "voteCount": 2,
        "content": "I've changed my opinion. Yes, B looks as correct answer"
      },
      {
        "date": "2024-10-14T07:32:00.000Z",
        "voteCount": 1,
        "content": "delta table returns new records in streaming read."
      },
      {
        "date": "2024-09-26T06:37:00.000Z",
        "voteCount": 1,
        "content": "B. Set the skipChangeCommits flag to true on raw_iot\n\nLet's break down the requirements and explain why this is the best solution:\n\nRetain manually deleted or updated records in raw_iot: The skipChangeCommits flag, when set to true, tells Delta Live Tables (DLT) to ignore any manual changes (updates or deletes) made to the table outside of the pipeline. This means that even if records are manually deleted or updated in the raw_iot table, these changes won't be reflected in the table when the pipeline runs again.\nRecompute downstream bpm_stats table: By default, DLT will recompute downstream tables when their upstream dependencies change. Since bpm_stats is based on raw_iot, it will naturally be recomputed when the pipeline updates, without any special configuration.\nWhy the other options are not correct:\n\nA. Setting pipelines.reset.allowed to false on raw_iot would prevent the table from being reset, but it wouldn't address the requirement to retain manually deleted or updated records."
      },
      {
        "date": "2024-09-23T14:15:00.000Z",
        "voteCount": 1,
        "content": "You have to know the CDF's current version and last processed the version in order to get not processed records. B does not provide those versions. It will just return content from the bronze table with CDF turned on. D is only possible solution."
      },
      {
        "date": "2024-07-27T07:25:00.000Z",
        "voteCount": 2,
        "content": "I did not test it. But i think D is wrong as it filtering agenst directory path using =="
      },
      {
        "date": "2024-05-29T11:46:00.000Z",
        "voteCount": 4,
        "content": "Seems D"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/databricks/view/142393-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer is working to implement logic for a Lakehouse table named silver_device_recordings. The source data contains 100 unique fields in a highly nested JSON structure.<br><br>The silver_device_recordings table will be used downstream to power several production monitoring dashboards and a production model. At present, 45 of the 100 fields are being used in at least one of these applications.<br><br>The data engineer is trying to determine the best approach for dealing with schema declaration given the highly-nested structure of the data and the numerous fields.<br><br>Which of the following accurately presents information about Delta Lake and Databricks that may impact their decision-making process?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Tungsten encoding used by Databricks is optimized for storing string data; newly-added native support for querying JSON strings means that string types are always most efficient.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause Delta Lake uses Parquet for data storage, data types can be easily evolved by just modifying file footer information in place.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchema inference and evolution on Databricks ensure that inferred types will always accurately match the data types used by downstream systems.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause Databricks will infer schema using types that allow all observed data to be processed, setting types manually provides greater assurance of data quality enforcement.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-14T11:56:00.000Z",
        "voteCount": 3,
        "content": "Agree with propopsed answer, D"
      },
      {
        "date": "2024-06-12T10:13:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/databricks/view/142394-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineering team maintains the following code:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image68.png\"><br><br>Assuming that this code produces logically correct results and the data in the source tables has been de-duplicated and validated, which statement describes what will occur when this code is executed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA batch job will update the enriched_itemized_orders_by_account table, replacing only those rows that have different values than the current version of the table, using accountID as the primary key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe enriched_itemized_orders_by_account table will be overwritten using the current valid version of data in each of the three tables referenced in the join logic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo computation will occur until enriched_itemized_orders_by_account is queried; upon query materialization, results will be calculated using the current valid version of data in each of the three tables referenced in the join logic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn incremental job will detect if new rows have been written to any of the source tables; if new rows are detected, all results will be recalculated and used to overwrite the enriched_itemized_orders_by_account table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-12T10:15:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/databricks/view/147232-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The marketing team is looking to share data in an aggregate table with the sales organization, but the field names used by the teams do not match, and a number of marketing-specific fields have not been approved for the sales org.<br><br>Which of the following solutions addresses the situation while emphasizing simplicity?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a view on the marketing table selecting only those fields approved for the sales team; alias the names of any fields that should be standardized to the sales naming conventions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new table with the required schema and use Delta Lake's DEEP CLONE functionality to sync up changes committed to one table to the corresponding table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a CTAS statement to create a derivative table from the marketing table; configure a production job to propagate changes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a parallel table write to the current production pipeline, updating a new sales table that varies as required from the marketing table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-09T14:25:00.000Z",
        "voteCount": 2,
        "content": "Creating a view is the simplest and most effective solution"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/databricks/view/144262-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Delta Lake table representing metadata about content posts from users has the following schema:<br><br>user_id LONG, post_text STRING, post_id STRING, longitude FLOAT, latitude FLOAT, post_time TIMESTAMP, date DATE<br><br>This table is partitioned by the date column. A query is run with the following filter:<br><br>longitude &lt; 20 &amp; longitude &gt; -20<br><br>Which statement describes how data will be filtered?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStatistics in the Delta Log will be used to identify partitions that might Include files in the filtered range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo file skipping will occur because the optimizer does not know the relationship between the partition column and the longitude.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Delta Engine will scan the parquet file footers to identify each row that meets the filter criteria.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStatistics in the Delta Log will be used to identify data files that might include records in the filtered range.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-20T03:39:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/databricks/view/142396-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A small company based in the United States has recently contracted a consulting firm in India to implement several new data engineering pipelines to power artificial intelligence applications. All the company's data is stored in regional cloud storage in the United States.<br><br>The workspace administrator at the company is uncertain about where the Databricks workspace used by the contractors should be deployed.<br><br>Assuming that all data governance considerations are accounted for, which statement accurately informs this decision?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks runs HDFS on cloud volume storage; as such, cloud virtual machines must be deployed in the region where the data is stored.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks workspaces do not rely on any regional infrastructure; as such, the decision should be made based upon what is most convenient for the workspace administrator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCross-region reads and writes can incur significant costs and latency; whenever possible, compute should be deployed in the same region the data is stored.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks notebooks send all executable code from the user\u2019s browser to virtual machines over the open internet; whenever possible, choosing a workspace region near the end users is the most secure."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-12T10:31:00.000Z",
        "voteCount": 2,
        "content": "option C"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/databricks/view/144263-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A CHECK constraint has been successfully added to the Delta table named activity_details using the following logic:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image69.png\"><br><br>A batch job is attempting to insert new records to the table, including a record where latitude = 45.50 and longitude = 212.67.<br><br>Which statement describes the outcome of this batch insert?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe write will insert all records except those that violate the table constraints; the violating records will be reported in a warning log.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe write will fail completely because of the constraint violation and no records will be inserted into the target table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe write will insert all records except those that violate the table constraints; the violating records will be recorded to a quarantine table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe write will include all records in the target table; any violations will be indicated in the boolean column named valid_coordinates."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-20T03:49:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/databricks/view/142397-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer is migrating a workload from a relational database system to the Databricks Lakehouse. The source system uses a star schema, leveraging foreign key constraints and multi-table inserts to validate records on write.<br><br>Which consideration will impact the decisions made by the engineer while migrating this workload?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks only allows foreign key constraints on hashed identifiers, which avoid collisions in highly-parallel writes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeign keys must reference a primary key field; multi-table inserts must leverage Delta Lake\u2019s upsert functionality.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCommitting to multiple tables simultaneously requires taking out multiple table locks and can lead to a state of deadlock.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll Delta Lake transactions are ACID compliant against a single table, and Databricks does not enforce foreign key constraints.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-12T10:32:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/databricks/view/142398-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data architect has heard about Delta Lake\u2019s built-in versioning and time travel capabilities. For auditing purposes, they have a requirement to maintain a full record of all valid street addresses as they appear in the customers table.<br><br>The architect is interested in implementing a Type 1 table, overwriting existing records with new values and relying on Delta Lake time travel to support long-term auditing. A data engineer on the project feels that a Type 2 table will provide better performance and scalability.<br><br>Which piece of information is critical to this decision?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData corruption can occur if a query fails in a partially completed state because Type 2 tables require setting multiple fields in a single update.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShallow clones can be combined with Type 1 tables to accelerate historic queries for long-term versioning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake time travel cannot be used to query previous versions of these tables because Type 1 changes modify data files in place.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake time travel does not scale well in cost or latency to provide a long-term versioning solution.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-12T10:32:00.000Z",
        "voteCount": 3,
        "content": "correct answer - D"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/databricks/view/141710-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A junior data engineer has manually configured a series of jobs using the Databricks Jobs UI. Upon reviewing their work, the engineer realizes that they are listed as the \"Owner\" for each job. They attempt to transfer \"Owner\" privileges to the \"DevOps\" group, but cannot successfully accomplish this task.<br><br>Which statement explains what is preventing this privilege transfer?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks jobs must have exactly one owner; \"Owner\" privileges cannot be assigned to a group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe creator of a Databricks job will always have \"Owner\" privileges; this configuration cannot be changed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly workspace administrators can grant \"Owner\" privileges to a group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA user can only transfer job ownership to a group if they are also a member of that group."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-27T07:11:00.000Z",
        "voteCount": 2,
        "content": "This is the correct answer for this question in a past Databricks version, however now you can indeed add a group as a owner to a job."
      },
      {
        "date": "2024-05-31T20:03:00.000Z",
        "voteCount": 1,
        "content": "A. Databricks jobs must have exactly one owner; \"Owner\" privileges cannot be assigned to a group.\nIt's only possivel that a databricks JOB has an owner, not a group."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/databricks/view/141558-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A table named user_ltv is being used to create a view that will be used by data analysts on various teams. Users in the workspace are configured into groups, which are used for setting up data access using ACLs.<br><br>The user_ltv table has the following schema:<br><br>email STRING, age INT, ltv INT<br><br>The following view definition is executed:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image71.png\"><br><br>An analyst who is not a member of the auditing group executes the following query:<br><br>SELECT * FROM user_ltv_no_minors<br><br>Which statement describes the results returned by this query?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll columns will be displayed normally for those records that have an age greater than 17; records not meeting this condition will be omitted.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll age values less than 18 will be returned as null values, all other columns will be returned with the values in user_ltv.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll values for the age column will be returned as null values, all other columns will be returned with the values in user_ltv.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll columns will be displayed normally for those records that have an age greater than 18; records not meeting this condition will be omitted."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-14T12:03:00.000Z",
        "voteCount": 3,
        "content": "Surely, it's an A"
      },
      {
        "date": "2024-06-12T10:43:00.000Z",
        "voteCount": 2,
        "content": "option A is correct"
      },
      {
        "date": "2024-06-08T05:26:00.000Z",
        "voteCount": 2,
        "content": "Greater than 17"
      },
      {
        "date": "2024-06-01T15:15:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: A\n(&gt;17) qual to (&gt;=18). So, all records above 17 years will get in result and other records will be omitted."
      },
      {
        "date": "2024-05-31T19:56:00.000Z",
        "voteCount": 1,
        "content": "A. All columns will be displayed normally for those records that have an age greater than 17; records not meeting this condition will be omitted.\n\nBecause the condition of age&gt;=18 only is respected in option A."
      },
      {
        "date": "2024-05-29T11:57:00.000Z",
        "voteCount": 1,
        "content": "Nope, A greater than 18 is 19. D is incorrect."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/databricks/view/141709-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "All records from an Apache Kafka producer are being ingested into a single Delta Lake table with the following schema:<br><br>key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG<br><br>There are 5 unique topics being ingested. Only the \"registration\" topic contains Personal Identifiable Information (PII). The company wishes to restrict access to PII. The company also wishes to only retain records containing PII in this table for 14 days after initial ingestion. However, for non-PII information, it would like to retain these records indefinitely.<br><br>Which solution meets the requirements?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll data should be deleted biweekly; Delta Lake's time travel functionality should be leveraged to maintain a history of non-PII information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData should be partitioned by the registration field, allowing ACLs and delete statements to be set for the PII directory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData should be partitioned by the topic field, allowing ACLs and delete statements to leverage partition boundaries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate object storage containers should be specified based on the partition field, allowing isolation at the storage level."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-12T10:43:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2024-05-31T19:53:00.000Z",
        "voteCount": 1,
        "content": "C.\nPartitioning the data by the topic field allows the company to apply different access control policies and retention policies for different topics. Althought there is a performance optmization gain because of the read in the partition path."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/databricks/view/141708-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data governance team is reviewing code used for deleting records for compliance with GDPR. The following logic has been implemented to propagate delete requests from the user_lookup table to the user_aggregates table.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image72.png\"><br><br>Assuming that user_id is a unique identifying key and that all users that have requested deletion have been removed from the user_lookup table, which statement describes whether successfully executing the above logic guarantees that the records to be deleted from the user_aggregates table are no longer accessible and why?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; the Delta Lake DELETE command only provides ACID guarantees when combined with the MERGE INTO command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo; the change data feed only tracks inserts and updates, not deleted records.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes; Delta Lake ACID guarantees provide assurance that the DELETE command succeeded fully and permanently purged these records."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-31T19:45:00.000Z",
        "voteCount": 3,
        "content": "B. No; files containing deleted records may still be accessible with time travel until a VACUUM command is used to remove invalidated data files."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/databricks/view/141559-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "An external object storage container has been mounted to the location /mnt/finance_eda_bucket.<br><br>The following logic was executed to create a database for the finance team:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image73.png\"><br><br>After the database was successfully created and permissions configured, a member of the finance team runs the following code:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-professional/image74.png\"><br><br>If all users on the finance team are members of the finance group, which statement describes how the tx_sales table will be created?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA logical table will persist the query plan to the Hive Metastore in the Databricks control plane.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn external table will be created in the storage container mounted to /mnt/finance_eda_bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA managed table will be created in the DBFS root storage container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn managed table will be created in the storage container mounted to /mnt/finance_eda_bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-14T12:05:00.000Z",
        "voteCount": 3,
        "content": "It will be created in database location, but it will be managed table (missing LOCATION keyword in CREATE TABLE)."
      },
      {
        "date": "2024-06-12T10:53:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2024-06-01T15:24:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: D\nThe table is still managed by Spark SQL in terms of metadata, but the data files are stored in the specified path due to the database\u2019s location setting.\n\nGiven the inherited location from the database, if the CREATE TABLE statement had explicitly used USING EXTERNAL or specified a LOCATION, it would definitely be an external table. However, since it inherits the database's location without these specifications, it creates a managed table."
      },
      {
        "date": "2024-05-29T12:02:00.000Z",
        "voteCount": 1,
        "content": "Seems D"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/databricks/view/141560-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineering team has been tasked with configuring connections to an external database that does not have a supported native connector with Databricks. The external database already has data security configured by group membership. These groups map directly to user groups already created in Databricks that represent various teams within the company.<br><br>A new login credential has been created for each group in the external database. The Databricks Utilities Secrets module will be used to make these credentials available to Databricks users.<br><br>Assuming that all the credentials are configured correctly on the external database and group membership is properly configured on Databricks, which statement describes how teams can be granted the minimum necessary access to using these credentials?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo additional configuration is necessary as long as all users are configured as administrators in the workspace where secrets have been added.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"Read\" permissions should be set on a secret key mapped to those credentials that will be used by a given team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"Read\" permissions should be set on a secret scope containing only those credentials that will be used by a given team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"Manage\" permissions should be set on a secret scope containing only those credentials that will be used by a given team."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-12T10:54:00.000Z",
        "voteCount": 3,
        "content": "C is correct. Read permission on secret scope should work here."
      },
      {
        "date": "2024-06-01T15:27:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer: C\nThis option is the best practice for managing access to sensitive data. By creating a secret scope dedicated to each team and setting \"Read\" permissions on the scope, you ensure that only the intended team members can access their respective credentials. This method aligns with security best practices by tightly controlling access based on group membership and reducing the risk of unauthorized access."
      },
      {
        "date": "2024-05-29T12:09:00.000Z",
        "voteCount": 2,
        "content": "Seems C"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/databricks/view/141707-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "What is the retention of job run history?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt is retained until you export or delete job run logs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt is retained for 30 days, during which time you can deliver job run logs to DBFS or S3",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt is retained for 60 days, during which you can export notebook run results to HTML\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt is retained for 60 days, after which logs are archived"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-14T14:36:00.000Z",
        "voteCount": 3,
        "content": "For most Databricks workspaces, the job run history is retained for 60 days."
      },
      {
        "date": "2024-06-12T10:55:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/databricks/view/141706-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A data engineer, User A, has promoted a new pipeline to production by using the REST API to programmatically create several jobs. A DevOps engineer, User B, has configured an external orchestration tool to trigger job runs through the REST API. Both users authorized the REST API calls using their personal access tokens.<br><br>Which statement describes the contents of the workspace audit logs concerning these events?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the REST API was used for job creation and triggering runs, a Service Principal will be automatically used to identify these events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause User A created the jobs, their identity will be associated with both the job creation events and the job run events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause these events are managed separately, User A will have their identity associated with the job creation events and User B will have their identity associated with the job run events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause the REST API was used for job creation and triggering runs, user identity will not be captured in the audit logs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-31T19:30:00.000Z",
        "voteCount": 5,
        "content": "C, because the users has their own personal access tokens."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/databricks/view/141705-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A distributed team of data analysts share computing resources on an interactive cluster with autoscaling configured. In order to better manage costs and query throughput, the workspace administrator is hoping to evaluate whether cluster upscaling is caused by many concurrent users or resource-intensive queries.<br><br>In which location can one review the timeline for cluster resizing events?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorkspace audit logs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDriver's log file",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGanglia",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster Event Log"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-31T19:28:00.000Z",
        "voteCount": 3,
        "content": "Its possible to see the metricks of compute with Ganglia, but the question is about a timeline so D, Cluster Event Log seems correct."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/databricks/view/141704-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "When evaluating the Ganglia Metrics for a given cluster with 3 executor nodes, which indicator would signal proper utilization of the VM's resources?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe five Minute Load Average remains consistent/flat",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCPU Utilization is around 75%",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNetwork I/O never spikes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTotal Disk Space remains constant"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-31T19:21:00.000Z",
        "voteCount": 3,
        "content": "B.\nThis level of CPU utilization indicates that the cluster is being used without being underutilized."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/databricks/view/141561-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "The data engineer is using Spark's MEMORY_ONLY storage level.<br><br>Which indicators should the data engineer look for in the Spark UI's Storage tab to signal that a cached table is not performing optimally?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn Heap Memory Usage is within 75% of Off Heap Memory Usage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe RDD Block Name includes the \u201c*\u201d annotation signaling a failure to cache",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSize on Disk is &gt; 0\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe number of Cached Partitions &gt; the number of Spark Partitions"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-29T05:36:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2024-06-27T07:30:00.000Z",
        "voteCount": 1,
        "content": "It's simple, if MEMORY_ONLY is used, anything spilled to disk would indicate a problem."
      },
      {
        "date": "2024-06-27T07:32:00.000Z",
        "voteCount": 1,
        "content": "The RDD answer is incorrect for this question due to the fact that while this indicates a failure to cache, it is more specific to identifying individual blocks that failed to cache rather than providing a general signal of a suboptimal performance for the entire cached table."
      },
      {
        "date": "2024-06-12T10:57:00.000Z",
        "voteCount": 2,
        "content": "C is correct here"
      },
      {
        "date": "2024-06-01T15:33:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: B\nOption B, is the most correct and relevant choice for an indicator that a cached table is not performing optimally in a MEMORY_ONLY scenario. If an RDD block includes a \"?\" annotation, it strongly suggests issues with caching, which would directly impact the performance and expected behavior of MEMORY_ONLY caching. This indication points to a failure to cache the data entirely in memory, which is what MEMORY_ONLY intends to do.\n\nOption C, could also be a relevant indicator in general caching scenarios (e.g., MEMORY_AND_DISK), but it contradicts the MEMORY_ONLY setting directly. Therefore, Option B is chosen based on the specific storage level described."
      },
      {
        "date": "2024-06-10T01:54:00.000Z",
        "voteCount": 2,
        "content": "*THE CORRECT ANSWER IS: C*\nPLEASE IGNORE MY PREVIOUS ANSWER. \n\nLong story short, B is correct in the context of non-functional requirement, but the question is based in functional requirement, and sorry for the confusion."
      },
      {
        "date": "2024-05-31T19:14:00.000Z",
        "voteCount": 1,
        "content": "B.\nThis annotation says that some partitions of the cached data have been spilled to disk because there wasn't enough memory to keep them."
      },
      {
        "date": "2024-05-29T12:16:00.000Z",
        "voteCount": 2,
        "content": "I would say C"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/databricks/view/141702-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "What is a method of installing a Python package scoped at the notebook level to all nodes in the currently active cluster?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun source env/bin/activate in a notebook setup script",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall libraries from PyPI using the cluster UI",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse %pip install in a notebook cell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse %sh pip install in a notebook cell"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-31T15:43:00.000Z",
        "voteCount": 1,
        "content": "Is necessary just run %pip install some_library inside a notebook cell\nC.\nOBS:\nFor the last update of a library can be executed %pip install some_library -U"
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/databricks/view/141701-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "What is the first line of a Databricks Python notebook when viewed in a text editor?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%python",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t// Databricks notebook source",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t# Databricks notebook source\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t-- Databricks notebook source"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-27T18:20:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is:\n\nC. # Databricks notebook source\n\nThis is the comment line that appears at the beginning of a Databricks Python notebook when viewed in a text editor."
      },
      {
        "date": "2024-05-31T15:41:00.000Z",
        "voteCount": 2,
        "content": "C. # Databricks notebook source\nThe commentary in the first like will indicate a magic command for a notebook source."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/databricks/view/141700-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "Incorporating unit tests into a PySpark application requires upfront attention to the design of your jobs, or a potentially significant refactoring of existing code.<br><br>Which benefit offsets this additional effort?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImproves the quality of your data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tValidates a complete use case of your application",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTroubleshooting is easier since all steps are isolated and tested individually",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsures that all steps interact correctly to achieve the desired end result"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-31T15:40:00.000Z",
        "voteCount": 1,
        "content": "C. Troubleshooting is easier since all steps are isolated and tested individually\nThe unit tests will ensuree that specific functions and transformations will work as intended."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/databricks/view/141699-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "What describes integration testing?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt validates an application use case.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt validates behavior of individual elements of an application,",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt requires an automated testing framework.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt validates interactions between subsystems of your application."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-31T15:38:00.000Z",
        "voteCount": 2,
        "content": "D. It validates interactions between subsystems of your application.\nAn integration test is used for different softwares validation components, subsystems, or applications that has dependencies."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/databricks/view/141697-exam-certified-data-engineer-professional-topic-1-question/",
    "body": "A Databricks job has been configured with three tasks, each of which is a Databricks notebook. Task A does not depend on other tasks. Tasks B and C run in parallel, with each having a serial dependency on task A.<br><br>What will be the resulting state if tasks A and B complete successfully but task C fails during a scheduled run?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic expressed in the notebook associated with tasks A and B will have been successfully completed; some operations in task C may have completed successfully.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnless all tasks complete successfully, no changes will be committed to the Lakehouse; because task C failed, all commits will be rolled back automatically.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBecause all tasks are managed as a dependency graph, no changes will be committed to the Lakehouse until all tasks have successfully been completed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll logic expressed in the notebook associated with tasks A and B will have been successfully completed; any changes made in task C will be rolled back due to task failure."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-31T15:34:00.000Z",
        "voteCount": 1,
        "content": "A: A. All logic expressed in the notebook associated with tasks A and B will have been successfully completed; some operations in task C may have completed successfully.\n\nBecause this type of orchestration indicates a Fan-Out."
      }
    ],
    "examNameCode": "certified-data-engineer-professional",
    "topicNumber": "1"
  }
]