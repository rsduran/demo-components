[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/databricks/view/105312-exam-certified-data-engineer-associate-topic-1-question-1/",
    "body": "A data organization leader is upset about the data analysis team\u2019s reports being different from the data engineering team\u2019s reports. The leader believes the siloed nature of their organization\u2019s data engineering and data analysis architectures is to blame.<br>Which of the following describes how a data lakehouse could alleviate this issue?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBoth teams would autoscale their work as data size evolves",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBoth teams would use the same source of truth for their work\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBoth teams would reorganize to report to the same department",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBoth teams would be able to collaborate on projects in real-time",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBoth teams would respond more quickly to ad-hoc requests"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-12T03:31:00.000Z",
        "voteCount": 8,
        "content": "Databricks Lakehouse enables using data as the single source of truth. Duplicating data often results in data silos in organizations. Correct answer B."
      },
      {
        "date": "2024-09-26T14:43:00.000Z",
        "voteCount": 1,
        "content": "CLEAR ANSWER"
      },
      {
        "date": "2024-08-28T10:04:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-06-09T14:37:00.000Z",
        "voteCount": 1,
        "content": "The answer is B!"
      },
      {
        "date": "2024-05-22T05:38:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-04-10T22:30:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer, I got 100%. all questions came from https://www.udemy.com/course/practice-exams-databricks-certified-data-engineer-associate-t/?couponCode=APR2024"
      },
      {
        "date": "2024-03-19T02:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-03-19T02:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-05T02:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-12-24T08:26:00.000Z",
        "voteCount": 1,
        "content": "Correct is B"
      },
      {
        "date": "2023-10-09T04:06:00.000Z",
        "voteCount": 1,
        "content": "Correct is B"
      },
      {
        "date": "2023-10-03T07:46:00.000Z",
        "voteCount": 1,
        "content": "Correct is B"
      },
      {
        "date": "2023-09-25T21:06:00.000Z",
        "voteCount": 1,
        "content": "Correct ans B"
      },
      {
        "date": "2023-09-25T00:37:00.000Z",
        "voteCount": 1,
        "content": "Both teams would use the same source of truth for their work"
      },
      {
        "date": "2023-09-06T01:10:00.000Z",
        "voteCount": 4,
        "content": "There are 2 versions in Databricks Certified Data Engineer Associate, which version we need to pick for this exam ?"
      },
      {
        "date": "2023-09-03T12:05:00.000Z",
        "voteCount": 3,
        "content": "B. Both teams would use the same source of truth for their work\n\nA data lakehouse is designed to unify the data engineering and data analysis architectures by integrating features of both data lakes and data warehouses. One of the key benefits of a data lakehouse is that it provides a common, centralized data repository (the \"lake\") that serves as a single source of truth for data storage and analysis. This allows both data engineering and data analysis teams to work with the same consistent data sets, reducing discrepancies and ensuring that the reports generated by both teams are based on the same underlying data.\n\nOption B addresses the issue of data consistency and alignment between the two teams, which is a common challenge in organizations with separate data engineering and data analysis architectures. By using the same source of truth, the data lakehouse helps alleviate this issue and promotes better collaboration and data integrity."
      },
      {
        "date": "2023-06-20T04:12:00.000Z",
        "voteCount": 2,
        "content": "Correct letter B"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/databricks/view/104728-exam-certified-data-engineer-associate-topic-1-question-2/",
    "body": "Which of the following describes a scenario in which a data team will want to utilize cluster pools?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn automated report needs to be refreshed as quickly as possible.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn automated report needs to be made reproducible.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn automated report needs to be tested to identify errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn automated report needs to be version-controlled across multiple collaborators.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn automated report needs to be runnable by all stakeholders."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 40,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-07T06:14:00.000Z",
        "voteCount": 17,
        "content": "Using cluster pools reduces the cluster startup time. So in this case, the reports can be refreshed quickly and not having to wait long for the cluster to start"
      },
      {
        "date": "2024-09-24T03:26:00.000Z",
        "voteCount": 4,
        "content": "Cluster pools in Databricks are used to ensure that a set of pre-warmed clusters is readily available to run workloads. This means that when a job is submitted, it can be executed more quickly because there is no need to wait for a cluster to spin up. Therefore, if a data team needs to refresh an automated report as quickly as possible, they will want to utilize cluster pools to ensure that the job can be executed as quickly as possible."
      },
      {
        "date": "2024-09-24T03:26:00.000Z",
        "voteCount": 3,
        "content": "A. An automated report needs to be refreshed as quickly as possible.\n\nCluster pools are typically used in distributed computing environments, such as cloud-based data platforms like Databricks. They allow you to pre-allocate a set of compute resources (a cluster) for specific tasks or workloads. In this case, if an automated report needs to be refreshed as quickly as possible, you can allocate a cluster pool with sufficient resources to ensure fast data processing and report generation. This helps ensure that the report is generated with minimal latency and can be delivered to stakeholders in a timely manner. Cluster pools allow you to optimize resource allocation for high-demand, time-sensitive tasks like real-time report generation."
      },
      {
        "date": "2024-09-24T03:25:00.000Z",
        "voteCount": 1,
        "content": "In Databricks, cluster pools are used to manage and optimize the allocation of cluster resources. They help ensure that clusters are efficiently provisioned and reused, which can reduce startup times and improve cost management.\n\nGiven the options:\n\nA. An automated report needs to be refreshed as quickly as possible. B. An automated report needs to be made reproducible. C. An automated report needs to be tested to identify errors. D. An automated report needs to be version-controlled across multiple collaborators. E. An automated report needs to be runnable by all stakeholders.\n\nThe most appropriate answer is:\n\nA. An automated report needs to be refreshed as quickly as possible.\n\nCluster pools are designed to minimize the time it takes to start up clusters by keeping a pool of pre-warmed instances available. This is particularly useful for scenarios where quick access to computing resources is crucial, such as in the case of refreshing automated reports quickly."
      },
      {
        "date": "2024-08-08T01:36:00.000Z",
        "voteCount": 1,
        "content": "we can reduce the start-up time of cluster using cluster pools."
      },
      {
        "date": "2024-06-09T14:38:00.000Z",
        "voteCount": 1,
        "content": "I believe it's A!"
      },
      {
        "date": "2024-05-22T05:40:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer as cluster pools are used to speed up the cluster startup time"
      },
      {
        "date": "2024-04-20T18:28:00.000Z",
        "voteCount": 2,
        "content": "Considering the recommendation to create pools based on workloads and to pre-populate pools to ensure instances are available when clusters need them, the most suitable option would be:\n\nE. An automated report needs to be runnable by all stakeholders.\n\nThis aligns with the concept of pre-populating pools to ensure that instances are readily available when needed, enabling the automated report to be executed promptly whenever stakeholders require it without waiting for instance acquisition."
      },
      {
        "date": "2024-04-02T06:28:00.000Z",
        "voteCount": 1,
        "content": "A : I think cluster pools are used mainly to accellerate cluster start up by using vms somehow."
      },
      {
        "date": "2024-03-19T02:09:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-03-07T19:47:00.000Z",
        "voteCount": 1,
        "content": "https://www.databricks.com/blog/2019/11/11/databricks-pools-speed-up-data-pipelines.html"
      },
      {
        "date": "2024-01-31T22:13:00.000Z",
        "voteCount": 1,
        "content": "E is correct for sure. For data team , their tasks is not just to refresh a report. They equally want to share the cluster for running their queries.  Please read at below:\nhttps://docs.databricks.com/en/compute/pool-best-practices.html#create-pools-based-on-workloads"
      },
      {
        "date": "2023-12-24T08:28:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-11-27T00:44:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-10-13T22:40:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-09-25T21:07:00.000Z",
        "voteCount": 1,
        "content": "Cluster pools are allows us to reduce the start time Ans A"
      },
      {
        "date": "2023-09-25T00:38:00.000Z",
        "voteCount": 1,
        "content": ".Cluster pools allow us to reserve VM's ahead of time, which means that its start-up time will be faster."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/databricks/view/104049-exam-certified-data-engineer-associate-topic-1-question-3/",
    "body": "Which of the following is hosted completely in the control plane of the classic Databricks architecture?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorker node",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJDBC data source",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks web application\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Filesystem",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDriver node"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 37,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-29T01:02:00.000Z",
        "voteCount": 14,
        "content": "I disagree with this answer. I think its the databricks web app that is always in the control plane"
      },
      {
        "date": "2023-04-01T07:28:00.000Z",
        "voteCount": 2,
        "content": "Agreed. Its Web app for sure"
      },
      {
        "date": "2023-04-04T10:09:00.000Z",
        "voteCount": 1,
        "content": "I think I meant to say that its option C. Not sure why I said that I agree with the answer in Examtopics. Option C for sure."
      },
      {
        "date": "2024-09-24T03:27:00.000Z",
        "voteCount": 8,
        "content": "C. Databricks web application\n\nIn the classic Databricks architecture, the control plane includes components like the Databricks web application, the Databricks REST API, and the Databricks Workspace. These components are responsible for managing and controlling the Databricks environment, including cluster provisioning, notebook management, access control, and job scheduling.\n\nThe other options, such as worker nodes, JDBC data sources, Databricks Filesystem (DBFS), and driver nodes, are typically part of the data plane or the execution environment, which is separate from the control plane. Worker nodes are responsible for executing tasks and computations, JDBC data sources are used to connect to external databases, DBFS is a distributed file system for data storage, and driver nodes are responsible for coordinating the execution of Spark jobs."
      },
      {
        "date": "2024-09-24T03:27:00.000Z",
        "voteCount": 3,
        "content": "The control plane in the classic Databricks architecture is responsible for managing the Databricks workspace, user and group management, and cluster management, among other things. The Databricks web application is a part of the control plane that enables users to interact with the workspace, create and manage clusters, and work with notebooks, jobs, and data. Worker nodes and driver nodes are part of the data plane, which is responsible for executing data processing tasks. JDBC data sources and the Databricks Filesystem are services that are used by both the control plane and the data plane."
      },
      {
        "date": "2024-09-24T03:27:00.000Z",
        "voteCount": 3,
        "content": "C. Databricks web application\n\nThe Databricks web application, which provides the user interface for interacting with Databricks, is hosted entirely in the control plane. It allows users to manage and monitor their clusters, notebooks, and jobs, among other functionalities.\n\nThe other components mentioned are associated with the compute layer in the classic Databricks architecture:\n\nA. Worker node: Worker nodes are responsible for executing the actual computations and processing the data.\n\nB. JDBC data source: A JDBC data source refers to an external database or data source accessed through the Java Database Connectivity (JDBC) interface. It is typically located outside of the control plane.\n\nD. Databricks Filesystem: The Databricks Filesystem (DBFS) is a distributed file system that stores and manages data within the compute layer.\n\nE. Driver node: The driver node is responsible for coordinating the execution of tasks across the worker nodes and managing the user session."
      },
      {
        "date": "2024-08-28T10:05:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-08-22T21:27:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-08-08T01:37:00.000Z",
        "voteCount": 1,
        "content": "its \"C\" -  Control plane has Databricks web application"
      },
      {
        "date": "2024-07-16T22:20:00.000Z",
        "voteCount": 1,
        "content": "Its C. \nother options, such as worker nodes, JDBC data sources, Databricks Filesystem (DBFS), and driver nodes, are typically part of the data plane or the execution environment, which is separate from the control plane."
      },
      {
        "date": "2024-06-09T14:42:00.000Z",
        "voteCount": 1,
        "content": "The answer is C! Accordinglu with the Databricks documentation, a  cluster consists of one driver node and zero or more worker nodes,  by default the driver node uses the same instance type as the worker node."
      },
      {
        "date": "2024-04-23T08:44:00.000Z",
        "voteCount": 1,
        "content": "Who decide the correct answer on this website ? CertiIQ says C ; ITExams says E.... For me it's C"
      },
      {
        "date": "2024-04-02T06:31:00.000Z",
        "voteCount": 1,
        "content": "Nodes are on the Data Plane. I think the Web App is the only one in the Control Pane."
      },
      {
        "date": "2024-03-19T02:10:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-03-05T08:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is C: https://docs.databricks.com/en/_images/databricks-architecture-aws.png"
      },
      {
        "date": "2024-01-31T22:03:00.000Z",
        "voteCount": 1,
        "content": "E.Driver Node , is the correct answer.\nIn the classic Databricks architecture, the control plane includes components responsible for managing and coordinating the execution of tasks. The driver node is part of the control plane, and it handles the coordination and execution of the overall Spark application."
      },
      {
        "date": "2024-04-26T13:34:00.000Z",
        "voteCount": 1,
        "content": "Cluster nodes (both driver and worker) are located on customer cloud account. So E is no the correct answer here."
      },
      {
        "date": "2023-12-26T01:25:00.000Z",
        "voteCount": 2,
        "content": "Webapplication always resides in Control Plane"
      },
      {
        "date": "2023-12-24T08:33:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-12-02T03:38:00.000Z",
        "voteCount": 1,
        "content": "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/databricks/view/104733-exam-certified-data-engineer-associate-topic-1-question-4/",
    "body": "Which of the following benefits of using the Databricks Lakehouse Platform is provided by Delta Lake?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe ability to manipulate the same data using a variety of languages",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe ability to collaborate in real time on a single notebook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe ability to set up alerts for query failures",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe ability to support batch and streaming workloads\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe ability to distribute complex data operations"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-03T12:12:00.000Z",
        "voteCount": 8,
        "content": "D. The ability to support batch and streaming workloads\n\nDelta Lake is a key component of the Databricks Lakehouse Platform that provides several benefits, and one of the most significant benefits is its ability to support both batch and streaming workloads seamlessly. Delta Lake allows you to process and analyze data in real-time (streaming) as well as in batch, making it a versatile choice for various data processing needs.\n\nWhile the other options may be benefits or capabilities of Databricks or the Lakehouse Platform in general, they are not specifically associated with Delta Lake."
      },
      {
        "date": "2024-08-28T10:06:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-08-08T01:37:00.000Z",
        "voteCount": 1,
        "content": "D. The ability to support batch and streaming workloads"
      },
      {
        "date": "2024-06-09T14:43:00.000Z",
        "voteCount": 1,
        "content": "The answer is D!"
      },
      {
        "date": "2024-03-19T02:32:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-10-09T04:44:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2023-09-25T21:08:00.000Z",
        "voteCount": 1,
        "content": "Correct and D"
      },
      {
        "date": "2023-07-02T09:56:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-04-07T06:16:00.000Z",
        "voteCount": 4,
        "content": "Delta Lake supports both Batch &amp; Stream workloads"
      },
      {
        "date": "2023-04-03T13:14:00.000Z",
        "voteCount": 4,
        "content": "Respuesta correcta es D"
      },
      {
        "date": "2023-04-03T00:39:00.000Z",
        "voteCount": 3,
        "content": "option D"
      },
      {
        "date": "2023-04-01T07:32:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/databricks/view/104735-exam-certified-data-engineer-associate-topic-1-question-5/",
    "body": "Which of the following describes the storage organization of a Delta table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta tables are stored in a single file that contains data, history, metadata, and other attributes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta tables store their data in a single file and all metadata in a collection of files in a separate location.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta tables are stored in a collection of files that contain data, history, metadata, and other attributes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta tables are stored in a collection of files that contain only the data stored within the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta tables are stored in a single file that contains only the data stored within the table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-24T03:29:00.000Z",
        "voteCount": 3,
        "content": "C. Delta tables are stored in a collection of files that contain data, history, metadata, and other attributes.\n\nDelta tables store data in a structured manner using Parquet files, and they also maintain metadata and transaction logs in separate directories. This organization allows for versioning, transactional capabilities, and metadata tracking in Delta Lake. Thank you for pointing out the error, and I appreciate your understanding."
      },
      {
        "date": "2024-08-08T01:38:00.000Z",
        "voteCount": 1,
        "content": "C. Delta tables are stored in a collection of files that contain data, history, metadata, and other attributes."
      },
      {
        "date": "2024-06-09T14:44:00.000Z",
        "voteCount": 1,
        "content": "The answer is C!"
      },
      {
        "date": "2024-04-02T23:56:00.000Z",
        "voteCount": 3,
        "content": "GPT4: \nDelta tables in Databricks use: \nParquet format files for data storage. \nA _delta_log folder for JSON log files that track transactions. \nScheme enforcement in metadata to ensure consistency.\n Checkpoint files to speed up the rebuilding of the table state."
      },
      {
        "date": "2024-03-19T02:33:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-12-24T08:36:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-10-09T04:49:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-10-01T09:58:00.000Z",
        "voteCount": 2,
        "content": "Reading Material:\n5 reasons to choose Delta format (on Databricks)\nhttps://medium.com/datalex/5-reasons-to-use-delta-lake-format-on-databricks-d9e76cf3e77d"
      },
      {
        "date": "2023-09-25T21:10:00.000Z",
        "voteCount": 1,
        "content": "Correct ans C"
      },
      {
        "date": "2023-08-15T23:30:00.000Z",
        "voteCount": 2,
        "content": "C is the right answer"
      },
      {
        "date": "2023-07-08T15:51:00.000Z",
        "voteCount": 2,
        "content": "C\nDelta tables in Databricks Delta Lake are stored in a collection of files organized in a directory structure. This directory structure includes data files, transaction log files, and metadata files. These files are stored in a specified location, typically in a distributed file system such as Hadoop Distributed File System (HDFS) or Amazon S3."
      },
      {
        "date": "2023-05-12T04:18:00.000Z",
        "voteCount": 3,
        "content": "First selected D as I assumed the data to be stored in the Delta lake and the transaction log to be stored separately. However, documentation states when a user creates a Delta Lake table, that table\u2019s transaction log is automatically created in the _delta_log subdirectory. The deltalog contains multiple files hence a collection of files. Answer C."
      },
      {
        "date": "2023-04-07T06:18:00.000Z",
        "voteCount": 3,
        "content": "C is the right option"
      },
      {
        "date": "2023-04-03T14:01:00.000Z",
        "voteCount": 1,
        "content": "C , respuesta correcta"
      },
      {
        "date": "2023-04-01T07:37:00.000Z",
        "voteCount": 2,
        "content": "C is correct answer\nhttps://docs.delta.io/latest/delta-faq.html#:~:text=Delta%20Lake%20uses%20versioned%20Parquet,directory%20to%20provide%20ACID%20transactions."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/databricks/view/104736-exam-certified-data-engineer-associate-topic-1-question-6/",
    "body": "Which of the following code blocks will remove the rows where the value in column age is greater than 25 from the existing Delta table my_table and save the updated table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT * FROM my_table WHERE age &gt; 25;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUPDATE my_table WHERE age &gt; 25;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDELETE FROM my_table WHERE age &gt; 25;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUPDATE my_table WHERE age &lt;= 25;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDELETE FROM my_table WHERE age &lt;= 25;"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-08T01:40:00.000Z",
        "voteCount": 1,
        "content": "to remove the data from a table we can use delete from table with condition."
      },
      {
        "date": "2024-06-09T14:45:00.000Z",
        "voteCount": 1,
        "content": "The answer is C!"
      },
      {
        "date": "2024-04-18T14:50:00.000Z",
        "voteCount": 1,
        "content": "there is not delete history option just the vacuum with its parameters of time retention."
      },
      {
        "date": "2024-03-25T10:26:00.000Z",
        "voteCount": 3,
        "content": "Answer is C. Just finished exam-got 100% [Databricks Associate Exam Practice Exams] All questions came from \nDatabricks Certified Data Engineer Associate\nhttps://www.udemy.com/share/10aEFa3@9M_uT6vrKbnl68tOK96kfy-YWitjwzLTlVCrzPs-0hGUu8fyX8V4Tn_x_y65bwLm/"
      },
      {
        "date": "2024-03-19T02:34:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-12-24T08:38:00.000Z",
        "voteCount": 1,
        "content": "C. DELETE FROM my_table WHERE age &gt; 25;"
      },
      {
        "date": "2023-10-09T04:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-09-28T02:38:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer as the SELECT statement allows to query a table, the UPDATE statement allows to modify values in columns. If you want to remove rows that don't match a specific condition you must use DELETE"
      },
      {
        "date": "2023-09-25T21:11:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-09-11T22:14:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-09-03T12:13:00.000Z",
        "voteCount": 1,
        "content": "C. DELETE FROM my_table WHERE age &gt; 25;"
      },
      {
        "date": "2023-07-02T10:04:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-05-12T04:25:00.000Z",
        "voteCount": 2,
        "content": "C is correct. use DELETE FROM to delete existing records from the table. UPDATE is used to modify existing records. SELECT only creates a view, it does not alter the table records."
      },
      {
        "date": "2023-04-20T22:44:00.000Z",
        "voteCount": 1,
        "content": "C - is correct answer"
      },
      {
        "date": "2023-04-03T13:10:00.000Z",
        "voteCount": 2,
        "content": "C es correcto"
      },
      {
        "date": "2023-04-03T01:11:00.000Z",
        "voteCount": 1,
        "content": "option c"
      },
      {
        "date": "2023-04-01T07:38:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/databricks/view/104051-exam-certified-data-engineer-associate-topic-1-question-7/",
    "body": "A data engineer has realized that they made a mistake when making a daily update to a table. They need to use Delta time travel to restore the table to a version that is 3 days old. However, when the data engineer attempts to time travel to the older version, they are unable to restore the data because the data files have been deleted.<br>Which of the following explains why the data files are no longer present?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe VACUUM command was run on the table\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe TIME TRAVEL command was run on the table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe DELETE HISTORY command was run on the table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe OPTIMIZE command was nun on the table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe HISTORY command was run on the table"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 33,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-16T03:07:00.000Z",
        "voteCount": 11,
        "content": "There is no DELETE HISTORY command in Databricks \nVACCUM command can remove history and we can also specify the retention period with VACCUM Command. Default Retention period is 7 days.\nTo allow changing the default retention period you can rum the following command \n\nALTER TABLE your_table SET TBLPROPERTIES ('delta.retentionDurationCheck.enabled' = 'true');"
      },
      {
        "date": "2024-09-25T03:42:00.000Z",
        "voteCount": 1,
        "content": "Its vacuum"
      },
      {
        "date": "2024-09-24T03:30:00.000Z",
        "voteCount": 3,
        "content": "A. The VACUUM command was run on the table\n\nThe VACUUM command in Delta Lake is used to clean up and remove unnecessary data files that are no longer needed for time travel or query purposes. When you run VACUUM with certain retention settings, it can delete older data files, which might include versions of data that are older than the specified retention period. If the data engineer is unable to restore the table to a version that is 3 days old because the data files have been deleted, it's likely because the VACUUM command was run on the table, removing the older data files as part of data cleanup."
      },
      {
        "date": "2024-08-26T08:45:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A\nA. The VACUUM command was run on the table\n\nThe VACUUM command in Delta Lake is used to clean up old data files that are no longer needed, which could include files that are older than a certain retention period. If the data engineer is unable to restore data to a version that is 3 days old, it is likely because the VACUUM command has deleted the old data files beyond the retention period."
      },
      {
        "date": "2024-08-08T01:42:00.000Z",
        "voteCount": 1,
        "content": "vacuum command is used to remove the history of the table."
      },
      {
        "date": "2024-07-16T22:25:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2024-07-08T13:38:00.000Z",
        "voteCount": 1,
        "content": "There is no DELETE HISTORY command anywhere in Databricks. The VACUUM command removes files older than the value that is set. The default value is 7 days."
      },
      {
        "date": "2024-06-16T02:15:00.000Z",
        "voteCount": 1,
        "content": "A is the good answer"
      },
      {
        "date": "2024-06-09T14:49:00.000Z",
        "voteCount": 1,
        "content": "The answer is A!"
      },
      {
        "date": "2024-03-25T10:26:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. Just finished exam-got 100% [Databricks Associate Exam Practice Exams] All questions came from \nDatabricks Certified Data Engineer Associate\nhttps://www.udemy.com/share/10aEFa3@9M_uT6vrKbnl68tOK96kfy-YWitjwzLTlVCrzPs-0hGUu8fyX8V4Tn_x_y65bwLm/"
      },
      {
        "date": "2024-03-19T02:35:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-12-24T08:39:00.000Z",
        "voteCount": 1,
        "content": "A i correct"
      },
      {
        "date": "2023-11-17T07:20:00.000Z",
        "voteCount": 2,
        "content": "I agree with the first post. A is the correct answer. There is no such thing as a Delete History Command"
      },
      {
        "date": "2023-11-07T05:10:00.000Z",
        "voteCount": 1,
        "content": "right answer is A"
      },
      {
        "date": "2023-10-29T22:56:00.000Z",
        "voteCount": 1,
        "content": "i think B is the answer, plz let me know if not correct"
      },
      {
        "date": "2023-10-29T22:55:00.000Z",
        "voteCount": 1,
        "content": "but vaccum allows to vaccum anything that's older than 7 days right"
      },
      {
        "date": "2023-10-09T04:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is A Vaccum"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/databricks/view/104737-exam-certified-data-engineer-associate-topic-1-question-8/",
    "body": "Which of the following Git operations must be performed outside of Databricks Repos?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCommit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPull",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPush",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClone",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-13T14:22:00.000Z",
        "voteCount": 20,
        "content": "According to the most recent one, all command is feasible in Repos"
      },
      {
        "date": "2023-08-31T04:09:00.000Z",
        "voteCount": 14,
        "content": "Not valid anymore... \nhttps://docs.databricks.com/en/repos/ci-cd-techniques-with-repos.html"
      },
      {
        "date": "2024-08-26T08:48:00.000Z",
        "voteCount": 3,
        "content": "D. Clone\n\nCloning a repository creates a local copy of the repository on your machine and must be done using a local Git client or command line. Once the repository is cloned, you can work with it in Databricks Repos, but the initial clone operation itself is outside the Databricks interface. Other operations like commit, pull, push, and merge can be managed within Databricks Repos or through other Git tools."
      },
      {
        "date": "2024-08-08T01:43:00.000Z",
        "voteCount": 2,
        "content": "D: Cloning a repository is typically done outside of Databricks Repos, often using a Git client or command line interface before the repository is linked to Databricks Repos."
      },
      {
        "date": "2024-04-26T13:31:00.000Z",
        "voteCount": 4,
        "content": "Confirmed on live environment - merging is now possible directly in Databricks Repos"
      },
      {
        "date": "2024-03-19T02:35:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-02-23T00:01:00.000Z",
        "voteCount": 1,
        "content": "Why not option B Pull\n\nThe following tasks are not supported by Databricks Repos, and must be performed in your Git provider:\nCreate a pull request\nDelete branches\nMerge and rebase branches *"
      },
      {
        "date": "2024-04-26T13:32:00.000Z",
        "voteCount": 2,
        "content": "Pull is not the same as pull request. Pulls are updating local version of the repo to the one present on remote. And it's surely feasible in Databricks repos."
      },
      {
        "date": "2024-01-07T05:44:00.000Z",
        "voteCount": 7,
        "content": "The new answer is F - Delete ."
      },
      {
        "date": "2023-12-24T08:41:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-12-01T03:03:00.000Z",
        "voteCount": 1,
        "content": "i think it supports merge now \nhttps://docs.databricks.com/en/repos/git-operations-with-repos.html\n\"If an operation such as pull, rebase, or merge causes a merge conflict, the Repos UI shows a list of files with conflicts and options for resolving the conflicts.\n\nYou have two primary options:\n\n    Use the Repos UI to resolve the conflict.\""
      },
      {
        "date": "2023-11-17T07:21:00.000Z",
        "voteCount": 1,
        "content": "E is the correct answer given the selections. You can clone."
      },
      {
        "date": "2023-10-30T07:30:00.000Z",
        "voteCount": 1,
        "content": "According to the recent version, all commands are supported under Repos !"
      },
      {
        "date": "2023-09-25T21:11:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2023-06-06T04:42:00.000Z",
        "voteCount": 2,
        "content": "Merge is the correct answer"
      },
      {
        "date": "2023-05-03T17:42:00.000Z",
        "voteCount": 2,
        "content": "For following tasks, work in your Git provider:\n\nCreate a pull request.\nResolve merge conflicts.\nMerge or delete branches.\nRebase a branch.\n\nhttps://docs.databricks.com/repos/index.html"
      },
      {
        "date": "2023-04-20T22:46:00.000Z",
        "voteCount": 1,
        "content": "Merge - A"
      },
      {
        "date": "2023-04-20T11:58:00.000Z",
        "voteCount": 1,
        "content": "merge is not supported"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/databricks/view/104743-exam-certified-data-engineer-associate-topic-1-question-9/",
    "body": "Which of the following data lakehouse features results in improved data quality over a traditional data lake?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA data lakehouse provides storage solutions for structured and unstructured data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA data lakehouse supports ACID-compliant transactions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA data lakehouse allows the use of SQL queries to examine data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA data lakehouse stores data in open formats.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA data lakehouse enables machine learning and artificial Intelligence workloads."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 48,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-12T05:31:00.000Z",
        "voteCount": 11,
        "content": "Option B - ACID Properties Source: https://www.databricks.com/blog/2021/08/30/frequently-asked-questions-about-the-data-lakehouse.html#five -&gt; \"Lakehouse tackles the fundamental issues that make data swamps out of data lakes. It adds ACID transactions to ensure consistency as multiple parties concurrently read or write data.\""
      },
      {
        "date": "2023-09-03T12:16:00.000Z",
        "voteCount": 8,
        "content": "B. A data lakehouse supports ACID-compliant transactions.\n\nOne of the key features of a data lakehouse that results in improved data quality over a traditional data lake is its support for ACID (Atomicity, Consistency, Isolation, Durability) transactions. ACID transactions provide data integrity and consistency guarantees, ensuring that operations on the data are reliable and that data is not left in an inconsistent state due to failures or concurrent access.\n\nIn a traditional data lake, such transactional guarantees are often lacking, making it challenging to maintain data quality, especially in scenarios involving multiple data writes, updates, or complex transformations. A data lakehouse, by offering ACID compliance, helps maintain data quality by providing strong consistency and reliability, which is crucial for data pipelines and analytics."
      },
      {
        "date": "2024-08-26T08:59:00.000Z",
        "voteCount": 1,
        "content": "B. A data lakehouse supports ACID-compliant transactions.\n\nACID compliance (Atomicity, Consistency, Isolation, Durability) ensures that transactions are processed reliably and helps maintain data integrity. This feature allows for reliable updates, deletions, and insertions in a data lakehouse, which significantly improves data quality by avoiding issues like partial updates or inconsistent states that are common in traditional data lakes."
      },
      {
        "date": "2024-08-08T01:44:00.000Z",
        "voteCount": 1,
        "content": "B. A data lakehouse supports ACID-compliant transactions.\nACID compliance ensures data integrity and consistency, which improves the overall quality of the data."
      },
      {
        "date": "2024-07-16T22:32:00.000Z",
        "voteCount": 1,
        "content": "answer is B"
      },
      {
        "date": "2024-06-09T14:50:00.000Z",
        "voteCount": 1,
        "content": "The answer is B!"
      },
      {
        "date": "2024-05-19T05:17:00.000Z",
        "voteCount": 1,
        "content": "B is Correct"
      },
      {
        "date": "2024-04-27T08:59:00.000Z",
        "voteCount": 1,
        "content": "b is correct as acid transactions are related to data quality"
      },
      {
        "date": "2024-04-03T23:47:00.000Z",
        "voteCount": 2,
        "content": "b is correct"
      },
      {
        "date": "2024-03-19T02:37:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-03T15:43:00.000Z",
        "voteCount": 1,
        "content": "Correct andwer is B"
      },
      {
        "date": "2023-12-29T13:38:00.000Z",
        "voteCount": 2,
        "content": "B. A data lakehouse supports ACID-compliant transactions.\n\nACID (Atomicity, Consistency, Isolation, Durability) compliance ensures that transactions are processed reliably and consistently, which is crucial for maintaining data integrity and quality. It helps in preventing data inconsistencies or errors that might occur during concurrent transactions or data operations. By supporting ACID-compliant transactions, a data lakehouse can enhance data quality by providing mechanisms to ensure the reliability and consistency of data operations, which is a notable improvement over traditional data lakes that might lack such transactional capabilities."
      },
      {
        "date": "2023-11-14T21:25:00.000Z",
        "voteCount": 1,
        "content": "point of correction: B is the correct answer, not A as posted in my previous comment"
      },
      {
        "date": "2023-11-14T21:23:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer. Allowing SQL query to examine data does not improve the quality of the data. For example, if the person writes the wrong query they will get a wrong answer. So how does that improve the quality of the data?"
      },
      {
        "date": "2023-11-07T05:30:00.000Z",
        "voteCount": 1,
        "content": "Correct : B"
      },
      {
        "date": "2023-10-09T04:58:00.000Z",
        "voteCount": 1,
        "content": "Acid Transaction is the correct answer"
      },
      {
        "date": "2023-09-25T21:12:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/databricks/view/104744-exam-certified-data-engineer-associate-topic-1-question-10/",
    "body": "A data engineer needs to determine whether to use the built-in Databricks Notebooks versioning or version their project using Databricks Repos.<br>Which of the following is an advantage of using Databricks Repos over the Databricks Notebooks versioning?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Repos automatically saves development progress",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Repos supports the use of multiple branches\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Repos allows users to revert to previous versions of a notebook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Repos provides the ability to comment on specific changes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Repos is wholly housed within the Databricks Lakehouse Platform"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-03T18:25:00.000Z",
        "voteCount": 9,
        "content": "While both Databricks Notebooks versioning and Databricks Repos allow for version control of code, Databricks Repos provides the additional benefit of supporting the use of multiple branches. This allows for multiple versions of a notebook or project to be developed in parallel, facilitating collaboration among team members and simplifying the process of merging changes into a single main branch."
      },
      {
        "date": "2024-08-31T11:32:00.000Z",
        "voteCount": 1,
        "content": "I read, Legacy notebook Git integration support was removed on January 31st, 2024. \nso , it means the git notebook integration not supported now. AM I correct?"
      },
      {
        "date": "2024-08-08T01:46:00.000Z",
        "voteCount": 1,
        "content": "B. Databricks Repos supports the use of multiple branches\n\nThis feature allows for more advanced version control and collaborative development workflows, enabling multiple branches for different features or experiments."
      },
      {
        "date": "2024-04-27T09:01:00.000Z",
        "voteCount": 1,
        "content": "b , multiple branches are not supported at all without a git integration and databricks repos have built in UI for governing such a thing"
      },
      {
        "date": "2024-04-27T09:01:00.000Z",
        "voteCount": 1,
        "content": "b , multiple branches are not supported at all without a git integration and databricks repos have built in UI for governing such a thing"
      },
      {
        "date": "2024-04-03T23:48:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-03-19T02:45:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-03T15:44:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2023-11-07T05:32:00.000Z",
        "voteCount": 1,
        "content": "Correct : B"
      },
      {
        "date": "2023-09-25T21:13:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-09-03T12:17:00.000Z",
        "voteCount": 3,
        "content": "B. Databricks Repos supports the use of multiple branches.\n\nAn advantage of using Databricks Repos over the built-in Databricks Notebooks versioning is the ability to work with multiple branches. Branching is a fundamental feature of version control systems like Git, which Databricks Repos is built upon. It allows you to create separate branches for different tasks, features, or experiments within your project. This separation helps in parallel development and experimentation without affecting the main branch or the work of other team members.\n\nBranching provides a more organized and collaborative development environment, making it easier to merge changes and manage different development efforts. While Databricks Notebooks versioning also allows you to track versions of notebooks, it may not provide the same level of flexibility and collaboration as branching in Databricks Repos."
      },
      {
        "date": "2023-08-11T12:01:00.000Z",
        "voteCount": 1,
        "content": "B\nbuilt in databricks notebook versioning does not allow multiple branches."
      },
      {
        "date": "2023-07-08T15:59:00.000Z",
        "voteCount": 2,
        "content": "B\nAn advantage of using Databricks Repos over the Databricks Notebooks versioning is that Databricks Repos supports the use of multiple branches. With Databricks Repos, you can create and manage multiple branches of your codebase, enabling parallel development, collaboration, and the ability to work on different features or bug fixes simultaneously."
      },
      {
        "date": "2023-04-20T22:49:00.000Z",
        "voteCount": 1,
        "content": "B. Databricks Repos supports the use of multiple branches"
      },
      {
        "date": "2023-04-03T22:40:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2023-04-03T01:20:00.000Z",
        "voteCount": 2,
        "content": "option B"
      },
      {
        "date": "2023-04-01T07:54:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/databricks/view/104745-exam-certified-data-engineer-associate-topic-1-question-11/",
    "body": "A data engineer has left the organization. The data team needs to transfer ownership of the data engineer\u2019s Delta tables to a new data engineer. The new data engineer is the lead engineer on the data team.<br>Assuming the original data engineer no longer has access, which of the following individuals must be the one to transfer ownership of the Delta tables in Data Explorer?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks account representative",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThis transfer is not possible",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorkspace administrator\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew lead data engineer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOriginal data engineer"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-20T00:52:00.000Z",
        "voteCount": 1,
        "content": "C. Workspace administrator"
      },
      {
        "date": "2024-08-08T01:50:00.000Z",
        "voteCount": 1,
        "content": "C. Workspace administrator"
      },
      {
        "date": "2024-07-21T03:51:00.000Z",
        "voteCount": 1,
        "content": "Workspace admin"
      },
      {
        "date": "2024-07-16T22:35:00.000Z",
        "voteCount": 2,
        "content": "Question is to give access to new data engineer, How will he/she have it ?\ncorrect answer is C - Workspace admin"
      },
      {
        "date": "2024-07-08T13:40:00.000Z",
        "voteCount": 1,
        "content": "The new data engineer would be unable to make this change. It's up to the WSA."
      },
      {
        "date": "2024-05-20T08:06:00.000Z",
        "voteCount": 1,
        "content": "Option c"
      },
      {
        "date": "2024-04-18T15:38:00.000Z",
        "voteCount": 2,
        "content": "workspace admin"
      },
      {
        "date": "2024-04-03T23:52:00.000Z",
        "voteCount": 1,
        "content": "workspace admin"
      },
      {
        "date": "2024-03-19T02:46:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-01-03T15:47:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C"
      },
      {
        "date": "2023-12-29T13:47:00.000Z",
        "voteCount": 3,
        "content": "The ownership of Delta tables in Data Explorer can be transferred by the current owner, a metastore admin, or the owner of the container1. In this case, since the original data engineer no longer has access, the Workspace Administrator (Option C) would be the most appropriate individual to transfer the ownership of the Delta tables to the new data engineer. This is because the Workspace Administrator typically has the necessary permissions to manage such resources2. Please note that the exact process may vary depending on the specific configurations and permissions set up in your workspace. It\u2019s always a good idea to consult with your organization\u2019s IT or data governance team to ensure the correct procedures are followed."
      },
      {
        "date": "2023-11-14T21:28:00.000Z",
        "voteCount": 2,
        "content": "The answer is C - the workspace admin. How can it be the new DE? You do not even know if the new DE has access. That was not stated and so you cannot consider it."
      },
      {
        "date": "2023-10-09T05:02:00.000Z",
        "voteCount": 1,
        "content": "Workspace Administrator"
      },
      {
        "date": "2023-09-25T21:13:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-09-25T01:10:00.000Z",
        "voteCount": 1,
        "content": "It should be \"workspace administrator\""
      },
      {
        "date": "2023-06-29T03:56:00.000Z",
        "voteCount": 1,
        "content": "It should be \"workspace administrator\""
      },
      {
        "date": "2023-06-19T01:43:00.000Z",
        "voteCount": 1,
        "content": "It's C"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/databricks/view/104748-exam-certified-data-engineer-associate-topic-1-question-12/",
    "body": "A data analyst has created a Delta table sales that is used by the entire data analysis team. They want help from the data engineering team to implement a series of tests to ensure the data is clean. However, the data engineering team uses Python for its tests rather than SQL.<br>Which of the following commands could the data engineering team use to access sales in PySpark?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT * FROM sales",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no way to share data between PySpark and SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql(\"sales\")D. spark.delta.table(\"sales\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.table(\"sales\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-08T16:05:00.000Z",
        "voteCount": 8,
        "content": "E\nThe spark.table() function in PySpark allows you to access tables registered in the catalog, including Delta tables. By specifying the table name (\"sales\"), the data engineering team can read the Delta table and perform various operations on it using PySpark.\n\nOption A, SELECT * FROM sales, is a SQL syntax and cannot be directly used in PySpark.\n\nOption B, \"There is no way to share data between PySpark and SQL,\" is incorrect. PySpark provides the capability to interact with data using both SQL and DataFrame/DataSet APIs.\n\nOption C, spark.sql(\"sales\"), is a valid command to execute SQL queries on registered tables in PySpark. However, in this case, the \"sales\" argument alone is not a valid SQL query.\n\nOption D, spark.delta.table(\"sales\"), is a specific method provided by Delta Lake to access Delta tables directly. While it can be used to access the \"sales\" table, it is not the most common approach in PySpark."
      },
      {
        "date": "2024-08-26T09:08:00.000Z",
        "voteCount": 1,
        "content": "To access the Delta table sales using PySpark, the data engineering team can use the following command:\n\nE. spark.table(\"sales\")\n\nThis command allows them to load the table into a PySpark DataFrame, which they can then use for their tests and data processing in Python.\nNo, the command spark.delta.table(\"table name\") does not exist in PySpark. To access a Delta table, you should use:\n\nspark.table(\"table name\")\n\nOr, if you need to use Delta-specific functionality, you would typically use Delta's APIs or spark.read.format(\"delta\").table(\"table name\") to read the table into a DataFrame."
      },
      {
        "date": "2024-08-08T01:52:00.000Z",
        "voteCount": 1,
        "content": "E. spark.table(\"sales\")\n\nThis command allows the team to access the table using PySpark, enabling them to implement their tests in Python."
      },
      {
        "date": "2024-07-21T03:55:00.000Z",
        "voteCount": 1,
        "content": "spark.table() . E is the correct one"
      },
      {
        "date": "2024-04-27T09:05:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-04-03T23:53:00.000Z",
        "voteCount": 2,
        "content": "e is correct"
      },
      {
        "date": "2024-03-19T02:47:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-01-03T15:49:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is E"
      },
      {
        "date": "2023-12-29T13:50:00.000Z",
        "voteCount": 4,
        "content": "E. spark.table(\"sales\")\n\nThe spark.table() function in PySpark allows access to a registered table within the SparkSession. In this case, \"sales\" is the name of the Delta table created by the data analyst, and the spark.table() function enables access to this table for performing data engineering tests using Python (PySpark)."
      },
      {
        "date": "2023-12-26T05:43:00.000Z",
        "voteCount": 1,
        "content": "C is correct Answer"
      },
      {
        "date": "2023-11-07T05:46:00.000Z",
        "voteCount": 1,
        "content": "Correct is E"
      },
      {
        "date": "2023-09-25T21:13:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-09-25T01:11:00.000Z",
        "voteCount": 1,
        "content": "delta is default."
      },
      {
        "date": "2023-06-12T06:03:00.000Z",
        "voteCount": 2,
        "content": "It's E. As stated by others, the default format is delta\n\nIf you try to run D, you get an error, that there are no \"delta\"-command for spark: \"AttributeError: 'SparkSession' object has no attribute 'delta'\". If you want to explicit tell it should be delta, then you need an \".option(format='delta')\" insted."
      },
      {
        "date": "2023-06-03T20:59:00.000Z",
        "voteCount": 1,
        "content": "You access data in Delta tables by the table name or the table path, as shown in the following examples:\npeople_df = spark.read.table(table_name)\n\ndisplay(people_df)"
      },
      {
        "date": "2023-05-12T04:55:00.000Z",
        "voteCount": 1,
        "content": "I believe the answer is E as in databricks the default tables are delta tables hence spark.table should be enough. Have not seen a spark.delta.table function before."
      },
      {
        "date": "2023-05-09T01:45:00.000Z",
        "voteCount": 2,
        "content": "E: spark.table or spark.read.table"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/databricks/view/104750-exam-certified-data-engineer-associate-topic-1-question-13/",
    "body": "Which of the following commands will return the location of database customer360?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDESCRIBE LOCATION customer360;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDROP DATABASE customer360;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDESCRIBE DATABASE customer360;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tALTER DATABASE customer360 SET DBPROPERTIES ('location' = '/user'};",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUSE DATABASE customer360;"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-03T13:42:00.000Z",
        "voteCount": 7,
        "content": "C. DESCRIBE DATABASE customer360;\n\nTo retrieve the location of a database named \"customer360\" in a database management system like Hive or Databricks, you can use the DESCRIBE DATABASE command followed by the database name. This command will provide information about the database, including its location."
      },
      {
        "date": "2024-08-08T01:54:00.000Z",
        "voteCount": 1,
        "content": "C. DESCRIBE DATABASE customer360;\nthis will show the location of the databaase."
      },
      {
        "date": "2024-04-27T09:06:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-03-19T02:51:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-01-03T15:50:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C"
      },
      {
        "date": "2023-11-07T05:51:00.000Z",
        "voteCount": 1,
        "content": "Correct :C"
      },
      {
        "date": "2023-09-25T21:14:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-08-10T21:07:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-08-04T23:30:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-04-20T23:06:00.000Z",
        "voteCount": 1,
        "content": "Option C -\nhttps://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux-describe-database.html"
      },
      {
        "date": "2023-04-03T02:07:00.000Z",
        "voteCount": 2,
        "content": "option c"
      },
      {
        "date": "2023-04-02T07:16:00.000Z",
        "voteCount": 2,
        "content": "Muy facil"
      },
      {
        "date": "2023-04-01T08:04:00.000Z",
        "voteCount": 3,
        "content": "Correct answer"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/databricks/view/104865-exam-certified-data-engineer-associate-topic-1-question-14/",
    "body": "A data engineer wants to create a new table containing the names of customers that live in France.<br>They have written the following command:<br><img title=\"image1\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image1.png\"><br>A senior data engineer mentions that it is organization policy to include a table property indicating that the new table includes personally identifiable information (PII).<br>Which of the following lines of code fills in the above blank to successfully complete the task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no way to indicate whether a table contains PII.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\"COMMENT PII\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTBLPROPERTIES PII",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCOMMENT \"Contains PII\"\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPII"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-14T21:40:00.000Z",
        "voteCount": 9,
        "content": "The correct answer is D. COMMENT \"Contains PII\". Context matters. Yes, you can use Table Property to add additional metadata. But you cannot view that property when you describe the table. With the Comment \"this is ...\" anyone who describe the table &lt;DESC &lt;table name&gt; will see the comment."
      },
      {
        "date": "2023-07-28T21:50:00.000Z",
        "voteCount": 6,
        "content": "D\nRef:https://www.databricks.com/discover/pages/data-quality-management\nCREATE TABLE my_table (id INT COMMENT 'Unique Identification Number', name STRING COMMENT 'PII', age INT COMMENT 'PII')\nTBLPROPERTIES ('contains_pii'=True)\nCOMMENT 'Contains PII';"
      },
      {
        "date": "2024-07-08T13:42:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D. There is no syntax for TBLPROPERTIES PII."
      },
      {
        "date": "2024-04-27T09:07:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-03-19T02:52:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-03-16T04:38:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html\nCOMMENT column_comment\nA string literal to describe the column."
      },
      {
        "date": "2024-01-31T22:37:00.000Z",
        "voteCount": 2,
        "content": "answer C :\nCREATE TABLE new_table\nAS\nSELECT customer_name\nFROM original_table\nWHERE country = 'France'\nTBLPROPERTIES ('PII'='true');"
      },
      {
        "date": "2024-01-03T15:53:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D"
      },
      {
        "date": "2023-11-07T05:54:00.000Z",
        "voteCount": 1,
        "content": "correct :D"
      },
      {
        "date": "2023-09-28T23:52:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-09-25T21:14:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-09-03T13:43:00.000Z",
        "voteCount": 1,
        "content": "D. COMMENT \"Contains PII\""
      },
      {
        "date": "2023-07-07T15:03:00.000Z",
        "voteCount": 3,
        "content": "D\nThe COMMENT keyword is used to add a comment to a table. The comment can be used to provide additional information about the table, such as its purpose or the data that it contains.\n\nIn this case, the data engineer wants to add a comment to the customersInFrance table indicating that the table contains PII. The following line of code will do this:\n\nCode snippet\nCOMMENT \"Contains PII\"\nUse code with caution. Learn more\nThis will add the comment \"Contains PII\" to the customersInFrance table.\n\nThe other options are not valid ways to indicate that a table contains PII. The TBLPROPERTIES keyword is used to set the table properties, but there is no property for indicating whether a table contains PII. The PII keyword is not a valid keyword in SQL.\n\nTherefore, the only valid way to indicate that a table contains PII is to use the COMMENT keyword."
      },
      {
        "date": "2023-05-21T03:25:00.000Z",
        "voteCount": 2,
        "content": "syntax of C is wrong."
      },
      {
        "date": "2023-05-23T17:32:00.000Z",
        "voteCount": 1,
        "content": "Exactly. The correct syntax for table properties is: TBLPROPERTIES ('foo'='bar');"
      },
      {
        "date": "2023-05-04T02:39:00.000Z",
        "voteCount": 2,
        "content": "Correct answer should be C asommand creates a new table called \"customersInFrance\" with the properties of Personally Identifiable Information (PII) and selects the columns ID, FIRSTNAME, LASTNAME, ADDRESS, and PHONE_NUMBER from the existing \"customers\" table where the country is France."
      },
      {
        "date": "2023-04-20T23:12:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/sql-ref-syntax-ddl-create-table-using"
      },
      {
        "date": "2023-04-20T12:09:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/databricks/view/104649-exam-certified-data-engineer-associate-topic-1-question-15/",
    "body": "Which of the following benefits is provided by the array functions from Spark SQL?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn ability to work with data in a variety of types at once",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn ability to work with data within certain partitions and windows",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn ability to work with time-related data in specified intervals",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn ability to work with complex, nested data ingested from JSON files\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn ability to work with an array of tables for procedural automation"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-07T15:02:00.000Z",
        "voteCount": 9,
        "content": "Array functions in Spark SQL allow you to work with complex, nested data ingested from JSON files. These functions can be used to extract data from nested structures, manipulate data within nested structures, and aggregate data within nested structures.\n\nThe other options are not benefits provided by the array functions from Spark SQL.\n\nOption A: Array functions do not allow you to work with data in a variety of types at once.\nOption B: Array functions do not allow you to work with data within certain partitions and windows.\nOption C: Array functions do not allow you to work with time-related data in specified intervals.\nOption E: Array functions do not allow you to work with an array of tables for procedural automation.\nTherefore, the only benefit provided by the array functions from Spark SQL is the ability to work with complex, nested data ingested from JSON files."
      },
      {
        "date": "2024-08-08T02:05:00.000Z",
        "voteCount": 2,
        "content": "D. An ability to work with complex, nested data ingested from JSON files\n\nArray functions in Spark SQL allow you to work with complex and nested data structures, such as those found in JSON files, enabling operations on arrays and nested elements."
      },
      {
        "date": "2024-07-16T22:40:00.000Z",
        "voteCount": 1,
        "content": "D is the correct one"
      },
      {
        "date": "2024-07-11T03:46:00.000Z",
        "voteCount": 1,
        "content": "The correct Answer is D"
      },
      {
        "date": "2024-07-08T13:42:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D."
      },
      {
        "date": "2024-05-18T01:04:00.000Z",
        "voteCount": 1,
        "content": "D is the right answeer"
      },
      {
        "date": "2024-04-04T00:03:00.000Z",
        "voteCount": 1,
        "content": "i thought sql arrays are usually seen in json files read"
      },
      {
        "date": "2024-03-19T02:53:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-01-03T15:54:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D"
      },
      {
        "date": "2023-12-29T13:58:00.000Z",
        "voteCount": 4,
        "content": "D. An ability to work with complex, nested data ingested from JSON files\n\nArray functions in Spark SQL enable users to work efficiently with arrays and complex, nested data structures that are often ingested from JSON files or other nested data formats. These functions allow manipulation, querying, and extraction of elements from arrays and nested structures within the dataset, facilitating operations on complex data types within Spark SQL."
      },
      {
        "date": "2023-11-14T21:43:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D. Array provides complex nesting of data and it is easy to query. That's why we use arrays for definding data domains."
      },
      {
        "date": "2023-11-07T06:02:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-10-09T05:05:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-09-28T23:54:00.000Z",
        "voteCount": 1,
        "content": "array functions allow you to work with JSON data"
      },
      {
        "date": "2023-09-25T21:15:00.000Z",
        "voteCount": 1,
        "content": "D is right ans"
      },
      {
        "date": "2023-09-03T13:44:00.000Z",
        "voteCount": 3,
        "content": "D. An ability to work with complex, nested data ingested from JSON files\n\nArray functions in Spark SQL are primarily used for working with arrays and complex, nested data structures, such as those often encountered when ingesting JSON files. These functions allow you to manipulate and query nested arrays and structures within your data, making it easier to extract and work with specific elements or values within complex data formats.\n\nWhile some of the other options (such as option A for working with different data types) are features of Spark SQL or SQL in general, array functions specifically excel at handling complex, nested data structures like those found in JSON files."
      },
      {
        "date": "2023-05-13T21:31:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D. Spark SQL Array functions allow us to work with nested datasets in JSON files"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/databricks/view/104863-exam-certified-data-engineer-associate-topic-1-question-16/",
    "body": "Which of the following commands can be used to write data into a Delta table while avoiding the writing of duplicate records?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDROP",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIGNORE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMERGE\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAPPEND",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tINSERT"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-08T02:07:00.000Z",
        "voteCount": 3,
        "content": "C. MERGE\n\nThe MERGE command allows you to perform upserts (update and insert) into a Delta table, effectively avoiding duplicates by updating existing records and inserting new ones as needed."
      },
      {
        "date": "2024-05-18T01:04:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer"
      },
      {
        "date": "2024-04-27T09:09:00.000Z",
        "voteCount": 1,
        "content": "C merge"
      },
      {
        "date": "2024-01-03T15:55:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C"
      },
      {
        "date": "2023-11-07T06:04:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-10-19T02:52:00.000Z",
        "voteCount": 1,
        "content": "Merge is correct"
      },
      {
        "date": "2023-10-08T23:18:00.000Z",
        "voteCount": 2,
        "content": "MERGE INTO is the one to choose if you want to avoid duplicates."
      },
      {
        "date": "2023-09-28T23:54:00.000Z",
        "voteCount": 1,
        "content": "Merge is correct"
      },
      {
        "date": "2023-09-25T21:16:00.000Z",
        "voteCount": 1,
        "content": "Merge will avoid duplicates by comparing the results based on primary key columns"
      },
      {
        "date": "2023-09-03T13:46:00.000Z",
        "voteCount": 3,
        "content": "C. MERGE\n\nThe MERGE command is used to write data into a Delta table while avoiding the writing of duplicate records. It allows you to perform an \"upsert\" operation, which means that it will insert new records and update existing records in the Delta table based on a specified condition. This helps maintain data integrity and avoid duplicates when adding new data to the table."
      },
      {
        "date": "2023-07-07T15:06:00.000Z",
        "voteCount": 2,
        "content": "C. MERGE\n\nTo write data into a Delta table while avoiding the writing of duplicate records, you can use the MERGE command. The MERGE command in Delta Lake allows you to combine the ability to insert new records and update existing records in a single atomic operation.\n\nThe MERGE command compares the data being written with the existing data in the Delta table based on specified matching criteria, typically using a primary key or unique identifier. It then performs conditional actions, such as inserting new records or updating existing records, depending on the comparison results.\n\nBy using the MERGE command, you can handle the prevention of duplicate records in a more controlled and efficient manner. It allows you to synchronize and reconcile data from different sources while avoiding duplication and ensuring data integrity.\n\nTherefore, option C, MERGE, is the correct command to use when writing data into a Delta table while avoiding the writing of duplicate records."
      },
      {
        "date": "2023-05-04T02:54:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. AS DROP is used to remove a table or database\nIGNORE is used to skip errors while executing a query.\nINSERT will add new records but will not avoid duplication so Merge is right answer"
      },
      {
        "date": "2023-04-20T23:17:00.000Z",
        "voteCount": 2,
        "content": "Ans - C\nhttps://docs.databricks.com/sql/language-manual/delta-merge-into.html"
      },
      {
        "date": "2023-04-20T12:12:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-04-04T10:32:00.000Z",
        "voteCount": 1,
        "content": "Wrong answer. The correct answer is D."
      },
      {
        "date": "2023-04-05T04:53:00.000Z",
        "voteCount": 4,
        "content": "'C' is a correct answer. https://docs.databricks.com/sql/language-manual/delta-merge-into.html"
      },
      {
        "date": "2023-04-05T08:25:00.000Z",
        "voteCount": 1,
        "content": "Thanks for the clarification"
      },
      {
        "date": "2023-04-02T07:29:00.000Z",
        "voteCount": 3,
        "content": "la unica opcion posible"
      },
      {
        "date": "2023-04-03T17:19:00.000Z",
        "voteCount": 1,
        "content": "Respuesta correcta C\nA) DROP: Elimina registros, B) IGNORE : NO existe C) MERGE: EN base a la data, registra, actualiza o elimina registros, D) NO existe E) Solo inserta"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/databricks/view/104636-exam-certified-data-engineer-associate-topic-1-question-17/",
    "body": "A data engineer needs to apply custom logic to string column city in table stores for a specific use case. In order to apply this custom logic at scale, the data engineer wants to create a SQL user-defined function (UDF).<br>Which of the following code blocks creates this SQL UDF?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image2\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image2.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image3\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image3.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image4\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image4.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image5\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image5.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image6\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image6.png\">"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-31T05:05:00.000Z",
        "voteCount": 14,
        "content": "E is wrong, the right answer is A.\nhttps://www.databricks.com/blog/2021/10/20/introducing-sql-user-defined-functions.html"
      },
      {
        "date": "2023-04-01T09:39:00.000Z",
        "voteCount": 7,
        "content": "The answer E is incorrect. A user defined function is never written as CREATE UDF. The correct way is CREATE FUNCTION. So that leaves us with the choices A and D. Out of that, in D, there is no such thing as RETURN CASE so the correct answer is A."
      },
      {
        "date": "2023-05-17T08:03:00.000Z",
        "voteCount": 4,
        "content": "Both A and D use RETURN CASE"
      },
      {
        "date": "2024-09-05T02:59:00.000Z",
        "voteCount": 1,
        "content": "A and D"
      },
      {
        "date": "2024-07-08T13:43:00.000Z",
        "voteCount": 1,
        "content": "A is the correct syntax for creating a UDF in Databricks."
      },
      {
        "date": "2024-04-27T09:10:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-03-16T04:53:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/udf/index.html#language-sql\n\nAnswer E is not correct when having CREATE UDF rather than CREATE FUNCTION"
      },
      {
        "date": "2024-02-10T11:26:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2024-01-31T22:43:00.000Z",
        "voteCount": 3,
        "content": "D, should be the answer. The fucntion is not returning STRING  , it is applied on a string column"
      },
      {
        "date": "2024-01-03T15:59:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A"
      },
      {
        "date": "2023-11-14T21:51:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is A. It is not E. First, you do not need to specify \"UDF\" in the syntax. You need to specify the return type and then what you want to return, usually your processed output. \nCREATE FUNCTION myUDF(udf STRING) \nRETURNS STRING \n&lt;...udf....&gt;"
      },
      {
        "date": "2023-10-16T00:54:00.000Z",
        "voteCount": 1,
        "content": "Create Function Return String"
      },
      {
        "date": "2023-10-09T05:23:00.000Z",
        "voteCount": 1,
        "content": "Correct is A"
      },
      {
        "date": "2023-10-08T23:20:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer.\nThe template to use is the following:\nCREATE FUNCTION &lt;name_function&gt; (&lt;function_parameter&gt;, ..) RETURNS &lt;return_type&gt;\n&lt;body&gt;"
      },
      {
        "date": "2023-09-28T02:52:00.000Z",
        "voteCount": 1,
        "content": "First need create a function and then need to register it as UDF"
      },
      {
        "date": "2023-09-25T21:16:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-09-25T01:16:00.000Z",
        "voteCount": 1,
        "content": "no UDF only FUNCTION needed."
      },
      {
        "date": "2023-07-18T19:10:00.000Z",
        "voteCount": 1,
        "content": "CORRECT ANSWER IS : A \nhttps://www.databricks.com/blog/2021/10/20/introducing-sql-user-defined-functions.html"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/databricks/view/105035-exam-certified-data-engineer-associate-topic-1-question-18/",
    "body": "A data analyst has a series of queries in a SQL program. The data analyst wants this program to run every day. They only want the final query in the program to run on Sundays. They ask for help from the data engineering team to complete this task.<br>Which of the following approaches could be used by the data engineering team to complete this task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey could submit a feature request with Databricks to add this functionality.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey could wrap the queries using PySpark and use Python\u2019s control flow system to determine when to run the final query.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey could only run the entire program on Sundays.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey could automatically restrict access to the source table in the final query so that it is only accessible on Sundays.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey could redesign the data model to separate the data used in the final query into a new table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-07T15:15:00.000Z",
        "voteCount": 11,
        "content": "B\n\nThe answer is B. \n\nOption A: Submitting a feature request with Databricks to add this functionality is not a feasible solution because it would require Databricks to implement new functionality.\n\nOption C: Only running the entire program on Sundays would be inconvenient for the data analyst because they would have to remember to run the program on Sundays.\n\nOption D: Automatically restricting access to the source table in the final query so that it is only accessible on Sundays would be difficult to implement and would not be a reliable solution.\n\nOption E: Redesigning the data model to separate the data used in the final query into a new table would be a major undertaking and would not be a feasible solution for this specific problem.\n\nTherefore, the only feasible solution to the problem is to wrap the queries using PySpark and use Python's control flow system to determine when to run the final query.\npython code"
      },
      {
        "date": "2024-05-31T00:45:00.000Z",
        "voteCount": 1,
        "content": "Also to add for option C is:\nthe requirement says: data analysts wants to run the sql program every day, but only the final query to run on sundays. So the option c violates the requirements"
      },
      {
        "date": "2024-08-08T02:12:00.000Z",
        "voteCount": 1,
        "content": "B. They could wrap the queries using PySpark and use Python\u2019s control flow system to determine when to run the final query.\n\nThis approach involves using PySpark to control the execution flow based on the day of the week, allowing the final query to execute conditionally while the other queries run daily."
      },
      {
        "date": "2024-01-03T16:02:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2023-10-09T05:24:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-09-25T21:17:00.000Z",
        "voteCount": 1,
        "content": "B is correct ans"
      },
      {
        "date": "2023-09-25T01:17:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2023-07-07T07:43:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-06-06T16:45:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-04-05T08:26:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2023-04-04T03:51:00.000Z",
        "voteCount": 1,
        "content": "i think the answer is correct"
      },
      {
        "date": "2023-04-03T17:49:00.000Z",
        "voteCount": 1,
        "content": "Respuesta Correcta es B"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/databricks/view/105038-exam-certified-data-engineer-associate-topic-1-question-19/",
    "body": "A data engineer runs a statement every day to copy the previous day\u2019s sales into the table transactions. Each day\u2019s sales are in their own file in the location \"/transactions/raw\".<br>Today, the data engineer runs the following command to complete this task:<br><img title=\"image7\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image7.png\"><br>After running the command today, the data engineer notices that the number of records in table transactions has not changed.<br>Which of the following describes why the statement might not have copied any new records into the table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe format of the files to be copied were not included with the FORMAT_OPTIONS keyword.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe names of the files to be copied were not included with the FILES keyword.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe previous day\u2019s file has already been copied into the table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe PARQUET file format does not support COPY INTO.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe COPY INTO statement requires the table to be refreshed to view the copied rows."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-27T00:04:00.000Z",
        "voteCount": 4,
        "content": "Just got 100% on the test. C was correct."
      },
      {
        "date": "2024-01-03T16:05:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C"
      },
      {
        "date": "2023-12-29T14:11:00.000Z",
        "voteCount": 3,
        "content": "C. The previous day\u2019s file has already been copied into the table.\n\nThe COPY INTO statement is generally used to copy data from files or a location into a table. If the data engineer runs this statement daily to copy the previous day\u2019s sales into the \"transactions\" table and the number of records hasn't changed after today's execution, it's possible that the data from today's file might not have differed from the data already present in the table.\n\nIf the files in the \"/transactions/raw\" location are expected to contain distinct data for each day and the number of records in the table remains the same, it implies that the data engineer might have already copied today's data previously, or today's data was identical to the data already present in the table.\n\nOptions A, B, D, and E don't accurately explain why the statement might not have copied new records into the table based on the provided scenario."
      },
      {
        "date": "2023-11-07T06:25:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2023-10-15T05:03:00.000Z",
        "voteCount": 1,
        "content": "If the table \"transaction\" is an external table, then option E, if its internal C should suffice."
      },
      {
        "date": "2023-10-09T00:06:00.000Z",
        "voteCount": 1,
        "content": "COPY INTO statement does skip already copied rows."
      },
      {
        "date": "2023-09-25T21:18:00.000Z",
        "voteCount": 1,
        "content": "C is correct ans"
      },
      {
        "date": "2023-09-24T06:12:00.000Z",
        "voteCount": 4,
        "content": "E is the correct answer, because immediately after using copy into you might query the cashed version of the table."
      },
      {
        "date": "2023-08-18T02:23:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/ingestion/copy-into/index.html \n\nThe COPY INTO SQL command lets you load data from a file location into a Delta table. This is a re-triable and idempotent operation; files in the source location that have already been loaded are skipped.\n\nif there are no new records, the only consistent choice is C no new files were loaded because already loaded files were skipped."
      },
      {
        "date": "2023-07-07T15:28:00.000Z",
        "voteCount": 1,
        "content": "C\nThe COPY INTO statement copies the data from the specified files into the target table. If the previous day's file has already been copied into the table, then the COPY INTO statement will not copy any new records into the table."
      },
      {
        "date": "2023-05-28T23:36:00.000Z",
        "voteCount": 1,
        "content": "COPY INTO\nLoads data from a file location into a Delta table. This is a retriable and idempotent operation\u2014files in the source location that have already been loaded are skipped."
      },
      {
        "date": "2023-05-17T21:23:00.000Z",
        "voteCount": 1,
        "content": "Answer: B\nFILES = ('f1.json', 'f2.json', 'f3.json', 'f4.json', 'f5.json')\nhttps://docs.databricks.com/ingestion/copy-into/examples.html"
      },
      {
        "date": "2023-05-23T17:43:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is letter C. \nThe use of specific files names with keyword \"FILES\"  is optional as the syntax of COPY INTO declares:\n [ FILES = ( file_name [, ...] ) | PATTERN = glob_pattern ]\nWhen keyword FILES is not used in the statement all files of the directory is used once (because this operation is idempotent)."
      },
      {
        "date": "2023-04-20T23:48:00.000Z",
        "voteCount": 1,
        "content": "C-\n\nhttps://docs.databricks.com/ingestion/copy-into/tutorial-notebook.html\nBecause this action is idempotent, you can run it multiple times but data will only be loaded once."
      },
      {
        "date": "2023-04-04T10:35:00.000Z",
        "voteCount": 3,
        "content": "Option C is the correct answer."
      },
      {
        "date": "2023-04-04T03:56:00.000Z",
        "voteCount": 1,
        "content": "i am not sure whether C is the correct answer, but A is definitely not right"
      },
      {
        "date": "2023-04-03T22:56:00.000Z",
        "voteCount": 1,
        "content": "option C"
      },
      {
        "date": "2023-04-03T18:03:00.000Z",
        "voteCount": 2,
        "content": "Respuesta C, por descarte, A) No es necesario B) No se coloca FILES D) PARQUET si es soportado E) No es necesario refrescar la vista, ya que se esta copiando un archivo"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/databricks/view/104862-exam-certified-data-engineer-associate-topic-1-question-20/",
    "body": "A data engineer needs to create a table in Databricks using data from their organization\u2019s existing SQLite database.<br>They run the following command:<br><img title=\"image8\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image8.png\"><br>Which of the following lines of code fills in the above blank to successfully complete the task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\torg.apache.spark.sql.jdbc\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tautoloader",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDELTA",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsqlite",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\torg.apache.spark.sql.sqlite"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-02T10:32:00.000Z",
        "voteCount": 8,
        "content": "A is correct"
      },
      {
        "date": "2024-08-27T12:02:00.000Z",
        "voteCount": 2,
        "content": "Nobody mentioned this, but the big hint in this question is the url, which has a \"jdbc:\" url prefix.  Hence, a JDBC type driver is required here."
      },
      {
        "date": "2024-07-16T22:59:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-04-27T09:22:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-01-03T16:06:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A"
      },
      {
        "date": "2023-11-14T21:56:00.000Z",
        "voteCount": 2,
        "content": "I think the correct answer is A. All that is missing the the jdbc drive. org.apache.spark.sql.jdbc"
      },
      {
        "date": "2023-09-29T00:04:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-09-25T21:21:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-06-20T04:28:00.000Z",
        "voteCount": 1,
        "content": "must be \"USING JDBC\", there is no such thing as \"USING org.apache.spark.sql.jdbc\". https://docs.databricks.com/external-data/jdbc.html#language-sql"
      },
      {
        "date": "2023-06-20T04:47:00.000Z",
        "voteCount": 4,
        "content": "I correct myself https://docs.yugabyte.com/preview/integrations/apache-spark/spark-sql/"
      },
      {
        "date": "2023-05-03T19:44:00.000Z",
        "voteCount": 2,
        "content": "To specify the JDBC driver and other options, the using clause should be followed by the fully qualified name of the JDBC data source, which is org.apache.spark.sql.jdbc."
      },
      {
        "date": "2023-04-20T23:55:00.000Z",
        "voteCount": 1,
        "content": "Answer A -\nCREATE TABLE new_employees_table\n  USING JDBC\nOPTIONS (\n  url \"&lt;jdbc_url&gt;\",\n  dbtable \"&lt;table_name&gt;\",\n  user '&lt;username&gt;',\n  password '&lt;password&gt;'\n) AS\nSELECT * FROM employees_table_vw"
      },
      {
        "date": "2023-04-20T12:19:00.000Z",
        "voteCount": 1,
        "content": "JDBC - Option A"
      },
      {
        "date": "2023-04-04T10:37:00.000Z",
        "voteCount": 2,
        "content": "Option A is correct answer"
      },
      {
        "date": "2023-04-03T22:56:00.000Z",
        "voteCount": 2,
        "content": "option A"
      },
      {
        "date": "2023-04-03T21:28:00.000Z",
        "voteCount": 1,
        "content": "option A"
      },
      {
        "date": "2023-04-03T18:08:00.000Z",
        "voteCount": 1,
        "content": "Es JDBC osea la A, pregunta con truco para confundir"
      },
      {
        "date": "2023-04-02T07:12:00.000Z",
        "voteCount": 3,
        "content": "es JDBC"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/databricks/view/104760-exam-certified-data-engineer-associate-topic-1-question-21/",
    "body": "A data engineering team has two tables. The first table march_transactions is a collection of all retail transactions in the month of March. The second table april_transactions is a collection of all retail transactions in the month of April. There are no duplicate records between the tables.<br>Which of the following commands should be run to create a new table all_transactions that contains all records from march_transactions and april_transactions without duplicate records?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE TABLE all_transactions AS<br>SELECT * FROM march_transactions<br>INNER JOIN SELECT * FROM april_transactions;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE TABLE all_transactions AS<br>SELECT * FROM march_transactions<br>UNION SELECT * FROM april_transactions;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE TABLE all_transactions AS<br>SELECT * FROM march_transactions<br>OUTER JOIN SELECT * FROM april_transactions;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE TABLE all_transactions AS<br>SELECT * FROM march_transactions<br>INTERSECT SELECT * from april_transactions;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE TABLE all_transactions AS<br>SELECT * FROM march_transactions<br>MERGE SELECT * FROM april_transactions;"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-08T03:07:00.000Z",
        "voteCount": 1,
        "content": "B. CREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nUNION SELECT * FROM april_transactions;"
      },
      {
        "date": "2024-01-04T00:53:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-11-07T06:28:00.000Z",
        "voteCount": 1,
        "content": "Correct: B"
      },
      {
        "date": "2023-09-18T00:49:00.000Z",
        "voteCount": 4,
        "content": "UNION [ALL | DISTINCT]\n\nReturns the result of subquery1 plus the rows of subquery2`.\n\nIf ALL is specified duplicate rows are preserved.\n\nIf DISTINCT is specified the result does not contain any duplicate rows. This is the default.\nhttps://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-setops.html#examples"
      },
      {
        "date": "2023-09-03T15:05:00.000Z",
        "voteCount": 1,
        "content": "B. CREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nUNION SELECT * FROM april_transactions;\n\nTo create a new table all_transactions that contains all records from march_transactions and april_transactions without duplicate records, you should use the UNION operator, as shown in option B. This operator combines the result sets of the two tables while automatically removing duplicate records."
      },
      {
        "date": "2023-07-07T19:33:00.000Z",
        "voteCount": 1,
        "content": "B\nCREATE TABLE all_transactions AS\nSELECT * FROM march_transactions\nUNION\nSELECT * FROM april_transactions;"
      },
      {
        "date": "2023-05-13T07:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is B."
      },
      {
        "date": "2023-04-03T21:30:00.000Z",
        "voteCount": 1,
        "content": "option B"
      },
      {
        "date": "2023-04-01T09:54:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/databricks/view/105262-exam-certified-data-engineer-associate-topic-1-question-22/",
    "body": "A data engineer only wants to execute the final block of a Python program if the Python variable day_of_week is equal to 1 and the Python variable review_period is True.<br>Which of the following control flow statements should the data engineer use to begin this conditionally executed code block?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tif day_of_week = 1 and review_period:",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tif day_of_week = 1 and review_period = \"True\":",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tif day_of_week == 1 and review_period == \"True\":",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tif day_of_week == 1 and review_period:\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tif day_of_week = 1 &amp; review_period: = \"True\":"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-05T03:41:00.000Z",
        "voteCount": 16,
        "content": "The correct control flow statement to begin the conditionally executed code block would be D. if day_of_week == 1 and review_period:.\n\nThis statement will check if the variable day_of_week is equal to 1 and if the variable review_period evaluates to a truthy value. The use of the double equal sign (==) in the comparison of day_of_week is important, as a single equal sign (=) would be used to assign a value to the variable instead of checking its value. The use of a single ampersand (&amp;) instead of the keyword and is not valid syntax in Python. The use of quotes around True in options B and C will result in a string comparison, which will not evaluate to True even if the value of review_period is True."
      },
      {
        "date": "2024-07-08T13:58:00.000Z",
        "voteCount": 1,
        "content": "You need the == to use the \"equals\" operation. A single \"=\" is an assignment operation."
      },
      {
        "date": "2024-03-04T01:13:00.000Z",
        "voteCount": 2,
        "content": "C fits if you're looking for a string == 'True', in this case you are using a boolean so D"
      },
      {
        "date": "2024-01-04T00:57:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-12-29T16:29:00.000Z",
        "voteCount": 1,
        "content": "D. if day_of_week == 1 and review_period:\n\n- In Python, the equality comparison operator is ==, not =. == is used to check if two values are equal.\n- The logical operator \"and\" is used to combine two conditions, ensuring that both conditions (day_of_week == 1 and review_period) are true for the subsequent code block to execute.\n- day_of_week == 1 checks if the variable day_of_week is equal to the integer value 1.\n- review_period is already assumed to be a Boolean variable since it is stated to be True (without quotes) in the question. Therefore, it should not be compared to a string \"True\".\n\nTherefore, option D correctly represents the condition for executing the final block of the Python program based on the given conditions."
      },
      {
        "date": "2023-11-07T06:34:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-10-16T01:37:00.000Z",
        "voteCount": 1,
        "content": "review_period == \"true\" is different from review_period == true"
      },
      {
        "date": "2023-09-03T15:06:00.000Z",
        "voteCount": 1,
        "content": "D. if day_of_week == 1 and review_period:\n\nThe correct control flow statement to begin the conditionally executed code block is option D. In Python, the == operator is used for equality comparison, and and is used for logical \"and\" operations. So, this statement checks if day_of_week is equal to 1 and review_period is True (a boolean value), which is the correct way to express the conditions you mentioned."
      },
      {
        "date": "2023-08-29T09:39:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2023-07-07T19:35:00.000Z",
        "voteCount": 1,
        "content": "D\nif day_of_week == 1 and review_period:"
      },
      {
        "date": "2023-05-13T05:20:00.000Z",
        "voteCount": 2,
        "content": "in python value comparison is done by double equal signs (==). in case of boolean values that are TRUE these may be omitted. Quotes around True would result in string comparison and here we are comparing to a bool value."
      },
      {
        "date": "2023-05-10T10:47:00.000Z",
        "voteCount": 1,
        "content": "Answer is 'D'\n\nday_of_week=1\nreview_period = True\n\n1)\nif day_of_week == 1 and review_period:\n    print(\"yes\")\n\noutput:\nAbove code block's output is yes\n    \n2)\nif day_of_week == 1 and review_period == \"True\":\n    print(\"yes\")\n    \noutput:\nThere is no output for above code block"
      },
      {
        "date": "2023-05-03T21:14:00.000Z",
        "voteCount": 1,
        "content": "The data engineer should use option D: if day_of_week == 1 and review_period:. This statement checks if the variable day_of_week is equal to 1 and if the variable review_period is True. It uses the double equal sign (==) to compare the values of the variables, and does not use quotes around the keyword True, which is a boolean value."
      },
      {
        "date": "2023-04-10T12:19:00.000Z",
        "voteCount": 2,
        "content": "option D"
      },
      {
        "date": "2023-04-05T08:29:00.000Z",
        "voteCount": 1,
        "content": "I believe the right answer is C"
      },
      {
        "date": "2023-06-20T04:59:00.000Z",
        "voteCount": 3,
        "content": "It's not C. Conditional check of \"True\" is treated as a string and not Boolean. Hence D is the right answer"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/databricks/view/104761-exam-certified-data-engineer-associate-topic-1-question-23/",
    "body": "A data engineer is attempting to drop a Spark SQL table my_table. The data engineer wants to delete all table metadata and data.<br>They run the following command:<br><br>DROP TABLE IF EXISTS my_table -<br>While the object no longer appears when they run SHOW TABLES, the data files still exist.<br>Which of the following describes why the data files still exist and the metadata files were deleted?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table\u2019s data was larger than 10 GB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table\u2019s data was smaller than 10 GB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table was external\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table did not have a location",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table was managed"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-08T03:14:00.000Z",
        "voteCount": 1,
        "content": "C. The table was external\n\nWhen dropping an external table in Spark SQL, only the metadata is removed. The actual data files remain in their original location because they are not managed by Spark but by the external source."
      },
      {
        "date": "2024-01-04T00:59:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-10-20T13:43:00.000Z",
        "voteCount": 1,
        "content": "THE QUESTION SHOULD BE \"Which of the following describes why the metadata files still exist and the data files were deleted?\""
      },
      {
        "date": "2023-09-03T15:09:00.000Z",
        "voteCount": 2,
        "content": "C. The table was external\n\nThe reason why the data files still exist while the metadata files were deleted is because the table was external. When a table is external in Spark SQL (or in other database systems), it means that the table metadata (such as schema information and table structure) is managed externally, and Spark SQL assumes that the data is managed and maintained outside of the system. Therefore, when you execute a DROP TABLE statement for an external table, it removes only the table metadata from the catalog, leaving the data files intact.\n\nOn the other hand, for managed tables (option E), Spark SQL manages both the metadata and the data files. When you drop a managed table, it deletes both the metadata and the associated data files, resulting in a complete removal of the table."
      },
      {
        "date": "2023-04-03T21:31:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2023-04-01T09:56:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer. For external tables, you need to go to the specific location using DESCRIBE EXTERNAL TABLE command and delete all files."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/databricks/view/104762-exam-certified-data-engineer-associate-topic-1-question-24/",
    "body": "A data engineer wants to create a data entity from a couple of tables. The data entity must be used by other data engineers in other sessions. It also must be saved to a physical location.<br>Which of the following data entities should the data engineer create?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabase",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFunction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTemporary view",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T10:53:00.000Z",
        "voteCount": 29,
        "content": "Questions says :  \n1. The data entity must be used by other data engineers in other sessions.\n2.  It also must be saved to a physical location.\n\nHere View doesn't store data in physical location , from the options only table stores data in physical location \n\nSo answer should be  'E' which is Table."
      },
      {
        "date": "2024-01-27T00:06:00.000Z",
        "voteCount": 8,
        "content": "Just got 100% in the exam. Table was a correct answer."
      },
      {
        "date": "2024-01-29T11:44:00.000Z",
        "voteCount": 1,
        "content": "Wow! Congratz!"
      },
      {
        "date": "2024-08-08T03:19:00.000Z",
        "voteCount": 1,
        "content": "E. Table\n\nCreating a table ensures that the data entity is saved to a physical location and can be used by other data engineers in different sessions. Tables persist data and metadata, making them suitable for long-term storage and sharing across sessions."
      },
      {
        "date": "2024-07-08T13:59:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is \"E\". A view does not save to a physical location; only caching a SELECT statement."
      },
      {
        "date": "2024-04-28T01:25:00.000Z",
        "voteCount": 2,
        "content": "physical location means table"
      },
      {
        "date": "2024-01-31T23:30:00.000Z",
        "voteCount": 1,
        "content": "E. as view doesnt has any location"
      },
      {
        "date": "2024-01-04T01:02:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-12-29T16:36:00.000Z",
        "voteCount": 1,
        "content": "E. Table\n\nUsage by Other Sessions: Tables in a database are persistent data structures that can be accessed by multiple users and sessions concurrently.\n\nSaved to a Physical Location: Tables store data physically in a structured manner on disk or in a storage system, making them suitable for long-term storage.\n\nUsage by Other Data Engineers: Other data engineers can query, access, and work with the data within the table, making it a feasible choice for shared access among multiple users or sessions.\n\nWhile other entities like views or temporary views can provide different ways to represent or filter data, a table fits the criteria best when the data engineer requires a persistent physical storage entity accessible by other sessions and users for data manipulation, retrieval, and storage."
      },
      {
        "date": "2023-12-11T01:31:00.000Z",
        "voteCount": 3,
        "content": "I think the key to the answer is that it refers to the Data Entinty, and not to the data itself, when it mentions \"the Data Entity must be used by other Data Engineers\", and \"It must be saved to a physical location\". From this PoV, both C and E would be correct, however, creating a new table would incur in processing to a static state the relationship from \"a couple of tables\". While this make sense to many use cases, this would require either a Workflow or a DLT to make it work, which goes over the requested scope. C is the best answer for the requested scenario."
      },
      {
        "date": "2023-11-30T18:30:00.000Z",
        "voteCount": 2,
        "content": "Key point to remember during answering this question:\n \" It also must be saved to a physical location\"\n\n\nSo answer should be 'E' which is Table."
      },
      {
        "date": "2023-11-30T17:05:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer. View is a data entity and its definition is physically saved so other users can consume view"
      },
      {
        "date": "2023-11-15T09:36:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is E because it has to be physically saved. View is in memory."
      },
      {
        "date": "2023-11-07T06:40:00.000Z",
        "voteCount": 1,
        "content": "Correct : E"
      },
      {
        "date": "2023-09-03T15:10:00.000Z",
        "voteCount": 2,
        "content": "E. Table\n\nTo create a data entity that can be used by other data engineers in other sessions and must be saved to a physical location, you should create a table. Tables in a database are physical storage structures that hold data, and they can be accessed and shared by multiple users and sessions. By creating a table, you provide a permanent and structured storage location for the data entity that can be used across different sessions and by other users as needed.\n\nOptions like databases (A) can be used to organize tables, views (C) can provide virtual representations of data, and temporary views (D) are temporary in nature and don't save data to a physical location. Functions (B) are typically used for processing data or performing calculations, not for storing data."
      },
      {
        "date": "2023-08-29T09:40:00.000Z",
        "voteCount": 1,
        "content": "View does not have a physical location so answer has to be E"
      },
      {
        "date": "2023-08-05T04:52:00.000Z",
        "voteCount": 2,
        "content": "View Doesn't physical location"
      },
      {
        "date": "2023-07-14T04:37:00.000Z",
        "voteCount": 2,
        "content": "The answer is E: \"Table\"\n\nIn the context described, creating a \"Table\" is the most suitable choice. Tables in SQL are data entities that exist independently of any session and are saved in a physical location. They can be accessed and manipulated by other data engineers in different sessions, which aligns with the requirements stated.\n\nA \"Database\" is a collection of tables, views, and other database objects. A \"Function\" is a stored procedure that performs an operation. A \"View\" is a virtual table based on the result-set of an SQL statement, but it is not stored physically. A \"Temporary view\" is a feature that allows you to store the result of a query as a view that disappears once your session with the database is closed."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/databricks/view/104764-exam-certified-data-engineer-associate-topic-1-question-25/",
    "body": "A data engineer is maintaining a data pipeline. Upon data ingestion, the data engineer notices that the source data is starting to have a lower level of quality. The data engineer would like to automate the process of monitoring the quality level.<br>Which of the following tools can the data engineer use to solve this problem?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnity Catalog",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Explorer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Live Tables\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAuto Loader"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-01T10:05:00.000Z",
        "voteCount": 17,
        "content": "The answer is incorrect. The correct answer is Delta Live Tables or (C)\nhttps://docs.databricks.com/delta-live-tables/expectations.html"
      },
      {
        "date": "2023-06-12T02:31:00.000Z",
        "voteCount": 2,
        "content": "upon reading this i think you are right"
      },
      {
        "date": "2023-09-05T14:56:00.000Z",
        "voteCount": 7,
        "content": "Delta Live Tables is a declarative framework for building reliable, maintainable, and testable data processing pipelines. You define the transformations to perform on your data and Delta Live Tables manages task orchestration, cluster management, monitoring, data quality, and error handling.\n\nQuality is explicitly mentioned in the definition."
      },
      {
        "date": "2024-08-08T03:20:00.000Z",
        "voteCount": 2,
        "content": "D. Delta Live Tables\n\nDelta Live Tables provides features for automating data quality monitoring and ensuring that the data in the pipeline meets certain quality standards. It allows you to define expectations and monitor data quality as part of the data pipeline."
      },
      {
        "date": "2024-04-28T01:26:00.000Z",
        "voteCount": 1,
        "content": "delta live table"
      },
      {
        "date": "2024-01-04T05:06:00.000Z",
        "voteCount": 1,
        "content": "Correct is D"
      },
      {
        "date": "2023-11-07T06:45:00.000Z",
        "voteCount": 1,
        "content": "Correct: D"
      },
      {
        "date": "2023-11-07T06:44:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-09-03T15:34:00.000Z",
        "voteCount": 3,
        "content": "D. Delta Live Tables\n\nDelta Live Tables is a tool provided by Databricks that can help data engineers automate the monitoring of data quality. It is designed for managing data pipelines, monitoring data quality, and automating workflows. With Delta Live Tables, you can set up data quality checks and alerts to detect issues and anomalies in your data as it is ingested and processed in real-time. It provides a way to ensure that the data quality meets your desired standards and can trigger actions or notifications when issues are detected.\n\nWhile the other tools mentioned may have their own purposes in a data engineering environment, Delta Live Tables is specifically designed for data quality monitoring and automation within the Databricks ecosystem."
      },
      {
        "date": "2023-07-07T19:45:00.000Z",
        "voteCount": 1,
        "content": "D\nDelta Live Tables.\n\nDelta Live Tables is a tool that can be used to automate the process of monitoring the quality level of data in a data pipeline. Delta Live Tables provides a number of features that can be used to monitor data quality, including:\n\nData lineage: Delta Live Tables tracks the lineage of data as it flows through the data pipeline. This allows the data engineer to see where the data came from and how it has been transformed.\nData quality checks: Delta Live Tables allows the data engineer to define data quality checks that can be run on the data as it is ingested. These checks can be used to identify data that is not meeting the expected quality standards.\nAlerts: Delta Live Tables can be configured to send alerts when data quality checks fail. This allows the data engineer to be notified of potential problems with the data pipeline."
      },
      {
        "date": "2023-05-03T21:21:00.000Z",
        "voteCount": 1,
        "content": "The data engineer can use the Data Explorer tool to monitor the quality level of the ingested data. Data Explorer is a feature of Databricks that provides data profiling and data quality metrics to monitor the health of data pipelines."
      },
      {
        "date": "2023-05-09T18:43:00.000Z",
        "voteCount": 3,
        "content": "After reading docs and more investigation I think in the terms of managing the data quality D would be better answer"
      },
      {
        "date": "2023-04-05T03:49:00.000Z",
        "voteCount": 1,
        "content": "B. Data Explorer can be used to monitor the quality level of data. It provides an interactive interface to analyze the data and define quality rules to identify issues. Data Explorer also offers automated validation rules that can be used to monitor data quality over time."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/databricks/view/104767-exam-certified-data-engineer-associate-topic-1-question-26/",
    "body": "A Delta Live Table pipeline includes two datasets defined using STREAMING LIVE TABLE. Three datasets are defined against Delta Lake table sources using LIVE TABLE.<br>The table is configured to run in Production mode using the Continuous Pipeline Mode.<br>Assuming previously unprocessed data exists and all definitions are valid, what is the expected outcome after clicking Start to update the pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated once and the pipeline will persist without any processing. The compute resources will persist but go unused.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated once and the pipeline will shut down. The compute resources will be terminated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for additional testing."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-29T19:57:00.000Z",
        "voteCount": 8,
        "content": "C. All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped.\n\nExplanation:\n\nContinuous Pipeline Mode in Production mode implies that the pipeline continuously processes incoming data updates at set intervals, ensuring the datasets are kept up-to-date as new data arrives.\nSince the pipeline is set to Continuous Pipeline Mode, it will keep running and updating the datasets until it is manually shut down.\nThe compute resources are allocated dynamically to process and update the datasets as needed, and they will be terminated when the pipeline is stopped or shut down.\nThis mode allows for real-time or near-real-time updates to the datasets from the streaming/live tables, ensuring that the data remains current and reflects the changes occurring in the data sources."
      },
      {
        "date": "2024-08-08T03:25:00.000Z",
        "voteCount": 1,
        "content": "In Continuous Pipeline Mode, Delta Live Tables processes data continuously and updates datasets at regular intervals. Compute resources are used to handle these updates and are terminated when the pipeline is stopped."
      },
      {
        "date": "2024-07-08T14:02:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"C\"."
      },
      {
        "date": "2024-04-28T01:28:00.000Z",
        "voteCount": 1,
        "content": "daje gianluca"
      },
      {
        "date": "2024-01-04T05:08:00.000Z",
        "voteCount": 2,
        "content": "Correct is C"
      },
      {
        "date": "2023-11-07T06:50:00.000Z",
        "voteCount": 1,
        "content": "Correct : C"
      },
      {
        "date": "2023-09-03T15:36:00.000Z",
        "voteCount": 4,
        "content": "C. All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped.\n\nIn the scenario described:\n\nThe Delta Live Table pipeline is configured in Production mode, which means it will continuously process data using the Continuous Pipeline Mode.\nThere are both STREAMING LIVE TABLE datasets and LIVE TABLE datasets defined.\nWhen you click Start to update the pipeline in Continuous Pipeline Mode:\n\nAll datasets, including both STREAMING LIVE TABLE and LIVE TABLE datasets, will be updated at set intervals.\nCompute resources will be deployed for the update, ensuring that the pipeline processes data.\nThe compute resources will be terminated when the pipeline is stopped or shut down.\nThis setup allows for continuous data processing while efficiently managing compute resources, and the pipeline can be stopped when no longer needed."
      },
      {
        "date": "2023-08-22T08:28:00.000Z",
        "voteCount": 1,
        "content": "All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped."
      },
      {
        "date": "2023-07-31T06:41:00.000Z",
        "voteCount": 3,
        "content": "No answer is correct. Prod Continuous mode processes data at set intervals until pipe is shutdown. However, compute must be always-on and will not terminate. https://docs.databricks.com/delta-live-tables/updates.html#continuous-triggered"
      },
      {
        "date": "2023-08-22T01:53:00.000Z",
        "voteCount": 1,
        "content": "You could be correct:\n\"Triggered pipelines can reduce resource consumption and expense since the cluster runs only long enough to execute the pipeline. However, new data won\u2019t be processed until the pipeline is triggered. Continuous pipelines require an always-running cluster, which is more expensive but reduces processing latency.\""
      },
      {
        "date": "2023-08-22T01:54:00.000Z",
        "voteCount": 2,
        "content": "Actually it could make A the correct answer?"
      },
      {
        "date": "2023-07-07T19:56:00.000Z",
        "voteCount": 2,
        "content": "C. \nAll datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped.\n\nIn a Delta Live Table pipeline running in Continuous Pipeline Mode, when you click Start to update the pipeline, the following outcome is expected:\n\nAll datasets defined using STREAMING LIVE TABLE and LIVE TABLE against Delta Lake table sources will be updated at set intervals.\nThe compute resources will be deployed for the update process and will be active during the execution of the pipeline.\nThe compute resources will be terminated when the pipeline is stopped or shut down.\nThis mode allows for continuous and periodic updates to the datasets as new data arrives or changes in the underlying Delta Lake tables occur. The compute resources are provisioned and utilized during the update intervals to process the data and perform the necessary operations."
      },
      {
        "date": "2023-06-01T06:14:00.000Z",
        "voteCount": 3,
        "content": "Answer: C"
      },
      {
        "date": "2023-05-03T02:23:00.000Z",
        "voteCount": 2,
        "content": "Answer: C\nPipeline mode - This specifies how the pipeline will be run. Choose the mode based on latency and cost requirements. \n* Triggered pipelines run once and then shut down until the next manual or scheduled update. \n* Continuous pipelines run continuously, ingesting new data as it arrives."
      },
      {
        "date": "2023-04-18T10:00:00.000Z",
        "voteCount": 3,
        "content": "Answer: D\nOfficial Databricks practice exam with answers - question 36"
      },
      {
        "date": "2023-04-19T02:25:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer is C. That question in the practice test is using a Triggered Pipeline. This question is using a Continuous."
      },
      {
        "date": "2023-04-21T09:43:00.000Z",
        "voteCount": 3,
        "content": "Yes, You're right. Thanks."
      },
      {
        "date": "2023-04-01T10:12:00.000Z",
        "voteCount": 3,
        "content": "E is not the right answer. The correct answer is C\nhttps://www.databricks.com/product/delta-live-tables"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/databricks/view/104770-exam-certified-data-engineer-associate-topic-1-question-27/",
    "body": "In order for Structured Streaming to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing, which of the following two approaches is used by Spark to record the offset range of the data being processed in each trigger?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheckpointing and Write-ahead Logs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming cannot record the offset range of the data being processed in each trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplayable Sources and Idempotent Sinks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite-ahead Logs and Idempotent Sinks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheckpointing and Idempotent Sinks"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-27T06:20:00.000Z",
        "voteCount": 3,
        "content": "Why the correct answer is E. Checkpointing and Idempotent Sinks:\nCheckpointing: Spark Structured Streaming uses checkpointing to track the state of the data being processed. Checkpoints allow the system to restart processing from where it left off in case of failure, ensuring reliability.\nIdempotent Sinks: Idempotent sinks ensure that reprocessing the same data multiple times (in case of a failure or restart) doesn\u2019t lead to duplicate results. The sink can handle repeated writes of the same data without issues.\nWhy A. Checkpointing and Write-ahead Logs is incorrect:\nSpark Structured Streaming does not use Write-ahead Logs (WAL) for tracking offsets or ensuring fault tolerance. While WALs are used in some systems for durability, Spark Structured Streaming relies on checkpointing and the concept of idempotent operations to ensure consistency and fault tolerance."
      },
      {
        "date": "2024-09-01T07:15:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is:\nE. Checkpointing and Idempotent Sinks\nIn Structured Streaming, Spark uses checkpointing to reliably track the progress of the streaming data. Checkpointing saves the state of the streaming computation to a reliable storage system. \nIdempotent sinks ensure that even if data is reprocessed, the results remain consistent and correct, preventing duplicate data from being written."
      },
      {
        "date": "2024-08-08T03:28:00.000Z",
        "voteCount": 1,
        "content": "Checkpointing: Spark saves metadata, including offsets, in a checkpoint directory, allowing it to recover from failures by replaying data starting from the last checkpoint.\nWrite-ahead Logs (WAL): Spark writes information about the data being processed to a log before the data is written to the sink. This ensures that even if a failure occurs, Spark can recover and reprocess the data from the log."
      },
      {
        "date": "2024-07-08T14:03:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer."
      },
      {
        "date": "2024-05-14T09:05:00.000Z",
        "voteCount": 2,
        "content": "The answer is A\n\n\"Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine. ... Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs.\" - Apache Spark Structured Streaming Programming Guide"
      },
      {
        "date": "2024-05-05T23:09:00.000Z",
        "voteCount": 1,
        "content": "Nice information and i hope best [url=https://keensolution.in/data-visualization-services/]Data visualization agencies in India[/url]"
      },
      {
        "date": "2024-04-30T05:03:00.000Z",
        "voteCount": 1,
        "content": "The answer is Checkpointing and idempotent sinks (E)\nHow does structured streaming achieves end to end fault tolerance:\n\u2022\tFirst, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval.\n\u2022\tNext, the streaming sinks are designed to be _idempotent_\u2014that is, multiple writes of the same data (as identified by the offset) do not result in duplicates being written to the sink.\nTaken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure end-to-end, exactly-once semantics under any failure condition"
      },
      {
        "date": "2024-04-28T01:32:00.000Z",
        "voteCount": 2,
        "content": "1 checkpointing and write ahead logs to record the offset range of data being processed \n2 checkpointing and idempotent sinks achieve end to end fault tolerance"
      },
      {
        "date": "2024-01-04T05:10:00.000Z",
        "voteCount": 1,
        "content": "Correct is A"
      },
      {
        "date": "2023-09-07T06:26:00.000Z",
        "voteCount": 3,
        "content": "The answer is Checkpointing and idempotent sinks\n\n\n\nHow does structured streaming achieves end to end fault tolerance:\n\nFirst, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval.\n\nNext, the streaming sinks are designed to be _idempotent_\u2014that is, multiple writes of the same data (as identified by the offset) do not result in duplicates being written to the sink.\n\nTaken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure end-to-end, exactly-once semantics under any failure condition."
      },
      {
        "date": "2023-09-03T15:54:00.000Z",
        "voteCount": 2,
        "content": "A. Checkpointing and Write-ahead Logs\n\nTo reliably track the exact progress of processing and handle failures in Spark Structured Streaming, Spark uses both checkpointing and write-ahead logs. Checkpointing allows Spark to periodically save the state of the streaming application to a reliable distributed file system, which can be used for recovery in case of failures. Write-ahead logs are used to record the offset range of data being processed, ensuring that the system can recover and reprocess data from the last known offset in the event of a failure."
      },
      {
        "date": "2023-07-19T20:24:00.000Z",
        "voteCount": 2,
        "content": "A:   \nThe engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger.\n-- in the link search for \"The engine uses \" youll find the answer.\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#:~:text=The%20engine%20uses%20checkpointing%20and,being%20processed%20in%20each%20trigger."
      },
      {
        "date": "2023-07-08T06:37:00.000Z",
        "voteCount": 3,
        "content": "A. Checkpointing and Write-ahead Logs.\n\nCheckpointing is a process of periodically saving the state of the streaming computation to a durable storage system. This ensures that if the streaming computation fails, it can be restarted from the last checkpoint and resume processing from where it left off.\nWrite-ahead logs are a type of log that records all changes made to a dataset. This allows Structured Streaming to recover from failures by replaying the write-ahead logs from the last checkpoint."
      },
      {
        "date": "2023-06-12T02:53:00.000Z",
        "voteCount": 2,
        "content": "why i think both A E are correct? https://learn.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-streaming-exactly-once#:~:text=Use%20idempotent%20sinks"
      },
      {
        "date": "2023-06-13T13:18:00.000Z",
        "voteCount": 5,
        "content": "spark handle streaming failure through:\n1. track the progress/offset(This is option A)\n2. fix failure(This is option E)\nBut the question is \"two approaches ... record the offset range\"\nTherefore, A"
      },
      {
        "date": "2023-06-01T06:16:00.000Z",
        "voteCount": 3,
        "content": "Answer is A:\nThe engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure end-to-end exactly-once semantics under any failure.\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#:~:text=The%20engine%20uses%20checkpointing%20and,being%20processed%20in%20each%20trigger."
      },
      {
        "date": "2023-05-13T05:35:00.000Z",
        "voteCount": 2,
        "content": "Answer is A.\nFrom Spark documentation: Every streaming source is assumed to have offsets to track the read position in the stream. The engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger."
      },
      {
        "date": "2023-05-03T21:31:00.000Z",
        "voteCount": 1,
        "content": "E. Checkpointing and Idempotent Sinks are the two approaches used by Spark to record the offset range of the data being processed in each trigger, enabling Structured Streaming to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing. Checkpointing periodically checkpoints the state of the streaming query to a fault-tolerant storage system, while idempotent sinks ensure that data can be written multiple times to the sink without affecting the final result."
      },
      {
        "date": "2023-05-03T21:36:00.000Z",
        "voteCount": 1,
        "content": "Answer is A:\nThe engine uses checkpointing and write-ahead logs to record the offset range of the data being processed in each trigger. The streaming sinks are designed to be idempotent for handling reprocessing. Together, using replayable sources and idempotent sinks, Structured Streaming can ensure end-to-end exactly-once semantics under any failure.\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#:~:text=The%20engine%20uses%20checkpointing%20and,being%20processed%20in%20each%20trigger."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/databricks/view/104772-exam-certified-data-engineer-associate-topic-1-question-28/",
    "body": "Which of the following describes the relationship between Gold tables and Silver tables?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGold tables are more likely to contain aggregations than Silver tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGold tables are more likely to contain valuable data than Silver tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGold tables are more likely to contain a less refined view of data than Silver tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGold tables are more likely to contain more data than Silver tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGold tables are more likely to contain truthful data than Silver tables."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-03T15:57:00.000Z",
        "voteCount": 6,
        "content": "A. Gold tables are more likely to contain aggregations than Silver tables.\n\nIn some data processing pipelines, especially those following a typical \"Bronze-Silver-Gold\" data lakehouse architecture, Silver tables are often considered a more refined version of the raw or Bronze data. Silver tables may include data cleansing, schema enforcement, and some initial transformations.\n\nGold tables, on the other hand, typically represent a stage where data is further enriched, aggregated, and processed to provide valuable insights for analytical purposes. This could indeed involve more aggregations compared to Silver tables."
      },
      {
        "date": "2024-09-07T11:06:00.000Z",
        "voteCount": 1,
        "content": "Gold data is often refined and aggregated."
      },
      {
        "date": "2024-09-01T07:20:00.000Z",
        "voteCount": 1,
        "content": "A. Gold tables are more likely to contain aggregations than Silver tables.\nIn the Delta Lake architecture, Silver tables typically contain cleaned and enriched data that has been transformed from raw data (Bronze tables). Gold tables, on the other hand, are often used for business-level aggregates, reporting, and analytics. They are built on top of Silver tables and provide a more refined and aggregated view of the data, making them more likely to contain aggregations."
      },
      {
        "date": "2024-08-08T03:29:00.000Z",
        "voteCount": 1,
        "content": "In the typical data pipeline architecture, Gold tables are often the final layer and contain aggregated, high-value insights that are ready for reporting and analysis. Silver tables usually contain more detailed and refined data that is processed from the raw or Bronze tables but may not yet be aggregated."
      },
      {
        "date": "2024-07-23T12:28:00.000Z",
        "voteCount": 1,
        "content": "A\n\nThis gold data is often highly refined and aggregated, containing data that powers analytics, machine learning, and production applications. While all tables in the lakehouse should serve an important purpose, gold tables represent data that has been transformed into knowledge, rather than just information.\n\nAnalysts largely rely on gold tables for their core responsibilities, and data shared with a customer would rarely be stored outside this level."
      },
      {
        "date": "2024-07-12T02:59:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B!"
      },
      {
        "date": "2024-07-08T14:04:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B. I saw this from the Databricks practice test. It's a little blurry, but this is the correct answer."
      },
      {
        "date": "2024-08-06T12:18:00.000Z",
        "voteCount": 1,
        "content": "No, it's not true I checked it in the databricks exam and the answer is A"
      },
      {
        "date": "2024-06-07T04:06:00.000Z",
        "voteCount": 1,
        "content": "In some data processing pipelines, particularly those following a \"Bronze-Silver-Gold\" data lakehouse architecture, Silver tables are indeed considered a more refined version of raw or Bronze data. Gold tables, which represent the final stage of data processing, typically contain highly refined, aggregated, and ready-to-consume data.\nTherefore, it's common for Gold tables to contain aggregations, as they often represent the final, summarized, and aggregated view of the data. On the other hand, Silver tables may contain partially aggregated or cleansed data but are not typically the final destination for aggregated data.\n\"Gold tables are more likely to contain aggregations than Silver tables\" is accurate, making option A a valid choice."
      },
      {
        "date": "2024-05-31T00:11:00.000Z",
        "voteCount": 1,
        "content": "A; row data = bronze data &gt; silver data &gt; golden data\nC is so opposite and wrong"
      },
      {
        "date": "2024-05-20T08:25:00.000Z",
        "voteCount": 1,
        "content": "Raw Data &gt; Bronze Data &gt; Silver Data &gt; Golden Data"
      },
      {
        "date": "2024-04-28T01:33:00.000Z",
        "voteCount": 1,
        "content": "correct is A"
      },
      {
        "date": "2024-01-04T05:11:00.000Z",
        "voteCount": 1,
        "content": "Correct is A"
      },
      {
        "date": "2023-11-07T07:16:00.000Z",
        "voteCount": 1,
        "content": "Correct: A"
      },
      {
        "date": "2023-08-11T00:24:00.000Z",
        "voteCount": 2,
        "content": "To me it seems A and E is equally correct. Truthfull is not very defined in the question. But Gold layer typically have more rules and transformations in order to be consumed by business and reports. So It could be intepreted as more \"truthfull\". Or am I wrong here?"
      },
      {
        "date": "2023-07-08T06:46:00.000Z",
        "voteCount": 1,
        "content": "B\n2 Type of Tables in Delta Lake data lake architecture\nGold tables are the most refined and valuable tables in the data lake, while Silver tables are less refined and less valuable.\nGold tables are typically used for downstream analysis and reporting, while Silver tables are typically used for data exploration and experimentation.\n\nGold tables typically contain the most refined, high-quality, and valuable data in an organization's data architecture. They often represent the final output or result of data processing pipelines, where data has undergone extensive cleansing, transformation, and aggregation. Gold tables are typically used for critical business analysis, reporting, and decision-making processes.\n\nOption A: Gold tables are not necessarily more likely to contain aggregations than Silver tables.\nOption C: Gold tables are more likely to contain a more refined view of data than Silver tables.\nOption D: Gold tables are not necessarily more likely to contain more data than Silver tables."
      },
      {
        "date": "2023-08-11T00:22:00.000Z",
        "voteCount": 2,
        "content": "The data itself should be the same. However the Transformations are not. Gold Layer, as I understand it, is more probable to have more transformations as its ready for reports and business consumptions. So A?\n\"The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins. The final layer of data transformations and data quality rules are applied here.\"\nhttps://www.databricks.com/glossary/medallion-architecture"
      },
      {
        "date": "2023-04-05T04:07:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. Gold tables are typically considered to be the most valuable and trusted data assets in an organization. They represent the final, refined view of the data after all cleaning, transformations, and enrichments have been performed. Silver tables are the intermediate tables that feed into the Gold tables, and are typically used to perform data cleansing, filtering, and enrichment before the data is promoted to Gold."
      },
      {
        "date": "2023-04-05T08:36:00.000Z",
        "voteCount": 6,
        "content": "Dude you are providing all the wrong answers and giving baseless explanations without any link to a documentation or something. Please stop misleading people."
      },
      {
        "date": "2023-04-04T08:05:00.000Z",
        "voteCount": 2,
        "content": "A os correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/databricks/view/104780-exam-certified-data-engineer-associate-topic-1-question-29/",
    "body": "Which of the following describes the relationship between Bronze tables and raw data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBronze tables contain less data than raw data files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBronze tables contain more truthful data than raw data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBronze tables contain aggregates while raw data is unaggregated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBronze tables contain a less refined view of data than raw data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBronze tables contain raw data with a schema applied.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 32,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-01T11:00:00.000Z",
        "voteCount": 14,
        "content": "Bronze tables are basically raw ingested data, often with schema borrowed from the original data source or table. Correct answer is E."
      },
      {
        "date": "2024-09-30T10:54:00.000Z",
        "voteCount": 1,
        "content": "Correct is E"
      },
      {
        "date": "2024-04-28T01:35:00.000Z",
        "voteCount": 2,
        "content": "still i am not sure about the schema as i thought that correct types are usually defined in silver while in bronze are all strings"
      },
      {
        "date": "2024-01-04T05:12:00.000Z",
        "voteCount": 4,
        "content": "Correct is E"
      },
      {
        "date": "2023-11-07T07:19:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-10-31T07:03:00.000Z",
        "voteCount": 3,
        "content": "E is the right answer. Bronze data are simply a more structured (in terms of schema) version of raw data to be found in the \"landing area\"."
      },
      {
        "date": "2023-09-03T15:59:00.000Z",
        "voteCount": 2,
        "content": "E. Bronze tables contain raw data with a schema applied.\n\nIn a typical data processing pipeline following a \"Bronze-Silver-Gold\" data lakehouse architecture, Bronze tables are the initial stage where raw data is ingested and transformed into a structured format with a schema applied. The schema provides structure and meaning to the raw data, making it more usable and accessible for downstream processing.\n\nTherefore, Bronze tables contain the raw data but in a structured and schema-enforced format, which makes them distinct from the unprocessed, unstructured raw data files."
      },
      {
        "date": "2023-07-21T12:11:00.000Z",
        "voteCount": 3,
        "content": "Ans : E\n\nThe Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\nhttps://www.databricks.com/glossary/medallion-architecture#:~:text=Bronze%20layer%20%28raw%20data%29"
      },
      {
        "date": "2023-07-21T12:09:00.000Z",
        "voteCount": 1,
        "content": "Ans: E\nhttps://www.databricks.com/glossary/medallion-architecture#:~:text=Bronze%20layer%20%28raw%20data%29"
      },
      {
        "date": "2023-07-08T06:49:00.000Z",
        "voteCount": 1,
        "content": "E\nBronze tables are the foundation of the Delta Lake data lake architecture. They are created from raw data files and contain a schema that describes the data. This makes it easy to query and analyze the data in Bronze tables.\n\nRaw data files, on the other hand, do not have a schema applied. This means that it can be difficult to query and analyze the data in raw data files.\n\nOption A: Bronze tables typically contain more data than raw data files, because they include the schema.\n\nOption B: There is no indication that Bronze tables contain more truthful data than raw data.\n\nOption C: Bronze tables can contain aggregates, but they do not have to.\n\nOption D: Bronze tables typically contain a more refined view of data than raw data, because they include the schema."
      },
      {
        "date": "2023-07-08T06:54:00.000Z",
        "voteCount": 1,
        "content": "Sorry this is meant to be on question #30"
      },
      {
        "date": "2023-07-08T06:55:00.000Z",
        "voteCount": 1,
        "content": "never mind :)"
      },
      {
        "date": "2023-04-04T08:07:00.000Z",
        "voteCount": 2,
        "content": "E option"
      },
      {
        "date": "2023-04-03T02:53:00.000Z",
        "voteCount": 3,
        "content": "Option E"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/databricks/view/104781-exam-certified-data-engineer-associate-topic-1-question-30/",
    "body": "Which of the following tools is used by Auto Loader process data incrementally?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheckpointing",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark Structured Streaming\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Explorer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnity Catalog",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks SQL"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-01T11:01:00.000Z",
        "voteCount": 9,
        "content": "B is the correct answer. Checkpointing is a method that is part of structured streaming."
      },
      {
        "date": "2024-08-08T21:12:00.000Z",
        "voteCount": 1,
        "content": "B. Spark Structured Streaming\n\nAuto Loader uses Spark Structured Streaming to incrementally and efficiently process new data as it arrives, enabling scalable and reliable data ingestion in Databricks."
      },
      {
        "date": "2024-01-17T03:59:00.000Z",
        "voteCount": 2,
        "content": "The answer should be A. \nAuto Loader is used by Structured Streaming to process data incrementaly, not the other way around."
      },
      {
        "date": "2024-01-04T05:13:00.000Z",
        "voteCount": 1,
        "content": "Correct is B"
      },
      {
        "date": "2023-11-07T07:20:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-10-10T01:45:00.000Z",
        "voteCount": 1,
        "content": "B is orrect"
      },
      {
        "date": "2023-09-03T16:02:00.000Z",
        "voteCount": 2,
        "content": "B. Spark Structured Streaming\n\nThe Auto Loader process in Databricks is typically used in conjunction with Spark Structured Streaming to process data incrementally. Spark Structured Streaming is a real-time data processing framework that allows you to process data streams incrementally as new data arrives. The Auto Loader is a feature in Databricks that works with Structured Streaming to automatically detect and process new data files as they are added to a specified data source location. It allows for incremental data processing without the need for manual intervention."
      },
      {
        "date": "2023-07-21T12:21:00.000Z",
        "voteCount": 1,
        "content": "ans:A\nHow does Auto Loader track ingestion progress?\nAs files are discovered, their metadata is persisted in a scalable key-value store (RocksDB) in the checkpoint location of your Auto Loader pipeline. This key-value store ensures that data is processed exactly once.\n\nIn case of failures, Auto Loader can resume from where it left off by information stored in the checkpoint location and continue to provide exactly-once guarantees when writing data into Delta Lake. You don\u2019t need to maintain or manage any state yourself to achieve fault tolerance or exactly-once semantics.\nhttps://docs.databricks.com/ingestion/auto-loader/index.html"
      },
      {
        "date": "2023-07-21T12:16:00.000Z",
        "voteCount": 1,
        "content": "ans:B\nHow does Auto Loader track ingestion progress?\nAs files are discovered, their metadata is persisted in a scalable key-value store (RocksDB) in the checkpoint location of your Auto Loader pipeline. This key-value store ensures that data is processed exactly once.\n\nIn case of failures, Auto Loader can resume from where it left off by information stored in the checkpoint location and continue to provide exactly-once guarantees when writing data into Delta Lake. You don\u2019t need to maintain or manage any state yourself to achieve fault tolerance or exactly-once semantics.\nhttps://docs.databricks.com/ingestion/auto-loader/index.html"
      },
      {
        "date": "2023-07-08T06:53:00.000Z",
        "voteCount": 2,
        "content": "B\nAuto Loader uses Spark Structured Streaming to process data incrementally. Spark Structured Streaming is a streaming engine that can be used to process data as it arrives. This makes it ideal for processing data that is being generated in real time.\n\nOption A: Checkpointing is a technique used to ensure that data is not lost in case of a failure. It is not used to process data incrementally.\n\nOption C: Data Explorer is a data exploration tool that can be used to explore data. It is not used to process data incrementally.\n\nOption D: Unity Catalog is a metadata management tool that can be used to store and manage metadata about data assets. It is not used to process data incrementally.\n\nOption E: Databricks SQL is a SQL engine that can be used to query data. It is not used to process data incrementally."
      },
      {
        "date": "2023-04-03T02:55:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/databricks/view/104854-exam-certified-data-engineer-associate-topic-1-question-31/",
    "body": "A data engineer has configured a Structured Streaming job to read from a table, manipulate the data, and then perform a streaming write into a new table.<br>The cade block used by the data engineer is below:<br><img title=\"image9\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image9.png\"><br>If the data engineer only wants the query to execute a micro-batch to process data every 5 seconds, which of the following lines of code should the data engineer use to fill in the blank?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(\"5 seconds\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(once=\"5 seconds\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(processingTime=\"5 seconds\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(continuous=\"5 seconds\")"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-02T06:14:00.000Z",
        "voteCount": 5,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-04-05T04:22:00.000Z",
        "voteCount": 5,
        "content": "The correct line of code to fill in the blank to execute a micro-batch to process data every 5 seconds is:\n\nD. trigger(processingTime=\"5 seconds\")\n\nOption A (\"trigger(\"5 seconds\")\") would not work because it does not specify that the trigger should be a processing time trigger, which is necessary to trigger a micro-batch processing at regular intervals.\n\nOption B (\"trigger()\") would not work because it would use the default trigger, which is not a processing time trigger.\n\nOption C (\"trigger(once=\"5 seconds\")\") would not work because it would only trigger the query once, not at regular intervals.\n\nOption E (\"trigger(continuous=\"5 seconds\")\") would not work because it would trigger the query to run continuously, without any pauses in between, which is not what the data engineer wants."
      },
      {
        "date": "2024-10-01T01:42:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer\nProcessingTime\nhttps://learn.microsoft.com/en-us/azure/databricks/structured-streaming/triggers\nContinues Processing : \n"
      },
      {
        "date": "2024-04-28T01:37:00.000Z",
        "voteCount": 1,
        "content": "correct syntax is D"
      },
      {
        "date": "2023-11-07T07:25:00.000Z",
        "voteCount": 1,
        "content": "Correct: D"
      },
      {
        "date": "2023-09-03T16:04:00.000Z",
        "voteCount": 2,
        "content": "# ProcessingTime trigger with two-seconds micro-batch interval\ndf.writeStream \\\n  .format(\"console\") \\\n  .trigger(processingTime='2 seconds') \\\n  .start()\n\nhttps://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers"
      },
      {
        "date": "2023-08-19T11:30:00.000Z",
        "voteCount": 1,
        "content": "https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers"
      },
      {
        "date": "2023-07-08T06:58:00.000Z",
        "voteCount": 1,
        "content": "D\nval query = sourceTable\n  .writeStream\n  .format(\"delta\")\n  .outputMode(\"append\")\n  .trigger(Trigger.ProcessingTime(\"5 seconds\"))\n  .start(destinationTable)"
      },
      {
        "date": "2023-09-03T16:05:00.000Z",
        "voteCount": 3,
        "content": "This is Scala example. Exam should be 100% on Python."
      },
      {
        "date": "2023-04-04T08:08:00.000Z",
        "voteCount": 2,
        "content": "D os correct"
      },
      {
        "date": "2023-04-03T02:55:00.000Z",
        "voteCount": 3,
        "content": "Option D"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/databricks/view/104855-exam-certified-data-engineer-associate-topic-1-question-32/",
    "body": "A dataset has been defined using Delta Live Tables and includes an expectations clause:<br>CONSTRAINT valid_timestamp EXPECT (timestamp &gt; '2020-01-01') ON VIOLATION DROP ROW<br>What is the expected behavior when a batch of data containing data that violates these constraints is processed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are dropped from the target dataset and loaded into a quarantine table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are added to the target dataset and flagged as invalid in a field added to the target dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are added to the target dataset and recorded as invalid in the event log.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation cause the job to fail."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 40,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-02T06:18:00.000Z",
        "voteCount": 17,
        "content": "I am simply appalled by the number of wrong answers in this series of questions. The statement in the question already says \"ON VIOLATE DROP ROW\" which means if condition is violated, there will be nothing saved to quarantine table and a log of all invalid entries will be recoded. All invalid data that doesn't meet condition will be dropped. \nSo C is the correct answer."
      },
      {
        "date": "2023-04-04T08:11:00.000Z",
        "voteCount": 5,
        "content": "C is correct"
      },
      {
        "date": "2024-09-20T01:08:00.000Z",
        "voteCount": 1,
        "content": "100% C"
      },
      {
        "date": "2024-08-28T06:29:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer."
      },
      {
        "date": "2024-07-08T14:08:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer. The DROP ROW clause will cause them to NOT be added to the destination; only marked in the log."
      },
      {
        "date": "2024-04-28T01:46:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-01-08T15:41:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-12-29T20:20:00.000Z",
        "voteCount": 3,
        "content": "C. Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.\n\nExplanation:\n\nThe defined expectation specifies that if the timestamp is not greater than '2020-01-01', the row will be considered in violation of the constraint.\nThe ON VIOLATION DROP ROW clause states that rows that violate the constraint will be dropped from the target dataset.\nAdditionally, the expectation clause will log these violations in the event log, indicating which records did not meet the specified constraint criteria.\nThis behavior ensures that the rows failing the defined constraint are not included in the target dataset and are logged as invalid in the event log for reference or further investigation, maintaining data integrity within the dataset based on the specified constraints."
      },
      {
        "date": "2023-11-15T11:29:00.000Z",
        "voteCount": 1,
        "content": "who choses these answers? The correct answer is C. The record is dropped. This is not about the default  behavior. It is explicit."
      },
      {
        "date": "2023-10-31T07:13:00.000Z",
        "voteCount": 1,
        "content": "Right answer: C\nInvalid rows will be dropped as requested by the constraint and flagged as such in log files. If you need a quarantine table, you'll have to write more code."
      },
      {
        "date": "2023-09-03T16:22:00.000Z",
        "voteCount": 2,
        "content": "C. Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.\n\nWith the defined constraint and expectation clause, when a batch of data is processed, any records that violate the expectation (in this case, where the timestamp is not greater than '2020-01-01') will be dropped from the target dataset. These dropped records will also be recorded as invalid in the event log, allowing for auditing and tracking of the data quality issues without causing the entire job to fail."
      },
      {
        "date": "2023-08-19T11:52:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/delta-live-tables/expectations.html"
      },
      {
        "date": "2023-07-08T07:03:00.000Z",
        "voteCount": 1,
        "content": "C\nWhen a batch of data is processed in Delta Live Tables and contains data that violates the defined expectations or constraints, the expected behavior is that the records violating the expectation are dropped from the target dataset. Additionally, these violated records are recorded as invalid in the event log."
      },
      {
        "date": "2023-07-07T08:01:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-04-13T06:11:00.000Z",
        "voteCount": 2,
        "content": "B is correct. This question is number 35 on the practice test on databricks patner academy. https://partner-academy.databricks.com/  correct answer is \"Records that violate the expectation are added to the target dataset and recorded as invalid in the event log\""
      },
      {
        "date": "2023-04-13T13:33:00.000Z",
        "voteCount": 1,
        "content": "Sorry, D"
      },
      {
        "date": "2023-04-13T23:55:00.000Z",
        "voteCount": 5,
        "content": "I was wrong, the ON VIOLATION DROP ROW makes C the correct answer"
      },
      {
        "date": "2023-04-03T02:57:00.000Z",
        "voteCount": 4,
        "content": "option C"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/databricks/view/104856-exam-certified-data-engineer-associate-topic-1-question-33/",
    "body": "Which of the following describes when to use the CREATE STREAMING LIVE TABLE (formerly CREATE INCREMENTAL LIVE TABLE) syntax over the CREATE LIVE TABLE syntax when creating Delta Live Tables (DLT) tables using SQL?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE STREAMING LIVE TABLE should be used when the subsequent step in the DLT pipeline is static.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE STREAMING LIVE TABLE should be used when data needs to be processed incrementally.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE STREAMING LIVE TABLE is redundant for DLT and it does not need to be used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE STREAMING LIVE TABLE should be used when data needs to be processed through complicated aggregations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE STREAMING LIVE TABLE should be used when the previous step in the DLT pipeline is static."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-02T06:19:00.000Z",
        "voteCount": 6,
        "content": "B is the correct answer."
      },
      {
        "date": "2023-04-05T04:39:00.000Z",
        "voteCount": 6,
        "content": "B. CREATE STREAMING LIVE TABLE should be used when data needs to be processed incrementally. The CREATE STREAMING LIVE TABLE syntax is used to create tables that read data incrementally, while the CREATE LIVE TABLE syntax is used to create tables that read data in batch mode. Delta Live Tables support both streaming and batch modes of processing data. When the data is streamed and needs to be processed incrementally, CREATE STREAMING LIVE TABLE should be used."
      },
      {
        "date": "2024-08-08T22:40:00.000Z",
        "voteCount": 1,
        "content": "B. CREATE STREAMING LIVE TABLE should be used when data needs to be processed incrementally.\nThis syntax is used to define a Delta Live Table that processes data incrementally as new data arrives, which is essential for handling streaming data or large datasets that need to be processed in chunks rather than all at once."
      },
      {
        "date": "2024-01-08T15:42:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-07T07:32:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-09-03T16:23:00.000Z",
        "voteCount": 3,
        "content": "B. CREATE STREAMING LIVE TABLE should be used when data needs to be processed incrementally.\n\nThe CREATE STREAMING LIVE TABLE syntax is used when you want to create Delta Live Tables (DLT) tables that are designed for processing data incrementally. This is typically used when your data pipeline involves streaming or incremental data updates, and you want the table to stay up to date as new data arrives. It allows you to define tables that can handle data changes incrementally without the need for full table refreshes.\n\nSo, option B correctly describes when to use CREATE STREAMING LIVE TABLE over CREATE LIVE TABLE in the context of Delta Live Tables."
      },
      {
        "date": "2023-06-14T08:04:00.000Z",
        "voteCount": 2,
        "content": "This is old version question, currently, Databricks only have Streaming Table (Create Live Table). The previous Streaming live table and Live table already combined."
      },
      {
        "date": "2023-06-20T09:09:00.000Z",
        "voteCount": 3,
        "content": "is this dump valid for v3?"
      },
      {
        "date": "2023-04-03T02:58:00.000Z",
        "voteCount": 4,
        "content": "option B"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/databricks/view/104857-exam-certified-data-engineer-associate-topic-1-question-34/",
    "body": "A data engineer is designing a data pipeline. The source system generates files in a shared directory that is also used by other processes. As a result, the files should be kept as is and will accumulate in the directory. The data engineer needs to identify which files are new since the previous run in the pipeline, and set up the pipeline to only ingest those new files with each run.<br>Which of the following tools can the data engineer use to solve this problem?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnity Catalog",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Lake",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Explorer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAuto Loader\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-08T22:45:00.000Z",
        "voteCount": 1,
        "content": "E. Auto Loader\nAuto Loader is designed to incrementally ingest new data files as they appear in a directory, making it ideal for scenarios where files accumulate and need to be ingested without reprocessing previously ingested files. It automatically tracks which files have already been processed, ensuring that only new files are ingested with each pipeline run."
      },
      {
        "date": "2024-04-28T01:49:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-01-08T15:44:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-11-15T11:39:00.000Z",
        "voteCount": 1,
        "content": "the  data engineer needs to identify which files are new since the previous run. This seems to be an analysis effort. If that is the case, and I might be wrong, then DB SQL is the correct answer."
      },
      {
        "date": "2023-10-31T07:50:00.000Z",
        "voteCount": 1,
        "content": "Autoloader can help if you want to ingest data incrementally."
      },
      {
        "date": "2023-08-19T11:57:00.000Z",
        "voteCount": 2,
        "content": "Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup.\n\nhttps://docs.databricks.com/en/ingestion/auto-loader/index.html"
      },
      {
        "date": "2023-04-03T02:58:00.000Z",
        "voteCount": 3,
        "content": "option E"
      },
      {
        "date": "2023-04-02T06:20:00.000Z",
        "voteCount": 4,
        "content": "E is the correct answer."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/databricks/view/104858-exam-certified-data-engineer-associate-topic-1-question-35/",
    "body": "Which of the following Structured Streaming queries is performing a hop from a Silver table to a Gold table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image10\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image10.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image11\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image11.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image12\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image12.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image13\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image13.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image14\" src=\"https://img.examtopics.com/certified-data-engineer-associate/image14.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T01:52:00.000Z",
        "voteCount": 2,
        "content": "E - Aggregations are performed from silver to gold"
      },
      {
        "date": "2024-01-31T23:51:00.000Z",
        "voteCount": 1,
        "content": "Answer shoiuld be A , for writestream data should be stream only and not static"
      },
      {
        "date": "2024-04-28T01:52:00.000Z",
        "voteCount": 3,
        "content": "that's a good point but i would say that A is performing a raw data ingestion into bronze"
      },
      {
        "date": "2024-01-08T15:45:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2023-11-09T09:53:00.000Z",
        "voteCount": 2,
        "content": "The best practice is to use \"Complete\" as output mode instead of \"append\" when working with aggregated tables. Since gold layer is work final aggregated tables, the only option with output mode as complete is option E."
      },
      {
        "date": "2023-10-31T07:51:00.000Z",
        "voteCount": 1,
        "content": "E is the right answer. The \"gold layer\" is used to store aggregated clean data, E is the only answer in wich aggregation is performed."
      },
      {
        "date": "2023-10-06T03:21:00.000Z",
        "voteCount": 2,
        "content": "E as we're doing an aggregation and we're rewriting the whole table and not just appending."
      },
      {
        "date": "2023-08-03T19:01:00.000Z",
        "voteCount": 2,
        "content": "E is correct as it includes group by as well by store."
      },
      {
        "date": "2023-04-04T08:20:00.000Z",
        "voteCount": 3,
        "content": "E option"
      },
      {
        "date": "2023-04-03T03:05:00.000Z",
        "voteCount": 3,
        "content": "Option E"
      },
      {
        "date": "2023-04-02T06:21:00.000Z",
        "voteCount": 4,
        "content": "E is the correct answer."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/databricks/view/105325-exam-certified-data-engineer-associate-topic-1-question-36/",
    "body": "A data engineer has three tables in a Delta Live Tables (DLT) pipeline. They have configured the pipeline to drop invalid records at each table. They notice that some data is being dropped due to quality concerns at some point in the DLT pipeline. They would like to determine at which table in their pipeline the data is being dropped.<br>Which of the following approaches can the data engineer take to identify the table that is dropping the records?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up separate expectations for each table when developing their DLT pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey cannot determine which table is dropping the records.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up DLT to notify them via email when records are dropped.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the DLT pipeline page, click on each table, and view the data quality statistics.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the DLT pipeline page, click on the \u201cError\u201d button, and review the present errors."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-03T16:30:00.000Z",
        "voteCount": 13,
        "content": "D. They can navigate to the DLT pipeline page, click on each table, and view the data quality statistics.\n\nTo identify the table in a Delta Live Tables (DLT) pipeline where data is being dropped due to quality concerns, the data engineer can navigate to the DLT pipeline page, click on each table in the pipeline, and view the data quality statistics. These statistics often include information about records dropped, violations of expectations, and other data quality metrics. By examining the data quality statistics for each table in the pipeline, the data engineer can determine at which table the data is being dropped."
      },
      {
        "date": "2024-09-01T08:04:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is:\nD. They can navigate to the DLT pipeline page, click on each table, and view the data quality statistics.\nDelta Live Tables provides detailed data quality statistics for each table in the pipeline. By navigating to the DLT pipeline page and clicking on each table, the data engineer can view these statistics and determine at which table the records are being dropped due to quality concerns. This allows them to identify and address the specific issues causing the data to be dropped."
      },
      {
        "date": "2024-07-08T14:11:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is \"D\"."
      },
      {
        "date": "2024-04-28T01:58:00.000Z",
        "voteCount": 1,
        "content": "I would say D but I have never really tested it, still other solutions smell wrong"
      },
      {
        "date": "2024-01-31T23:53:00.000Z",
        "voteCount": 1,
        "content": "D is correct \nBy clicking on each table in the DLT pipeline page, the data engineer may be able to access data quality statistics, error logs, or other information related to dropped records. This can help them pinpoint at which table in the pipeline the data is being dropped."
      },
      {
        "date": "2023-12-06T03:40:00.000Z",
        "voteCount": 2,
        "content": "E is for when an error occur. But pipeline is defined to drop some records that will not result on error"
      },
      {
        "date": "2023-11-07T07:39:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-07-08T07:26:00.000Z",
        "voteCount": 2,
        "content": "E\nWhen records are dropped due to quality concerns in a DLT pipeline, the errors are logged in the event log. The data engineer can navigate to the DLT pipeline page and click on the \u201cError\u201d button to view the present errors. The errors will show the table where the records were dropped.\nOption A: Setting up separate expectations for each table will not help the data engineer determine which table is dropping the records.\n\nOption B: The data engineer cannot determine which table is dropping the records without looking at the event log.\n\nOption C: Setting up DLT to notify the data engineer via email when records are dropped will not help the data engineer determine which table is dropping the records.\n\nOption D: Viewing the data quality statistics for each table will not help the data engineer determine which table is dropping the records."
      },
      {
        "date": "2023-10-31T08:07:00.000Z",
        "voteCount": 1,
        "content": "Don't you have to select a table generated in a single step of the pipeline to access the errors through the buttton though? Probably D is the right one here"
      },
      {
        "date": "2023-05-13T05:59:00.000Z",
        "voteCount": 4,
        "content": "Think answer is D.\nThe pipeline is configured to drop invalid records, i.e. a SQL equivalent query with a ON VIOLATION DROP ROW clause. This will not result in a failed pipeline execution because there are no errors. Instead, you'd have to go to each table and review the quality charactistics."
      },
      {
        "date": "2023-07-08T19:12:00.000Z",
        "voteCount": 2,
        "content": "Option D is incorrect because viewing the data quality statistics for each table will not help the data engineer identify which table is dropping the records. The data quality statistics will show the overall quality of the data in each table, but they will not show which table is dropping the records.\n\nFor example, if the data quality statistics for a table show that 10% of the records are invalid, this does not mean that 10% of the records are being dropped. The invalid records could be being updated, inserted, or deleted."
      },
      {
        "date": "2023-04-24T08:10:00.000Z",
        "voteCount": 3,
        "content": "Is this for v2 or v3"
      },
      {
        "date": "2023-04-05T12:57:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is D"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/databricks/view/105268-exam-certified-data-engineer-associate-topic-1-question-37/",
    "body": "A data engineer has a single-task Job that runs each morning before they begin working. After identifying an upstream data issue, they need to set up another task to run a new notebook prior to the original task.<br>Which of the following approaches can the data engineer use to set up the new task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can clone the existing task in the existing Job and update it to run the new notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can create a new task in the existing Job and then add it as a dependency of the original task.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can create a new task in the existing Job and then add the original task as a dependency of the new task.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can create a new job from scratch and add both tasks to run concurrently.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can clone the existing task to a new Job and then edit it to run the new notebook."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 52,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 25,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-07T10:04:00.000Z",
        "voteCount": 15,
        "content": "B is the right answer."
      },
      {
        "date": "2023-04-14T11:34:00.000Z",
        "voteCount": 15,
        "content": "It seems there is some confusion on what dependency means in this case. Option B is correct because adding the new task as a dependency of the original task means that the new task will run BEFORE the original task, which is the goal defined in the question."
      },
      {
        "date": "2024-09-27T06:38:00.000Z",
        "voteCount": 1,
        "content": "C. They can create a new task in the existing Job and then add the original task as a dependency of the new task.\n\nWhy this is correct: In Databricks, you can set up a task dependency chain by adding a new task and specifying that the original task depends on the new one. This ensures that the new task will run first, followed by the original task."
      },
      {
        "date": "2024-09-23T19:31:00.000Z",
        "voteCount": 1,
        "content": "Both B and C involve dependencies between tasks, but the difference is in how the dependencies are structured:\n\nB: \"They can create a new task in the existing Job and then add it as a dependency of the original task.\"\n\nIn this case, the new task is added as a prerequisite (dependency) for the original task. This means the new task will run first, and once it's completed, the original task will run.\n\nC: \"They can create a new task in the existing Job and then add the original task as a dependency of the new task.\"\n\nIn this case, the original task is added as a dependency for the new task, meaning the new task will wait for the original task to finish before running.\n\nThe correct answer is B:\nYou want the new task (the one handling the upstream issue) to run before the original task, so it should be set as a dependency of the original task."
      },
      {
        "date": "2024-09-20T01:18:00.000Z",
        "voteCount": 1,
        "content": "B is correct as Redwings538 says"
      },
      {
        "date": "2024-09-01T08:13:00.000Z",
        "voteCount": 1,
        "content": "I think the Correct answer is C.\nBecause as per the statement in the question \"they need to set up another task to run a new notebook prior to the original task.\" i.e. original task should run AFTER the new task.\n\nSo, By creating a new task in the existing job and setting the original task as a dependency of the new task, the data engineer ensures that the new notebook runs first, followed by the original task. This approach maintains the sequence of execution required to address the upstream data issue."
      },
      {
        "date": "2024-08-26T10:50:00.000Z",
        "voteCount": 2,
        "content": "Below is the info I am convinced after checking with AI.....\nHere's the break down the differences between options B and C:\n\nOption B:\n Create a new task in the existing Job and then add it as a dependency of the original task:\nResult: The new task will run after the original task.\n\nOption C:\nCreate a new task in the existing Job and then add the original task as a dependency of the new task:\n\nResult: The new task will run before the original task.\n\nSummary:\nOption B: Original task \u2192 New task\nOption C: New task \u2192 Original task\nIn your case, Option C is the correct choice because you need the new task to run first to resolve the upstream data issue before the original task executes."
      },
      {
        "date": "2024-08-16T01:09:00.000Z",
        "voteCount": 1,
        "content": "C is correct because it correctly handles the sequence of execution. By creating a new task in the existing Job and adding the original task as a dependency of the new task, the new task will run first, and once it completes successfully, the original task will run. This ensures that the upstream data issue is addressed before the original task runs."
      },
      {
        "date": "2024-05-23T07:09:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer."
      },
      {
        "date": "2024-04-28T22:56:00.000Z",
        "voteCount": 1,
        "content": "original depends on new"
      },
      {
        "date": "2024-03-05T00:21:00.000Z",
        "voteCount": 3,
        "content": "C because the new task has to run prior the original one"
      },
      {
        "date": "2024-01-27T00:09:00.000Z",
        "voteCount": 6,
        "content": "Just got 100% on the test. B was correct."
      },
      {
        "date": "2024-01-18T07:22:00.000Z",
        "voteCount": 3,
        "content": "This has become more of a English grammatical test as the word dependency is confusing people.  When the Original task has a dependency on the new task this means the original task needs to depend on the new task.  So it's Option C."
      },
      {
        "date": "2023-12-29T22:11:00.000Z",
        "voteCount": 3,
        "content": "The data engineer can create a new task in the existing Job and then add the original task as a dependency of the new task (Option C). This way, the new task will run first, and once it\u2019s completed, the original task will run. Here are the steps to do this:\n\nClick Workflows in the sidebar and click New and select Job.\nThe Tasks tab appears with the create task dialog.\nReplace Add a name for your job\u2026 with your job name.\nEnter a name for the task in the Task name field.\nIn the Type drop-down menu, select the type of task to run.\nConfigure the cluster where the task runs.\nTo add dependent libraries, click + Add next to Dependent libraries.\nYou can pass parameters for your task.\nPlease note that the exact process may vary depending on the specific configurations and permissions set up in your workspace. It\u2019s always a good idea to consult with your organization\u2019s IT or data governance team to ensure the correct procedures are followed."
      },
      {
        "date": "2023-12-26T21:56:00.000Z",
        "voteCount": 5,
        "content": "Answer is C"
      },
      {
        "date": "2023-12-12T07:29:00.000Z",
        "voteCount": 1,
        "content": "I am pretty sure its B - \"they need to set up another task to run a new notebook prior to the original task.\" - so NEW task need to run BEFORE ORIGINAL task. So NEW TASK should be DEPENDENCY of ORIGINAL TASK (or in other words: original task is dependent on new task)"
      },
      {
        "date": "2023-11-08T04:51:00.000Z",
        "voteCount": 5,
        "content": "\"A data engineer has a single-task Job that runs each morning before they begin working. After identifying an upstream data issue, they need to set up another task to run a new notebook prior to the original task.\"\n\nIn the tasks UI of the Job:\n1. Create a *new task*\n2. Select *original task*\n3. In *original task* for \"depends on\" enter *new task\" - as *new task* needs to run prior to *original task*, ie, original task has a dependency on new task\n\nfrom 1. create new task ..... from 3. original task has a dependency on new task\n\nAnswer is C ... They can *create a new task* in the existing Job and then add the *original task as a dependency of the new task*."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/databricks/view/105269-exam-certified-data-engineer-associate-topic-1-question-38/",
    "body": "An engineering manager wants to monitor the performance of a recent project using a Databricks SQL query. For the first week following the project\u2019s release, the manager wants the query results to be updated every minute. However, the manager is concerned that the compute resources used for the query will be left running and cost the organization a lot of money beyond the first week of the project\u2019s release.<br>Which of the following approaches can the engineering team use to ensure the query does not cost the organization any money beyond the first week of the project\u2019s release?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set a limit to the number of DBUs that are consumed by the SQL Endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set the query\u2019s refresh schedule to end after a certain number of refreshes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey cannot ensure the query does not cost the organization money beyond the first week of the project\u2019s release.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set a limit to the number of individuals that are able to manage the query\u2019s refresh schedule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set the query\u2019s refresh schedule to end on a certain date in the query scheduler.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-27T00:11:00.000Z",
        "voteCount": 15,
        "content": "Just got 100% on the test. E was correct. C was not in the available options."
      },
      {
        "date": "2023-04-24T21:37:00.000Z",
        "voteCount": 10,
        "content": "The query scheduler only gives the option on what the interval is to run the query. It does not provide a way to stop after x iterations or at a point in time. \nThe question is confusing. From what i found the only option is to limit users access to the query (and therefore query scheduler). \nhttps://docs.databricks.com/security/auth-authz/access-control/query-acl.html\nNot convinced how this would be helping the organization save money if no-one is manually stopping the schedule. \nAnswer C seems most correct\nAnswer D can be achieved using acl however how is this helpful in the use case described?"
      },
      {
        "date": "2024-09-19T02:25:00.000Z",
        "voteCount": 1,
        "content": "Answer is E. There is an option to specify schedule with CRON syntax which enables to set schedule for a chosen week. \nFor example, when you specify CRON: 0 0 0 23-29 SEP ? 2024, the query will be run At 12:00 AM, between day 23 and 29 of the month, only in September 2024."
      },
      {
        "date": "2024-08-16T01:17:00.000Z",
        "voteCount": 1,
        "content": "E is correct because the engineering team can use the query scheduler in Databricks to set a specific end date for the query refresh schedule. This way, after the first week, the automatic refreshes will stop, and the associated compute costs will be avoided."
      },
      {
        "date": "2024-07-08T14:15:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is E for this question."
      },
      {
        "date": "2024-05-22T22:26:00.000Z",
        "voteCount": 1,
        "content": "Answer is E"
      },
      {
        "date": "2024-02-27T11:27:00.000Z",
        "voteCount": 4,
        "content": "Answer is E\nIt\u00b4s true natively the query can\u00b4t be scheduled to stop, but the scheduler allow us to use cron syntax.\nSo we can define the year, month and days of the first week and the trigger won\u00b4t run after that"
      },
      {
        "date": "2024-01-10T05:59:00.000Z",
        "voteCount": 2,
        "content": "The query scheduler does not give option to have end date (or iterations). Dashboards might give one, but the question specifically mentions queries.\nhttps://learn.microsoft.com/en-gb/azure/databricks/sql/user/queries/schedule-query"
      },
      {
        "date": "2023-12-29T22:18:00.000Z",
        "voteCount": 3,
        "content": "E. They can set the query\u2019s refresh schedule to end on a certain date in the query scheduler.\n\nExplanation:\n\nQuery Scheduler: Databricks offers a Query Scheduler that allows users to schedule the execution of SQL queries at specific intervals or for specific durations.\n\nSetting a Specific End Date: The team can configure the query's refresh schedule to conclude or end on a certain date. By specifying an end date within the first week of the project's release, the query will automatically stop refreshing after that date. This action ensures that compute resources aren't continuously utilized beyond the specified timeframe, preventing unnecessary costs.\n\nThis approach allows the team to control and limit the execution of the query to the required duration without incurring additional costs beyond the first week of the project's release."
      },
      {
        "date": "2023-12-03T07:30:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer \n\nSource : https://docs.databricks.com/en/sql/user/queries/schedule-query.html"
      },
      {
        "date": "2023-10-29T17:42:00.000Z",
        "voteCount": 2,
        "content": "E is the correct answer.\n\nFrom the docs:\n\nIf a dashboard is configured for automatic updates, it has a Scheduled button at the top, rather than a Schedule button. To stop automatically updating the dashboard and remove its subscriptions:\n\n    Click Scheduled.\n    In the Refresh every drop-down, select Never.\n    Click Save. The Scheduled button label changes to Schedule.\nSource: https://learn.microsoft.com/en-us/azure/databricks/sql/user/dashboards/"
      },
      {
        "date": "2024-01-10T05:58:00.000Z",
        "voteCount": 2,
        "content": "This is Dashboard, not SQL query."
      },
      {
        "date": "2023-10-27T16:30:00.000Z",
        "voteCount": 1,
        "content": "Option E is correct answer"
      },
      {
        "date": "2023-10-27T16:17:00.000Z",
        "voteCount": 1,
        "content": "The picker scrolls and allows you to choose:\nAn interval: 1-30 minutes, 1-12 hours, 1 or 30 days, 1 or 2 weeks\n\nSince the schedule picker allows to choose interval to refresh query every 1 or 2 weeks. If we choose 1 week the schedule ends after a week. So the answer is B."
      },
      {
        "date": "2023-10-27T16:30:00.000Z",
        "voteCount": 1,
        "content": "Based on my explanation E can be the correct answer."
      },
      {
        "date": "2023-09-06T12:56:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer E."
      },
      {
        "date": "2023-08-29T09:49:00.000Z",
        "voteCount": 1,
        "content": "agree with BigDaddyAus"
      },
      {
        "date": "2023-08-15T05:20:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. According to documentation it cant be scheduled up until a certain date. It has to be in intervals and then canceled manually. They don't mention end date. Only start date and intervals.\nhttps://docs.databricks.com/en/workflows/jobs/schedule-jobs.html"
      },
      {
        "date": "2023-08-15T05:21:00.000Z",
        "voteCount": 1,
        "content": "Also this link seems to verify C as the correct answer:\nhttps://docs.databricks.com/en/sql/user/queries/schedule-query.html"
      },
      {
        "date": "2023-07-08T07:53:00.000Z",
        "voteCount": 1,
        "content": "E\nOption A:  The query will still run, but it will be throttled if it exceeds the DBU limit.\nOption B:The query will still run, but it will only run a certain number of times before it stops.\nOption C: The engineering team can ensure \nOption D:  The query will still run, but only the individuals who are authorized to manage the refresh schedule will be able to stop it.\nE-Answer \nTherefore, the correct answer is that the engineering team can set the query\u2019s refresh schedule to end on a certain date in the query scheduler to ensure the query does not cost the organization any money beyond the first week of the project\u2019s release."
      },
      {
        "date": "2023-10-20T14:08:00.000Z",
        "voteCount": 1,
        "content": "Refresh schedule doesn't have any option to expire. So E is not correct option. https://docs.databricks.com/en/sql/user/queries/schedule-query.html"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/databricks/view/104898-exam-certified-data-engineer-associate-topic-1-question-39/",
    "body": "A data analysis team has noticed that their Databricks SQL queries are running too slowly when connected to their always-on SQL endpoint. They claim that this issue is present when many members of the team are running small queries simultaneously. They ask the data engineering team for help. The data engineering team notices that each of the team\u2019s queries uses the same SQL endpoint.<br>Which of the following approaches can the data engineering team use to improve the latency of the team\u2019s queries?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can increase the cluster size of the SQL endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can increase the maximum bound of the SQL endpoint\u2019s scaling range.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can turn on the Auto Stop feature for the SQL endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can turn on the Serverless feature for the SQL endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy to \u201cReliability Optimized.\u201d"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-06T13:06:00.000Z",
        "voteCount": 30,
        "content": "Answer is B.\nAccording to databricks documentation:\n-Sequentially -&gt; Increase cluster size\n-Concurrent --&gt; Scale out cluster"
      },
      {
        "date": "2023-11-07T03:11:00.000Z",
        "voteCount": 15,
        "content": "Answer B is correct\n For those who's selected the same answer as the question 40 in the Databricks exam training, be careful becaue it's quite different:\n- Here the question is about simultaneously runs -&gt;  Scale Out clusters (involves adding more clusters)\n- In the Databricks exam training, the question is about  \"sequentially run queries\" -&gt; Scale Up (increasing the size of the nodes)\n\nPleas refer to the this  accepted answer\nhttps://community.databricks.com/t5/data-engineering/sequential-vs-concurrency-optimization-questions-from-query/td-p/36696"
      },
      {
        "date": "2024-09-21T18:33:00.000Z",
        "voteCount": 1,
        "content": "Correct Answers B\nThrough put &gt; Sequential &gt; Scale Up\nPerformance &gt; Concurrent &gt; Scale Out"
      },
      {
        "date": "2024-08-16T01:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct because increasing the maximum bound of the SQL endpoint\u2019s scaling range allows the endpoint to handle a larger number of queries by automatically scaling up the resources (e.g., adding more clusters). This approach addresses the issue of slow queries due to high concurrent usage, as more resources will become available to handle the increased load from simultaneous queries."
      },
      {
        "date": "2024-04-28T22:59:00.000Z",
        "voteCount": 1,
        "content": "simultaneously probably means concurrently so scaling out the cluster is better"
      },
      {
        "date": "2024-04-01T05:49:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-02-08T22:21:00.000Z",
        "voteCount": 2,
        "content": "A data analysis team has noticed that their Databricks SQL queries are running too slowly when connected to their always-on SQL endpoint. They claim that this issue is present when many members of the team are running small queries simultaneously4"
      },
      {
        "date": "2024-02-01T00:01:00.000Z",
        "voteCount": 3,
        "content": "Answer is A , Q40 -- https://files.training.databricks.com/assessments/practice-exams/PracticeExam-DataEngineerAssociate.pdf"
      },
      {
        "date": "2024-09-08T02:02:00.000Z",
        "voteCount": 1,
        "content": "differenct question"
      },
      {
        "date": "2024-02-07T15:12:00.000Z",
        "voteCount": 2,
        "content": "the question on Practice set is slightly different if you look closely :-In the first scenario, the data analyst notes slow query performance for sequentially run queries on a SQL endpoint that isn't shared with other users. This suggests that the problem may be related to the configuration or performance of the SQL endpoint itself rather than contention with other users.\n\nIn the second scenario, the data analysis team experiences slow query performance when multiple team members are running queries simultaneously on the same SQL endpoint. This indicates potential resource contention or limitations on the SQL endpoint when handling concurrent queries from multiple users.\n\nGiven these differences, the approaches to address the issues may also differ:"
      },
      {
        "date": "2024-01-27T00:14:00.000Z",
        "voteCount": 5,
        "content": "Just got 100% on the exam. B was correct. Also, here is the link to good explanation:\nhttps://docs.databricks.com/en/compute/cluster-config-best-practices.html"
      },
      {
        "date": "2024-01-14T12:38:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-01-13T16:43:00.000Z",
        "voteCount": 2,
        "content": "correct answer is A\nQuestion 40: https://files.training.databricks.com/assessments/practice-exams/PracticeExam-DataEngineerAssociate.pdf"
      },
      {
        "date": "2024-09-12T10:23:00.000Z",
        "voteCount": 1,
        "content": "Completely different question"
      },
      {
        "date": "2024-01-08T15:57:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-12-12T07:47:00.000Z",
        "voteCount": 3,
        "content": "its B because its \"simultanously by many users\" so you have to scale it horizontally by increasing number of nodes : https://community.databricks.com/t5/data-engineering/sequential-vs-concurrency-optimization-questions-from-query/td-p/36696"
      },
      {
        "date": "2023-11-13T06:20:00.000Z",
        "voteCount": 5,
        "content": "Issues occur when too many users are running queries at the same time -&gt; Increase scaling so more clusters handle the queries"
      },
      {
        "date": "2023-10-29T17:51:00.000Z",
        "voteCount": 2,
        "content": "Increasing cluster size is for vertical scalability of query execution, while scaling out cluster is for horizontal scalability of query execution"
      },
      {
        "date": "2023-09-16T00:21:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B\n(we can check this under databricks sql WH tool tip option. It is clearly mentioend that scaling is used to improve query \"LATANCY\")"
      },
      {
        "date": "2023-09-03T16:37:00.000Z",
        "voteCount": 1,
        "content": "A. They can increase the cluster size of the SQL endpoint.\n\nTo improve the latency of the team's queries when many members are running small queries simultaneously, you can increase the cluster size of the SQL endpoint. Increasing the cluster size allocates more compute resources to handle query execution, which can help reduce query execution times and improve overall performance, especially during periods of high query concurrency.\n\nOption B refers to adjusting scaling settings, which can also be beneficial, but increasing the cluster size (Option A) directly allocates more resources, making it a more direct approach to improving query performance.\n\nOptions C, D, and E relate to different features and configurations (Auto Stop, Serverless, and Spot Instance Policy), but they may not directly address the issue of improving query latency during high concurrency, which is the primary concern in this scenario."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/databricks/view/104899-exam-certified-data-engineer-associate-topic-1-question-40/",
    "body": "A data engineer wants to schedule their Databricks SQL dashboard to refresh once per day, but they only want the associated SQL endpoint to be running when it is necessary.<br>Which of the following approaches can the data engineer use to minimize the total running time of the SQL endpoint used in the refresh schedule of their dashboard?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can ensure the dashboard\u2019s SQL endpoint matches each of the queries\u2019 SQL endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up the dashboard\u2019s SQL endpoint to be serverless.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can turn on the Auto Stop feature for the SQL endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can reduce the cluster size of the SQL endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can ensure the dashboard\u2019s SQL endpoint is not one of the included query\u2019s SQL endpoint."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-05T05:16:00.000Z",
        "voteCount": 10,
        "content": "The data engineer can use the Auto Stop feature to minimize the total running time of the SQL endpoint used in the refresh schedule of their dashboard. The Auto Stop feature allows the SQL endpoint to automatically shut down when there are no active connections, which will minimize the total running time of the SQL endpoint. By scheduling the dashboard to refresh once per day, the SQL endpoint will only be running for a short period of time each day, which will minimize the total running time and reduce costs."
      },
      {
        "date": "2024-08-08T23:17:00.000Z",
        "voteCount": 2,
        "content": "The Auto Stop feature ensures that the SQL endpoint will automatically shut down when not in use, which helps in reducing unnecessary running time and associated costs. The endpoint will only be running when it's needed for refreshing the dashboard."
      },
      {
        "date": "2024-01-08T16:00:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-11-30T16:34:00.000Z",
        "voteCount": 1,
        "content": "Why it can't be B ? . They can set up the dashboard\u2019s SQL endpoint to be serverless. ? they can use a serverless endpoint and it will only be active when required."
      },
      {
        "date": "2023-11-08T02:34:00.000Z",
        "voteCount": 1,
        "content": "correct : C"
      },
      {
        "date": "2023-10-30T00:23:00.000Z",
        "voteCount": 1,
        "content": "C. They can turn on the Auto Stop feature for the SQL endpoint."
      },
      {
        "date": "2023-09-03T16:40:00.000Z",
        "voteCount": 2,
        "content": "C. They can turn on the Auto Stop feature for the SQL endpoint.\n\nTo minimize the total running time of the SQL endpoint used in the refresh schedule of their dashboard while ensuring that it only runs when necessary, the data engineer can turn on the Auto Stop feature for the SQL endpoint. This feature will automatically stop the SQL endpoint when it is idle for a specified period, reducing costs by avoiding unnecessary running time.\n\nOption C allows you to efficiently manage the SQL endpoint's lifecycle, ensuring it's active only when needed, which aligns with the goal of minimizing running time and associated costs.\n\nOption B (setting the dashboard's SQL endpoint to be serverless) can also be a valid approach, as it allows the SQL endpoint to be provisioned on-demand and incurs costs only when queries are executed. However, it depends on the specific requirements of your dashboard and queries.\n\nOptions A, D, and E do not directly address the goal of minimizing the SQL endpoint's running time while ensuring it runs when necessary."
      },
      {
        "date": "2023-08-28T08:59:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-20T03:57:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/clusters/clusters-manage.html#automatic-termination"
      },
      {
        "date": "2023-07-08T08:14:00.000Z",
        "voteCount": 2,
        "content": "C\nThe Auto Stop feature automatically terminates the compute resources (cluster) associated with the SQL endpoint after a specified period of inactivity. By enabling this feature, the SQL endpoint will be automatically stopped when it is no longer needed, reducing the total running time and associated costs."
      },
      {
        "date": "2023-04-02T11:25:00.000Z",
        "voteCount": 4,
        "content": "Correct answer"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/databricks/view/104900-exam-certified-data-engineer-associate-topic-1-question-41/",
    "body": "A data engineer has been using a Databricks SQL dashboard to monitor the cleanliness of the input data to an ELT job. The ELT job has its Databricks SQL query that returns the number of input records containing unexpected NULL values. The data engineer wants their entire team to be notified via a messaging webhook whenever this value reaches 100.<br>Which of the following approaches can the data engineer use to notify their entire team via a messaging webhook whenever the number of NULL values reaches 100?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up an Alert with a custom template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up an Alert with a new email alert destination.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up an Alert with a new webhook alert destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up an Alert with one-time notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up an Alert without notifications."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-02T11:27:00.000Z",
        "voteCount": 5,
        "content": "Correct answer."
      },
      {
        "date": "2023-12-19T19:09:00.000Z",
        "voteCount": 3,
        "content": "https://docs.databricks.com/en/lakehouse-monitoring/monitor-alerts.html \n\nMonitor alerts are created and used the same way as other Databricks SQL alerts. You create a Databricks SQL query on the monitor profile metrics table or drift metrics table. You then create a Databricks SQL alert for this query. You can configure the alert to evaluate the query at a desired frequency, and send a notification if the alert is triggered. By default, email notification is sent. You can also set up a webhook or send notifications to other applications such as Slack or Pagerduty."
      },
      {
        "date": "2023-11-08T02:51:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-09-03T16:43:00.000Z",
        "voteCount": 2,
        "content": "C. They can set up an Alert with a new webhook alert destination.\n\nTo notify their entire team via a messaging webhook whenever the number of NULL values reaches 100, the data engineer can set up an Alert in Databricks with a new webhook alert destination. This allows them to configure the alert to trigger when the specified condition (reaching 100 NULL values) is met, and the notification can be sent to the team's messaging webhook.\n\nOption C provides the specific approach to achieve the desired outcome of notifying the team via a messaging webhook when the condition is met."
      },
      {
        "date": "2023-07-08T08:57:00.000Z",
        "voteCount": 3,
        "content": "C\nAlerts allow you to be notified when something goes wrong in your Databricks environment. You can set up alerts to be notified by email, webhook, or Slack.\nWebhooks are a way to send data from one application to another. You can use a webhook to send data from Databricks to a messaging service, such as Slack or PagerDuty.\nOne-time notifications allow you to be notified only once when an alert is triggered. This is useful if you only want to be notified about a specific event.\nCustom templates allow you to customize the email or webhook notification that is sent when an alert is triggered. This is useful if you want to include additional information in the notification, such as the name of the alert or the value of the metric that triggered the alert."
      },
      {
        "date": "2023-04-05T05:19:00.000Z",
        "voteCount": 4,
        "content": "The approach the data engineer can use to notify their entire team via a messaging webhook whenever the number of NULL values reaches 100 is:\n\nC. They can set up an Alert with a new webhook alert destination.\n\nExplanation:\nTo achieve this, the data engineer can set up an Alert in the Databricks workspace that triggers when the query results exceed the threshold of 100 NULL values. They can create a new webhook alert destination in the Alert's configuration settings and provide the necessary messaging webhook URL to receive notifications. When the Alert is triggered, it will send a message to the configured webhook URL, which will then notify the entire team of the issue."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/databricks/view/105272-exam-certified-data-engineer-associate-topic-1-question-42/",
    "body": "A single Job runs two notebooks as two separate tasks. A data engineer has noticed that one of the notebooks is running slowly in the Job\u2019s current run. The data engineer asks a tech lead for help in identifying why this might be the case.<br>Which of the following approaches can the tech lead use to identify why the notebook is running slowly as part of the Job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the Runs tab in the Jobs UI to immediately review the processing notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the Tasks tab in the Jobs UI and click on the active run to review the processing notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the Runs tab in the Jobs UI and click on the active run to review the processing notebook.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no way to determine why a Job task is running slowly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the Tasks tab in the Jobs UI to immediately review the processing notebook."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T23:08:00.000Z",
        "voteCount": 1,
        "content": "c is correct"
      },
      {
        "date": "2023-12-29T22:33:00.000Z",
        "voteCount": 1,
        "content": "The tech lead can navigate to the Runs tab in the Jobs UI and click on the active run to review the processing notebook (Option C). This will allow them to inspect the details of the job run, including the duration of each task, which can help identify potential performance issues.\n\nThere could be several reasons why a notebook is running slowly as part of a job. For instance, there might be a delay when the job cluster has to be spun up, or the table gets delta cached in memory and copies of files will be stored on local node\u2019s storage. Even certain operations like pandas UDFs can be slow.\n\nPlease note that the exact process may vary depending on the specific configurations and permissions set up in your workspace. It\u2019s always a good idea to consult with your organization\u2019s IT or data governance team to ensure the correct procedures are followed."
      },
      {
        "date": "2023-12-29T06:26:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer as we monitor job and performance of task in same way in my current project .\nTask tab to add another task or edit existing one"
      },
      {
        "date": "2023-11-08T03:25:00.000Z",
        "voteCount": 1,
        "content": "C is correct."
      },
      {
        "date": "2023-08-19T18:31:00.000Z",
        "voteCount": 4,
        "content": "The job run details page contains job output and links to logs, including information about the success or failure of each task in the job run. You can access job run details from the Runs tab for the job. To view job run details from the Runs tab, click the link for the run in the Start time column in the runs list view. To return to the Runs tab for the job, click the Job ID value.\n\nIf the job contains multiple tasks, click a task to view task run details, including:\n\nthe cluster that ran the task\n\nthe Spark UI for the task\n\nlogs for the task\n\nmetrics for the task\n\nhttps://docs.databricks.com/en/workflows/jobs/monitor-job-runs.html#job-run-details"
      },
      {
        "date": "2023-07-08T09:18:00.000Z",
        "voteCount": 3,
        "content": "C\nIn the Databricks Jobs UI, the Runs tab provides detailed information about the execution of each run in a Job. By clicking on the active run associated with the notebook running slowly, you can access the specific run details, including the notebook execution logs, execution duration, resource utilization, and any error messages or warnings."
      },
      {
        "date": "2023-04-27T04:24:00.000Z",
        "voteCount": 2,
        "content": "\"Job runs\" tab"
      },
      {
        "date": "2023-04-05T09:12:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer. See link\nhttps://docs.databricks.com/workflows/jobs/jobs.html"
      },
      {
        "date": "2023-04-05T05:22:00.000Z",
        "voteCount": 2,
        "content": "B. They can navigate to the Tasks tab in the Jobs UI and click on the active run to review the processing notebook.\n\nThe Tasks tab in the Jobs UI provides detailed information about each task in the job, including the task's execution time, the task's logs, and the task's output. By clicking on the active run for the notebook that is running slowly, the tech lead can review the task's logs and output to identify any issues that might be causing the slowdown. The Runs tab provides an overview of all runs of the job, but it does not provide detailed information about each task in the job."
      },
      {
        "date": "2023-04-05T09:12:00.000Z",
        "voteCount": 2,
        "content": "Wrong answer. Please see documentation and you will realize the correct answer is C\nhttps://docs.databricks.com/workflows/jobs/jobs.html"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/databricks/view/104901-exam-certified-data-engineer-associate-topic-1-question-43/",
    "body": "A data engineer has a Job with multiple tasks that runs nightly. Each of the tasks runs slowly because the clusters take a long time to start.<br>Which of the following actions can the data engineer perform to improve the start up time for the clusters used for the Job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can use endpoints available in Databricks SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can use jobs clusters instead of all-purpose clusters",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can configure the clusters to be single-node",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can use clusters that are from a cluster pool\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can configure the clusters to autoscale for larger data sizes"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-08T09:21:00.000Z",
        "voteCount": 8,
        "content": "D\nCluster pools are a way to pre-provision clusters that are ready to use. This can reduce the start up time for clusters, as they do not have to be created from scratch.\nAll-purpose clusters are not pre-provisioned, so they will take longer to start up.\nJobs clusters are a type of cluster pool, but they are not the best option for this use case. Jobs clusters are designed for long-running jobs, and they can be more expensive than other types of cluster pools.\nSingle-node clusters are the smallest type of cluster, and they will start up the fastest. However, they may not be powerful enough to run the Job's tasks.\nAutoscaling clusters can scale up or down based on demand. This can help to improve the start up time for clusters, as they will only be created when they are needed. However, autoscaling clusters can also be more expensive than other types of cluster pool"
      },
      {
        "date": "2024-08-08T23:42:00.000Z",
        "voteCount": 1,
        "content": "Cluster pools help to reduce cluster startup times by maintaining a pool of pre-warmed clusters that can be quickly allocated when needed. This minimizes the overhead associated with starting a new cluster from scratch, thus improving the efficiency and speed of running tasks in the Job."
      },
      {
        "date": "2024-04-28T23:10:00.000Z",
        "voteCount": 1,
        "content": "to be fair B might seem correct but D is more appropriate for reducing start up times"
      },
      {
        "date": "2023-12-29T22:37:00.000Z",
        "voteCount": 3,
        "content": "D. They can use clusters that are from a cluster pool.\n\nExplanation:\n\nCluster Pools: Cluster pools in Databricks allow for the pre-creation and management of clusters in a pool that are readily available for use. With cluster pools, clusters are pre-initialized and kept in a ready state, minimizing the startup time when tasks need to run. This reduces the overhead of cluster initialization as the clusters are already provisioned and waiting for the tasks to be assigned.\n\nUsing clusters from a pool ensures that there is no wait time for cluster initialization when the tasks start running in the nightly Job. This approach significantly reduces the time taken for clusters to start, thereby improving the overall performance and efficiency of the tasks by minimizing the overhead of cluster startup delays."
      },
      {
        "date": "2023-11-01T09:50:00.000Z",
        "voteCount": 3,
        "content": "They must use clusters from a pool if they want to reduce the startup time."
      },
      {
        "date": "2023-09-03T16:45:00.000Z",
        "voteCount": 3,
        "content": "D. They can use clusters that are from a cluster pool.\n\nTo improve startup time for the clusters used for the Job, the data engineer can configure the clusters to be sourced from a cluster pool. Cluster pools are pre-allocated clusters that are kept in a running state, ready for use. This eliminates the need to start new clusters from scratch each time a Job runs, significantly reducing startup times.\n\nCluster pools are designed to optimize cluster reuse, making them an efficient choice for recurring jobs like the one described in the scenario.\n\nOption D provides a practical solution to address the slow cluster startup time issue."
      },
      {
        "date": "2023-08-19T18:40:00.000Z",
        "voteCount": 3,
        "content": "You can minimize instance acquisition time by creating a pool for each instance type and Databricks runtime your organization commonly uses. \n\nSOURCE : https://docs.databricks.com/en/clusters/pool-best-practices.html"
      },
      {
        "date": "2023-04-11T11:55:00.000Z",
        "voteCount": 4,
        "content": "D: use clusters that are from a cluster pool.\n\nUsing clusters from a cluster pool can improve the start-up time for the clusters used in the Job because the pool contains preconfigured and pre-started clusters that can be used immediately. This can save time and resources compared to starting new clusters for each task."
      },
      {
        "date": "2023-04-05T05:25:00.000Z",
        "voteCount": 4,
        "content": "D. They can use clusters that are from a cluster pool. Cluster pools allow you to pre-create a pool of ready-to-use clusters that can be used for running jobs, thereby eliminating the need to start new clusters each time a job runs. This can greatly reduce the startup time for each task."
      },
      {
        "date": "2023-04-02T11:32:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer. Job clusters are best suited for automated tasks running on a schedule."
      },
      {
        "date": "2023-04-03T10:11:00.000Z",
        "voteCount": 1,
        "content": "\"Cluster pools allow us to reserve VM's ahead of time, when a new job cluster is created VM are grabbed from the pool. Note: when the VM's are waiting to be used by the cluster only cost incurred is Azure. Databricks run time cost is only billed once VM is allocated to a cluster. Use Databricks cluser pools feature to reduce the startup time\""
      },
      {
        "date": "2023-04-03T21:25:00.000Z",
        "voteCount": 2,
        "content": "D es la respuesta correcta"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/databricks/view/104902-exam-certified-data-engineer-associate-topic-1-question-44/",
    "body": "A new data engineering team team. has been assigned to an ELT project. The new data engineering team will need full privileges on the database customers to fully manage the project.<br>Which of the following commands can be used to grant full permissions on the database to the new data engineering team?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT USAGE ON DATABASE customers TO team;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT ALL PRIVILEGES ON DATABASE team TO customers;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT SELECT PRIVILEGES ON DATABASE customers TO teams;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT SELECT CREATE MODIFY USAGE PRIVILEGES ON DATABASE customers TO team;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT ALL PRIVILEGES ON DATABASE customers TO team;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T23:12:00.000Z",
        "voteCount": 1,
        "content": "e is correct"
      },
      {
        "date": "2024-03-16T23:05:00.000Z",
        "voteCount": 1,
        "content": "Examtopics not showing all the questions and asking for contributor access.  I can only see qestions till 44. Anyone is able to see all 99 questions??????"
      },
      {
        "date": "2024-03-18T17:42:00.000Z",
        "voteCount": 1,
        "content": "i think the 99 questions have been relabelled as Q1-44 then Q1-45"
      },
      {
        "date": "2024-01-18T06:35:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-12-29T07:03:00.000Z",
        "voteCount": 4,
        "content": "E is correct \nTemplate to give access--&gt;\nGRANT Privilege ON Object &lt;object-name&gt; TO &lt;user or group&gt;\n\nALL PRIVILEGES = gives all privilege"
      },
      {
        "date": "2023-11-08T03:35:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2023-11-01T09:52:00.000Z",
        "voteCount": 2,
        "content": "Right answer is E.\nThe template to respect is the following: GRANT &lt;privilege&gt; ON &lt;resource&gt; TO &lt;user/group&gt;"
      },
      {
        "date": "2023-09-03T16:46:00.000Z",
        "voteCount": 2,
        "content": "E. GRANT ALL PRIVILEGES ON DATABASE customers TO team;\n\nTo grant full privileges on the database \"customers\" to the new data engineering team, you can use the GRANT ALL PRIVILEGES command as shown in option E. This command provides the team with all possible privileges on the specified database, allowing them to fully manage it.\n\nOption A is not correct because it grants only the USAGE privilege, which is not sufficient for full management.\n\nOption B has the syntax reversed, and it is attempting to grant privileges on the \"team\" database to the \"customers\" database, which is not the desired action.\n\nOption C contains incorrect syntax and should use \"team\" instead of \"teams.\"\n\nOption D has incorrect syntax and is not a valid SQL command for granting privileges in most database management systems."
      },
      {
        "date": "2023-07-08T09:24:00.000Z",
        "voteCount": 1,
        "content": "E\nGRANT ALL PRIVILEGES ON DATABASE customers TO team;"
      },
      {
        "date": "2023-04-05T07:05:00.000Z",
        "voteCount": 2,
        "content": "Option E"
      },
      {
        "date": "2023-04-02T11:33:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is E. Please take note of how the questions are worded to avoid confusion and not make the wrong choice."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/databricks/view/104870-exam-certified-data-engineer-associate-topic-1-question-45/",
    "body": "A new data engineering team has been assigned to work on a project. The team will need access to database customers in order to see what tables already exist. The team has its own group team.<br>Which of the following commands can be used to grant the necessary permission on the entire database to the new team?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT VIEW ON CATALOG customers TO team;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT CREATE ON DATABASE customers TO team;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT USAGE ON CATALOG team TO customers;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT CREATE ON DATABASE team TO customers;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT USAGE ON DATABASE customers TO team;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-07T10:48:00.000Z",
        "voteCount": 14,
        "content": "E is the correct answer."
      },
      {
        "date": "2023-12-12T12:12:00.000Z",
        "voteCount": 7,
        "content": "GRANT USAGE, answer E is correct. I dont see such privilage as GRANT VIEW https://docs.databricks.com/en/sql/language-manual/sql-ref-privileges.html#privilege-types"
      },
      {
        "date": "2024-08-16T01:36:00.000Z",
        "voteCount": 1,
        "content": "E is correct\n\nA: GRANT VIEW ON CATALOG is not a valid syntax in many SQL environments for database-level access.\nB: GRANT CREATE ON DATABASE customers would grant the team the ability to create objects in the database, which is more than just viewing the existing tables.\nC: This command incorrectly reverses the order of the team and the customers database.\nD: This would grant creation privileges on the team's database to the customers group, which is not relevant to the scenario."
      },
      {
        "date": "2024-06-19T05:53:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2024-03-17T07:30:00.000Z",
        "voteCount": 1,
        "content": "The question is asking \"The team will need access to database customers in order to see what tables already exist.\" The presumption is, hoever, the team already has usage privilege on the catalog containing the database."
      },
      {
        "date": "2023-10-18T18:34:00.000Z",
        "voteCount": 4,
        "content": "Customers is a Database and not a catalog. \nThe three-space naming convention has Catalog at the top level(catalog.database(schema).table). \nSo granting a usage on catalog will expose other objects."
      },
      {
        "date": "2023-10-11T04:56:00.000Z",
        "voteCount": 1,
        "content": "Option A, \"GRANT VIEW ON CATALOG customers TO team,\" grants the privilege to view the catalog but not access the database's tables, so it might not fulfill the requirement of seeing the existing tables in the database.\nSo answer is E"
      },
      {
        "date": "2023-10-04T23:45:00.000Z",
        "voteCount": 3,
        "content": "E.\nIt can NOT be A.\nYou can GRANT SELECT, but you cannot GRANT VIEW.\nVIEW is a securable OBJECT and not a PRIVILEGE TYPE, so you cannot grant it.\nSee also https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/manage-privileges/privileges"
      },
      {
        "date": "2023-09-11T08:51:00.000Z",
        "voteCount": 2,
        "content": "It's A....I got 100% on the Data Governance section...it's A"
      },
      {
        "date": "2023-10-07T08:26:00.000Z",
        "voteCount": 1,
        "content": "There is no such thing as GRANT VIEW, so A is not a valid option"
      },
      {
        "date": "2023-09-03T16:48:00.000Z",
        "voteCount": 1,
        "content": "A. GRANT VIEW ON CATALOG customers TO team;\n\nTo grant the new data engineering team the necessary permission to view the tables that exist in the \"customers\" database, you can use the GRANT VIEW ON CATALOG command as shown in option A. This command allows the team to see the metadata and information about the tables in the specified catalog or database, which is what you want in this case.\n\nOption B grants the CREATE privilege on the \"customers\" database to the team, which is not necessary for simply viewing existing tables.\n\nOption C and Option D have the syntax reversed, attempting to grant permissions from the team to the \"customers\" database, which is not the desired action.\n\nOption E grants USAGE privilege on the \"customers\" database to the team, which allows them to use the database but may not provide the necessary view permissions to see existing tables."
      },
      {
        "date": "2024-09-02T08:55:00.000Z",
        "voteCount": 1,
        "content": "Wrong, customers is a database, not catalog."
      },
      {
        "date": "2023-08-20T04:00:00.000Z",
        "voteCount": 2,
        "content": "SOURCE : https://docs.databricks.com/en/sql/language-manual/security-grant.html\n\nThe perfect answer would be : GRANT SHOW METADATA ON DATABASE customers TO team\nBut because, it is not suggested, we need to find an impertect answer allowing listing tables on database customers for project team's group : team\n\nA. GRANT VIEW ON CATALOG customers TO team  -- Incorrect : \"GRANT VIEW\" does not exist\nB. GRANT CREATE ON DATABASE customers TO team  -- Correct : only possible answer by elimitation. \nC. GRANT USAGE ON CATALOG team TO customers  -- Incorrect : \"GRANT USAGE\" does not allow listing tables\nD. GRANT CREATE ON DATABASE team TO customers  -- Incorrect : \"team\" and \"customers\" are inverted\nE. GRANT USAGE ON DATABASE customers TO team  -- Incorrect : \"GRANT USAGE\" does not allow listing tables"
      },
      {
        "date": "2023-08-20T08:49:00.000Z",
        "voteCount": 1,
        "content": "In addition to what was typed previously, I'm adding an extra source : https://docs.databricks.com/en/data-governance/table-acls/object-privileges.html#usage-privilege\n\nSo correct syntax for what I previously wrote is : \nGRANT SHOW READ_METADATA ON DATABASE customers TO team\nand not : \nGRANT SHOW METADATA ON DATABASE customers TO team"
      },
      {
        "date": "2023-07-25T04:27:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is E. GRANT ALL PRIVILEGES ON DATABASE customers TO team;\n\nThe GRANT statement is used to grant privileges on a database, table, or view to a user or role. The ALL PRIVILEGES option grants all possible privileges on the specified object, such as CREATE, SELECT, MODIFY, and USAGE. The syntax of the GRANT statement is:\n\nGRANT privilege_type ON object TO user_or_role;\n\nTherefore, to grant full permissions on the database customers to the new data engineering team, the command should be:\n\nGRANT ALL PRIVILEGES ON DATABASE customers TO team;\n\nOption A is incorrect because it only grants the USAGE privilege, which allows the team to access the database but not to create or modify any tables or views in it."
      },
      {
        "date": "2023-07-08T21:27:00.000Z",
        "voteCount": 1,
        "content": "E\nThe GRANT USAGE ON DATABASE command grants the USAGE privilege on the specified database to the specified group. The USAGE privilege allows the group to see the tables that exist in the database, but it does not allow them to do anything else with the database.\n\nThe other commands are incorrect. The GRANT VIEW ON CATALOG command grants the VIEW privilege on the specified catalog to the specified group. The CREATE privilege allows the group to create new tables in the database. The USAGE privilege on the CATALOG does not exist."
      },
      {
        "date": "2023-07-01T08:34:00.000Z",
        "voteCount": 2,
        "content": "The correct is GRANT both \"USAGE\" and \"SELECT\" on DATABASE CUSTOMERS TO TEAMS"
      },
      {
        "date": "2023-06-01T06:35:00.000Z",
        "voteCount": 1,
        "content": "e is the right answer"
      },
      {
        "date": "2023-05-13T06:43:00.000Z",
        "voteCount": 2,
        "content": "Think the Answer is E. \nThe privilege types are CREATE, MODIFY, READ_METADATA, SELECT and USAGE. There is no such thing as GRANT VIEW. (it can however be GRANT &lt;p-type&gt; ON VIEW TO &lt;team&gt;). For our use case we want to GRANT USAGE for reading the Database. C has wrong syntax."
      },
      {
        "date": "2023-05-10T13:19:00.000Z",
        "voteCount": 1,
        "content": "option A is correct .\n\nQuestion says that  \"The team will need access to database customers in order to see what tables already exist\" this looks like they need view access"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/databricks/view/124124-exam-certified-data-engineer-associate-topic-1-question-46/",
    "body": "A data engineer is running code in a Databricks Repo that is cloned from a central Git repository. A colleague of the data engineer informs them that changes have been made and synced to the central Git repository. The data engineer now needs to sync their Databricks Repo to get the changes from the central Git repository.<br><br>Which of the following Git operations does the data engineer need to run to accomplish this task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPush",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPull\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCommit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClone"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T23:33:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2024-03-12T09:10:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-10-29T18:05:00.000Z",
        "voteCount": 1,
        "content": "This is more of a Git question.\n\nFrom the docs:\nIn Databricks Repos, you can use Git functionality to:\n    Clone, push to, and pull from a remote Git repository.\n    Create and manage branches for development work, including merging, rebasing, and resolving conflicts.\n    Create notebooks&amp;mdash;including IPYNB notebooks&amp;mdash;and edit them and other files.\n    Visually compare differences upon commit and resolve merge conflicts.\n\nSource: https://docs.databricks.com/en/repos/index.html"
      },
      {
        "date": "2023-10-20T07:48:00.000Z",
        "voteCount": 2,
        "content": "pull is required from the Databricks Repo to sync the changes b/w local and central repo."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/databricks/view/124066-exam-certified-data-engineer-associate-topic-1-question-47/",
    "body": "Which of the following is a benefit of the Databricks Lakehouse Platform embracing open source technologies?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud-specific integrations",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSimplified governance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAbility to scale storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAbility to scale workloads",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAvoiding vendor lock-in\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T06:26:00.000Z",
        "voteCount": 1,
        "content": "By embracing open-source technologies, the platform allows users to avoid being locked into a single vendor's ecosystem, offering flexibility and the ability to integrate with a wide range of tools and systems."
      },
      {
        "date": "2024-04-28T23:33:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-01-25T17:42:00.000Z",
        "voteCount": 4,
        "content": "E is correct as open-source is opposite of proprietary technology, so not being a proprietary means it is free of vendor lock in, if that makes sense."
      },
      {
        "date": "2023-10-22T11:24:00.000Z",
        "voteCount": 3,
        "content": "its avoiding vendor lock in  : - https://double.cloud/blog/posts/2023/01/break-free-from-vendor-lock-in-with-open-source-tech/"
      },
      {
        "date": "2023-10-20T07:51:00.000Z",
        "voteCount": 2,
        "content": "E looks to be the correct one, as Databricks Lakeshouse platform supports Delta table which is an open-source format for storage."
      },
      {
        "date": "2023-10-19T12:36:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/databricks/view/124368-exam-certified-data-engineer-associate-topic-1-question-48/",
    "body": "A data engineer needs to use a Delta table as part of a data pipeline, but they do not know if they have the appropriate permissions.<br><br>In which of the following locations can the data engineer review their permissions on the table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Filesystem",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJobs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDashboards",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRepos",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Explorer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T06:28:00.000Z",
        "voteCount": 2,
        "content": "Data Explorer in Databricks allows users to view and manage permissions for tables, schemas, and databases."
      },
      {
        "date": "2024-04-28T23:34:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-12-06T12:50:00.000Z",
        "voteCount": 4,
        "content": "E is correct answer"
      },
      {
        "date": "2023-10-22T11:23:00.000Z",
        "voteCount": 2,
        "content": "E is correct Data explorer"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/databricks/view/124125-exam-certified-data-engineer-associate-topic-1-question-49/",
    "body": "Which of the following describes a scenario in which a data engineer will want to use a single-node cluster?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen they are working interactively with a small amount of data\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen they are running automated reports to be refreshed as quickly as possible",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen they are working with SQL within Databricks SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen they are concerned about the ability to automatically scale with larger data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen they are manually running reports with a large amount of data"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-20T07:55:00.000Z",
        "voteCount": 5,
        "content": "Single node clusters can be used for interactive queries with small dataset"
      },
      {
        "date": "2024-04-28T23:35:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-01-20T09:07:00.000Z",
        "voteCount": 2,
        "content": "A seems correct for this"
      },
      {
        "date": "2023-10-22T11:26:00.000Z",
        "voteCount": 4,
        "content": "ans A : A Single Node cluster is a cluster consisting of an Apache Spark driver and no Spark workers. A Single Node cluster supports Spark jobs and all Spark data sources, including Delta Lake. A Standard cluster requires a minimum of one Spark worker to run Spark jobs.\nhttps://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwidg8mSsYqCAxUmg2oFHbkTDJsQFnoECA4QAw&amp;url=https%3A%2F%2Fdocs.databricks.com%2Fen%2Fclusters%2Fsingle-node.html%23%3A~%3Atext%3DA%2520Single%2520Node%2520cluster%2520is%2Cworker%2520to%2520run%2520Spark%2520jobs.&amp;usg=AOvVaw3PFq3_Qyt2gAAa4id0j6CS&amp;opi=89978449"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/databricks/view/124370-exam-certified-data-engineer-associate-topic-1-question-50/",
    "body": "A data engineer has been given a new record of data:<br><br>id STRING = 'a1'<br>rank INTEGER = 6<br>rating FLOAT = 9.4<br><br>Which of the following SQL commands can be used to append the new record to an existing Delta table my_table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tINSERT INTO my_table VALUES ('a1', 6, 9.4)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmy_table UNION VALUES ('a1', 6, 9.4)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tINSERT VALUES ( 'a1' , 6, 9.4) INTO my_table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUPDATE my_table VALUES ('a1', 6, 9.4)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUPDATE VALUES ('a1', 6, 9.4) my_table"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T06:33:00.000Z",
        "voteCount": 1,
        "content": "insert into table comment"
      },
      {
        "date": "2024-04-28T23:35:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-01-20T09:09:00.000Z",
        "voteCount": 2,
        "content": "A is correct because syntax is correct"
      },
      {
        "date": "2024-01-09T07:30:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-10-22T11:27:00.000Z",
        "voteCount": 3,
        "content": "Ans A : check the correct syntax for insert into"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/databricks/view/124126-exam-certified-data-engineer-associate-topic-1-question-51/",
    "body": "A data engineer has realized that the data files associated with a Delta table are incredibly small. They want to compact the small files to form larger files to improve performance.<br><br>Which of the following keywords can be used to compact the small files?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tREDUCE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOPTIMIZE\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCOMPACTION",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tREPARTITION",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVACUUM"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-20T07:57:00.000Z",
        "voteCount": 5,
        "content": "OPTIMIZE can be used to club small files into 1 and improve performance."
      },
      {
        "date": "2024-08-10T06:30:00.000Z",
        "voteCount": 1,
        "content": "The OPTIMIZE command in Databricks is used to compact small files into larger ones, improving the performance of Delta tables."
      },
      {
        "date": "2024-08-09T00:18:00.000Z",
        "voteCount": 1,
        "content": "The OPTIMIZE command in Delta Lake merges small files into larger files, which can help improve query performance and manage storage more efficiently."
      },
      {
        "date": "2024-04-28T23:36:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-25T17:49:00.000Z",
        "voteCount": 2,
        "content": "OPTIMIZE is the correct answer. Compacting small files using the OPTIMIZE command improves table performance such as by combining multiple small files into larger ones."
      },
      {
        "date": "2024-01-20T09:10:00.000Z",
        "voteCount": 2,
        "content": "OPTIMIZE would help in this scenario"
      },
      {
        "date": "2023-12-12T12:41:00.000Z",
        "voteCount": 2,
        "content": "Its B https://docs.databricks.com/en/delta/optimize.html"
      },
      {
        "date": "2023-10-22T11:27:00.000Z",
        "voteCount": 3,
        "content": "Ans B : optimize is used to compact small files which in turn improves perf"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/databricks/view/124287-exam-certified-data-engineer-associate-topic-1-question-52/",
    "body": "In which of the following file formats is data from Delta Lake tables primarily stored?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA proprietary, optimized format specific to Databricks"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T06:32:00.000Z",
        "voteCount": 1,
        "content": "Delta Lake builds on top of the Parquet file format, adding features like ACID transactions, versioning, and more, while leveraging Parquet's efficient columnar storage capabilities."
      },
      {
        "date": "2024-08-09T00:19:00.000Z",
        "voteCount": 1,
        "content": "Delta Lake tables use the Parquet file format for storing data, which is a columnar storage format optimized for performance and efficient data processing."
      },
      {
        "date": "2024-04-28T23:36:00.000Z",
        "voteCount": 1,
        "content": "Parquet for data and JSON for metadata"
      },
      {
        "date": "2024-01-20T09:11:00.000Z",
        "voteCount": 1,
        "content": "Parquet"
      },
      {
        "date": "2023-12-05T06:17:00.000Z",
        "voteCount": 1,
        "content": "Parquet format because its columnar format, much faster alternative to CSV because it supports partition pruning for example. No such file format as \"Delta\""
      },
      {
        "date": "2023-10-30T05:12:00.000Z",
        "voteCount": 3,
        "content": "Parquet format is correct"
      },
      {
        "date": "2023-10-27T22:15:00.000Z",
        "voteCount": 1,
        "content": "Parquet it is"
      },
      {
        "date": "2023-10-23T23:33:00.000Z",
        "voteCount": 1,
        "content": "parquet format"
      },
      {
        "date": "2023-10-27T22:14:00.000Z",
        "voteCount": 4,
        "content": "Buddy it should be Parquet, hence C"
      },
      {
        "date": "2023-10-22T11:30:00.000Z",
        "voteCount": 1,
        "content": "so i think data from delta lake is stored in parquet format .. while the storage format seems to be delta .. very confusing\nsome notes :\nWhat format does Delta Lake use to store data? Delta Lake uses versioned Parquet files to store your data in your cloud storage. Apart from the versions, Delta Lake also stores a transaction log to keep track of all the commits made to the table or blob store directory to provide ACID transactions.\nhttps://docs.delta.io/latest/delta-faq.html"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/databricks/view/124286-exam-certified-data-engineer-associate-topic-1-question-53/",
    "body": "Which of the following is stored in the Databricks customer's cloud account?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks web application",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster management metadata",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRepos",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNotebooks"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T06:35:00.000Z",
        "voteCount": 1,
        "content": "Data stored in Delta Lake or other formats on cloud storage is managed within the customer's own cloud account, while other components like the Databricks web application and cluster management metadata are managed by Databricks itself."
      },
      {
        "date": "2024-04-28T23:38:00.000Z",
        "voteCount": 2,
        "content": "Data is in Data plane"
      },
      {
        "date": "2024-02-21T21:56:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B\nBecause \nWhen the customer sets up a Spark cluster, the cluster virtual machines are deployed in the data plane in the customer's cloud account."
      },
      {
        "date": "2024-01-20T09:15:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2024-01-09T03:25:00.000Z",
        "voteCount": 2,
        "content": "D. Data"
      },
      {
        "date": "2023-10-21T20:25:00.000Z",
        "voteCount": 4,
        "content": "D. Data"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/databricks/view/124127-exam-certified-data-engineer-associate-topic-1-question-54/",
    "body": "Which of the following can be used to simplify and unify siloed data architectures that are specialized for specific use cases?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNone of these",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData lake",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData warehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll of these",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData lakehouse\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T23:38:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-01-20T09:17:00.000Z",
        "voteCount": 2,
        "content": "Lakehouse, so E is correct"
      },
      {
        "date": "2023-10-20T08:01:00.000Z",
        "voteCount": 4,
        "content": "Data Lakehouse can be used as a single source of truth for multiple specific use cases"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/databricks/view/124291-exam-certified-data-engineer-associate-topic-1-question-55/",
    "body": "A data architect has determined that a table of the following format is necessary:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image15.png\"><br><br>Which of the following code blocks uses SQL DDL commands to create an empty Delta table in the above format regardless of whether a table already exists with this name?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image16.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image17.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image18.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image19.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image20.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-21T20:36:00.000Z",
        "voteCount": 6,
        "content": "E is correct you dont need to specify Delta as its the default storage format for tables."
      },
      {
        "date": "2024-08-27T05:11:00.000Z",
        "voteCount": 1,
        "content": "The closest correct option could be C but it shouldn't have with column clause\nTo create an empty Delta table with the specified schema in SQL, you can use the CREATE TABLE statement with the USING DELTA clause. The IF NOT EXISTS option ensures that the table is created only if it does not already exist. Here\u2019s the SQL code block that accomplishes this:\n\n\nCREATE TABLE IF NOT EXISTS your_table_name (\n  employeeld STRING,\n  startDate DATE,\n  avgRating DOUBLE\n) USING DELTA;"
      },
      {
        "date": "2024-08-10T06:40:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer. create table if not exists will check if the table already exists in database. if not it will create the new table."
      },
      {
        "date": "2024-08-27T12:33:00.000Z",
        "voteCount": 1,
        "content": "Disagree.  The question states that it wants to create an empty table \"regardless of whether the table already exists\".  This means that your chosen answer A will NOT recreate the table if it exists."
      },
      {
        "date": "2024-04-28T23:39:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2024-01-26T05:33:00.000Z",
        "voteCount": 2,
        "content": "A and E have both correct syntax, but the question mentioned \"regardless of whether a table already exists with this name\". Hence the correct answer is E"
      },
      {
        "date": "2024-01-20T09:23:00.000Z",
        "voteCount": 2,
        "content": "E is correct option"
      },
      {
        "date": "2024-01-09T03:28:00.000Z",
        "voteCount": 2,
        "content": "E. correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/databricks/view/124290-exam-certified-data-engineer-associate-topic-1-question-56/",
    "body": "A data engineer has a Python notebook in Databricks, but they need to use SQL to accomplish a specific task within a cell. They still want all of the other cells to use Python without making any changes to those cells.<br><br>Which of the following describes how the data engineer can use SQL within a cell of their Python notebook?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIt is not possible to use SQL in a Python notebook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can attach the cell to a SQL endpoint rather than a Databricks cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can simply write SQL syntax in the cell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can add %sql to the first line of the cell\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can change the default language of the notebook to SQL"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T06:46:00.000Z",
        "voteCount": 1,
        "content": "we can use magic comment in notebook to indicate the cell to be run in specific language %SQL."
      },
      {
        "date": "2024-01-20T09:24:00.000Z",
        "voteCount": 1,
        "content": "Magic command % can be used to switch the language, so D is correct"
      },
      {
        "date": "2024-01-09T03:29:00.000Z",
        "voteCount": 1,
        "content": "D. Correct. Use %sql magic in first line."
      },
      {
        "date": "2023-11-23T09:13:00.000Z",
        "voteCount": 3,
        "content": "Use magic command %sql"
      },
      {
        "date": "2023-10-30T03:46:00.000Z",
        "voteCount": 3,
        "content": "correct answer D"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/databricks/view/124289-exam-certified-data-engineer-associate-topic-1-question-57/",
    "body": "Which of the following SQL keywords can be used to convert a table from a long format to a wide format?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTRANSFORM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPIVOT\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSUM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCONVERT",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWHERE"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T06:49:00.000Z",
        "voteCount": 1,
        "content": "The PIVOT operation is used to rotate data from rows to columns, which effectively converts a table from a long format (where each row represents a single observation or measurement) to a wide format (where each row represents a single entity with multiple observations or measurements as columns)."
      },
      {
        "date": "2024-04-28T23:40:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-01-20T09:26:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2023-12-19T19:38:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-qry-select-pivot.html  \n\n\u201cPivot\u201d transforms the rows of the table_reference by rotating unique values of a specified column list into separate columns. \n\nSYNTAX :  \n\ntable_reference PIVOT ( { aggregate_expression [ [ AS ] agg_column_alias ] } [, ...] \n    FOR column_list IN ( expression_list ) ) \n \ncolumn_list \n { column_name | \n   ( column_name [, ...] ) } \n \nexpression_list \n { expression [ AS ] [ column_alias ] | \n   { ( expression [, ...] ) [ AS ] [ column_alias] } [, ...] ) }"
      },
      {
        "date": "2023-10-24T08:38:00.000Z",
        "voteCount": 2,
        "content": "PIVOT is correct."
      },
      {
        "date": "2023-10-21T20:35:00.000Z",
        "voteCount": 3,
        "content": "PIVOT is correct."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/databricks/view/124209-exam-certified-data-engineer-associate-topic-1-question-58/",
    "body": "Which of the following describes a benefit of creating an external table from Parquet rather than CSV when using a CREATE TABLE AS SELECT statement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet files can be partitioned",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE TABLE AS SELECT statements cannot be used on files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet files have a well-defined schema\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet files have the ability to be optimized",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet files will become Delta tables"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-13T11:34:00.000Z",
        "voteCount": 1,
        "content": "Vote for D\nParquet files are a columnar storage file format that allows for efficient data compression and encoding schemes, enabling optimization and faster query performance compared to CSV files. This format supports efficient reading and writing of large datasets, making it a preferred choice for big data applications."
      },
      {
        "date": "2024-05-27T04:46:00.000Z",
        "voteCount": 1,
        "content": "The keywords are \"CREATE TABLE AS SELECT \""
      },
      {
        "date": "2024-04-28T23:42:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-01-25T18:02:00.000Z",
        "voteCount": 2,
        "content": "CREATE TABLE AS SELECT adopts the schema details from the source. Parquet files have a defined schema."
      },
      {
        "date": "2024-01-09T03:31:00.000Z",
        "voteCount": 1,
        "content": "C. Paruqet has well defined schema unline csv"
      },
      {
        "date": "2023-12-30T13:58:00.000Z",
        "voteCount": 1,
        "content": "C. Parquet files have a well-defined schema.\n\nExplanation:\n\nParquet files inherently store metadata about the schema within the files themselves, allowing for a well-defined schema. This schema information includes data types, column names, and other structural information. When creating an external table from Parquet, this schema is retained, providing a structured and well-defined format for the data. It ensures consistency and enables more efficient processing, query optimization, and compatibility across various systems or tools that work with the Parquet format.\nThis structured schema within Parquet files offers advantages in terms of data integrity, ease of data processing, and compatibility, making it a beneficial choice over CSV, which lacks inherent schema information and might need additional handling or inference of schema during data ingestion."
      },
      {
        "date": "2023-12-19T20:01:00.000Z",
        "voteCount": 1,
        "content": "The key word here is : CREATE TABLE AS SELECT\n\nnot A : partitioning is not relevant in a create table as statement because the data will be created in a delta table \nnot C : Parquet schema is not well defined and there can be parquet files with multiple schema in a folder\nnot D : Parquet are already optimized and are not relevant in a create table as statement because the data will be created in a delta table \nnot E : both CSV &amp; Parquet will become delta tables in a create table as statement\nB : correct answer by elimination"
      },
      {
        "date": "2023-12-12T12:50:00.000Z",
        "voteCount": 1,
        "content": "I disagree i think its D. Schema can be inferred from CSV as well, but CSV cannot provide same optimizations as Parquet"
      },
      {
        "date": "2023-10-30T19:01:00.000Z",
        "voteCount": 4,
        "content": "CTAS - CTAS automatically infer schema information from query\nresults and do not support manual schema declaration.This means\nthat CTAS statements are useful for external data ingestion from\nsources with well-defined schema, such as Parquet files and\ntables.CTAS statements also do not support specifying additional\nfile options."
      },
      {
        "date": "2023-10-30T05:42:00.000Z",
        "voteCount": 2,
        "content": "C is the correct option"
      },
      {
        "date": "2023-10-24T18:01:00.000Z",
        "voteCount": 2,
        "content": "c is correct"
      },
      {
        "date": "2023-10-21T20:40:00.000Z",
        "voteCount": 4,
        "content": "Ans : C \nhttps://www.databricks.com/glossary/what-is-parquet#:~:text=Columnar%20storage%20like%20Apache%20Parquet,compared%20to%20row%2Doriented%20databases.\n\nColumnar storage like Apache Parquet is designed to bring efficiency compared to row-based files like CSV. When querying, columnar storage you can skip over the non-relevant data very quickly. As a result, aggregation queries are less time-consuming compared to row-oriented databases."
      },
      {
        "date": "2023-10-21T08:31:00.000Z",
        "voteCount": 4,
        "content": "C.\nit supports well-defined schema, such as Parquet files and tables and do not support specifying additional file options such as Delimeter if you were to use CSV"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/databricks/view/124293-exam-certified-data-engineer-associate-topic-1-question-59/",
    "body": "A data engineer wants to create a relational object by pulling data from two tables. The relational object does not need to be used by other data engineers in other sessions. In order to save on storage costs, the data engineer wants to avoid copying and storing physical data.<br><br>Which of the following relational objects should the data engineer create?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark SQL Table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabase",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTemporary view\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta Table"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-21T20:47:00.000Z",
        "voteCount": 8,
        "content": "D is correct.\n\ufeff\ufeffTemp view : session based\n\tCreate temp view view_name as query\n\tAll these are termed as session ended:\n\t\tOpening a new notebook\n\t\tDetaching and reattaching a cluster\n\t\tInstalling a python package\nRestarting a cluster"
      },
      {
        "date": "2024-08-10T06:59:00.000Z",
        "voteCount": 1,
        "content": "A temporary view is created in memory and does not persist beyond the session. It does not require physical storage, making it ideal for avoiding storage costs."
      },
      {
        "date": "2023-12-30T14:05:00.000Z",
        "voteCount": 2,
        "content": "D. Temporary view\n\nExplanation:\n\nTemporary View: A temporary view in database systems like Apache Spark provides a temporary and ephemeral representation of data based on an SQL query's result set. It exists for the duration of a Spark session and is not persisted to storage. Similar to a regular view, a temporary view allows the data engineer to define a logical schema by pulling and combining data from multiple tables using SQL queries, but it does not store any physical data on disk. Temporary views are suitable when there's no need for long-term storage of the combined data and are helpful for immediate analysis or processing within the current session without incurring storage costs."
      },
      {
        "date": "2023-12-19T20:09:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html  \n\nShould be a view or temporary view to avoid copying and storing data.  \n\nDoes not need to be used by other data engineers in other sessions. So this view should be temporary. TEMPORARY views are visible only to the session that created them and are dropped when the session ends."
      },
      {
        "date": "2023-11-15T15:24:00.000Z",
        "voteCount": 2,
        "content": "Answer is D. key phrase is \"...does not need to be used by other data engineers in other sessions...\""
      },
      {
        "date": "2023-10-30T05:47:00.000Z",
        "voteCount": 1,
        "content": "D is right option"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/databricks/view/124128-exam-certified-data-engineer-associate-topic-1-question-60/",
    "body": "A data analyst has developed a query that runs against Delta table. They want help from the data engineering team to implement a series of tests to ensure the data returned by the query is clean. However, the data engineering team uses Python for its tests rather than SQL.<br><br>Which of the following operations could the data engineering team use to run the query and operate with the results in PySpark?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT * FROM sales",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.delta.table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no way to share data between PySpark and SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.table"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-20T08:10:00.000Z",
        "voteCount": 7,
        "content": "spark.sql() should be used to execute a SQL query with Pyspark\nspark.table() can only be used to load a table and not run a query."
      },
      {
        "date": "2024-04-28T23:47:00.000Z",
        "voteCount": 1,
        "content": "I am not sure wheter it is C or E . I see majority went for E but you can still query your data with spark.table by using purely pyspark syntax . I don't see any part of the question specifying you HAVE to use SQL syntax."
      },
      {
        "date": "2024-09-25T10:00:00.000Z",
        "voteCount": 1,
        "content": "I think that is the first part of the question \"has developed a query\".\nFrom a quick google, it looks like queries as SQL, scripts are python. So when it says a query was developed, it means its coded in sql."
      },
      {
        "date": "2023-10-21T20:49:00.000Z",
        "voteCount": 3,
        "content": "C is correct \nEG :\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.getOrCreate()\n\ndf = spark.sql(\"SELECT * FROM sales\")\n\nprint(df.count())"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/databricks/view/124129-exam-certified-data-engineer-associate-topic-1-question-61/",
    "body": "Which of the following commands will return the number of null values in the member_id column?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT count(member_id) FROM my_table;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT count(member_id) - count_null(member_id) FROM my_table;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT count_if(member_id IS NULL) FROM my_table;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT null(member_id) FROM my_table;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT count_null(member_id) FROM my_table;"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T23:45:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-01-09T03:39:00.000Z",
        "voteCount": 1,
        "content": "C: There are no 'null' and 'count_null' functions in SparkSQL"
      },
      {
        "date": "2023-11-29T03:32:00.000Z",
        "voteCount": 4,
        "content": "https://docs.databricks.com/en/sql/language-manual/functions/count_if.html"
      },
      {
        "date": "2023-10-21T20:52:00.000Z",
        "voteCount": 3,
        "content": "Ans C :\nhttps://docs.databricks.com/en/sql/language-manual/functions/count.html\n\nReturns\nA BIGINT.\n\nIf * is specified also counts row containing NULL values.\n\nIf expr are specified counts only rows for which all expr are not NULL.\n\nIf DISTINCT duplicate rows are not counted."
      },
      {
        "date": "2023-10-20T08:15:00.000Z",
        "voteCount": 3,
        "content": "count_if() can be used in this scenario"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/databricks/view/124294-exam-certified-data-engineer-associate-topic-1-question-62/",
    "body": "A data engineer needs to apply custom logic to identify employees with more than 5 years of experience in array column employees in table stores. The custom logic should create a new column exp_employees that is an array of all of the employees with more than 5 years of experience for each row. In order to apply this custom logic at scale, the data engineer wants to use the FILTER higher-order function.<br><br>Which of the following code blocks successfully completes this task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image21.png\"><br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image22.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image23.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image24.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image25.png\">"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T23:49:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-12-19T20:26:00.000Z",
        "voteCount": 2,
        "content": "B &amp; E incorrect : source is employees not exp_employees \n\nD incorrect : does not use FILTER higher-order function) \n\nC incorrect : syntax errror \n\nA : correct by elimination &amp; based on https://docs.databricks.com/en/sql/language-manual/functions/filter.html#examples"
      },
      {
        "date": "2023-12-06T13:11:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-11-29T03:50:00.000Z",
        "voteCount": 3,
        "content": "https://docs.databricks.com/en/sql/language-manual/functions/filter.html"
      },
      {
        "date": "2023-10-21T20:56:00.000Z",
        "voteCount": 4,
        "content": "A is correct."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/databricks/view/124295-exam-certified-data-engineer-associate-topic-1-question-63/",
    "body": "A data engineer has a Python variable table_name that they would like to use in a SQL query. They want to construct a Python code block that will run the query using table_name.<br><br>They have the following incomplete code block:<br><br>____(f\"SELECT customer_id, spend FROM {table_name}\")<br><br>Which of the following can be used to fill in the blank to successfully complete the task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.delta.sql",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.delta.table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbutils.sql",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T23:50:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2024-01-20T09:47:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2023-10-21T20:57:00.000Z",
        "voteCount": 4,
        "content": "E is correct you use spark.sql to execute python comamands"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/databricks/view/124130-exam-certified-data-engineer-associate-topic-1-question-64/",
    "body": "A data engineer has created a new database using the following command:<br><br>CREATE DATABASE IF NOT EXISTS customer360;<br><br>In which of the following locations will the customer360 database be located?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbfs:/user/hive/database/customer360",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbfs:/user/hive/warehouse\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbfs:/user/hive/customer360",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMore information is needed to determine the correct response",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbfs:/user/hive/database"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-21T09:24:00.000Z",
        "voteCount": 9,
        "content": "B\nB. dbfs:/user/hive/warehouse Thereby showing  \"dbfs:/user/hive/warehouse/customer360.db"
      },
      {
        "date": "2024-07-25T12:04:00.000Z",
        "voteCount": 1,
        "content": "D , The database could be created after using use catalog statement . In that case location would be different, not in hive warehouse"
      },
      {
        "date": "2024-05-22T22:38:00.000Z",
        "voteCount": 1,
        "content": "B. B is \"default\""
      },
      {
        "date": "2024-05-19T05:25:00.000Z",
        "voteCount": 3,
        "content": "D is correct. We do not know if this is a Unity Catalog enabled database. If so it would be created in default location of catalog as managed table. Therefore too little info to answer."
      },
      {
        "date": "2024-04-28T23:50:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-02-24T23:04:00.000Z",
        "voteCount": 1,
        "content": "While usage schema and database is interchangeable, schema is preferred. Option B is correct"
      },
      {
        "date": "2024-01-25T18:14:00.000Z",
        "voteCount": 2,
        "content": "Creating tables without using the LOCATION keyword to specify a location will create the table (a managed table) in the default directory which is: dbfs:/user/hive/warehouse\nhttps://docs.databricks.com/en/dbfs/root-locations.html"
      },
      {
        "date": "2023-12-30T14:15:00.000Z",
        "voteCount": 2,
        "content": "B. dbfs:/user/hive/warehouse\n\nExplanation:\n\nIn Databricks, the default location for databases created in the Hive Metastore is often under the warehouse directory. The CREATE DATABASE command usually creates the metadata entry for the database in the Hive Metastore, but it doesn\u2019t directly create the physical database directory within DBFS (Databricks File System).\n\nThe exact path structure may differ based on configuration or settings in the Databricks environment, but generally, the warehouse directory is where Hive databases' metadata resides. The physical data within the database will be stored in DBFS, but the metadata for the customer360 database should be within the warehouse directory in Hive Metastore."
      },
      {
        "date": "2023-12-06T13:13:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-15T15:32:00.000Z",
        "voteCount": 1,
        "content": "correct answer is B. dbfs:/user/hive/warehouse. All managed objects are stored in the default location unless specified."
      },
      {
        "date": "2023-10-24T18:15:00.000Z",
        "voteCount": 1,
        "content": "b is correct"
      },
      {
        "date": "2023-10-23T23:39:00.000Z",
        "voteCount": 2,
        "content": "dbfs:/user/hive/warehouse - which is the default location"
      },
      {
        "date": "2023-10-21T21:02:00.000Z",
        "voteCount": 1,
        "content": "Ans A :  https://community.databricks.com/t5/data-engineering/database-within-a-database-in-databricks/td-p/20731#:~:text=The%20default%20location%20of%20a,and%20Table%20location%20are%20independent.\nThe default location of a database will be in the /user/hive/warehouse/&lt;databasename. db&gt;. Irrespective of the location of the database the tables in the database can have different locations and they can be specified at the time of creation. Database location and Table location are independent."
      },
      {
        "date": "2023-10-23T23:39:00.000Z",
        "voteCount": 3,
        "content": "dbfs:/user/hive/warehouse - which is the default location"
      },
      {
        "date": "2023-10-20T08:19:00.000Z",
        "voteCount": 3,
        "content": "dbfs:/user/hive/warehouse - which is the default location of any object created"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/databricks/view/124296-exam-certified-data-engineer-associate-topic-1-question-65/",
    "body": "A data engineer is attempting to drop a Spark SQL table my_table and runs the following command:<br><br>DROP TABLE IF EXISTS my_table;<br><br>After running this command, the engineer notices that the data files and metadata files have been deleted from the file system.<br><br>Which of the following describes why all of these files were deleted?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table was managed\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t The table's data was smaller than 10 GB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table's data was larger than 10 GB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table was external",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table did not have a location"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T23:51:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-01-25T18:17:00.000Z",
        "voteCount": 1,
        "content": "Two types of tables, managed and external. Both table types are treated the same, except when the table is dropped. \nFor a managed table the data is stored in the managed storage location that is configured to the meta store. By default this is dbfs:/user/hive/warehouse. When the table is dropped the meta data and the underlying data is deleted.\nFor external tables the data is stored in a cloud storage location outside of the managed storage location. The underlying data is retained when an external table is dropped, only the metadata is dropped."
      },
      {
        "date": "2023-12-30T14:17:00.000Z",
        "voteCount": 1,
        "content": "A. The table was managed.\n\nExplanation:\n\nIn Spark SQL, when a table is managed (or internal), both the metadata that contains information about the table and the actual data files associated with the table are managed by the SQL engine.\n\nThe DROP TABLE command, when used on a managed table, deletes not only the metadata but also the underlying data files associated with that table from the file system.\n\nWhen a managed table is dropped, it removes all information about the table, including metadata and data files, leading to the deletion of both the metadata and data files from the file system.\n\nOptions B, C, D, and E don't specifically relate to why the data files and metadata files were deleted. The fact that the table was managed (or internal) is the reason for the removal of both the metadata and data files when the table was dropped using the DROP TABLE command."
      },
      {
        "date": "2023-12-06T13:14:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-10-21T21:05:00.000Z",
        "voteCount": 4,
        "content": "A is correct , managed tables files and metadata are managed by metastore and will be deleted when the table is dropped . while external tables the metadata is stored in a external location. hence when a external table is dropped you clear off only the metadata and the files (data) remain."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/databricks/view/124297-exam-certified-data-engineer-associate-topic-1-question-66/",
    "body": "A data engineer that is new to using Python needs to create a Python function to add two integers together and return the sum?<br><br>Which of the following code blocks can the data engineer use to complete this task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image26.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image27.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image28.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image29.png\"><br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image30.png\">"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-25T18:19:00.000Z",
        "voteCount": 1,
        "content": "Python functions start with the def keyword followed by the function name. Function also ends with the return keyword."
      },
      {
        "date": "2024-01-20T10:02:00.000Z",
        "voteCount": 1,
        "content": "D is to choose here"
      },
      {
        "date": "2023-12-06T13:15:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-11-29T06:09:00.000Z",
        "voteCount": 3,
        "content": "D : https://www.geeksforgeeks.org/python-functions/"
      },
      {
        "date": "2023-10-28T20:58:00.000Z",
        "voteCount": 2,
        "content": "D is correct.\nhttps://www.w3schools.com/python/python_functions.asp"
      },
      {
        "date": "2023-10-21T21:06:00.000Z",
        "voteCount": 3,
        "content": "D is correct. if you get this answer wrong you need to learn the basics of python."
      },
      {
        "date": "2024-09-11T09:24:00.000Z",
        "voteCount": 1,
        "content": "Like the data engineer in the question"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/databricks/view/124298-exam-certified-data-engineer-associate-topic-1-question-67/",
    "body": "In which of the following scenarios should a data engineer use the MERGE INTO command instead of the INSERT INTO command?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the location of the data needs to be changed",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the target table is an external table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the source table can be deleted",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the target table cannot contain duplicate records\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen the source is not a Delta table"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-10T10:49:00.000Z",
        "voteCount": 2,
        "content": "correct answer: D\nexplanation: The MERGE INTO command is used when you need to perform both insertions and updates (or deletes) in one operation based on whether a match exists. It is particularly useful for maintaining up-to-date data and ensuring there are no duplicate records in the target table. This is often referred to as an \"upsert\" operation (update + insert). When the target table needs to be kept free of duplicate records, and there's a need to update existing records or insert new ones based on some matching condition, MERGE INTO is the appropriate command. The INSERT INTO command, on the other hand, is used to add new records to a table without regard for whether they duplicate existing records. Options A, B, C, and E do not specifically require the use of MERGE INTO. Therefore, D is the correct answer."
      },
      {
        "date": "2024-01-25T18:20:00.000Z",
        "voteCount": 2,
        "content": "MERGE INTO you can upsert (update insert) data from a source table, view or dataframe into the target table. Merge operation allows updates, insets and deletes to be completed in a single atomic transaction. The main benefit of using the MERGE INTO is to avoid duplicates but does not inherently remove duplicates."
      },
      {
        "date": "2024-01-20T10:03:00.000Z",
        "voteCount": 2,
        "content": "D is answer here"
      },
      {
        "date": "2023-12-06T13:16:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-10-21T21:10:00.000Z",
        "voteCount": 1,
        "content": "Ans D : With merge , you can avoid inserting the duplicate records. The dataset containing the new logs needs to be deduplicated within itself. By the SQL semantics of merge, it matches and deduplicates the new data with the existing data in the table, but if there is duplicate data within the new dataset, it is inserted.\nhttps://docs.databricks.com/en/delta/merge.html#:~:text=With%20merge%20%2C%20you%20can%20avoid%20inserting%20the%20duplicate%20records.&amp;text=The%20dataset%20containing%20the%20new,new%20dataset%2C%20it%20is%20inserted."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/databricks/view/124091-exam-certified-data-engineer-associate-topic-1-question-68/",
    "body": "A data engineer is working with two tables. Each of these tables is displayed below in its entirety.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image31.png\"><br><br>The data engineer runs the following query to join these tables together:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image32.png\"><br><br>Which of the following will be returned by the above query?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image33.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image34.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image35.png\"><br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image36.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image37.png\">"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-06T13:18:00.000Z",
        "voteCount": 7,
        "content": "C is correct"
      },
      {
        "date": "2024-08-17T23:26:00.000Z",
        "voteCount": 1,
        "content": "c is correct when performing left join the sales values are only taken."
      },
      {
        "date": "2024-06-21T07:17:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-12-05T07:45:00.000Z",
        "voteCount": 3,
        "content": "C is correct answer"
      },
      {
        "date": "2023-11-29T06:18:00.000Z",
        "voteCount": 3,
        "content": "The LEFT JOIN keyword returns all records from the left table (table1), and the matching records from the right table (table2). The result is 0 records from the right side, if there is no match."
      },
      {
        "date": "2023-11-21T02:44:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-11-15T15:40:00.000Z",
        "voteCount": 2,
        "content": "The answer is C. this is a Left Join. In this case you show everything on the left side regardless of whether they appear on the right. When it does not appear on the right you represent that with a Null. So, for a3, store id is null."
      },
      {
        "date": "2023-11-10T08:34:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer.\nIn a LEFT JOIN, all the records from the left table are included, and only the matching records from the right table are added. In this case, \"a1\" and \"a4\" from the left table (favorite_stores) match with \"a1\" and \"a4\" from the right table (sales). So, these matching records are fetched. Additionally, all the records from the left table, including \"a3,\" are included. Since \"a3\" has no corresponding store_id in the right table, the store_id for \"a3\" will be NULL. Therefore, after the LEFT JOIN, the result will include \"a1,\" \"a3\" (with a NULL store_id), and \"a4.\""
      },
      {
        "date": "2023-11-07T06:32:00.000Z",
        "voteCount": 1,
        "content": "C is correct.\nplease refer to this simple blog if any confusion regarding JOINS\nhttps://sql.sh/cours/jointures"
      },
      {
        "date": "2023-10-24T18:20:00.000Z",
        "voteCount": 2,
        "content": "c is corret"
      },
      {
        "date": "2023-10-21T21:13:00.000Z",
        "voteCount": 1,
        "content": "Ans C: Left join only keeps left recs and only the matching recs from Right table.\nin other words : the left table is preserved as is."
      },
      {
        "date": "2023-10-20T08:44:00.000Z",
        "voteCount": 1,
        "content": "A typical LEFT JOIN scenario"
      },
      {
        "date": "2023-10-19T23:59:00.000Z",
        "voteCount": 3,
        "content": "The LEFT JOIN keyword returns all records from the left table, even if there are no matches in the right table."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/databricks/view/124133-exam-certified-data-engineer-associate-topic-1-question-69/",
    "body": "A data engineer needs to create a table in Databricks using data from a CSV file at location /path/to/csv.<br><br>They run the following command:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image38.png\"><br><br>Which of the following lines of code fills in the above blank to successfully complete the task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNone of these lines of code are needed to successfully complete the task",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUSING CSV\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFROM CSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUSING DELTA",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFROM \"path/to/csv\""
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-10T10:54:00.000Z",
        "voteCount": 2,
        "content": "correct answer: B\nexplanation: To create a table in Databricks using data from a CSV file, the correct syntax after specifying the table name and schema (if applicable) would be to use the USING CSV clause to define the format of the source data. This clause tells Databricks that the data source format is CSV. The command would typically look"
      },
      {
        "date": "2024-02-21T19:38:00.000Z",
        "voteCount": 1,
        "content": "I have a question \n\nWhy can option using delta"
      },
      {
        "date": "2023-12-06T13:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-29T06:23:00.000Z",
        "voteCount": 3,
        "content": "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html#parameters"
      },
      {
        "date": "2023-10-21T21:14:00.000Z",
        "voteCount": 3,
        "content": "Ans B : Using csv is correct. that is the correct syntax"
      },
      {
        "date": "2023-10-20T08:45:00.000Z",
        "voteCount": 2,
        "content": "USING CSV"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/databricks/view/124299-exam-certified-data-engineer-associate-topic-1-question-70/",
    "body": "A data engineer has configured a Structured Streaming job to read from a table, manipulate the data, and then perform a streaming write into a new table.<br><br>The code block used by the data engineer is below:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image39.png\"><br><br>If the data engineer only wants the query to process all of the available data in as many batches as required, which of the following lines of code should the data engineer use to fill in the blank?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tprocessingTime(1)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(availableNow=True)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(parallelBatch=True)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(processingTime=\"once\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(continuous=\"once\")"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T23:55:00.000Z",
        "voteCount": 1,
        "content": "b is ok"
      },
      {
        "date": "2024-03-10T10:58:00.000Z",
        "voteCount": 4,
        "content": "correct answer: B\nexplanation: In Structured Streaming, if a data engineer wants to process all the available data in as many batches as required without any explicit trigger interval, they can use the option trigger(availableNow=True). This feature, availableNow, is used to specify that the query should process all the data that is available at the moment and not wait for more data to arrive."
      },
      {
        "date": "2023-12-20T02:55:00.000Z",
        "voteCount": 1,
        "content": "it\u2019s the only answer with a correct syntax"
      },
      {
        "date": "2023-11-29T06:57:00.000Z",
        "voteCount": 2,
        "content": "https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.trigger.html"
      },
      {
        "date": "2023-10-24T05:11:00.000Z",
        "voteCount": 4,
        "content": "B\navailableNowbool, optional\nif set to True, set a trigger that processes all available data in multiple batches then terminates the query. Only one trigger can be set."
      },
      {
        "date": "2023-10-22T12:08:00.000Z",
        "voteCount": 4,
        "content": "sorry Ans is B : https://stackoverflow.com/questions/71061809/trigger-availablenow-for-delta-source-streaming-queries-in-pyspark-databricks\n\nfor batch we use available now"
      },
      {
        "date": "2023-10-21T21:16:00.000Z",
        "voteCount": 1,
        "content": "Correct Ans is D : \n%python\n\nspark.readStream.format(\"delta\").load(\"&lt;delta_table_path&gt;\")\n.writeStream\n.format(\"delta\")\n.trigger(processingTime='5 seconds')  #Added line of code that defines .trigger processing time.\n.outputMode(\"append\")\n.option(\"checkpointLocation\",\"&lt;checkpoint_path&gt;\")\n.options(**writeConfig)\n.start()\n\nhttps://kb.databricks.com/streaming/optimize-streaming-transactions-with-trigger"
      },
      {
        "date": "2024-07-25T06:38:00.000Z",
        "voteCount": 1,
        "content": "Nopes ! Use trigger(availableNow=True)"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/databricks/view/124300-exam-certified-data-engineer-associate-topic-1-question-71/",
    "body": "A data engineer has developed a data pipeline to ingest data from a JSON source using Auto Loader, but the engineer has not provided any type inference or schema hints in their pipeline. Upon reviewing the data, the data engineer has noticed that all of the columns in the target table are of the string type despite some of the fields only including float or boolean values.<br><br>Which of the following describes why Auto Loader inferred all of the columns to be of the string type?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere was a type mismatch between the specific schema and the inferred schema",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON data is a text-based format\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAuto Loader only works with string data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll of the fields had at least one null value",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAuto Loader cannot infer the schema of ingested data"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-20T03:07:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/ingestion/auto-loader/schema.html#how-does-auto-loader-schema-inference-work  \n\nBy default, Auto Loader schema inference seeks to avoid schema evolution issues due to type mismatches. For formats that don\u2019t encode data types (JSON and CSV), Auto Loader infers all columns as strings (including nested fields in JSON files)."
      },
      {
        "date": "2023-12-13T02:08:00.000Z",
        "voteCount": 2,
        "content": "Its B \"By default, Auto Loader schema inference seeks to avoid schema evolution issues due to type mismatches. For formats that don\u2019t encode data types (JSON and CSV), Auto Loader infers all columns as strings (including nested fields in JSON files). For formats with typed schema (Parquet and Avro), Auto Loader samples a subset of files and merges the schemas of individual files. This behavior is summarized in the following table:\" https://docs.databricks.com/en/ingestion/auto-loader/schema.html"
      },
      {
        "date": "2023-11-29T07:15:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/ingestion/auto-loader/schema.html#how-does-auto-loader-schema-inference-work"
      },
      {
        "date": "2023-10-21T21:19:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is: B. JSON data is a text-based format\n\nJSON data is a text-based format that uses strings to represent all values. When Auto Loader infers the schema of JSON data, it assumes that all values are strings. This is because Auto Loader cannot determine the type of a value based on its string representation.\n\nhttps://docs.databricks.com/en/ingestion/auto-loader/schema.html\n\nFor example, the following JSON string represents a value that is logically a boolean:\n\nJSON\n\"true\"\nUse code with caution. Learn more\nHowever, Auto Loader would infer that the type of this value is string. This is because Auto Loader cannot determine that the value is a boolean based on its string representation.\n\nIn order to get Auto Loader to infer the correct types for columns, the data engineer can provide type inference or schema hints. Type inference hints can be used to specify the types of specific columns. Schema hints can be used to provide the entire schema of the data.\n\nTherefore, the correct answer is B. JSON data is a text-based format."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/databricks/view/124177-exam-certified-data-engineer-associate-topic-1-question-72/",
    "body": "A Delta Live Table pipeline includes two datasets defined using STREAMING LIVE TABLE. Three datasets are defined against Delta Lake table sources using LIVE TABLE.<br><br>The table is configured to run in Development mode using the Continuous Pipeline Mode.<br><br>Assuming previously unprocessed data exists and all definitions are valid, what is the expected outcome after clicking Start to update the pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated once and the pipeline will shut down. The compute resources will be terminated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist until the pipeline is shut down.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated once and the pipeline will persist without any processing. The compute resources will persist but go unused.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for additional testing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-21T21:24:00.000Z",
        "voteCount": 10,
        "content": "Ans E : Development and production modes\nYou can optimize pipeline execution by switching between development and production modes. Use the Delta Live Tables Environment Toggle Icon buttons in the Pipelines UI to switch between these two modes. By default, pipelines run in development mode.\n\nWhen you run your pipeline in development mode, the Delta Live Tables system does the following:\n\nReuses a cluster to avoid the overhead of restarts. By default, clusters run for two hours when development mode is enabled. You can change this with the pipelines.clusterShutdown.delay setting in the Configure your compute settings.\n\nDisables pipeline retries so you can immediately detect and fix errors.\n\nIn production mode, the Delta Live Tables system does the following:\n\nRestarts the cluster for specific recoverable errors, including memory leaks and stale credentials.\n\nRetries execution in the event of specific errors, for example, a failure to start a cluster.\n\nhttps://docs.databricks.com/en/delta-live-tables/updates.html#optimize-execution"
      },
      {
        "date": "2024-08-16T05:38:00.000Z",
        "voteCount": 1,
        "content": "E is correct !\n\nOption B: \"All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist until the pipeline is shut down.\"\n\nThis option correctly reflects that the pipeline continues running, updating datasets at intervals, and only stops when manually shut down. Compute resources persist throughout this process.\nOption E: \"All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing.\"\n\nWhile this is very similar, it adds the phrase \"to allow for additional testing,\" which might imply that the resources are persisting just for testing purposes. This can be misleading because the primary reason for resource persistence in Continuous mode is to keep the pipeline active and processing data, not solely for testing."
      },
      {
        "date": "2024-07-08T15:22:00.000Z",
        "voteCount": 1,
        "content": "The answer is E. The compute resources will persist even after the pipeline is shut down."
      },
      {
        "date": "2024-04-28T23:57:00.000Z",
        "voteCount": 1,
        "content": "e as teh cluster actually persits differently from b"
      },
      {
        "date": "2023-12-30T18:42:00.000Z",
        "voteCount": 2,
        "content": "E. All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing.\n\nExplanation:\n\nIn Development mode, Delta Live Tables persistently updates datasets at set intervals. The pipeline continuously processes incoming data until manually stopped or shut down.\n\nCompute resources, including the cluster used for processing, persist without automatic restarts or retries (as it is the behavior in Development mode). This persistence allows for ongoing processing of data, enabling additional testing or continued data processing until the pipeline is manually shut down.\n\nTherefore, option E accurately captures the behavior expected in Development mode, emphasizing the continuous update of datasets and the persistence of compute resources until the pipeline is manually terminated."
      },
      {
        "date": "2023-12-06T13:27:00.000Z",
        "voteCount": 2,
        "content": "E seems the correct answer"
      },
      {
        "date": "2023-12-06T01:54:00.000Z",
        "voteCount": 2,
        "content": "Why E? It persists with same functionality as was before, not for \"additional testing\"?"
      },
      {
        "date": "2023-12-20T06:23:00.000Z",
        "voteCount": 1,
        "content": "because \"The table is configured to run in Development mode\" when tables are set in dev mode, \"The compute resources will persist to allow for additional testing.\""
      },
      {
        "date": "2023-12-20T06:23:00.000Z",
        "voteCount": 1,
        "content": "So correct answer is E"
      },
      {
        "date": "2023-11-30T06:40:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/delta-live-tables/updates.html#continuous-vs-triggered-pipeline-execution\n\nhttps://docs.databricks.com/en/delta-live-tables/testing.html#use-development-mode-to-run-pipeline-updates"
      },
      {
        "date": "2023-10-24T18:28:00.000Z",
        "voteCount": 2,
        "content": "E is correct"
      },
      {
        "date": "2023-10-21T04:06:00.000Z",
        "voteCount": 2,
        "content": "E. All datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/databricks/view/124301-exam-certified-data-engineer-associate-topic-1-question-73/",
    "body": "Which of the following data workloads will utilize a Gold table as its source?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA job that enriches data by parsing its timestamps into a human-readable format",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA job that aggregates uncleaned data to create standard summary statistics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA job that cleans data by removing malformatted records",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA job that queries aggregated data designed to feed into a dashboard\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA job that ingests raw data from a streaming source into the Lakehouse"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-11T09:49:00.000Z",
        "voteCount": 1,
        "content": "A gold table is most likely to contain aggregated data. It is also the table where cleaned up data is stored, so that is where data will be used from for a dashboard."
      },
      {
        "date": "2023-11-30T06:50:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/lakehouse/medallion.html#power-analytics-with-the-gold-layer"
      },
      {
        "date": "2023-10-21T21:25:00.000Z",
        "voteCount": 3,
        "content": "D is correct : std medallion arch"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/databricks/view/124135-exam-certified-data-engineer-associate-topic-1-question-74/",
    "body": "Which of the following must be specified when creating a new Delta Live Tables pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA key-value pair configuration",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe preferred DBU/hour cost",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA path to cloud storage location for the written data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA location of a target database for the written data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAt least one notebook library to be executed\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-26T06:34:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is E. storage location is optional. \n\"(Optional) Enter a Storage location for output data from the pipeline. The system uses a default location if you leave Storage location empty\""
      },
      {
        "date": "2024-09-27T08:31:00.000Z",
        "voteCount": 1,
        "content": "D. A location of a target database for the written data\n\nWhy this is correct: When creating a Delta Live Tables (DLT) pipeline, you must specify the target database where the resulting data will be written. This ensures that the output of the pipeline is stored properly.\n\nWhy the other options are incorrect:\n\nA. A key-value pair configuration: While configurations are useful, they are not mandatory when setting up a DLT pipeline.\n\nB. The preferred DBU/hour cost: You don't specify a cost directly; the DBU is associated with the cluster used.\n\nC. A path to cloud storage location for the written data: While storage paths may be specified, the target database location is required.\n\nE. At least one notebook library: You specify the transformation logic (which could be in notebooks), but this is not a strict requirement for setting up the pipeline itself."
      },
      {
        "date": "2024-08-10T08:01:00.000Z",
        "voteCount": 1,
        "content": "This is a key requirement for creating a Delta Live Tables pipeline. You need to specify notebooks that contain the ETL logic to be executed by the pipeline."
      },
      {
        "date": "2024-06-11T01:01:00.000Z",
        "voteCount": 1,
        "content": "C, just tested on databricks DLT"
      },
      {
        "date": "2024-04-28T23:59:00.000Z",
        "voteCount": 1,
        "content": "tbf C is correct as well but the question is probably hinting for E"
      },
      {
        "date": "2024-03-18T11:20:00.000Z",
        "voteCount": 1,
        "content": "Per Databaricks documentation (see below), you need to select a destination for datasets published by the pipeline, either the Hive metastore or Unity Catalog I think A is incorrect because it uses the term \"Notebook Library\" and not just \"Notebook\". \nDatabricks doc: https://docs.databricks.com/en/delta-live-tables/tutorial-pipelines.html"
      },
      {
        "date": "2024-08-26T13:18:00.000Z",
        "voteCount": 1,
        "content": "\"you need to select a destination for datasets published by the pipeline\".   This is true if you have a notebook that is writing out a result dataset.  However, nothing in this question or documentation states that a Delta Live Tables Pipeline --MUST-- contain a notebook that write dataset results."
      },
      {
        "date": "2024-01-20T10:25:00.000Z",
        "voteCount": 1,
        "content": "As per Pipeline creating steps, choosing a Notebook is mandatory whereas specifying a location is optional. I would go with answer E"
      },
      {
        "date": "2024-01-16T06:37:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/delta-live-tables/tutorial-pipelines.html\n\nE. The only non-optional selection is a notebook"
      },
      {
        "date": "2023-12-30T18:58:00.000Z",
        "voteCount": 2,
        "content": "E. At least one notebook library to be executed.\n\nExplanation:\nhttps://docs.databricks.com/en/delta-live-tables/tutorial-pipelines.html\n\nDelta Live Tables pipelines execute notebook libraries as part of their operations. These notebooks contain the logic, code, or instructions defining the data processing steps, transformations, or actions to be performed within the pipeline.\n\nSpecifying at least one notebook library to be executed is crucial when creating a new Delta Live Tables pipeline, as it defines the sequence of operations and the logic to be executed on the data within the pipeline, aligning with the documentation provided."
      },
      {
        "date": "2023-12-18T20:49:00.000Z",
        "voteCount": 2,
        "content": "This should be E. As per the link https://docs.databricks.com/en/delta-live-tables/tutorial-pipelines.html\n\nCreate a pipeline\n\nClick Jobs Icon Workflows in the sidebar, click the Delta Live Tables tab, and click Create Pipeline.\n\nGive the pipeline a name and click File Picker Icon to select a notebook.\n\nSelect Triggered for Pipeline Mode.\n\n(Optional) Enter a Storage location for output data from the pipeline. The system uses a default location if you leave Storage location empty.\n\n(Optional) Specify a Target schema to publish your dataset to the Hive metastore or a Catalog and a Target schema to publish your dataset to Unity Catalog. See Publish datasets.\n\n(Optional) Click Add notification to configure one or more email addresses to receive notifications for pipeline events. See Add email notifications for pipeline events.\n\nClick Create."
      },
      {
        "date": "2023-11-30T07:40:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/delta-live-tables/index.html#what-is-a-delta-live-tables-pipeline"
      },
      {
        "date": "2023-11-15T15:50:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is E. DLT tables needs a notebook where you have to specify the processing info"
      },
      {
        "date": "2023-10-28T03:10:00.000Z",
        "voteCount": 2,
        "content": "storage location is required to be specified to control the object storage location for data written by the pipeline."
      },
      {
        "date": "2023-10-21T21:29:00.000Z",
        "voteCount": 3,
        "content": "Ans E : i think it might be E - https://docs.databricks.com/en/delta-live-tables/settings.html - this doc says that target schema and storage may be optional so it leaves us with E"
      },
      {
        "date": "2023-10-28T21:24:00.000Z",
        "voteCount": 1,
        "content": "Answer is E\nStorage and location are optional.\nhttps://docs.databricks.com/en/delta-live-tables/tutorial-pipelines.html"
      },
      {
        "date": "2023-10-20T08:54:00.000Z",
        "voteCount": 3,
        "content": "A path to a cloud storage location for the written data - considering this option is talking about the source data being stored in cloud storage and being ingested to DLT using an autoloader."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/databricks/view/124304-exam-certified-data-engineer-associate-topic-1-question-75/",
    "body": "A data engineer has joined an existing project and they see the following query in the project repository:<br><br>CREATE STREAMING LIVE TABLE loyal_customers AS<br><br>SELECT customer_id -<br>FROM STREAM(LIVE.customers)<br>WHERE loyalty_level = 'high';<br><br>Which of the following describes why the STREAM function is included in the query?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe STREAM function is not needed and will cause an error.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table being created is a live table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customers table is a streaming live table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe customers table is a reference to a Structured Streaming query on a PySpark DataFrame.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe data in the customers table has been updated since its last run."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-21T21:35:00.000Z",
        "voteCount": 7,
        "content": "Ans C is correct : \nhttps://docs.databricks.com/en/sql/load-data-streaming-table.html\nLoad data into a streaming table\nTo create a streaming table from data in cloud object storage, paste the following into the query editor, and then click Run:\n\nSQL\nCopy to clipboardCopy\n/* Load data from a volume */\nCREATE OR REFRESH STREAMING TABLE &lt;table-name&gt; AS\nSELECT * FROM STREAM read_files('/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/&lt;path&gt;/&lt;folder&gt;')\n\n/* Load data from an external location */\nCREATE OR REFRESH STREAMING TABLE &lt;table-name&gt; AS\nSELECT * FROM STREAM read_files('s3://&lt;bucket&gt;/&lt;path&gt;/&lt;folder&gt;')"
      },
      {
        "date": "2024-08-10T08:05:00.000Z",
        "voteCount": 1,
        "content": "The STREAM function is used to indicate that LIVE.customers is a streaming live table. This allows the query to process real-time streaming data."
      },
      {
        "date": "2024-04-29T00:02:00.000Z",
        "voteCount": 1,
        "content": "c is correct . about D: it can be correct but it is not given the fact it comes from pyspark ; sql supports (at least in databricks) the creation of streaming live table as well so it is not necessasarily from pyspark"
      },
      {
        "date": "2024-04-29T00:00:00.000Z",
        "voteCount": 1,
        "content": "c is ok"
      },
      {
        "date": "2024-04-27T06:11:00.000Z",
        "voteCount": 1,
        "content": "Option E, specifying \"at least one notebook library to be executed,\" is not a requirement for setting up a Delta Live Tables pipeline. Delta Live Tables are built on top of Databricks and use notebooks to define the pipeline's logic, but the actual requirement when setting up the pipeline is typically the location where the data will be written to, like a target database or a path to cloud storage. While notebooks may contain the business logic for the transformations and actions within the pipeline, the fundamental requirement for setting up a pipeline is knowing where the data will reside after processing, hence why the location of the target database for the written data is crucial."
      },
      {
        "date": "2024-05-24T18:19:00.000Z",
        "voteCount": 1,
        "content": "Wrong question, that's for #73"
      },
      {
        "date": "2024-05-24T18:19:00.000Z",
        "voteCount": 1,
        "content": "I mean question #74"
      },
      {
        "date": "2024-01-20T10:27:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-01-03T02:17:00.000Z",
        "voteCount": 1,
        "content": "Ans is A.\nCREATE STREAMING LIVE TABLE  syntax is does not exist.\nIt should be CREATE LIVE TABLE AS SELECT * FROM STREAM."
      },
      {
        "date": "2024-01-10T03:07:00.000Z",
        "voteCount": 1,
        "content": "LIVE references schema name\ncustomer_table references table name"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/databricks/view/124305-exam-certified-data-engineer-associate-topic-1-question-76/",
    "body": "Which of the following describes the type of workloads that are always compatible with Auto Loader?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStreaming workloads\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMachine learning workloads",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tServerless workloads",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBatch workloads",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDashboard workloads"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-21T21:36:00.000Z",
        "voteCount": 6,
        "content": "A is correct Structured streaming for autoloader"
      },
      {
        "date": "2024-08-10T08:06:00.000Z",
        "voteCount": 1,
        "content": "Auto Loader is designed to handle streaming data ingestion. It continuously processes new data as it arrives, making it well-suited for streaming workloads."
      },
      {
        "date": "2024-04-29T00:03:00.000Z",
        "voteCount": 1,
        "content": "A is ok"
      },
      {
        "date": "2024-01-20T10:27:00.000Z",
        "voteCount": 1,
        "content": "A is correct here"
      },
      {
        "date": "2023-12-20T07:15:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/ingestion/auto-loader/unity-catalog.html#using-auto-loader-with-unity-catalog\t\n\nAuto Loader relies on Structured Streaming for incremental processing"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/databricks/view/124306-exam-certified-data-engineer-associate-topic-1-question-77/",
    "body": "A data engineer and data analyst are working together on a data pipeline. The data engineer is working on the raw, bronze, and silver layers of the pipeline using Python, and the data analyst is working on the gold layer of the pipeline using SQL. The raw source of the pipeline is a streaming input. They now want to migrate their pipeline to use Delta Live Tables.<br><br>Which of the following changes will need to be made to the pipeline when migrating to Delta Live Tables?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNone of these changes will need to be made",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe pipeline will need to stop using the medallion-based multi-hop architecture",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe pipeline will need to be written entirely in SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe pipeline will need to use a batch source in place of a streaming source\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe pipeline will need to be written entirely in Python"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-06-10T12:43:00.000Z",
        "voteCount": 8,
        "content": "I had the exam today and option A &amp; B weren't exist, correct answer is D."
      },
      {
        "date": "2024-06-20T07:15:00.000Z",
        "voteCount": 7,
        "content": "None is never a solution"
      },
      {
        "date": "2024-09-22T02:39:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2024-08-27T06:42:00.000Z",
        "voteCount": 1,
        "content": "A. None of these changes will need to be made.\n\nYou can continue using the medallion-based architecture, and you do not need to switch entirely to SQL or Python. Delta Live Tables will work with your existing streaming sources and support both SQL and Python."
      },
      {
        "date": "2024-08-17T23:33:00.000Z",
        "voteCount": 2,
        "content": "When migrating to Delta Live Tables, you can continue using the medallion-based architecture, work with streaming sources, and write the pipeline in either SQL or Python. Therefore, no major changes are required for the pipeline in this scenario."
      },
      {
        "date": "2024-06-10T06:54:00.000Z",
        "voteCount": 4,
        "content": "D:\nDelta Live Tables is primarily designed to work with batch processing rather than streaming. This means that when migrating a pipeline to Delta Live Tables, any streaming sources used in the original pipeline will need to be replaced with batch sources.\n\nIn the scenario described, where the raw source of the pipeline is a streaming input, the data engineer and data analyst will need to modify their pipeline to read data from a batch source instead. This could involve changing the way data is ingested and processed to align with batch processing paradigms rather than streaming.\n\nAdditionally, Delta Live Tables enables the integration of both SQL and Python code within a pipeline, so there's no strict requirement to write the pipeline entirely in SQL or Python. Both the data engineer's Python code for the raw, bronze, and silver layers and the data analyst's SQL code for the gold layer can still be used within the Delta Live Tables environment.\n\nOverall, the key change needed when migrating to Delta Live Tables in this scenario is transitioning from a streaming input source to a batch source to align with the batch processing nature of Delta Live Tables."
      },
      {
        "date": "2024-06-10T07:23:00.000Z",
        "voteCount": 1,
        "content": "Yes It must be A:\nLanguage Support: DLT allows the use of both SQL and Python, so you can integrate the existing Python and SQL code within the DLT framework."
      },
      {
        "date": "2024-04-29T00:04:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-04-19T02:24:00.000Z",
        "voteCount": 3,
        "content": "Cleared the exam today . Option A and B were not available in the exam . There was a different option which was correct."
      },
      {
        "date": "2023-12-20T07:32:00.000Z",
        "voteCount": 4,
        "content": "B - DLT support medallion architecture (see example in : https://docs.databricks.com/en/delta-live-tables/transform.html#combine-streaming-tables-and-materialized-views-in-a-single-pipeline) \nC - DLT can mix Python and SQL using multiple notebooks (according to https://docs.databricks.com/en/delta-live-tables/tutorial-python.html You cannot mix languages within a Delta Live Tables source code file. You can use multiple notebooks or files with different languages in a pipeline) \nD - DLT manage streaming sources using streaming tables (ex : https://docs.databricks.com/en/delta-live-tables/load.html#load-data-from-a-message-bus) \nE - DLT support python and sql (https://docs.databricks.com/en/delta-live-tables/tutorial-python.html and https://docs.databricks.com/en/delta-live-tables/tutorial-sql.html) \n\nCorrect answer is A by elimination"
      },
      {
        "date": "2023-12-06T13:35:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is A"
      },
      {
        "date": "2023-12-06T02:54:00.000Z",
        "voteCount": 2,
        "content": "It should be A. Medallion architecture can be used in DLT pipeline https://www.databricks.com/glossary/medallion-architecture \"Databricks provides tools like Delta Live Tables&nbsp;(DLT) that allow users to instantly build data pipelines with Bronze, Silver and Gold tables from just a few lines of code.\""
      },
      {
        "date": "2023-11-15T15:55:00.000Z",
        "voteCount": 3,
        "content": "the correct answer is A. DLT needs a notebook where you specify the processing"
      },
      {
        "date": "2023-11-07T06:53:00.000Z",
        "voteCount": 1,
        "content": "Response A: They have to adapt their notebook's code to be able to decalre the DLT pipeline.\nHowever, this option is not proposed in the answers so I think it might be A"
      },
      {
        "date": "2023-10-27T01:44:00.000Z",
        "voteCount": 2,
        "content": "Answer should be A."
      },
      {
        "date": "2023-10-24T09:38:00.000Z",
        "voteCount": 2,
        "content": "In my opinion, this should be A. Assuming they were working on the same notebook, and weren't declaring the Streaming or Live keywords during development, the would probably need to do so before adding to the DLT workflow. and that is not in the option."
      },
      {
        "date": "2023-10-21T21:41:00.000Z",
        "voteCount": 4,
        "content": "i think its A ;"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/databricks/view/124307-exam-certified-data-engineer-associate-topic-1-question-78/",
    "body": "A data engineer is using the following code block as part of a batch ingestion pipeline to read from a composable table:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image40.png\"><br><br>Which of the following changes needs to be made so this code block will work when the transactions table is a stream source?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace predict with a stream-friendly prediction function",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace schema(schema) with option (\"maxFilesPerTrigger\", 1)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace \"transactions\" with the path to the location of the Delta table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace format(\"delta\") with format(\"stream\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace spark.read with spark.readStream\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-29T00:05:00.000Z",
        "voteCount": 1,
        "content": "E is ok"
      },
      {
        "date": "2023-12-20T07:39:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/structured-streaming/tutorial.html#use-auto-loader-to-read-streaming-data-from-object-storage"
      },
      {
        "date": "2023-11-30T08:47:00.000Z",
        "voteCount": 4,
        "content": "Example from https://docs.databricks.com/en/structured-streaming/delta-lake.html\n\nspark.readStream.table(\"table_name\")\n\nspark.readStream.load(\"/path/to/table\")"
      },
      {
        "date": "2023-12-02T04:00:00.000Z",
        "voteCount": 2,
        "content": "have you cleared the exam"
      },
      {
        "date": "2023-10-21T21:43:00.000Z",
        "voteCount": 3,
        "content": "Ans E; for streaming source you use readstream.\n\nhttps://docs.databricks.com/en/structured-streaming/delta-lake.html"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/databricks/view/125329-exam-certified-data-engineer-associate-topic-1-question-79/",
    "body": "Which of the following queries is performing a streaming hop from raw data to a Bronze table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image41.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image42.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image43.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image44.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image45.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-07T07:04:00.000Z",
        "voteCount": 6,
        "content": "answer E: Raw to Bronze is simply an integration of source data in the lakehouse without any schema needed nor extra operationss (e;g filtering, aggregation, joins etc..)\nPlease refer to this Medaillon Architecture article\nhttps://www.databricks.com/glossary/medallion-architecture"
      },
      {
        "date": "2024-07-16T04:36:00.000Z",
        "voteCount": 1,
        "content": "Yes E is correct. But There are filtering or aggregation in silver layer . We need to check if it have readstream and writestream"
      },
      {
        "date": "2024-04-29T00:06:00.000Z",
        "voteCount": 1,
        "content": "E is ok , all others are incorrect"
      },
      {
        "date": "2023-12-20T07:44:00.000Z",
        "voteCount": 2,
        "content": "sourcename is \u201crawSalesLocation\u201d (bronze tables contain raw data) and code includes \u201creadStream\u201d to indicate that it is a streaming hop"
      },
      {
        "date": "2023-11-30T08:50:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/lakehouse/medallion.html#ingest-raw-data-to-the-bronze-layer"
      },
      {
        "date": "2023-11-03T22:25:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2024-07-16T04:41:00.000Z",
        "voteCount": 1,
        "content": "I think we should have read stream and writestream that should the important point"
      },
      {
        "date": "2023-11-04T16:04:00.000Z",
        "voteCount": 4,
        "content": "Answer should be E. Filtering and cleaning usually happens from bronze to silver layer"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/databricks/view/124308-exam-certified-data-engineer-associate-topic-1-question-80/",
    "body": "A dataset has been defined using Delta Live Tables and includes an expectations clause:<br><br>CONSTRAINT valid_timestamp EXPECT (timestamp &gt; '2020-01-01') ON VIOLATION FAIL UPDATE<br><br>What is the expected behavior when a batch of data containing data that violates these constraints is processed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation cause the job to fail.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are dropped from the target dataset and loaded into a quarantine table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are added to the target dataset and recorded as invalid in the event log.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are added to the target dataset and flagged as invalid in a field added to the target dataset."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-11T10:01:00.000Z",
        "voteCount": 1,
        "content": "B is the way"
      },
      {
        "date": "2024-04-29T00:07:00.000Z",
        "voteCount": 1,
        "content": "b is ok"
      },
      {
        "date": "2023-11-30T09:08:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/delta-live-tables/sql-ref.html#sql-properties\nON VIOLATION\nOptional action to take for failed rows:\nFAIL UPDATE: Immediately stop pipeline execution.\nDROP ROW: Drop the record and continue processing."
      },
      {
        "date": "2023-11-07T17:28:00.000Z",
        "voteCount": 2,
        "content": "ON VIOLATION\nFAIL UPDATE:  Immediately stop pipeline execution.\nDROP ROW:  Drop the record and continue processing."
      },
      {
        "date": "2023-10-21T21:46:00.000Z",
        "voteCount": 4,
        "content": "Ans B : delta live tables data quality expectations . - https://docs.databricks.com/en/delta-live-tables/expectations.html\nAction\n\nResult\n\nwarn (default)\n\nInvalid records are written to the target; failure is reported as a metric for the dataset.\n\ndrop\n\nInvalid records are dropped before data is written to the target; failure is reported as a metrics for the dataset.\n\nfail\n\nInvalid records prevent the update from succeeding. Manual intervention is required before re-processing."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/databricks/view/124309-exam-certified-data-engineer-associate-topic-1-question-81/",
    "body": "Which of the following statements regarding the relationship between Silver tables and Bronze tables is always true?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSilver tables contain a less refined, less clean view of data than Bronze data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSilver tables contain aggregates while Bronze data is unaggregated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSilver tables contain more data than Bronze tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSilver tables contain a more refined and cleaner view of data than Bronze tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSilver tables contain less data than Bronze tables."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-11T10:02:00.000Z",
        "voteCount": 1,
        "content": "Silver tables are used to clean the raw imported data from a bronze table"
      },
      {
        "date": "2024-04-29T00:07:00.000Z",
        "voteCount": 1,
        "content": "d is ok"
      },
      {
        "date": "2024-01-20T10:35:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer"
      },
      {
        "date": "2023-10-21T21:47:00.000Z",
        "voteCount": 2,
        "content": "Ans D : medallion arch databricks\nhttps://www.databricks.com/glossary/medallion-architecture"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/databricks/view/124310-exam-certified-data-engineer-associate-topic-1-question-82/",
    "body": "A data engineering team has noticed that their Databricks SQL queries are running too slowly when they are submitted to a non-running SQL endpoint. The data engineering team wants this issue to be resolved.<br><br>Which of the following approaches can the team use to reduce the time it takes to return results in this scenario?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can turn on the Serverless feature for the SQL endpoint and change the Spot Instance Policy to \"Reliability Optimized.\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can turn on the Auto Stop feature for the SQL endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can increase the cluster size of the SQL endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can turn on the Serverless feature for the SQL endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can increase the maximum bound of the SQL endpoint's scaling range."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T08:11:00.000Z",
        "voteCount": 16,
        "content": "The important point of this scenario is \"when they are submitted to a non-running SQL endpoint\". So its not about increasing the instance size or the amount of instances to improve the query performance, but its about reducing the start-up time.\nA: Not possible, serverless can't be combined with spot instance policies, see https://docs.databricks.com/en/compute/sql-warehouse/serverless.html#limitations\nB: Auto Stop is about terminating a SQL warehouse after x minutes of being idle.\nC: Increasing the cluster size provides more capacities for running queries, but doesn't reduce start-up time.\nD: Serverless reduces start-up time from minutes to seconds. Jackpot!\nE: Increasing the max bound of the SQL endpoints scaling range will help with lots of sequencial queries, which is not the case here."
      },
      {
        "date": "2024-01-20T10:43:00.000Z",
        "voteCount": 1,
        "content": "D is correct. Key phrase is \"submitted to a non-running SQL endpoint\". Increasing cluster size is not going to help if that's in a state like non-running."
      },
      {
        "date": "2024-01-10T03:15:00.000Z",
        "voteCount": 1,
        "content": "\"when they are submitted to a non-running SQL endpoint\" ANSWER D"
      },
      {
        "date": "2023-12-30T19:34:00.000Z",
        "voteCount": 1,
        "content": "C. They can increase the cluster size of the SQL endpoint.\n\nExplanation:\n\nIncreasing the cluster size of the SQL endpoint can enhance query performance by providing more computational resources to execute queries. This can potentially speed up query processing by allowing more parallelism, handling larger workloads, and reducing the time taken for query execution."
      },
      {
        "date": "2023-12-20T12:47:00.000Z",
        "voteCount": 4,
        "content": "key word, \u201cnon-running SQL endpoint\u201d which implies that the query is slow because the cluster needs time to get started.  \n\nI suggest answer D because :  \n\nA : Serverless &amp; spot instances cannot be mixed ? \n\nB : autostop means that jobs are submitted to non-running SQL endpoints \n\nC : increasing the clustersize can compensate for slow startup time \n\nD : serverless is able to start and scale faster than non-running SQL endpoints (seconds intead of minutes)  \n\nE : increasing maximum bound will help only if there are simultaneous queries \n\nhttps://docs.gcp.databricks.com/en/lakehouse-architecture/cost-optimization/best-practices.html#use-serverless-for-your-workloads"
      },
      {
        "date": "2023-12-15T07:54:00.000Z",
        "voteCount": 2,
        "content": "maximum bound of the SQL endpoint's scaling range"
      },
      {
        "date": "2023-12-06T04:05:00.000Z",
        "voteCount": 2,
        "content": "D is wrong - its already Serverless (non running SQL endpoint) how would turning Serverless ON help? They also says C here https://community.databricks.com/t5/data-engineering/when-to-increase-maximum-bound-vs-when-to-increase-cluster-size/td-p/27880 . E is only true for autoscaling clusters"
      },
      {
        "date": "2023-12-01T20:17:00.000Z",
        "voteCount": 2,
        "content": "https://community.databricks.com/t5/data-engineering/sql-query-takes-too-long-to-run/td-p/21884"
      },
      {
        "date": "2023-10-28T21:57:00.000Z",
        "voteCount": 2,
        "content": "Answer E:\n\nhttps://www.databricks.com/blog/2022/03/10/top-5-databricks-performance-tips.html"
      },
      {
        "date": "2023-10-28T21:58:00.000Z",
        "voteCount": 1,
        "content": "I mean answer C"
      },
      {
        "date": "2023-10-21T21:52:00.000Z",
        "voteCount": 1,
        "content": "Ans E : you re welcome :) \nhttps://community.databricks.com/t5/data-engineering/when-to-increase-maximum-bound-vs-when-to-increase-cluster-size/td-p/27880"
      },
      {
        "date": "2023-10-28T11:50:00.000Z",
        "voteCount": 1,
        "content": "I don't agree. Your answer is only valid when 'sequential' is mentioned, which is not the case here."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/databricks/view/124311-exam-certified-data-engineer-associate-topic-1-question-83/",
    "body": "A data engineer has a Job that has a complex run schedule, and they want to transfer that schedule to other Jobs.<br><br>Rather than manually selecting each value in the scheduling form in Databricks, which of the following tools can the data engineer use to represent and submit the schedule programmatically?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpyspark.sql.types.DateType",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdatetime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpyspark.sql.types.TimestampType",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCron syntax\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no way to represent and submit this information programmatically"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-11T06:37:00.000Z",
        "voteCount": 2,
        "content": "Cron syntax is a powerful way to define complex schedules programmatically. In Databricks, you can use Cron syntax to set up the scheduling of jobs, which allows for more flexibility and ease when transferring the schedule to other jobs without manually selecting each value in the scheduling form."
      },
      {
        "date": "2023-11-30T09:36:00.000Z",
        "voteCount": 2,
        "content": "https://docs.databricks.com/en/sql/user/queries/schedule-query.html"
      },
      {
        "date": "2023-10-21T21:53:00.000Z",
        "voteCount": 3,
        "content": "Ans D : Cron Syntax with that you can easily copy all the syntax"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/databricks/view/124312-exam-certified-data-engineer-associate-topic-1-question-84/",
    "body": "Which of the following approaches should be used to send the Databricks Job owner an email in the case that the Job fails?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually programming in an alert system in each cell of the Notebook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetting up an Alert in the Job page\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetting up an Alert in the Notebook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no way to notify the Job owner in the case of Job failure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMLflow Model Registry Webhooks"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-11T06:39:00.000Z",
        "voteCount": 2,
        "content": "In Databricks, you can configure job notifications directly from the Jobs page, where you can specify that an email should be sent to the Job owner or other specified individuals in the case of Job failure. This is the most straightforward and automated way to ensure notifications are sent."
      },
      {
        "date": "2023-12-08T03:06:00.000Z",
        "voteCount": 2,
        "content": "Setting up an alert in Jobs page"
      },
      {
        "date": "2023-10-21T21:55:00.000Z",
        "voteCount": 3,
        "content": "Ans B : https://docs.databricks.com/en/workflows/jobs/job-notifications.html"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/databricks/view/124344-exam-certified-data-engineer-associate-topic-1-question-85/",
    "body": "An engineering manager uses a Databricks SQL query to monitor ingestion latency for each data source. The manager checks the results of the query every day, but they are manually rerunning the query each day and waiting for the results.<br><br>Which of the following approaches can the manager use to ensure the results of the query are updated each day?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can schedule the query to refresh every 1 day from the SQL endpoint's page in Databricks SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can schedule the query to refresh every 12 hours from the SQL endpoint's page in Databricks SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can schedule the query to refresh every 1 day from the query's page in Databricks SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can schedule the query to run every 1 day from the Jobs UI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can schedule the query to run every 12 hours from the Jobs UI."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-30T19:44:00.000Z",
        "voteCount": 3,
        "content": "The manager can schedule the query to refresh every 1 day from the query\u2019s page in Databricks SQL (Option C). Here are the steps to do this:\n\n- In the Query Editor, click Schedule &gt; Add schedule to open a menu with schedule settings.\n- Choose when to run the query. Use the dropdown pickers to specify the frequency, period, starting time, and time zone.\n- Click Create."
      },
      {
        "date": "2023-12-20T12:23:00.000Z",
        "voteCount": 1,
        "content": "has to be every 1 day to run once day. https://docs.databricks.com/en/sql/user/queries/schedule-query.html"
      },
      {
        "date": "2023-12-06T13:49:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C"
      },
      {
        "date": "2023-10-28T06:03:00.000Z",
        "voteCount": 2,
        "content": "From the query editor page we have option to schedule the queries"
      },
      {
        "date": "2023-10-22T09:14:00.000Z",
        "voteCount": 1,
        "content": "Ans D : think option A might not be right since we are not doing scheduling in sql end points page"
      },
      {
        "date": "2023-10-23T03:03:00.000Z",
        "voteCount": 6,
        "content": "it is C, Question 41 of Practice Exam Databricks"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/databricks/view/124345-exam-certified-data-engineer-associate-topic-1-question-86/",
    "body": "In which of the following scenarios should a data engineer select a Task in the Depends On field of a new Databricks Job Task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen another task needs to be replaced by the new task",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen another task needs to fail before the new task begins",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen another task has the same dependency libraries as the new task",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen another task needs to use as little compute resources as possible",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen another task needs to successfully complete before the new task begins\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-15T07:18:00.000Z",
        "voteCount": 1,
        "content": "E is correct"
      },
      {
        "date": "2023-12-08T03:00:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/workflows/jobs/conditional-tasks.html"
      },
      {
        "date": "2023-10-22T09:15:00.000Z",
        "voteCount": 2,
        "content": "Ans E : E is correct since dependency means the dependent job must complete successfully."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/databricks/view/124346-exam-certified-data-engineer-associate-topic-1-question-87/",
    "body": "A data engineer has been using a Databricks SQL dashboard to monitor the cleanliness of the input data to a data analytics dashboard for a retail use case. The job has a Databricks SQL query that returns the number of store-level records where sales is equal to zero. The data engineer wants their entire team to be notified via a messaging webhook whenever this value is greater than 0.<br><br>Which of the following approaches can the data engineer use to notify their entire team via a messaging webhook whenever the number of stores with $0 in sales is greater than zero?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up an Alert with a custom template.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up an Alert with a new email alert destination.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up an Alert with one-time notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up an Alert with a new webhook alert destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up an Alert without notifications."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-20T10:50:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer here"
      },
      {
        "date": "2023-12-08T02:55:00.000Z",
        "voteCount": 2,
        "content": "Set up an Alert with a new webhook alert destination"
      },
      {
        "date": "2023-10-22T09:15:00.000Z",
        "voteCount": 2,
        "content": "Ans D : the questions specifically says via a messaging webhook"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/databricks/view/124178-exam-certified-data-engineer-associate-topic-1-question-88/",
    "body": "A data engineer wants to schedule their Databricks SQL dashboard to refresh every hour, but they only want the associated SQL endpoint to be running when it is necessary. The dashboard has multiple queries on multiple datasets associated with it. The data that feeds the dashboard is automatically processed using a Databricks Job.<br><br>Which of the following approaches can the data engineer use to minimize the total running time of the SQL endpoint used in the refresh schedule of their dashboard?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can turn on the Auto Stop feature for the SQL endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can ensure the dashboard's SQL endpoint is not one of the included query's SQL endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can reduce the cluster size of the SQL endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can ensure the dashboard's SQL endpoint matches each of the queries' SQL endpoints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up the dashboard's SQL endpoint to be serverless."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-22T09:31:00.000Z",
        "voteCount": 6,
        "content": "Ans A : the keyword in the question is \" running only when its necessary \""
      },
      {
        "date": "2024-06-09T07:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is A.\nThe question about minimize the total running times not enhance the performance."
      },
      {
        "date": "2023-12-08T02:43:00.000Z",
        "voteCount": 2,
        "content": "They can turn on the Auto Stop feature"
      },
      {
        "date": "2023-11-15T16:08:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is A. They can use the Auto Stop feature. When the query is idel it will stop the compute"
      },
      {
        "date": "2023-10-21T04:21:00.000Z",
        "voteCount": 3,
        "content": "A. They can turn on the Auto Stop feature for the SQL endpoint."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/databricks/view/124362-exam-certified-data-engineer-associate-topic-1-question-89/",
    "body": "A data engineer needs access to a table new_table, but they do not have the correct permissions. They can ask the table owner for permission, but they do not know who the table owner is.<br><br>Which of the following approaches can be used to identify the owner of new_table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the Permissions tab in the table's page in Data Explorer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll of these options can be used to identify the owner of the table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the Owner field in the table's page in Data Explorer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the Owner field in the table's page in the cloud storage solution",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere is no way to identify the owner of the table"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-08T02:40:00.000Z",
        "voteCount": 3,
        "content": "Review the Owner field in the table's page in Data Explorer"
      },
      {
        "date": "2023-11-15T16:11:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is C since you are looking for the owner and not the permissions on the table"
      },
      {
        "date": "2023-10-22T10:39:00.000Z",
        "voteCount": 2,
        "content": "ANS C : C IS CORRECT ,"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/databricks/view/124373-exam-certified-data-engineer-associate-topic-1-question-90/",
    "body": "A new data engineering team team has been assigned to an ELT project. The new data engineering team will need full privileges on the table sales to fully manage the project.<br><br>Which of the following commands can be used to grant full permissions on the database to the new data engineering team?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT ALL PRIVILEGES ON TABLE sales TO team;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT SELECT CREATE MODIFY ON TABLE sales TO team;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT SELECT ON TABLE sales TO team;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT USAGE ON TABLE sales TO team;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT ALL PRIVILEGES ON TABLE team TO sales;"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-22T11:59:00.000Z",
        "voteCount": 5,
        "content": "Ans A : \ufeff\n\ufeffgrant \"privilege\" on \"object\" object_name to &lt;user or group&gt;"
      },
      {
        "date": "2024-06-09T07:54:00.000Z",
        "voteCount": 2,
        "content": "Correct answer A."
      },
      {
        "date": "2024-01-20T10:55:00.000Z",
        "voteCount": 2,
        "content": "A is correct as provided syntax is right"
      },
      {
        "date": "2023-12-08T02:36:00.000Z",
        "voteCount": 2,
        "content": "GRANT ALL grants all privileges on \"Sales\" Table"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/databricks/view/142524-exam-certified-data-engineer-associate-topic-1-question-91/",
    "body": "Which data lakehouse feature results in improved data quality over a traditional data lake?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA data lakehouse stores data in open formats.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA data lakehouse allows the use of SQL queries to examine data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA data lakehouse provides storage solutions for structured and unstructured data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA data lakehouse supports ACID-compliant transactions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-11T06:58:00.000Z",
        "voteCount": 1,
        "content": "ACID-compliant transactions ensure that data operations are Atomic, Consistent, Isolated, and Durable. This greatly enhances data reliability, consistency, and quality compared to traditional data lakes, which typically lack strong transactional guarantees."
      },
      {
        "date": "2024-06-14T05:42:00.000Z",
        "voteCount": 1,
        "content": "it's D"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/databricks/view/141229-exam-certified-data-engineer-associate-topic-1-question-92/",
    "body": "In which scenario will a data team want to utilize cluster pools?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn automated report needs to be version-controlled across multiple collaborators.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn automated report needs to be runnable by all stakeholders.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn automated report needs to be refreshed as quickly as possible.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn automated report needs to be made reproducible."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-11T06:59:00.000Z",
        "voteCount": 1,
        "content": "Cluster pools help in reducing the start-up time of clusters by maintaining a set of idle, ready-to-use instances. This allows jobs, such as automated reports, to start faster, ensuring that the report is refreshed as quickly as possible."
      },
      {
        "date": "2024-05-25T12:11:00.000Z",
        "voteCount": 2,
        "content": "This question is repeated, a cluster pool helps to reduce the times of cold start and change the scaling ."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/databricks/view/145563-exam-certified-data-engineer-associate-topic-1-question-93/",
    "body": "What is hosted completely in the control plane of the classic Databricks architecture?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorker node",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks web application\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDriver node",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Filesystem"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-11T07:00:00.000Z",
        "voteCount": 1,
        "content": "The Databricks web application, which includes the UI, job management, and other control functionalities, is hosted entirely in the control plane. The other components, such as worker nodes, driver nodes, and the Databricks Filesystem, are part of the data plane, which runs in the customer's cloud account."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/databricks/view/147886-exam-certified-data-engineer-associate-topic-1-question-94/",
    "body": "A data engineer needs to determine whether to use the built-in Databricks Notebooks versioning or version their project using Databricks Repos.<br><br>What is an advantage of using Databricks Repos over the Databricks Notebooks versioning?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Repos allows users to revert to previous versions of a notebook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Repos is wholly housed within the Databricks Data Intelligence Platform",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Repos provides the ability to comment on specific changes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Repos supports the use of multiple branches\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T07:37:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/databricks/view/145564-exam-certified-data-engineer-associate-topic-1-question-95/",
    "body": "What is a benefit of the Databricks Lakehouse Architecture embracing open source technologies?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAvoiding vendor lock-in\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSimplified governance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAbility to scale workloads",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud-specific integrations"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T07:38:00.000Z",
        "voteCount": 1,
        "content": "A is Correct"
      },
      {
        "date": "2024-08-11T07:04:00.000Z",
        "voteCount": 1,
        "content": "By using open-source technologies, Databricks allows organizations to avoid being tied to a single vendor's ecosystem, offering flexibility and interoperability across different platforms and tools."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/databricks/view/145566-exam-certified-data-engineer-associate-topic-1-question-96/",
    "body": "A data engineer needs to use a Delta table as part of a data pipeline, but they do not know if they have the appropriate permissions.<br><br>In which location can the data engineer review their permissions on the table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJobs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDashboards",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCatalog Explorer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRepos"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T07:38:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-08-11T07:06:00.000Z",
        "voteCount": 1,
        "content": "Catalog Explorer (also known as Data Explorer in Databricks) is where users can explore data assets like tables and databases and view permissions and metadata associated with those assets."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/databricks/view/141230-exam-certified-data-engineer-associate-topic-1-question-97/",
    "body": "A data engineer is running code in a Databricks Repo that is cloned from a central Git repository. A colleague of the data engineer informs them that changes have been made and synced to the central Git repository. The data engineer now needs to sync their Databricks Repo to get the changes from the central Git repository.<br><br>Which Git operation does the data engineer need to run to accomplish this task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClone",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPull\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPush"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-11T10:18:00.000Z",
        "voteCount": 1,
        "content": "Have to pull the updated repo from Git"
      },
      {
        "date": "2024-05-25T12:12:00.000Z",
        "voteCount": 1,
        "content": "Repeated, also correct."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/databricks/view/140894-exam-certified-data-engineer-associate-topic-1-question-98/",
    "body": "Which file format is used for storing Delta Lake Table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-24T23:17:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-05-20T02:37:00.000Z",
        "voteCount": 1,
        "content": "The answer is D"
      },
      {
        "date": "2024-05-22T22:56:00.000Z",
        "voteCount": 1,
        "content": "No, file format is Parquet! The correct answer is B"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/databricks/view/141231-exam-certified-data-engineer-associate-topic-1-question-99/",
    "body": "A data architect has determined that a table of the following format is necessary:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image46.png\"><br><br>Which code block is used by SQL DDL command to create an empty Delta table in the above format regardless of whether a table already exists with this name?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE OR REPLACE TABLE table_name ( employeeId STRING, startDate DATE, avgRating FLOAT )\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE OR REPLACE TABLE table_name WITH COLUMNS ( employeeId STRING, startDate DATE, avgRating FLOAT ) USING DELTA",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE TABLE IF NOT EXISTS table_name ( employeeId STRING, startDate DATE, avgRating FLOAT )",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE TABLE table_name AS SELECT employeeId STRING, startDate DATE, avgRating FLOAT"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T07:42:00.000Z",
        "voteCount": 1,
        "content": "Repeated, correct A"
      },
      {
        "date": "2024-09-11T10:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2024-05-25T12:12:00.000Z",
        "voteCount": 2,
        "content": "Repeated, correct."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/databricks/view/141232-exam-certified-data-engineer-associate-topic-1-question-100/",
    "body": "A data engineer has been given a new record of data:<br><br>id STRING = 'a1'<br>rank INTEGER = 6<br>rating FLOAT = 9.4<br><br>Which SQL commands can be used to append the new record to an existing Delta table my_table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tINSERT INTO my_table VALUES ('a1', 6, 9.4)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tINSERT VALUES ('a1', 6, 9.4) INTO my_table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUPDATE my_table VALUES ('a1', 6, 9.4)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUPDATE VALUES ('a1', 6, 9.4) my_table"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-25T12:13:00.000Z",
        "voteCount": 3,
        "content": "Repeated, correct."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/databricks/view/141233-exam-certified-data-engineer-associate-topic-1-question-101/",
    "body": "A data engineer has realized that the data files associated with a Delta table are incredibly small. They want to compact the small files to form larger files to improve performance.<br><br>Which keyword can be used to compact the small files?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOPTIMIZE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVACUUM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCOMPACTION",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tREPARTITION"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-06-18T10:58:00.000Z",
        "voteCount": 2,
        "content": "The OPTIMIZE command is used to compact small files into larger ones, which helps improve the performance of Delta Lake tables. It consolidates small files into fewer larger files to reduce the overhead associated with having many small files. This process is often referred to as \"compaction\" but the specific keyword in Databricks Delta Lake is OPTIMIZE."
      },
      {
        "date": "2024-05-25T12:13:00.000Z",
        "voteCount": 1,
        "content": "Repeated, correct."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/databricks/view/142913-exam-certified-data-engineer-associate-topic-1-question-102/",
    "body": "A data engineer wants to create a data entity from a couple of tables. The data entity must be used by other data engineers in other sessions. It also must be saved to a physical location.<br><br>Which of the following data entities should the data engineer create?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFunction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tView",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTemporary view"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-09-29T01:23:00.000Z",
        "voteCount": 1,
        "content": "VIEW will not be physically like Meterialized VIEW. Answer is Table"
      },
      {
        "date": "2024-09-21T19:48:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is C\nAs View is persited they are physically stored and accessable across cluster even when restarted or detactched ."
      },
      {
        "date": "2024-06-25T07:10:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer as the question is asking for physical location"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/databricks/view/141235-exam-certified-data-engineer-associate-topic-1-question-103/",
    "body": "A data engineer runs a statement every day to copy the previous day\u2019s sales into the table transactions. Each day\u2019s sales are in their own file in the location \"/transactions/raw\".<br><br>Today, the data engineer runs the following command to complete this task:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image47.png\"><br><br>After running the command today, the data engineer notices that the number of records in table transactions has not changed.<br><br>What explains why the statement might not have copied any new records into the table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe format of the files to be copied were not included with the FORMAT_OPTIONS keyword.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe COPY INTO statement requires the table to be refreshed to view the copied rows.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe previous day\u2019s file has already been copied into the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe PARQUET file format does not support COPY INTO."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-25T12:14:00.000Z",
        "voteCount": 1,
        "content": "Repeated, correct."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/databricks/view/141234-exam-certified-data-engineer-associate-topic-1-question-104/",
    "body": "Which command can be used to write data into a Delta table while avoiding the writing of duplicate records?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDROP",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tINSERT",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMERGE\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAPPEND"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-18T09:36:00.000Z",
        "voteCount": 1,
        "content": "The Merge command checks for duplicates and ignores them"
      },
      {
        "date": "2024-05-25T12:13:00.000Z",
        "voteCount": 2,
        "content": "Repeated, correct."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/databricks/view/141236-exam-certified-data-engineer-associate-topic-1-question-105/",
    "body": "A data analyst has created a Delta table sales that is used by the entire data analysis team. They want help from the data engineering team to implement a series of tests to ensure the data is clean. However, the data engineering team uses Python for its tests rather than SQL.<br><br>Which command could the data engineering team use to access sales in PySpark?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT * FROM sales",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.table(\"sales\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql(\"sales\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.delta.table(\"sales\")"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-25T12:14:00.000Z",
        "voteCount": 1,
        "content": "Repeated, correct."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/databricks/view/141237-exam-certified-data-engineer-associate-topic-1-question-106/",
    "body": "A data engineer has created a new database using the following command:<br><br>CREATE DATABASE IF NOT EXISTS customer360;<br><br>In which location will the customer360 database be located?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbfs:/user/hive/database/customer360",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbfs:/user/hive/warehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbfs:/user/hive/customer360",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbfs:/user/hive/database"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-25T12:14:00.000Z",
        "voteCount": 2,
        "content": "Repeated, correct."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/databricks/view/140233-exam-certified-data-engineer-associate-topic-1-question-107/",
    "body": "A data engineer is attempting to drop a Spark SQL table my_table and runs the following command:<br><br>DROP TABLE IF EXISTS my_table;<br><br>After running this command, the engineer notices that the data files and metadata files have been deleted from the file system.<br><br>What is the reason behind the deletion of all these files?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table was managed\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table's data was smaller than 10 GB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table did not have a location",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table was external"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-29T05:32:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-09-20T07:45:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A"
      },
      {
        "date": "2024-07-08T15:54:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A. BOTH data and metadata were deleted meaning the table was managed. If the metadata was gone, but the actual data files themselves were still there then it would be an external table."
      },
      {
        "date": "2024-06-21T23:57:00.000Z",
        "voteCount": 1,
        "content": "also go with A"
      },
      {
        "date": "2024-06-14T06:15:00.000Z",
        "voteCount": 1,
        "content": "it's A"
      },
      {
        "date": "2024-05-24T18:41:00.000Z",
        "voteCount": 1,
        "content": "For D to be correct, the metadata would have been deleted, but the data would still exist.  The answer is A"
      },
      {
        "date": "2024-05-22T13:28:00.000Z",
        "voteCount": 1,
        "content": "A - Both Data was deleted as well along with Metadata"
      },
      {
        "date": "2024-05-10T06:44:00.000Z",
        "voteCount": 1,
        "content": "Answer should be A as data was deleted table was managed"
      },
      {
        "date": "2024-05-09T10:19:00.000Z",
        "voteCount": 3,
        "content": "The answer should be A --&gt; the table was MANAGED. If the metadata and the underlined files have been deleted, then this is a MANAGED table and not an external table."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/databricks/view/147416-exam-certified-data-engineer-associate-topic-1-question-108/",
    "body": "A data engineer needs to create a table in Databricks using data from a CSV file at location /path/to/csv.<br><br>They run the following command:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image48.png\"><br><br>Which of the following lines of code fills in the above blank to successfully complete the task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFROM \"path/to/csv\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUSING CSV\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFROM CSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUSING DELTA"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-12T09:36:00.000Z",
        "voteCount": 2,
        "content": "B: using csv is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/databricks/view/140622-exam-certified-data-engineer-associate-topic-1-question-109/",
    "body": "What is a benefit of creating an external table from Parquet rather than CSV when using a CREATE TABLE AS SELECT statement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet files can be partitioned",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet files will become Delta tables",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet files have a well-defined schema\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet files have the ability to be optimized"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-25T12:17:00.000Z",
        "voteCount": 1,
        "content": "Parquetare columnar and can be optimized. However, I think the key part is  \" when using a CREATE TABLE AS SELECT statement?\", that's C"
      },
      {
        "date": "2024-05-14T06:12:00.000Z",
        "voteCount": 1,
        "content": "D partquet file are columnar and otimised"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/databricks/view/146487-exam-certified-data-engineer-associate-topic-1-question-110/",
    "body": "Which SQL keyword can be used to convert a table from a long format to a wide format?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTRANSFORM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPIVOT",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSUM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCONVERT"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-08-26T14:30:00.000Z",
        "voteCount": 2,
        "content": "Answer is (B. PIVOT)\nThe PIVOT keyword is used to transform rows into columns, converting data from a long format (where each row represents a single measurement) to a wide format (where multiple measurements are represented as columns). For example, if you have a table of sales data with columns for Date, Product, and Sales, you can use PIVOT to create a table where each Product has its own column, and sales are aggregated by Date."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/databricks/view/147887-exam-certified-data-engineer-associate-topic-1-question-111/",
    "body": "A data engineer has a Python variable table_name that they would like to use in a SQL query. They want to construct a Python code block that will run the query using table_name.<br><br>They have the following incomplete code block:<br><br>____(f\"SELECT customer_id, spend FROM {table_name}\")<br><br>What can be used to fill in the blank to successfully complete the task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.delta.sql",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdbutils.sql"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T07:46:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/databricks/view/141061-exam-certified-data-engineer-associate-topic-1-question-112/",
    "body": "A data engineer is working with two tables. Each of these tables is displayed below in its entirety.<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image49.png\"><br><br>The data engineer runs the following query to join these tables together:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image50.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image51.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image52.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image53.png\"><br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image54.png\">"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-12T09:47:00.000Z",
        "voteCount": 3,
        "content": "Answer is C, we're using a LEFT JOIN."
      },
      {
        "date": "2024-06-20T08:18:00.000Z",
        "voteCount": 3,
        "content": "LEFT join. D is the result of a FULL join"
      },
      {
        "date": "2024-06-18T23:31:00.000Z",
        "voteCount": 1,
        "content": "C - left outer join"
      },
      {
        "date": "2024-06-14T06:21:00.000Z",
        "voteCount": 1,
        "content": "IT'S D"
      },
      {
        "date": "2024-09-18T09:44:00.000Z",
        "voteCount": 1,
        "content": "D is the result of a full join, we're using a left join. Answer should be C."
      },
      {
        "date": "2024-05-22T14:09:00.000Z",
        "voteCount": 1,
        "content": "Sorry wrong it is C for the left join"
      },
      {
        "date": "2024-05-22T14:07:00.000Z",
        "voteCount": 1,
        "content": "should be D"
      },
      {
        "date": "2024-05-23T14:32:00.000Z",
        "voteCount": 1,
        "content": "LEFT JOIN so should be C"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/databricks/view/147418-exam-certified-data-engineer-associate-topic-1-question-113/",
    "body": "A data engineer needs to apply custom logic to identify employees with more than 5 years of experience in array column employees in table stores. The custom logic should create a new column exp_employees that is an array of all of the employees with more than 5 years of experience for each row. In order to apply this custom logic at scale, the data engineer wants to use the FILTER higher-order function.<br><br>Which code block successfully completes this task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image55.png\"><br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image56.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image57.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image58.png\">"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-12T09:50:00.000Z",
        "voteCount": 1,
        "content": "Answer is A, if for no other reason than it is the only correct syntax of filter."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/databricks/view/140234-exam-certified-data-engineer-associate-topic-1-question-114/",
    "body": "A data engineer that is new to using Python needs to create a Python function to add two integers together and return the sum?<br><br>Which code block can the data engineer use to complete this task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image59.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image60.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image61.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image62.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-29T05:39:00.000Z",
        "voteCount": 1,
        "content": "correct is D"
      },
      {
        "date": "2024-08-13T20:55:00.000Z",
        "voteCount": 1,
        "content": "def function must have \"return\" present."
      },
      {
        "date": "2024-07-08T15:58:00.000Z",
        "voteCount": 1,
        "content": "For the function to perform the actions \"RETURN\" must be present."
      },
      {
        "date": "2024-06-09T08:01:00.000Z",
        "voteCount": 1,
        "content": "Answer is D."
      },
      {
        "date": "2024-05-23T05:49:00.000Z",
        "voteCount": 2,
        "content": "RETURN is needed"
      },
      {
        "date": "2024-05-22T13:33:00.000Z",
        "voteCount": 1,
        "content": "D, RETURN in Python needed"
      },
      {
        "date": "2024-05-11T13:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is D. \nC is wrong answer, as it missed return key workd"
      },
      {
        "date": "2024-05-10T06:48:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D"
      },
      {
        "date": "2024-05-09T17:26:00.000Z",
        "voteCount": 3,
        "content": "Correct is D"
      },
      {
        "date": "2024-05-09T10:21:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D because the python function needs to provide a return for the function, so C is incorrect."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/databricks/view/146490-exam-certified-data-engineer-associate-topic-1-question-115/",
    "body": "A data engineer has configured a Structured Streaming job to read from a table, manipulate the data, and then perform a streaming write into a new table.<br><br>The code block used by the data engineer is below:<br><br><img src=\"https://img.examtopics.com/certified-data-engineer-associate/image63.png\"><br><br>Which line of code should the data engineer use to fill in the blank if the data engineer only wants the query to execute a micro-batch to process data every 5 seconds?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(\"5 seconds\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(continuous=\"5 seconds\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(once=\"5 seconds\")",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttrigger(processingTime=\"5 seconds\")\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-12T09:51:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer"
      },
      {
        "date": "2024-08-26T16:45:00.000Z",
        "voteCount": 2,
        "content": "Repeated, Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/databricks/view/146531-exam-certified-data-engineer-associate-topic-1-question-117/",
    "body": "A data engineer has three tables in a Delta Live Tables (DLT) pipeline. They have configured the pipeline to drop invalid records at each table. They notice that some data is being dropped due to quality concerns at some point in the DLT pipeline. They would like to determine at which table in their pipeline the data is being dropped.<br><br>Which approach can the data engineer take to identify the table that is dropping the records?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up separate expectations for each table when developing their DLT pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the DLT pipeline page, click on the \u201cError\u201d button, and review the present errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can set up DLT to notify them via email when records are dropped.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the DLT pipeline page, click on each table, and view the data quality statistics."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-08-27T03:21:00.000Z",
        "voteCount": 3,
        "content": "Repeated, Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/databricks/view/146534-exam-certified-data-engineer-associate-topic-1-question-118/",
    "body": "What is used by Spark to record the offset range of the data being processed in each trigger in order for Structured Streaming to reliably track the exact progress of the processing so that it can handle any kind of failure by restarting and/or reprocessing?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheckpointing and Write-ahead Logs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplayable Sources and Idempotent Sinks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite-ahead Logs and Idempotent Sinks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheckpointing and Idempotent Sinks"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-08-27T04:05:00.000Z",
        "voteCount": 1,
        "content": "Repeated, Correct\n\nThe correct answer is A. Checkpointing and Write-ahead Logs. Checkpointing records the progress of streaming queries, while write-ahead logs (WALs) capture the data before it is processed, allowing Spark to recover and process data reliably in case of failures."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/databricks/view/146532-exam-certified-data-engineer-associate-topic-1-question-119/",
    "body": "What describes the relationship between Gold tables and Silver tables?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGold tables are more likely to contain aggregations than Silver tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGold tables are more likely to contain valuable data than Silver tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGold tables are more likely to contain a less refined view of data than Silver tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGold tables are more likely to contain truthful data than Silver tables."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-08-27T04:02:00.000Z",
        "voteCount": 2,
        "content": "Repeated, Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/databricks/view/146533-exam-certified-data-engineer-associate-topic-1-question-120/",
    "body": "What describes when to use the CREATE STREAMING LIVE TABLE (formerly CREATE INCREMENTAL LIVE TABLE) syntax over the CREATE LIVE TABLE syntax when creating Delta Live Tables (DLT) tables using SQL?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE STREAMING LIVE TABLE should be used when the subsequent step in the DLT pipeline is static.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE STREAMING LIVE TABLE should be used when data needs to be processed incrementally.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE STREAMING LIVE TABLE should be used when data needs to be processed through complicated aggregations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE STREAMING LIVE TABLE should be used when the previous step in the DLT pipeline is static."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-08-27T04:05:00.000Z",
        "voteCount": 2,
        "content": "Repeated, Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/databricks/view/146535-exam-certified-data-engineer-associate-topic-1-question-121/",
    "body": "A Delta Live Table pipeline includes two datasets defined using STREAMING LIVE TABLE. Three datasets are defined against Delta Lake table sources using LIVE TABLE.<br><br>The table is configured to run in Production mode using the Continuous Pipeline Mode.<br><br>What is the expected outcome after clicking Start to update the pipeline assuming previously unprocessed data exists and all definitions are valid?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated at set intervals until the pipeline is shut down. The compute resources will persist to allow for additional testing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated once and the pipeline will shut down. The compute resources will persist to allow for additional testing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated at set intervals until the pipeline is shut down. The compute resources will be deployed for the update and terminated when the pipeline is stopped.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll datasets will be updated once and the pipeline will shut down. The compute resources will be terminated."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-18T09:53:00.000Z",
        "voteCount": 1,
        "content": "Continuous Pipeline Mode in Production mode implies that the pipeline continuously processes incoming data updates at set intervals, ensuring the datasets are kept up-to-date as new data arrives.\nSince the pipeline is set to Continuous Pipeline Mode, it will keep running and updating the datasets until it is manually shut down.\nThe compute resources are allocated dynamically to process and update the datasets as needed, and they will be terminated when the pipeline is stopped or shut down.\nThis mode allows for real-time or near-real-time updates to the datasets from the streaming/live tables, ensuring that the data remains current and reflects the changes occurring in the data sources."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/databricks/view/149736-exam-certified-data-engineer-associate-topic-1-question-122/",
    "body": "Which type of workloads are compatible with Auto Loader?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStreaming workloads",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMachine learning workloads",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tServerless workloads",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBatch workloads"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-18T03:16:00.000Z",
        "voteCount": 1,
        "content": "Auto Loader is designed to ingest data continuously from data lakes and process it in real-time. It can efficiently handle streaming data by detecting and processing new files as they arrive."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/databricks/view/144151-exam-certified-data-engineer-associate-topic-1-question-123/",
    "body": "A data engineer has developed a data pipeline to ingest data from a JSON source using Auto Loader, but the engineer has not provided any type inference or schema hints in their pipeline. Upon reviewing the data, the data engineer has noticed that all of the columns in the target table are of the string type despite some of the fields only including float or boolean values.<br><br>Why has Auto Loader inferred all of the columns to be of the string type?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAuto Loader cannot infer the schema of ingested data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON data is a text-based format\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAuto Loader only works with string data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll of the fields had at least one null value"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-18T03:19:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B as JSON files do not include datatypes"
      },
      {
        "date": "2024-07-18T20:59:00.000Z",
        "voteCount": 1,
        "content": "JSON file does not include datatype and all columns are defaulted as string"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/databricks/view/140248-exam-certified-data-engineer-associate-topic-1-question-124/",
    "body": "Which statement regarding the relationship between Silver tables and Bronze tables is always true?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSilver tables contain a less refined, less clean view of data than Bronze data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSilver tables contain aggregates while Bronze data is unaggregated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSilver tables contain more data than Bronze tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSilver tables contain less data than Bronze tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-10T06:53:00.000Z",
        "voteCount": 10,
        "content": "looks like there is no correct answer. Shold be like A but Silver and Bronze should be changed in their places"
      },
      {
        "date": "2024-09-12T10:06:00.000Z",
        "voteCount": 1,
        "content": "Silver table is the step of reducing and refining data after raw data was imported into bronze table. Gold table would be the last step where data aggregation is applied."
      },
      {
        "date": "2024-08-22T04:25:00.000Z",
        "voteCount": 1,
        "content": "Cannot be D. If raw data are clean enough already and have no aggregations or other functions before to be in the silver table, there will be the same amount of data both side. A seems to be a better answer because from raw you will have more chance to make at least one transformation to refine data."
      },
      {
        "date": "2024-08-17T23:56:00.000Z",
        "voteCount": 1,
        "content": "D. Silver tables contain less data than Bronze tables.\n\nThis is because Silver tables are typically more refined and processed versions of the raw data found in Bronze tables. Bronze tables often contain raw, unprocessed data, while Silver tables contain cleaned, filtered, or aggregated data. Therefore, Silver tables usually contain a subset or a refined version of the data in the Bronze tables, leading to less overall data in the Silver tables."
      },
      {
        "date": "2024-06-24T13:17:00.000Z",
        "voteCount": 2,
        "content": "silver have some data filters"
      },
      {
        "date": "2024-06-07T07:10:00.000Z",
        "voteCount": 2,
        "content": "in my opinion it is D Silver has only the data with no error"
      },
      {
        "date": "2024-05-22T14:13:00.000Z",
        "voteCount": 2,
        "content": "I think there is no correct answer"
      },
      {
        "date": "2024-05-22T13:59:00.000Z",
        "voteCount": 2,
        "content": "Silver tables contain a more refined and cleaner view of data than Bronze tables."
      },
      {
        "date": "2024-05-09T17:31:00.000Z",
        "voteCount": 1,
        "content": "Correct is A"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/databricks/view/147427-exam-certified-data-engineer-associate-topic-1-question-125/",
    "body": "Which query is performing a streaming hop from raw data to a Bronze table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image64.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image65.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image66.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/certified-data-engineer-associate/image67.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-18T03:23:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is D."
      },
      {
        "date": "2024-09-12T10:08:00.000Z",
        "voteCount": 1,
        "content": "Question specified streaming hop. D input from raw and is writeStream."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/databricks/view/141060-exam-certified-data-engineer-associate-topic-1-question-126/",
    "body": "A dataset has been defined using Delta Live Tables and includes an expectations clause:<br><br>CONSTRAINT valid_timestamp EXPECT (timestamp &gt; '2020-01-01') ON VIOLATION DROP ROW<br><br>What is the expected behavior when a batch of data containing data that violates these constraints is processed?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation cause the job to fail.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are added to the target dataset and flagged as invalid in a field added to the target dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecords that violate the expectation are added to the target dataset and recorded as invalid in the event log."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-26T12:19:00.000Z",
        "voteCount": 2,
        "content": "Repeated, Correct answer is C"
      },
      {
        "date": "2024-06-24T13:19:00.000Z",
        "voteCount": 2,
        "content": "ON VIOLATION DROP ROW"
      },
      {
        "date": "2024-06-14T06:36:00.000Z",
        "voteCount": 2,
        "content": "it's C"
      },
      {
        "date": "2024-05-29T02:11:00.000Z",
        "voteCount": 2,
        "content": "C. Records that violate the expectation are dropped from the target dataset and recorded as invalid in the event log.\n\nWhen a constraint defined using the EXPECT clause is violated, Delta Live Tables will drop the records that violate the expectation from the target dataset. Additionally, information about the dropped records and the reason for their exclusion will be recorded in the event log for audit and monitoring purposes. This ensures that only valid data meeting the specified constraints is included in the target dataset."
      },
      {
        "date": "2024-05-22T14:05:00.000Z",
        "voteCount": 1,
        "content": "C should be correct, A is for ON VIOLATION FAIL UPDATE"
      },
      {
        "date": "2024-05-22T14:03:00.000Z",
        "voteCount": 1,
        "content": "A should be correct"
      },
      {
        "date": "2024-05-25T12:23:00.000Z",
        "voteCount": 1,
        "content": "i don't agree, it shouldn't make the job to fail."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/databricks/view/146479-exam-certified-data-engineer-associate-topic-1-question-127/",
    "body": "A data engineer has a Job with multiple tasks that runs nightly. Each of the tasks runs slowly because the clusters take a long time to start.<br><br>Which action can the data engineer perform to improve the start up time for the clusters used for the Job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can use endpoints available in Databricks SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can use jobs clusters instead of all-purpose clusters",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can configure the clusters to autoscale for larger data sizes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can use clusters that are from a cluster pool"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-18T04:41:00.000Z",
        "voteCount": 1,
        "content": "pools are a set of idle, ready-to-use instances hence minimizing start-up times"
      },
      {
        "date": "2024-08-26T12:16:00.000Z",
        "voteCount": 1,
        "content": "Repeated, Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/databricks/view/140290-exam-certified-data-engineer-associate-topic-1-question-128/",
    "body": "A data engineer has a single-task Job that runs each morning before they begin working. After identifying an upstream data issue, they need to set up another task to run a new notebook prior to the original task.<br><br>Which approach can the data engineer use to set up the new task?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can clone the existing task in the existing Job and update it to run the new notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can create a new task in the existing Job and then add it as a dependency of the original task.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can create a new task in the existing Job and then add the original task as a dependency of the new task.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can create a new job from scratch and add both tasks to run concurrently."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-12T10:13:00.000Z",
        "voteCount": 1,
        "content": "B is correct. the new task needs to be the dependancy."
      },
      {
        "date": "2024-08-26T12:16:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: B\n\nExplanation: To set up the new task to run a new notebook prior to the original task in a single-task Job, the data engineer can use the following approach: In the existing Job, create a new task that corresponds to the new notebook that needs to be run. Set up the new task with the appropriate configuration, specifying the notebook to be executed and any necessary parameters or dependencies. Once the new task is created, designate it as a dependency of the original task in the Job configuration. This ensures that the new task is executed before the original task."
      },
      {
        "date": "2024-06-09T08:06:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\nNew task is prior than the original task."
      },
      {
        "date": "2024-05-22T14:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-05-19T07:08:00.000Z",
        "voteCount": 1,
        "content": "B is correct, \"needs to run prior to the original task\""
      },
      {
        "date": "2024-05-19T04:31:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-05-11T14:03:00.000Z",
        "voteCount": 1,
        "content": "B is correct, as new task runs first"
      },
      {
        "date": "2024-05-10T06:56:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/databricks/view/146477-exam-certified-data-engineer-associate-topic-1-question-129/",
    "body": "A single Job runs two notebooks as two separate tasks. A data engineer has noticed that one of the notebooks is running slowly in the Job\u2019s current run. The data engineer asks a tech lead for help in identifying why this might be the case.<br><br>Which approach can the tech lead use to identify why the notebook is running slowly as part of the Job?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the Runs tab in the Jobs UI to immediately review the processing notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the Tasks tab in the Jobs UI and click on the active run to review the processing notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the Runs tab in the Jobs UI and click on the active run to review the processing notebook.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can navigate to the Tasks tab in the Jobs UI to immediately review the processing notebook."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-12T10:15:00.000Z",
        "voteCount": 1,
        "content": "Question states it is Running slowly, nothing is wrong with the job itself, so the Run needs to be checked."
      },
      {
        "date": "2024-08-26T12:10:00.000Z",
        "voteCount": 1,
        "content": "Repeated, Correct"
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/databricks/view/145701-exam-certified-data-engineer-associate-topic-1-question-130/",
    "body": "A data analysis team has noticed that their Databricks SQL queries are running too slowly when connected to their always-on SQL endpoint. They claim that this issue is present when many members of the team are running small queries simultaneously. They ask the data engineering team for help. The data engineering team notices that each of the team\u2019s queries uses the same SQL endpoint.<br><br>Which approach can the data engineering team use to improve the latency of the team\u2019s queries?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can increase the cluster size of the SQL endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can increase the maximum bound of the SQL endpoint\u2019s scaling range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can turn on the Auto Stop feature for the SQL endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey can turn on the Serverless feature for the SQL endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-10T02:51:00.000Z",
        "voteCount": 1,
        "content": "It's always on, no need for serverless to speed up start-up."
      },
      {
        "date": "2024-08-26T14:25:00.000Z",
        "voteCount": 1,
        "content": "The question states that the developers are connected to their \"always-on\" SQL Endpoint.  This means there is no startup delay.  We can increase performance of many simultaneous queries by scaling out."
      },
      {
        "date": "2024-08-26T12:09:00.000Z",
        "voteCount": 1,
        "content": "further,\n\n\nA. Increase the cluster size of the SQL endpoint: While increasing the cluster size might help if the current cluster size is insufficient, it does not necessarily address the scaling needs dynamically. The SQL endpoint might still be limited by its scaling configuration.\n\nC. Turn on the Auto Stop feature: Auto Stop helps manage costs by automatically stopping the SQL endpoint when it is idle. However, it doesn't address performance issues related to simultaneous query execution and would not improve query latency directly.\n\nD. Turn on the Serverless feature: The Serverless SQL endpoint is designed for ad-hoc querying without requiring dedicated clusters. While it could help in certain scenarios, it may not be directly applicable if the issue is specifically related to high concurrency and resource contention in an always-on environment.\n\nBy increasing the scaling range, the SQL endpoint can handle more concurrent queries and improve overall performance."
      },
      {
        "date": "2024-08-26T12:09:00.000Z",
        "voteCount": 1,
        "content": "Given the scenario where the data analysis team is experiencing slow query performance due to multiple small queries running simultaneously on the same SQL endpoint, the best approach to improve the latency of these queries is:\n\nB. They can increase the maximum bound of the SQL endpoint\u2019s scaling range.\n\nHere\u2019s why this approach is suitable:\n\nIncreasing the maximum bound of the SQL endpoint\u2019s scaling range allows the SQL endpoint to scale out more resources as needed. If many users are running queries simultaneously, the increased scaling range ensures that additional compute resources are available to handle the load, which can reduce query latency and improve performance.\nHere's why the other options are less suitable:"
      },
      {
        "date": "2024-08-13T22:11:00.000Z",
        "voteCount": 1,
        "content": "Turning on the Serverless feature allows the SQL endpoint to scale automatically and efficiently handle a large number of small queries, improving performance and reducing latency."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/databricks/view/146471-exam-certified-data-engineer-associate-topic-1-question-135/",
    "body": "A new data engineering team team has been assigned to an ELT project. The new data engineering team will need full privileges on the table sales to fully manage the project.<br><br>Which command can be used to grant full permissions on the database to the new data engineering team?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT ALL PRIVILEGES ON TABLE sales TO team;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT SELECT CREATE MODIFY ON TABLE sales TO team;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT SELECT ON TABLE sales TO team;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGRANT ALL PRIVILEGES ON TABLE team TO sales;"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-12T10:29:00.000Z",
        "voteCount": 1,
        "content": "A is correct. Grant all on table sales to team, not table team to sales."
      }
    ],
    "examNameCode": "certified-data-engineer-associate",
    "topicNumber": "1"
  }
]